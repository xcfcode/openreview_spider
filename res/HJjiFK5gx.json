{"notes": [{"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1489576397803, "tcdate": 1478298019357, "number": 513, "id": "HJjiFK5gx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJjiFK5gx", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396648364, "tcdate": 1486396648364, "number": 1, "id": "HJge6z8_x", "invitation": "ICLR.cc/2017/conference/-/paper513/acceptance", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper demonstrates a novel (although somewhat obvious) extension to NPI, namely moving away from training exclusively on full traces (in order to model the nested calling of subprograms) to training, in part on low-level program traces. By exploiting a continuous stack in the vein of Das et al. 1992, Joulin and Mikolov 2015, or Grefenstette et al. 2015 (any of which could plausibly have been used here, by my reading, contrary to claims made by the authors in discussion), they can model nested calls with a stack-like structure which needs only partial supervision of a few full traces. The claim is that the resulting model is more sample efficient than the NPI of Reed and de Freitas.\n \n The reviews are mixed. R1 is grumpy about citations, which I see as grounds for rejection, but raises the point that the claims to data efficiency may be overblown since it is only shown that few full traces are needed to train the model, not that it can infer structure without full traces, which would be more impressive. Confusingly, they refer to continuous stacks as probabilistic, which is misleading as while there is (in all variants mentioned above) a possible probabilistic interpretation of non-{0, 1} push/pop operations, the updates to the stack state are deterministic, as are all other aspects of the network except test-time sampling of actions from the multivariate distribution induced by the softmax on output.\n \n R2 has not understood the paper or background material if they mistakenly believe that the stack-like structure and resulting model are probabilistic, as there is no sampling of actions on the stack, but only deterministic state updates. The superficiality of the review combined with the fairly crucial misunderstanding sadly means I cannot rely on it to make a recommendation.\n \n R3 is broadly sympathetic to the paper, while recognising that it still requires information about the program structure through partial FULL supervision in order to learn to manipulate the continuous stack (and thus nested function calls). It is disappointing that the authors did not reply to this point in this review. I acknowledge that they address this claim in part in a response to R1, but the reply is mostly that they will rewrite the formulation of their claims.\n \n Overall, the novelty of this paper lies in the integration of an existing flavour of differentiable data structure, variants of which have recently been presented at NIPS in 2015, into NPI in order to learn program structure. This would have made for an excellent paper if this augmentation had shown that, without partial supervision on the stack operations (thus training solely from low level traces) the model had been able to infer program structures. The need to provide some full traces is disappointing in this respect, although the claim about providing a more data-efficient training regime thanks to the data structure is plausible. \n\nStil, overall, the PCs encourage the authors to address the remainig issues in the camera reasy version of their paper and to present this as a poster at the main conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396648886, "id": "ICLR.cc/2017/conference/-/paper513/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396648886}}}, {"tddate": null, "tmdate": 1484489731470, "tcdate": 1484489731470, "number": 8, "id": "SyjbVWY8g", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Updated Draft", "comment": "We\u2019ve significantly revised the draft to address the concerns voiced by the reviewers.  This includes a rewrite of the intro and technical sections to clarify our contributions and our relationship to past work, esp. that of Joulin and Mikolov."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1484334697285, "tcdate": 1484334697285, "number": 7, "id": "Bk-_8jUUx", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "r1ZLUoBHg", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Follow up on comparison to Zaremba et. al.", "comment": "Thanks again for your thorough review.  Per our response, we have looked into comparing directly with Zaremba et. al. using their posted source code.  Unfortunately, it appears that the posted code only supports the reinforcement learning setup, and not the \u2018supervised\u2019 setup which is analogous to the results in our paper.  It\u2019s not clear what we would learn from comparing to the reinforcement learning setup, since it\u2019s an inherently more challenging than our \u2018supervised\u2019 setup. Further, the supervised model is similar to our BiSeq baseline and it appears from the code that the results in the Zaremba et al. paper rely on a much larger number of training samples than are used in our work.  So it's not clear to us how valuable it would be to compare to a modified version of their code."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1483220553057, "tcdate": 1483220553057, "number": 6, "id": "r1ZLUoBHg", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "rkLqZvB4g", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Response", "comment": "Thank you for your detailed and thorough review.  We would like to address each of your concerns point-by-point:\n\n\"\"\"\" \"a couple of examples which include the full abstraction hierarchy\". This may limit the scope of this work.\" \"\"\"\n\nWe agree that this is a limitation of our work, but still believe that our work represents a substantial step towards a solution which can induce the abstraction hierarchy without any supervision of the hierarchy itself.\n\n\"\"\"  It would be great if the authors clarify what they mean by \"relatively easy to gather in many natural settings\".  \"\"\"\n\nThis was simply meant as a backpointer to the motivation section in the introduction.  In the motivation section we gave the examples of using 3-d body tracking hardware to record a human performing real world tasks, and recording users mouse clicks and keystrokes as they perform virtual tasks in an online web application.\n\n\"\"\" What are the \"existing techniques\", they are referring to? \"\"\"\nThat claim should have referred directly to NPI, and we will update the writing to reflect this.\n\n\n\"\"\" In the conclusion, the author states that \"remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.\" However for all the experiments, they \"include a small number of FULL samples\" (FULL == \"samples with full program traces\"). Unfortunately even if this means that they need less FULL examples, they still need \"full program traces\", contradicting their final claim. \"\"\"\n\nThis claim of less supervision should also have referred directly to NPI, and we will update the writing to reflect this.\n\n\"\"\" does the environment change as well between the runs, i.e. are the FULL examples different between the runs? \"\"\"\n\nThe set of FULL examples is the same for all runs.  We only change the random seed used for initializing the weights.\n\n\"\"\" Concerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples. \"\"\"\n\nWe generate a large set of random samples, and throw out any duplicates.  We then shuffle this set, and pull the train/val/test sets from this large set.  The number of possible samples is much larger than your calculations because we randomly choose a subset of the blocks to be already in place in the environment.  Thus for each possible rectangle with total wall length $n$, there are $2^n-1$ possible samples/tasks. \n\n\"\"\" Concerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( \"123+45\" would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.? \"\"\"\n\nThe setup you describe is exactly the one we used to generate the set of baselines from figure 8 called BiSeq-*. We will attempt to compare directly with Zaremba et al. using their posted source code.\n\n\"\"\" Finally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version. \"\"\"\n\nWe are in the process of updating the technical section to make this relationship more clear.\n\n\"\"\" Missing references\u2026\"\"\"\n\n\nThanks for pointing these out, we will include these in the next version"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1482156198742, "tcdate": 1482154382402, "number": 3, "id": "rkLqZvB4g", "invitation": "ICLR.cc/2017/conference/-/paper513/official/review", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "First I would like to apologize for the late review.\n\nThis paper proposes an extension of the NPI model (Reed & de Freitas) by using an extension of the probabilistic stacks introduced in Mikolov et al.. This allows them to train their model with less supervision than Reed & de Freitas. \n\nOverall the model is a nice extension of NPI. While it requires less supervision than NPI, it still requires \"sequences of elementary operations paired with environment observations, and [...] a couple of examples which include the full abstraction hierarchy\". This may limit the scope of this work.\n\nThe paper claims that their \"method is leverages stronger supervision in the form of elementary action sequences rather than just input-output examples (sic).  Such sequences are relatively easy to gather in many natural settings\". It would be great if the authors clarify what they mean by \"relatively easy to gather in many natural settings\". They also claim that \"the additional supervision improves the data efficiency and allow our technique to scale to more complicated problems\". However, this paper only addresses two toy problems which are neither \"natural settings\" nor of a large scale (or at least not larger than those addressed in the related literature, see Zaremba et al. for addition). \n\nIn the introduction, the author states that \"Existing techniques, however, cannot be applied on data like this because it does not contain the abstraction hierarchy.\" What are the \"existing techniques\", they are referring to? This work only addresses the problem of long addition and puzzle solving in a block world. Afaik, Zaremba et al. has shown that with no supervision, it can solve the long addition problem and Sukhbaatar et al. (\"Mazebase: A sandbox for learning from games\") shows that a memory network can solve puzzles in a blockworld with little supervision.\n\nIn the conclusion,  the author states that \"remarkably, NPL achieves state-of-the-art performances with much less supervision compared to existing models, making itself more applicable to real-world applications where full program traces are hard to get.\" However for all the experiments, they \"include a small number of FULL samples\" (FULL == \"samples with full program traces\"). Unfortunately even if this means that they need less FULL examples, they still need \"full program traces\", contradicting their final claim. Moreover, as shown figure 7, their model does not use a \"small number of FULL samples\" but rather a significantly smaller amount of FULL examples than NPI, i.e., 16 vs 128. \n\n\"All experiments were run with 10 different random seeds\": does the environment change as well between the runs, i.e. are the FULL examples different between the runs? If it is the case and since you select the best run (on a validation set), the NPL model does not consume 16 FULL examples but 160 FULL examples for nanoCraft. \n\nConcerning the NanoCraft example, it would be good to have more details about how the examples are generated: how do you make sure that the train/val/test sets are different? How the rectangular shape are generated? If I consider all possible rectangles in a 6x6 grid, there are (6x6)x(6x6)/2 = 648 possibilities, thus taking 256 examples sum up to ~40% of the total number of rectangles. This does not even account for the fact that from an initial state, many rectangles can be made, making my estimate probably lower than the real coverage of examples.\n\nConcerning the addition, it would interesting to show what an LSTM would do: Take a 2 layer LSTM that takes the 2 current digits as an input and produce the current output ( \"123+45\" would be input[0] = [3,5], input[1]=[2,4], input[2]=[1, 0] and output[0] = 8...). I would be curious to see how such baseline would work. It can be trained on input/output and it is barely different from a standard sequence model. Also, would it be possible to compare with Zaremba et al.?\n\nFinally, as discussed previously with the authors, it would be good if they discuss more in length the relation between their probabilistic stacks and Mikolov et al.. They have a lot of similarities and it is not addressed in the current version. It should be addressed in the section describing the approach. I believe the authors agreed on this and I will wait for the updated version.\n\nOverall, it is a nice extension of Reed & de Freitas, but I'm a bit surprised by the lack of discussion about the rest of the literature (beside Reed & de Freitas, most previous work are only lightly discussed in the related work). This would have been fine if this paper would not suffer from a relatively weak experiment section that does not support the claims made in this work or show results that were not obtained by others before. \n\nMissing references:\n\"Learning simple arithmetic procedures\", Cottrell et al.\n\"Neural gpus learn algorithms\", Kaiser & Sutskever\n\"Mazebase: A sandbox for learning from games\", Sukhbaatar et al.\n\"Learning simple algorithms from examples\", Zaremba et al.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512557784, "id": "ICLR.cc/2017/conference/-/paper513/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper513/AnonReviewer3", "ICLR.cc/2017/conference/paper513/AnonReviewer2", "ICLR.cc/2017/conference/paper513/AnonReviewer1"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512557784}}}, {"tddate": null, "tmdate": 1481926408134, "tcdate": 1481926408134, "number": 2, "id": "SylfwJf4g", "invitation": "ICLR.cc/2017/conference/-/paper513/official/review", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer2"], "content": {"title": "well formulated paper", "rating": "7: Good paper, accept", "review": "The paper presents the Neural Program Lattice (NPL), extending the previous Neural Programmer-Interpreters (NPI). The main idea is to generalize stack manipulation of NPI by making it probabilistic. This allows the content of the stack to be stochastic than deterministic, and the paper describes the feed-forward steps of NPL's program inference similar to the NPI formulation. A new objective function is provided to train the model that maximizes the probability of NPL model correctly predicting operation sequences, from execution traces. We believe this is an important extension. The experimental results illustrate that the NPL is able to learn task executions in a clean setting with perfect observations.\n\nThe paper is clearly presented and its background literature (i.e., NPI) is well covered. We also believe the paper is presenting a conceptually/technically meaningful extension of NPI, which will be of interest to a broad audience. We are still a bit concerned whether the NPL would be directly applicable for noisy observations (e.g., human skeletons) in a continuous space with less explicit structure, so more discussions will be interesting.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512557784, "id": "ICLR.cc/2017/conference/-/paper513/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper513/AnonReviewer3", "ICLR.cc/2017/conference/paper513/AnonReviewer2", "ICLR.cc/2017/conference/paper513/AnonReviewer1"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512557784}}}, {"tddate": null, "tmdate": 1481916928161, "tcdate": 1481916928161, "number": 5, "id": "HyOZfTbVl", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "B1Zkmo-4e", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "No Problem.", "comment": "We will definitely highlight these differences more clearly in the upcoming update to the technical sections.  Thanks again for your feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1481908953502, "tcdate": 1481908953502, "number": 4, "id": "B1Zkmo-4e", "invitation": "ICLR.cc/2017/conference/-/paper513/official/comment", "forum": "HJjiFK5gx", "replyto": "H127Js-Ng", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "content": {"title": "Thanks", "comment": "Thanks for the clarification. It would be great if these differences would be discussed in the paper too.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287543429, "id": "ICLR.cc/2017/conference/-/paper513/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287543429}}}, {"tddate": null, "tmdate": 1481908003877, "tcdate": 1481908003877, "number": 4, "id": "H127Js-Ng", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "rJk20UxEe", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Further Response", "comment": "Thanks again for your questions.  I will try to answer them point-by-point in the following:\n\n\"I believe that Joulin & Mikolov also uses a greedy decoding schemes at test time, so it is actually related.\"\n--> You are correct, my previous statement was not very precise.  All I meant was that at test time our model is essentially the same as the model of Reed & de Freitas. All three models are related in that they use RNNs with the ability to keep around additional information in a stack.\n\n\"\"\"what do you mean by \"function abstraction\"? In the paper it seems that you re pushing on the stack E[g], that is the embedding of the program. In Joulin & Mikolov, they use multiple stacks, which means that they can push an embedding as well if required (each stack pushing one number, if you have 10 stacks it can push an embedding of size 10).\n- \"without any constraints to encourage/enforce its use as a mechanism for function abstraction\": The value on the top of the stacks are used to update the hidden states, which then control other stacks. Doesn't that mean that it acts actually as a \"mechanism for function abstraction\"?\"\"\"\n\nThe Reed and de Freitas model on which we build enables/enforces explicit function abstraction in 2 main ways that the Joulin and Mikolov model does not:\n\t1. The model enforces a separation of state between the caller function, and the callee function.  Specifically, when calling into a function, the LSTM hidden state is reset to zero, and the only information passed into the function must come through the learned program and argument embedding vectors; and when returning from a function, the LSTM state from before the call is restored.\n\tFurthermore, the model enforces a coupling between calling into a function and returning from that function, such that the model always returns from every function that it calls into, and returns only once from the top/main function.  In contrast the stack mechanism in Joulin and Mikolov is very general and does not contain any mechanisms to encourage/enforce this separation.  This is similar to how function abstraction can be implemented on top of a simple stack, but there's much more to this abstraction than just the stack itself.  We believe the separation of state in the function abstraction is part of what enables the strong generalization exhibited by the model. Additionally, this separation of state forces the program and argument embedding vectors to contain all the information necessary to execute the function, and thus only these embeddings needed to be update to learn new subroutines.  While we did not explore this in our experiments, Reed and de Freitas showed this in their original paper, and in future work we would like to explore this in the context of our training framework.\n\t2. When the model pushes/pops from the stack in order to call into/return from a function it *does not* perform an elementary  operation (i.e., produce observable output).   This enables the model to make a decision to call into a function at one timestep, and at the next timestep make a separate decision about whether to call into another function or directly perform an elementary operation.  This nested decision making can be arbitrarily deep (up to some constant set at training time).   In contrast, the training setup of the Joulin & Mikolov framework requires an output token to be generated at *every* timestep, and enables only one element to be pushed onto each stack in a given timestep.  This creates a strong coupling between the decision to call into a function, and the decision around the first step in that function, and makes it difficult for the model to call into a function which in turn immediately calls into another function.  Consider for example a calling program performing the operation \"if cond then A() else B()\".  In the Joulin & Mikolov setting, the model would not only need to learn to branch between different functions like this, but also the first output token produced by A and B. In tasks that require many recursive calls (such as the NanoCraft example), this becomes extremely hard.\n\n\"I believe that Joulin et Mikolov also use multiple stacks that interact with each other, how is that different?\"\n\n--> If one considers the set of possible program execution paths as a tree with an exponential number of leaves, then the goal of much of the neural program inference literature is to represent the intermediate and final states in that tree as accurately as possible to enable gradient descent to push the parameters towards following the set of paths that generate correct outputs.  One extreme (albeit impractical) point in that space is to exactly represent all possible paths in that tree through exhaustive exploration.  Joulin and Mikolov (along with most other work in this space) choose what is in some sense, the opposite extreme, which maintains a single state representation (i.e. a single value for the state of each stack) for each timestep (i.e. depth in the tree) and they generate that approximate state by probabalistically averaging together all states at that depth.  Our training framework chooses a different point in this space by maintaining at each timestep (i.e. tree depth) a separate state of the stacks for each call depth $l$ and number of elementary operations performed so far, $i$ (i.e. $L*I$ different states for each call depth).  These states are generated by probabalistically averaging all paths which are at call depth $l$ and elementary operation $i$ at timestep $t$.  This results in an increase in memory and computation time during training, but more accurate training.  We have not tested this explicitly, but we believe this additional accuracy reduces the number of local minima, resulting in superior performance of the learned models.  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1481904692262, "tcdate": 1481904692262, "number": 1, "id": "Bk3Efq-Nl", "invitation": "ICLR.cc/2017/conference/-/paper513/official/review", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer3"], "content": {"title": "Progress in reducing the supervision required by NPI", "rating": "7: Good paper, accept", "review": "Neural Programmer-Interpreters (NPI) achieves greatly reduced sample complexity and better generalization than flat seq2seq models for program induction, but requires program traces at multiple levels of abstraction for training, which is a very strong form of supervision. One obvious way to improve this situation, addressed in this work, is to only train on the lowest-level traces, with a latent compositional program structure. This makes sense because the \"raw\" low-level traces can be cheaply gathered in many cases just by watching expert demonstrations, without being explicitly told the more temporally abstract structures.\n\nThis paper shows that a variant of NPI, named NPL, can achieve even better generalization performance with weaker supervision (mostly flat traces), and also extends the model to a new grid world task. Unfortunately, it still requires being told the overall program structure by being given a few *full* execution traces. Still, I see this as important progress. It extends NPI in a quite nontrivial way by introducing a stack mechanism modeling the latent program call structure, which makes the training process much more closely match what the model does at test time. The results tell us that flat execution traces can take us almost all the way toward learning compositional programs from demonstrations - the hard part is of course learning to actually discover the subprogram structure.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512557784, "id": "ICLR.cc/2017/conference/-/paper513/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper513/AnonReviewer3", "ICLR.cc/2017/conference/paper513/AnonReviewer2", "ICLR.cc/2017/conference/paper513/AnonReviewer1"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512557784}}}, {"tddate": null, "tmdate": 1481826114469, "tcdate": 1481825959486, "number": 3, "id": "rJk20UxEe", "invitation": "ICLR.cc/2017/conference/-/paper513/official/comment", "forum": "HJjiFK5gx", "replyto": "BkIXBre4g", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "content": {"title": "Further questions", "comment": "Thanks for the answer!\n\nLet me ask a few more questions though:\n\n\"In this scenario there is no use of a differentiable stack, and thus no relation to Joulin & Mikolov\"\n--> I believe that Joulin & Mikolov also uses a greedy decoding schemes at test time, so it seems actually related.\n\n\n\"We use the stack to implement explicit function abstraction.  In contrast, the stack in Joulin & Mikolov is used simply as an additional memory beyond that in the internal RNN cell/hidden state without any constraints to encourage/enforce its use as a mechanism for function abstraction.\"\n--> Sorry if I miss something, but:\n- what do you mean by \"function abstraction\"? In the paper it seems that you re pushing on the stack E[g], that is the embedding of the program. In Joulin & Mikolov, they use multiple stacks, which means that they can push an embedding as well if required (each stack pushing one number, if you have 10 stacks it can push an embedding of size 10). That being said, they don't push explicitly embeddings on their stacks.\n- \"without any constraints to encourage/enforce its use as a mechanism for function abstraction\": The value on the top of the stacks are used to update the hidden states, which then control other stacks. Doesn't that mean that it acts actually as a \"mechanism for function abstraction\"?\n\n\n\"We trade off computation for accuracy by representing many possible stack states at each time step, one for each number of elementary operations performed so far, $i$ and each possible stack depth, $l$.  The resulting stack updates are more complicated and more computationally intensive, but also more accurate.\"\n--> I believe that Joulin et Mikolov also use multiple stacks that interact with each other, how is that different?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287543429, "id": "ICLR.cc/2017/conference/-/paper513/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287543429}}}, {"tddate": null, "tmdate": 1481819421907, "tcdate": 1481819421901, "number": 3, "id": "BkIXBre4g", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "SyIIhy1Nl", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Response", "comment": "At test time, our current implementation is equivalent to the model of Reed and de Freitas since we use a greedy decoding process just as they do.  In this scenario there is no use of a differentiable stack, and thus no relation to Joulin & Mikolov.  At training time, our framework uses the same primitive operation used by Joulin & Mikolov of stack averaging weighted by probabilities.  However, we extend/modify their model in two important ways:\n\t1. We use the stack to implement explicit function abstraction.  In contrast, the stack in Joulin & Mikolov is used simply as an additional memory beyond that in the internal RNN cell/hidden state without any constraints to encourage/enforce its use as a mechanism for function abstraction.\n\t2. We trade off computation for accuracy by representing many possible stack states at each time step, one for each number of elementary operations performed so far, $i$ and each possible stack depth, $l$.  The resulting stack updates are more complicated and more computationally intensive, but also more accurate.\nThat said, we recognize these distinctions are not very clear in the current version of the paper, and we are in the process of restructuring the technical sections to make this comparison more explicit and easier to understand and will include this in our next update to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1481731150178, "tcdate": 1481731150172, "number": 2, "id": "SyIIhy1Nl", "invitation": "ICLR.cc/2017/conference/-/paper513/official/comment", "forum": "HJjiFK5gx", "replyto": "ByO4UNJXx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer1"], "content": {"title": "Clarify the relation between your stack and Joulin & Mikolov.", "comment": "Would it be possible that you clarify the relation between your stack and the one from Joulin & Mikolov in the paper? Currently you're not making any connection with this work in your paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287543429, "id": "ICLR.cc/2017/conference/-/paper513/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287543429}}}, {"tddate": null, "tmdate": 1481636401154, "tcdate": 1481636401147, "number": 2, "id": "BJFEqdpXg", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "BysqhV87e", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Response", "comment": "Thank you for your question.\u00a0 That paragraph in the introduction was only intended to motivate the general direction of learning from action sequences by claiming that it is quite natural to gather this type of data for real world problems.\u00a0 We have not yet tested on data with noise so we did not intend to make any claims about that scenario.  We will make this distinction more clear in the writing.\u00a0\u00a0\u00a0 That said, we don't have any reason to believe that the basic framework won't work with noisy data without much modification,  since even a standard RNN trained with maximum likelihood will correctly learn a distribution over action sequences given enough data.  We  would like to explore this scenario in our future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1481161874992, "tcdate": 1481161874986, "number": 2, "id": "BysqhV87e", "invitation": "ICLR.cc/2017/conference/-/paper513/pre-review/question", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer2"], "content": {"title": "noisy observations", "question": "In the introduction, the paper mentions an example of abstracting human tasks from Kinect skeletons. However, it is difficult to easily imagine how the proposed approach can learn the model from such noisy observations possibly having multiple variations. We would like to see more clarifications/discussions on this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481731454049, "id": "ICLR.cc/2017/conference/-/paper513/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper513/AnonReviewer3", "ICLR.cc/2017/conference/paper513/AnonReviewer2", "ICLR.cc/2017/conference/paper513/AnonReviewer1"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481731454049}}}, {"tddate": null, "tmdate": 1480714890786, "tcdate": 1480701488251, "number": 1, "id": "ByO4UNJXx", "invitation": "ICLR.cc/2017/conference/-/paper513/public/comment", "forum": "HJjiFK5gx", "replyto": "BJfD_DCzl", "signatures": ["~Nate_Kushman1"], "readers": ["everyone"], "writers": ["~Nate_Kushman1"], "content": {"title": "Response:", "comment": "For each timestep, our lattice maintains a grid of different possible stacks, one for each number of elementary operations performed so far, $i$ and each possible stack depth, $l$.  Each grid node in the following timestep will contain a merged (weighted average) version of three different stacks from the previous timestep.  In the general case, these three different stacks will have no particular relationship to each other, and will not necessarily share any elements.  Our stack merging operation is a natural extension of the stack model from Mikolov et al.  However, the stack model from https://arxiv.org/abs/1506.02516 takes advantage of the fact that entities are added to the stack in a temporal order, which is not the case in our setting since at each timestep entities are pushed/popped from all nodes in the grid in parallel. Thus we need the ability to take a weighted average of full stacks, and it seems less natural to do this with the representation of https://arxiv.org/abs/1506.02516"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287544404, "id": "ICLR.cc/2017/conference/-/paper513/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJjiFK5gx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper513/reviewers", "ICLR.cc/2017/conference/paper513/areachairs"], "cdate": 1485287544404}}}, {"tddate": null, "tmdate": 1480648794553, "tcdate": 1480648794549, "number": 1, "id": "BJfD_DCzl", "invitation": "ICLR.cc/2017/conference/-/paper513/pre-review/question", "forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "signatures": ["ICLR.cc/2017/conference/paper513/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper513/AnonReviewer3"], "content": {"title": "Choice of neural stack", "question": "The stack in this work resembles the one proposed by Mikolov et al (https://arxiv.org/abs/1503.01007). Was there a particular requirement that led to the choice of this type of stack, versus other neural stack/queue models, e.g. https://arxiv.org/abs/1506.02516?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Program Lattices", "abstract": "We propose the Neural Program Lattice (NPL), a neural network that learns to perform complex tasks by composing low-level programs to express high-level programs. Our starting point is the recent work on Neural Programmer-Interpreters (NPI), which can only learn from strong supervision that contains the whole hierarchy of low-level and high-level programs. NPLs remove this limitation by providing the ability to learn from weak supervision consisting only of sequences of low-level operations. We demonstrate the capability of NPL to learn to perform long-hand addition and arrange blocks in a grid-world environment. Experiments show that it performs on par with NPI while using weak supervision in place of most of the strong supervision, thus indicating its ability to infer the high-level program structure from examples containing only the low-level operations.", "pdf": "/pdf/8d4a4645cf9a906b7cc8e5da7d7d8d54045bcda1.pdf", "paperhash": "li|neural_program_lattices", "keywords": ["Deep learning", "Semi-Supervised Learning"], "conflicts": ["mit.edu", "microsoft.com"], "authors": ["Chengtao Li", "Daniel Tarlow", "Alexander L. Gaunt", "Marc Brockschmidt", "Nate Kushman"], "authorids": ["ctli@mit.edu", "dtarlow@microsoft.com", "algaunt@microsoft.com", "mabrocks@microsoft.com", "nkushman@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481731454049, "id": "ICLR.cc/2017/conference/-/paper513/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper513/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper513/AnonReviewer3", "ICLR.cc/2017/conference/paper513/AnonReviewer2", "ICLR.cc/2017/conference/paper513/AnonReviewer1"], "reply": {"forum": "HJjiFK5gx", "replyto": "HJjiFK5gx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper513/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481731454049}}}], "count": 18}