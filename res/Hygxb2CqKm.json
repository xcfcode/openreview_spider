{"notes": [{"id": "Hygxb2CqKm", "original": "H1xtrh-cYm", "number": 1136, "cdate": 1538087927658, "ddate": null, "tcdate": 1538087927658, "tmdate": 1548123791197, "tddate": null, "forum": "Hygxb2CqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xscrn0b4", "original": null, "number": 14, "cdate": 1546728850998, "ddate": null, "tcdate": 1546728850998, "tmdate": 1546728850998, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "HylIggEFW4", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your interest in our paper and highlighting additional related work. We will incorporate these references into the final version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "HylIggEFW4", "original": null, "number": 1, "cdate": 1546366958026, "ddate": null, "tcdate": 1546366958026, "tmdate": 1546367032404, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Public_Comment", "content": {"comment": "I like the way the problem of RNN stability is tackled and the feasibility of replacing them with feed-forward networks is demonstrated in this paper.\n\nThere are a couple of papers on stabilizing RNN training which were published in ICML 2018 and NeurIPS 2018 which can be included into related work.\n\n1) Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization - (Zhang et al, ICML 2018)\n2) Kronecker Recurrent Units - (Jose et al, ICML 2018)\n3) FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network - (Kusupati et al, NeurIPS 2018)\n\nAlso, in ICML 2017, along with (Vorontsov et al.) there were two more paper dealing with stabilization of RNNs \n1)  Efficient orthogonal parametrisation of recurrent neural networks using householder reflections - (Mhammedi et al., ICML 2017)\n2) Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs - (Jing et al., ICML 2017).\n\nIt would be great if the authors could add these to the camera ready version of the paper make their related search more comprehensive and complete.\n\nThanks.", "title": "Interesting take on RNN stability and needs small updates in Related Work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311670248, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hygxb2CqKm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311670248}}}, {"id": "SJgGCBrZx4", "original": null, "number": 1, "cdate": 1544799689747, "ddate": null, "tcdate": 1544799689747, "tmdate": 1545354526101, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Meta_Review", "content": {"metareview": "The paper presents both theoretical analysis (based upon lambda-stability) and experimental evidence on stability of recurrent neural networks. The results are convincing but is concerns with a restricted definition of stability. Even with this restriction acceptance is recommended. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Important topic, favorable reviews but are the stated implications general?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1136/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352952100, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352952100}}}, {"id": "Ske9haIsRX", "original": null, "number": 13, "cdate": 1543364018383, "ddate": null, "tcdate": 1543364018383, "tmdate": 1543364018383, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "SJeu1Uh7aQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the clarification and fixing the notations in Theorem 1. I think the discussion of unitary RNN models makes the paper more well-rounded. I hope this work will inspire more research in this direction in the future and help us understand the dynamics of recurrent networks. I would like to keep my rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "ByeBXzI5AQ", "original": null, "number": 11, "cdate": 1543295517017, "ddate": null, "tcdate": 1543295517017, "tmdate": 1543295517017, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "SJxBPyJHam", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Well-written, thorough responses", "comment": "I don't have much to add to the thorough discussion below. I was already in the \"accept\" camp, and I remain there. I will confer with the other reviewers and consider a revised score."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "r1xhUKOThQ", "original": null, "number": 2, "cdate": 1541405012088, "ddate": null, "tcdate": 1541405012088, "tmdate": 1542346723013, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "content": {"title": "Interesting theoretical and practical results but false claims on RNNs", "review": "+ An interesting problem to study on the stability of RNNs\n+ Investigation of spectral normalization to sequential predictions is worthwhile, especially Figure 2\n+ Some theoretical justification of SGD for learning dynamic systems following Hardt et al. (2016b).\n\n- The take-home message of the paper is not clear. First, it defines a  notion of stability based on Lipchitz-continuity and proves SGD can learn it. Then the experiments show such a definition is actually not correct, but rather a data-dependent one. \n- The theory only looks at the instantaneous dynamics from time t to t+1, without unrolling the RNNs over time. Then it is not much different from analyzing feed-forward networks. The theorem on SGD is remotely related to the contribution of the paper. \n- The spectral normalization technique that is actually used in experiments is not new", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "cdate": 1542234297916, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880728, "tmdate": 1552335880728, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gN66Jqam", "original": null, "number": 9, "cdate": 1542221243708, "ddate": null, "tcdate": 1542221243708, "tmdate": 1542221243708, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "rkgdKTUw6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Revision to paper", "comment": "Thank you for your response. We have updated the paper to reflect our discussion. In particular, \n- we make clear the sufficient stability conditions are only new in the case of the LSTM and appropriately cite Jin et al. for the 1-layer RNN\n- we added a discussion around the relationship between stability and data-dependent stability\n- we clarify our notion of \"equivalence\" is only in terms of the context required to make predictions and not, e.g., in terms of number of parameters or some other measure, and added further discussion of this distinction to Section 5.\n\nWe're happy to address any additional concerns with the current presentation."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "rkgdKTUw6X", "original": null, "number": 8, "cdate": 1542053248286, "ddate": null, "tcdate": 1542053248286, "tmdate": 1542053259052, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "BJeX698vpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Reasonable response", "comment": "Appreciate your response.  I am willing to upgrade the rating if the authors can tone down the theoretical claims."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "BJeX698vpQ", "original": null, "number": 7, "cdate": 1542052538562, "ddate": null, "tcdate": 1542052538562, "tmdate": 1542052567002, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "SkgZ4aMPTm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your prompt response. We address these concerns in turn.\n\nGap between stability conditions: \nThe data-dependent condition is a strict relaxation of the Lipschitz condition. Two additional comments are in order.\n1) Stability is a clarifying concept. The Lipschitz condition is clean and allows us to understand the core phenomena associated with stability. The data-dependent definition is a useful diagnostic--  when our sufficient (Lipschitz) stability conditions fail to hold, the data-dependent condition addresses whether the model is still operating in the stable regime. \n2) In many cases, we can still prove results with the data-dependent guarantee.\n--If the input representation is fixed, then all of the proofs go through with the data-dependent condition. If S is the set of inputs from the data distribution, we can simply replace all instances of \u201cfor all x\u201d with \u201cfor all x in S\u201d. This is the case with polyphonic music modeling. \n--When the input is not fixed (e.g. word vectors that are updated during training), the proofs go through provided S is interpreted as \u201call word vectors generated during training.\u201d\n\nIn section 2.2, the subscript t is dropped because the Lipschitz definition of stability (eq 2) must hold for all x. \n\nTheoretical contribution: \nOur main theoretical contribution is feed-forward approximation of stable recurrent models, especially Proposition 3 and Theorem 1. The results in section 2.2 give concrete examples of our general stability definition. For a 1-layer RNN, the cited paper [1] gives similar stability conditions. However, [1] does not touch on the question of feed-forward approximation, particularly approximation during training, nor does it mention LSTMs. We will add the appropriate citation, but note the RNN stability conditions are a routine one-line calculation and far from our main technical contribution. \n\nEquilibrium states: \nWe only claim equivalence between *stable* RNNs and feed-forward networks. In stable RNNs, all trajectories converge to an equilibrium state. Certainly, general (unstable) RNNs cannot be approximated with feed-forward networks. Understanding to what extent models trained in practice are stable or can be made stable is then an empirical question, and we address this question in Section 4. \n\nImplementing truncated models as feed-forward networks increases the number of weights by a factor of $k$. This increase is an artifact of our analysis, and it is an interesting open question to find more parsimonious approximations. From a memory perspective, a feed-forward network with more weights is still a feed-forward network, and our result establishes stable recurrent models cannot have more memory than feed-forward models."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "SkgZ4aMPTm", "original": null, "number": 6, "cdate": 1542036777070, "ddate": null, "tcdate": 1542036777070, "tmdate": 1542036777070, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "rkeKirnQp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "significance of the theoretical claim", "comment": "- there is a gap between 'Lipschitz' and 'data-dependent' stability. why is that? In the proof of Section 2.2,  in order to satisfy the contractive mapping condition,  input data x does not have subscript t, can you justify?\n\n- the global stability property for one-layer RNN based on the Lipschitz condition of the activation function is a known result (e.g.[1]). what is the new contribution here?\n\nJin, Liang, Peter N. Nikiforuk, and Madan M. Gupta. \"Absolute stability conditions for discrete-time recurrent neural networks.\" IEEE Transactions on Neural Networks 5.6 (1994): 954-964.\n\n- The equivalence between RNN and feedforward networks is at the equilibrium state. But how about non-equilibrium states? and the number of weights? It is misleading to claim the two to be equivalent.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "SJxBPyJHam", "original": null, "number": 5, "cdate": 1541889885195, "ddate": null, "tcdate": 1541889885195, "tmdate": 1541889885195, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "HJe5FH37pX", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Thanks!", "comment": "Thank you for the prompt and thoughtful response. I wanted to let you know that I have read it (and your other responses) and am thinking about follow-up questions. Expect me to reply by mid-next week."}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "SJeu1Uh7aQ", "original": null, "number": 4, "cdate": 1541813728349, "ddate": null, "tcdate": 1541813728349, "tmdate": 1541813728349, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "rylX3rB_27", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your detailed comments and feedback. We have incorporated some of these suggestions into a revision of the paper. We discuss your concerns below.\n\nMotivation of stable models: \nThere are two reasons to consider stability in recurrent models: \n1) Stability is natural criterion for learnability in recurrent models. Outside the stable regime, learning recurrent models requires a delicate mix of heuristics. Studying stable models addresses whether this collection of tricks is actually necessary, and our results suggest a better-behaved model class can solve many of the same problems. \n\n2) Understanding whether models trained in practice are in the stable regime helps answer when recurrent models are truly necessary. As the reviewer noted, whether the stable model is \u201cdesirable\u201d depends on experimentation. However, when a stable model achieves similar performance with an unstable model, the conclusion is a feed-forward network suffices to solve the task. We demonstrate sequence learning happens in the stable regime, and this helps explain the widespread success of feed-forward models on sequence problems.\n\n\nVanishing Gradients: \nStable recurrent models always have vanishing gradients, and vanishing gradients are an important part of proving our approximation results. However, vanishing gradients are not unique to stable models. In the updated version of the paper, we show unstable language models also exhibit vanishing gradients. This corroborates the evidence in section 4.3 showing these models operate in the stable regime.\n\nThe cited unitary RNN models may help reduce vanishing gradients. Even in these works, there is still gradient decay over time (e.g. Figure 4, ii in [1]), but the rate of decay is slower. The updated version of the paper includes a brief discussion of these works. At minimum, these models have not yet seen widespread use, and our work demonstrates models frequently trained in practice are either stable or can be made stable without performance loss.\n\nEmpirical study of the difference between recurrent and truncated models: \nIn the revision, we added experiments studying truncation in the unstable models and also show unstable models satisfy a qualitative version of Theorem 1. All of the models considered, including the LSTM language models, exhibit sharply diminishing returns to larger values of the truncation parameter. As predicted by theorem 1, the difference between the truncated and full recurrent matrix during training becomes small for moderate values of the truncation parameter.\n\nComparison between stable and unstable models: \nWe disagree with the interpretation of Table 1. Except for the LSTM language models, the variation in performance between stable and unstable models is within standard-error. We do not retune the hyperparameters when imposing stability, and the near equivalence of the results is evidence the unstable models do not offer a large performance boost. For the LSTM language models, in section 4.3 and 4.4, we argue the unstable LSTM language models are close to the stable regime, and the gap between stable and unstable models is an artifact of the particular way we impose stability. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "rkeKirnQp7", "original": null, "number": 3, "cdate": 1541813665285, "ddate": null, "tcdate": 1541813665285, "tmdate": 1541813665285, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "r1xhUKOThQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "Thank you for your comments and feedback. We address each of your concerns below.\n\nTake-home message: \nThe message of the paper is that sequence learning happens, or can be made to happen, in the stable regime. The Lipschitz definition of stability (eq. 2) and the \u201cdata-dependent\u201d definition introduced in the experiments are complementary. The data-dependent definition is just a relaxation of the Lipschitz criteria-- we only require equation 2 to hold for inputs from the data-distribution. For the proofs and the majority of the experiments, the strict Lipschitz condition suffices. Most models can be made stable in the sense of equation 2 without performance loss. For LSTMs on language modeling, the data-dependent version illustrates even the nominally unstable LSTMs are close to the stable regime-- a truly unstable model would not satisfy even this weaker definition. We view results with both definitions as evidence recurrent models trained in practice operate in the stable regime.\n\nInstantaneous dynamics: \nThe theory in our paper does consider unrolling the RNNs over time.  While the stability condition is stated purely in terms of the the state-transition function from step t to step t+1, the main theoretical results (Proposition 3 and Theorem 1) specifically concern the unrolled RNN. In particular, our results show that the unrolled (stable) RNN can be approximated by a feed-forward network. \n\nSpectral Normalization: \nIn our experiments, our focus is more on comparing the performance of stable and unstable models and less on the particular form of normalization used to achieve stability. In the RNN case, enforcing stability via constraining the spectral norm of the recurrent matrix is fairly routine. In the LSTM case, the stability conditions given in Proposition 2 are new and allow one to experiment with stable LSTMs. The updated version of the paper includes a discussion of these other works.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "HJe5FH37pX", "original": null, "number": 2, "cdate": 1541813634059, "ddate": null, "tcdate": 1541813634059, "tmdate": 1541813634059, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "HJeGRbFZT7", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your detailed comments and feedback. \n\nWe agree it is difficult to know a priori whether particular dataset will be amenable to stable models. However, stability can still be a clarifying idea in practice. Given a dataset where stable models perform comparably with unstable models, either the dataset does not require long-term memory (i.e. feed-forward approximation suffices), or the unstable models do not take advantage of it. We conjecture most recurrent models successfully trained in practice are operating in the stable regime. To further test this claim, it would be interesting to find datasets (if any) where unstable models significantly outperform stable models, or datasets where non-recurrent models aren\u2019t competitive with their recurrent counterparts. \n\nIn the revision, we added discussion of the several recent works constraining RNN matrices. These works try to keep the model just outside the stable regime to avoid vanishing gradients and side-step exploding gradients (i.e. take lambda ~ 1). The spectral norm thresholding technique for RNNs is straightforward, whereas the stability conditions for the LSTM is new. In either case, our focus is on using these techniques to understand the consequences of imposing stability on recurrent models.\n\nIn general, answering the question of accuracy is fairly delicate. We\u2019re able to show stable and truncated/feed-forward models have the same accuracy. Bounds relating the accuracy of an unstable model with the accuracy of an stable one almost certainly require further assumptions on the data distribution. Obtaining such accuracy bounds for neural networks has been elusive, and part of the contribution of our work is proving a connection between the performance two model classes (stable RNNs and truncated/feed-forward models) without needing to resolve these questions. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620242, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hygxb2CqKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1136/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1136/Authors|ICLR.cc/2019/Conference/Paper1136/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers", "ICLR.cc/2019/Conference/Paper1136/Authors", "ICLR.cc/2019/Conference/Paper1136/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620242}}}, {"id": "HJeGRbFZT7", "original": null, "number": 3, "cdate": 1541669321524, "ddate": null, "tcdate": 1541669321524, "tmdate": 1541669321524, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "content": {"title": "Interesting theoretical angle on RNNs that provides insights but also feel incomplete", "review": "This is an interesting paper that I expect will generate some interest within the ICLR community and from deep learning researchers in general. The definition of stability is both intuitive and sound and the connection to exploding gradients is perhaps the most interesting and useful part of the paper. The sufficient conditions yield practical techniques for increasing the stability of, e.g., an LSTM, by constraining the weight matrices. They also show that stable recurrent models can be approximated by models with finite historical windows, e.g., truncated RNNs. Experiments in Sec 4 suggest that stable models produced by constraining standard RNN architectures can compete with their unconstrained unstable counterparts, and often without necessitating significant changes to architecture or hyperparameters. The perhaps most interesting observations are in Sec 4.3, in which the authors claim that even fundamentally unstable models, e.g., unconstrained RNNs, often operate in a stable regime, at least when being applied to in-sample data. I lean toward acceptance at the moment, but I am eager to discuss with the authors and other reviewers as I am not 100% confident that I fully understood the theory.\n\nSUMMARY\n\nThis paper proposes a simple, generic definition of \u201cstability\u201d for recurrent, non-linear dynamical systems such as RNNs: that given two hidden states h, h\u2019, the difference between their updated states given input x is bounded by the product between the difference between the states themselves and a small multiplier. The paper then immediately draws a connection between stability, asserting that unstable models are prone to gradient explosions during gradient descent-based training. In Sec 2.2, the paper presents sufficient conditions for basic RNNs and LSTMs to be stable. Secs 3.2 and 3.3 argue that stable recurrent models can be approximated by feedforward models during both inference and training with a finite history horizon, such as a RNN with a truncated history. Experiments in language and music modeling substantiate this claim: constrained, stable models are competitive with standard unconstrained models. Sec 4.3 sheds some light on this phenomenon, arguing that there is a weaker form of data-dependent stability and that even unstable models may operate in a stable regime for some problems, thus explaining the parity between stable and unstable models.\n\nSTRENGTHS\n\n* This paper is surprisingly engaging and easy to read.\n* The theorems are clearly stated and the proofs appear sound to me, though I will admit that I am not confident that I would catch a significant bug.\n* This paper provides a new (to me, anyway) and thought-provoking analysis of RNNs. In particular, I was especially interested in the observation that stable models can be approximated by truncated models and that there is a connection between stability and long-term dependencies. This seems consistent with the fact that for many problems, non-recurrent models (ConvNets, Transformers, etc.) are often competitive with more complex architectures.\n\nWEAKNESSES\n\n* In practice it seems as though stability may depend on not only choice of  model architecture but also the data themselves. There is probably no good way to know a priori what the stability characteristics of a given data set are, making it tough to apply the ideas of this paper in practice\n* The literature review seems a bit limited and appears to ignore the growing body of work on constraining RNN weight matrices to address both exploding and vanishing gradients. For example, I am pretty confident that the singular thresholding trick for renormalizing neural net weights has been  described in the literature previously.\n* Although stable and unstable models appear to be competitive in experiments, the theoretical analysis provides no insights into stability and how it relates to accuracy.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "cdate": 1542234297916, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880728, "tmdate": 1552335880728, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylX3rB_27", "original": null, "number": 1, "cdate": 1541064107052, "ddate": null, "tcdate": 1541064107052, "tmdate": 1541533391282, "tddate": null, "forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "content": {"title": "Review", "review": "In this paper, the authors study the stability property of recurrent neural networks. Adopting the definition of stability from the dynamical system literature, the authors present a generic definition of stable recurrent models and provide sufficient conditions of stable linear RNNs and LSTMs. The authors also study the \"feed-forward\" approximation of recurrent networks and theoretically show that the approximation works for both inference and training. Experimental studies compare the performance of stable and unstable models on various tasks.\n\nThe paper is well-written and very pleasant to read. The notations are clear and the claims are relatively easy to follow. The theoretical analysis in Section 3 is novel, interesting and solid. However, the reviewer has concerns about the motivation of the presented analysis and insufficient empirical results.\n\nThe stability property only eliminates the exploding gradient problem, but not the vanishing gradient problem. The reviewer suspects that a stable recurrent model always suffers from vanishing gradient. Therefore, stability might not necessarily be a desirable property. There has been a line of work that constrain the weight matrix in RNNs to be orthogonal or unitary so that the gradient won't explode, e.g. [1], [2], [3]. It seems that the orthogonal or unitary conditions are stronger than the stability condition, and are probably less prone to the vanishing gradient problem. \n\nThe vanishing gradient problem is also related to the analysis in Section 3. If a recurrent network is very stable and has vanishing gradient, then a small perturbation of the initial hidden state has little effect on later time steps. This intuitively explains why it can be well approximated by using only the last k time steps. However, the recurrent model itself might not be a desirable model.  In other words, although Theorem 1 shows that $y_T$ and $y_T^k$ can be arbitrarily close, $y_T$ might not be a good prediction.\n\nThe experimental study seems weak. Again, in the RNN case, constraining the singular values of the weight matrix is not a new idea. Furthermore, the results in Table 1 seem to suggest that the stable models perform worse than unstable ones. What is the benefit in using stable models? Proposition 2 is only a sufficient condition of a stable LSTM and it seems very restrictive, as the authors point out. This might explain the worse performance of the stable LSTMs in Table 1. The reviewer was expecting more experimental results to support the claims in Section 3. For example, an empirically study of the difference between a recurrent model and a \"feed-forward\" or truncation approximation.\n\nMinor comments:\n* Lemma 1: $\\lambda$-contractive => $\\lambda$-contractive in $h$?\n* Theorem 1: $k=O(...)$ => $k=\\Omega(...)$? Intuitively, a bigger k leads to a better feed-forward approximation.\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. ICML, 2016.\n[2] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary recurrent neural networks. NIPS, 2016.\n[3] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning recurrent networks with long term dependencies. ICML, 2017.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1136/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stable Recurrent Models", "abstract": "Stability is a fundamental property of dynamical systems, yet to this date it has had little bearing on the practice of recurrent neural networks. In this work, we conduct a thorough investigation of stable recurrent models. Theoretically, we prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Empirically, we demonstrate stable recurrent models often perform as well as their unstable counterparts on benchmark sequence tasks. Taken together, these findings shed light on the effective power of recurrent networks and suggest much of sequence learning happens, or can be made to happen, in the stable regime. Moreover, our results help to explain why in many cases practitioners succeed in replacing recurrent models by feed-forward models.\n", "keywords": ["stability", "gradient descent", "non-convex optimization", "recurrent neural networks"], "authorids": ["miller_john@berkeley.edu", "hardt@berkeley.edu"], "authors": ["John Miller", "Moritz Hardt"], "TL;DR": "Stable recurrent models can be approximated by feed-forward networks and empirically perform as well as unstable models on benchmark tasks.", "pdf": "/pdf/2e602c275cb2de7c656905c69b101bd66f2df83f.pdf", "paperhash": "miller|stable_recurrent_models", "_bibtex": "@inproceedings{\nmiller2018stable,\ntitle={Stable Recurrent Models},\nauthor={John Miller and Moritz Hardt},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Hygxb2CqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1136/Official_Review", "cdate": 1542234297916, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hygxb2CqKm", "replyto": "Hygxb2CqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1136/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335880728, "tmdate": 1552335880728, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1136/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 17}