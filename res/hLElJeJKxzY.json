{"notes": [{"id": "hLElJeJKxzY", "original": "ETKe2h0KGEa", "number": 2859, "cdate": 1601308317320, "ddate": null, "tcdate": 1601308317320, "tmdate": 1614985703049, "tddate": null, "forum": "hLElJeJKxzY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "c9OoehBL-p6", "original": null, "number": 1, "cdate": 1610040441606, "ddate": null, "tcdate": 1610040441606, "tmdate": 1610474042699, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewer acknowledged that the proposed method is simple and seems to work well on the chosen benchmarks. Yet the expressed several concerns that were not fully addressed by the authors in their responses. The major concern is about the experimental setup. The chosen tasks have been judged too simple and quite different from those where the baselines were tested initially (e.g. DQfD was demonstrated on a diverse set of Atari games). \n\nThe clarity of the paper should also be improved. For instance, the way the number of trajectories that are added to the replay buffer increases with time is not well explained and it seems to be crucial for the algorithm to outperform the baselines. The authors also seemed to select the experiments so that the \"results are more persuasive\", discarding experiments where you can be unlucky. This looks very much like cherry-picking and didn't convince the reviewers. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040441592, "tmdate": 1610474042681, "id": "ICLR.cc/2021/Conference/Paper2859/-/Decision"}}}, {"id": "d275UPzeZcH", "original": null, "number": 1, "cdate": 1603170062734, "ddate": null, "tcdate": 1603170062734, "tmdate": 1606803856295, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review", "content": {"title": "Review", "review": "=====POST-REBUTTAL COMMENTS======== \n\nI thank the authors for the response and the efforts in the updated draft. Most of my concerns were addressed. This is a simple, but nice idea. After reading the rebuttal and the other reviews I am recommending to accept the paper.\n\n##########################################################################\n\nSummary:\n \n\nThis paper seeks to improve deep reinforcement learning from demonstrations via a supervised learning loss based on behavioral cloning. The results show that adding the BC loss stabilizes training and shows improvements over baselines for OpenAI Gym environments. \n\n##########################################################################\n\nReasons for score: \n \n\nI think the idea is nice, but it is hard to follow as written and the experiments are on simple problems. It would be better to compare against more complex benchmarks such as Atari. The writing needs improvement for clarity.\n \n\n##########################################################################\nPros: \n \n\n1. The ablation study in Figure 4 shows the benefit of the proposed method.\n\n2. The ability to outperform a suboptimal demonstrator is important for imitation learning in the real world.\n \n\n\n\n##########################################################################\n\nCons: \n \n1. The idea of adding new trajectories from high scores was interesting, but I was left wondering why this works. Won't this add a lot of really bad trajectories in the beginning of learning which seem like they would have a negative impact on the BC policy. Also, the function \"get the best episode score\" was never rigorously defined.\n\n2. Equations should be introduced before they are included. Equations 1-4 come before the text that talks about them and this makes the paper more confusing that it needs to be.\n\n3. Equation (5) seems incomplete. Shouldn't l(\\pi, \\pi_bc, s_t) be a function of the action? Otherwise it is just a constant per state, but it is in the max_a brackets. Also it is unclear what the difference between s_t and s are in Eqn (5).\n\n4. Average stop episode is not clearly defined in the text. In the appendix it explains more but the thresholds are not justified.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069217, "tmdate": 1606915784244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2859/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review"}}}, {"id": "1URtDDEb6F", "original": null, "number": 9, "cdate": 1606225793034, "ddate": null, "tcdate": 1606225793034, "tmdate": 1606225793034, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "bNgV4V6NMMg", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": " ", "comment": "Thanks to the authors for replying to each of the points mentioned in the review.\n\nThe feedback for comments 1.1 to 1.6 is fine.\n\n1.7.1 The two versions of CartPole indeed have only that difference on the maximum duration of the episodes, but still, the observations, the actions, the dynamics, and the reward function are the same, so not much-added value with one of the two cases, which could have been replaced by another problem with different challenges.\n\n1.7.2 \"To make the results more persuasive\" does not sound very objective, are results not shown if they are not good enough?\n\n1.7.3 It is good you could add results from a higher number of runs, although I wonder what has changed because the new plots do not overlap with the previous ones considering the low variance, then it looks like they are obtained with different methods.\n\n1.7.4 I notice the results have been also updated apart from the clarification of the table. I wonder why does it take longer for the CartPole-v0 than the v1 since as you also pointed out, the v1 is more challenging.\n\n1.7.5 Ok.\n\n1.7.6 This is nice to be included somewhere, perhaps in the supplementary material."}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "bNgV4V6NMMg", "original": null, "number": 3, "cdate": 1606099496069, "ddate": null, "tcdate": 1606099496069, "tmdate": 1606114150221, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "U2thcg5eBr", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Reply-part2", "comment": "Comment 1.7:\n\n1. Regarding the experiments section- There are missing details of the experiments that do not allow for reproducing the procedure, how many repetitions were run for each case in Fig 2 and 3?, and also for the ablation study? It is not clear whether the results of Fig 2 and 3 are obtained from the same experiments that gave the results in Table 1, at least they do not seem to match. Why testing with the 2 variants of the CartPole environment instead of another kind of problem? since the difference between them is very simple\n2. Why is there no data in Table 1 for the cases of imperfect demonstrations for the CartPole-V0 environment?\n3. Why the learning curves of DDQN for the CartPole-V0 and CartPole-V1 cases are not similar at least during the first 200 episodes, looking at the mean and variance, both environments are not close to reach the maximum time steps, therefore in that region they should behave exactly the same.\n4. In Table 1, it is not so clear what average stop episode means, if it is the episode number in which the stop condition was reached, why are there negative numbers for the acrobot? if they are rewards obtained in the last episode, why are rewards higher than the possible to obtain in CartPole-V0.\n5. It is mentioned  \"...but the DDQN and DQfD fail to meet the experimental termination condition with imperfect demonstration\", However this is completely wrong, DDQN do not use demonstrations.\n6. It would be interesting to additionally mention how to define the size of the margin for the expert cost function, because this may depend on the kind of reward functions/range of value functions.\n\n**Feedback 1.7**:\n\n1. In all the experiments, the allowed maximum episodes are set to 1000 and 10 repetitions are completed to get the final results. Since different repetitions trigger the termination condition at different episodes, the data in the table are the statistic value (mean\u00b1std) and the data in the figure shows more detailed information about the learning process. The reason why testing with two CartPole environments is that the difficulty of the two environments is different. The maximum step of CartPole-v1 is 500 but it is 200 in CartPole-v0, more steps mean more carefully controlling are needed to satisfy the requirement of the environment. Thus, we complete different experiments on these two environments.\n2. Compared to the Cartpole-v1, the termination condition of Cartpole-v0 is much easier to be satisfied. In some lucky episodes, the termination condition can be triggered in Cartpole-v0, which is almost impossible in Cartpole-v1. To make the results more persuasive, we only test the method in Cartpole-v1 and we add this reason in the revised manuscript.\n3. Thanks for your comment, we have checked the experimental results and updated the results. We test our method in the CartPole environments again and the reward curves of DDQN are similar in newly obtained results. Due to time constraints, we only repeated the experiment 5 times and the maximum number of episodes was adjusted to 500 when we re-run the experiments. We also updated the ablation results.\n4. In Table 1, the average stop episode means the average stop episode number among different runs. The data in the \u201cDemo\u201d columns is the average score and a negative number in an Acrobot environment is normal because the reward in Acrobot environment is negative. The demonstration score is evaluated through the episode score, so it is negative in Acrobot. We update the table to make it clear.\n5. We have corrected the sentence to avoid the misunderstanding as follow: \u201c\u2026but DQfD fail to meet the experimental termination condition with imperfect demonstration and DDQN also failed to trigger the termination condition before the experiment was forced to stop\u201d\n6. The margin is a hyperparameter and it is fine-tuned in the experiments. A margin that is too large will cause the model to heavily rely on the supervised learning process and lose its ability to learn independently, while a margin that is too small will cause the advantages of supervised learning to be unused. In our experience, it has to be in the same order of magnitude and smaller as other loss items, so that it can have a more obvious impact on the complete loss while retaining a high self-learning ability."}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "suqJuhjPa-", "original": null, "number": 4, "cdate": 1606099689206, "ddate": null, "tcdate": 1606099689206, "tmdate": 1606114120808, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "U2thcg5eBr", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Reply-part1", "comment": "Comment 1.1:\n1. First sentence of Section 2 could be improved for it to be more meaningful \"Learning from demonstration (LfD) is a class of decision-making methods by learning from demonstration and  imitation learning is a subset of LfD\", additionally LfD is not exactly decision-making methods.\n2. It is stated that BC received extensive attention \"due to its great performance\", I'd say rather \"due to its simplicity\", since it is simply the application of supervised learning, there is extensive literature arguing that it does not always perform well, therefore several other methods have been proposed.\n\n**Feedback 1.1**:\n1. We agree with you that this sentence can be more meaningful and we checked the literature and found that LfD is a class of approaches to coping with different tasks quickly by learning from human or other intelligent methods. It is often used in robot control problems. To focus on the model proposed in the paper, we updated the manuscript in which there is no concept of LfD.\n2. We partly agree with your idea that BC is simple and does not always perform well, but BC also has its own significant advantages, such as fast learning speed, simple and effective utilization of demonstration, and so on. We have updated the original text to make it more accurate. \n\nComment 1.2:\n\"Behavioral cloning is an end-to-end learning method\" is not a right statement.\n\n**Feedback 1.2**:\nThis statement is not precise enough, therefore we delete this sentence.\n\nComment 1.3:\nAuthors mention DAgger that \"... address the shortcoming of limited state-action space covered by demonstration in traditional imitation learning\". This is an indirect result, but actually, it is intended to cope with the covariate shift problem.\n\n**Feedback 1.3**:\nWe agree with your statement that DAgger is proposed to cope with the covariate shift problem. In the Dagger algorithm, experts are required to respond to self-generated states. Thus, totally new transitions are generated and they can expand the state-action space covered by demonstration. \n\nComment 1.4:\nin \"...adversarial learning method and has yielded impressive results...\", impressive could be replaced by an objective word.\n\n**Feedback 1.4**:\nWe have updated the sentence, which becomes \"\u2026 an adversarial learning method and has made efforts on  sparse and dense reward environments\"\n\nComment 1.5:\nRegarding section 4: In \"As the transition quality of historical demonstrations is usually much higher than that associated with stochastic policies...\" stochastic policies are not necessarily something meaningless, I guess the authors meant a completely random policy.\n\n**Feedback 1.5**:\nWe have replaced the \u201cstochastic policies\u201d with \u201crandom policies\u201d.\n\nComment 1.6:\n1. In the line 20 of Algorithm 1. how the \"get the best episode score\" is computed? it is just taken from the final cumulative reward of each episode compared to the previous history? if that's the case how to deal with lucky episodes in which the conditions are simply easier for the agent, and not necessarily that the current policy is indeed better.\n2. It's not so clear why the self-learning process is executed, my interpretation is that authors use it to update the BC policy towards \\pi(s), such that suboptimal behaviors in the demonstrations don't keep pulling the learning policy.\n3. lines 8 and 18 have different notations for the same operation,  authors could use only one of them to be more consistent.\n\n**Feedback 1.6**:\n1. The best episode score is taken from the final cumulative reward of each episode compared to the previous history. To reduce the influence of the lucky episode, we do not update the demonstration in the early stage of the self-learning process. In the experiments, we begin to update the demonstration after the self-learning process has last for 10 episodes. \n2. The reason is indeed the same as your interpretation. We have clarified the significance of the self-learning process. The supervised learning process and self-learning process can promote each other. The supervised model provides a basic reference and the self-learning process overcomes the negative influence on the BC model brought by the suboptimal samples. \n3. Thanks for your advice and we have updated the manuscript.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "JnxnkNCtke", "original": null, "number": 8, "cdate": 1606113178389, "ddate": null, "tcdate": 1606113178389, "tmdate": 1606113281729, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Update the manuscript", "comment": "We thank all the reviewers for their valuable comments and advice. Please find the uploaded revised manuscript. We revised the manuscript according to the reviewers' comments to address concerns and to avoid any unnecessary confusion. All changes in the revised manuscript are highlighted in blue."}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "Lqj0YmxDGh", "original": null, "number": 6, "cdate": 1606100683956, "ddate": null, "tcdate": 1606100683956, "tmdate": 1606112075084, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "tZdEeok0gxC", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Reply", "comment": "Comment 3.1:\nOne thing I do not understand well is the necessity of building the BC model. What if we use some heuristic approaches to replace the replay buffer with higher quality data? Can some simple strategies like this achieve similar performance as building a BC model?\n\n**Feedback 3.1**:\nThank you for the suggestion. Heuristic methods usually contain more prior knowledge, resulting in that the samples selected may be biased. In this paper, the BC model was selected is mainly due to its advantage of the characteristics of fast training of supervised learning, and it can also provide decision-making results which have totally different mechanism from reinforcement learning. We clarify the advantages of BC in the revised manuscript to support our method.\n\nComment 3.2:\nSome obvious typos:\nSection 5 Experimental results: environmets -> environments\nSection 5.3, paragraph 2: combins -> combines\n\n**Feedback 3.2**:\nThanks for your careful review and we have polished the manuscript carefully to make the paper better. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "ipD9la7rH4-", "original": null, "number": 7, "cdate": 1606101118898, "ddate": null, "tcdate": 1606101118898, "tmdate": 1606101118898, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "d275UPzeZcH", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Reply", "comment": "Comment 4.1\n\nI think the idea is nice, but it is hard to follow as written and the experiments are on simple problems. It would be better to compare against more complex benchmarks such as Atari. The writing needs improvement for clarity.\n\n**Feedback 4.1**:\n\nThanks for your advice. We tried our best to compare against more complex benchmarks but the experiments have not finished before we submit the updated manuscripts. And we have polished the English writing of the paper.\n\n\nComment 4.2:\n\n1. The idea of adding new trajectories from high scores was interesting, but I was left wondering why this works. \n2. Won't this add a lot of really bad trajectories in the beginning of learning which seem like they would have a negative impact on the BC policy. \n3. Also, the function \"get the best episode score\" was never rigorously defined.\n4. Equations should be introduced before they are included. Equations 1-4 come before the text that talks about them and this makes the paper more confusing that it needs to be.\n5. Equation (5) seems incomplete. Shouldn't l(\\pi, \\pi_bc, s_t) be a function of the action? Otherwise it is just a constant per state, but it is in the max_a brackets. Also it is unclear what the difference between s_t and s are in Eqn (5).\n6. Average stop episode is not clearly defined in the text. In the appendix it explains more but the thresholds are not justified. \n\n**Feedback 4.2**:\n\n1. Trajectories with high scores own a greater probability of containing some transitions that successfully deal with difficult scenarios, which can not only expand the state-action space covered by the transition samples in the experience replay buffer but also help the supervised learning model provide more effective information to guide the convergence of the model. \n2. We have also noticed this feature and to reduce the negative influence as much as possible, only a small number of samples are added to the demonstration during the early stage of the learning and more volume of samples are generated after the model has gained better performance.\n3. The best episode score is taken from the final cumulative reward of each episode compared to the previous history. To reduce the influence of the lucky episode, we don\u2019t update the demonstration in the early stage of the self-learning process. In the experiments, we begin to update the demonstration after the self-learning process has last for at least 10 episodes.\n4. Thanks for your suggestion, we will improve the layout to make the paper clearer.\n5. $l(\\pi, \\pi_{bc};s_t)$ is indeed a function of action and it determines the loss value based on the difference between $\\pi_{bc}(s)$ and other actions in the action space. We have modified the equation, which looks like $l(a, \\pi_{bc}(s))$, to clarify the meaning. $s_t$ is the same as s and we have made them consistent.\n6. The average stop episode is the average value of the stop episode number among different runs. Because of the randomness in the experiments and environments, the stop episode number (after which the termination condition is satisfied) of different runs varies from each other. The threshold in the termination condition is equal to or higher than the *rThresh*, which is a reward threshold. If the agent gets a higher score than the *rThresh*, the task can be considered as solved."}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "Hlq7-b5bAx", "original": null, "number": 5, "cdate": 1606100588957, "ddate": null, "tcdate": 1606100588957, "tmdate": 1606100588957, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "ElwfW-0Maw", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment", "content": {"title": "Reply", "comment": "Comment 2.1:\n\nThe extensiveness of experiments is underwhelming, both in the diversity and complexity of the tasks chosen. The four tasks are among the most simple ones typically used for comparable algorithms, having very low degrees of freedom and very simple dynamics. For example, DQfD was validated on a broader and more challenging set of Atari tasks, including Montezuma Revenge where (human) demonstrations are critical for a learner agent to tackle the task.\n\nBy introducing a BC-learned helper policy, the approach introduces dependencies on how well this helper policy is (in terms of approximating the demonstrations) in order to provide better loss signals to large-margin loss. It\u2019d be good to design additional experiments to showcase shortcomings, if any, of this proposed approach. For example, the authors used ten thousands transitions as initial demonstrations. It\u2019d be interesting to see how the proposed approach would fare when different amounts of demonstrations are available.\n\n**Feedback 2.1**:\n\nThank you for the advice. We will try our best to test our method on broader and more challenging environments. The influence of the number of demonstrations also deserves more research work.\n\nComment 2.2:\n\nTable 1 is confusing to read. Especially in the top half, \u201cDemo\u201d rewards are juxtaposed with other numbers in completely different units (number of episodes).\n\nThere are some typos. In section 2, it\u2019s not clear what determination is meant in \u201cdetermination of DAGGER\u201d. In Section 1 \u201cdifficult consistent\u201d -> \u201cdifficult to be consistent\u201d.\n\n**Feedback 2.2**:\n\nThanks for your careful review and we have revised the manuscripts to make it clearer."}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hLElJeJKxzY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2859/Authors|ICLR.cc/2021/Conference/Paper2859/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843766, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Comment"}}}, {"id": "tZdEeok0gxC", "original": null, "number": 2, "cdate": 1603357115184, "ddate": null, "tcdate": 1603357115184, "tmdate": 1605023932928, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review", "content": {"title": "Tend to accept", "review": "Summary: \n\nThis paper proposes integrating deep Q-learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC). Compared with DQfD, the proposed approach introduces a behavior cloning model, which was first pre-trained by leveraging historical demonstrations and then updated using generated dynamic demonstration. The BC model is used in the expert loss function, where the DRL model's actions are compared with those obtained from the BC model for policy improvement guidance. The experimental results in OpenAI Gym environments show that the proposed approach adapts well to different demonstrations' imperfection levels and accelerates the learning processes. The ablation study also indicates that the new method improves the learning convergence performance compared with the original DQfD model.\n\n========================\n\nReasons for score:\nOverall, based on the description of the paper, the proposed approach works well.\n\n========================\n\nDetailed comments:\n- In the proposed approach, the BC model is fine-tuned if the model achieves a relative high-performance score. With this mechanism,  high-quality transition samples are included, and it helps avoid adverse impacts caused by imperfect demonstrations.\n- The experiments show that the performance of DQfDD-BC is better than DQfQ, and it works well for imperfect demonstration scenarios.\n- In this paper, I think the primary idea is about how to leverage high-quality data in the generated trajectory well. This paper aims to build a BC model, initialized by demonstration dataset, and later finetuned with newly generated high quality data. One thing I do not understand well is the necessity of building the BC model. What if we use some heuristic approaches to replace the replay buffer with higher quality data? Can some simple strategies like this achieve similar performance as building a BC model?\n\n========================\n\nSome obvious typos:\n- Section 5 Experimental results: environmets -> environments \n-Section 5.3, paragraph 2: combins -> combines", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069217, "tmdate": 1606915784244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2859/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review"}}}, {"id": "ElwfW-0Maw", "original": null, "number": 3, "cdate": 1603826832564, "ddate": null, "tcdate": 1603826832564, "tmdate": 1605023932857, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review", "content": {"title": "Official Blind Review", "review": "Summary\n\nThis paper proposes a new approach (DQfDD-BC) to leverage demonstrations in the framework of deep Q-learning. The new approach augments the previous approach called DQfD by \na) adding self-generated trajectories with high rewards as additional demonstrations (a feature the authors called \u201cdynamic demonstrations\u201d), \nb) replacing large-margin loss with a cross-entropy loss between the learner agent and a helper policy learned by behavior cloning demonstrations. \nThe new approach improves upon the previous by better leveraging demonstrations and being more robust against imperfect demonstrations. The authors demonstrated these improvements in experiments by showing the new approach leads to accelerated training speed (in terms of episodes) when using either perfect or imperfect demonstrations, and higher performance (in terms of rewards) when using imperfect demonstrations. \n\n==============\n\nPositives\n\nThis paper is well motivated. The two intended benefits of the new approach are meaningful and important.\n\nThe experiments back up the authors\u2019 claim on the benefits, especially on the improvement of training speed. In the CartPole tasks, the training speed is improved by 2 folds.\n\nThe paper is written clearly. It was easy to understand the new approach and the differences on top of the previous one. \n\n==============\n\nNegatives\n\nThe extensiveness of experiments is underwhelming, both in the diversity and complexity of the tasks chosen. The four tasks are among the most simple ones typically used for comparable algorithms, having very low degrees of freedom and very simple dynamics. For example, DQfD was validated on a broader and more challenging set of Atari tasks, including Montezuma Revenge where (human) demonstrations are critical for a learner agent to tackle the task.\n\nBy introducing a BC-learned helper policy, the approach introduces dependencies on how well this helper policy is (in terms of approximating the demonstrations) in order to provide better loss signals to large-margin loss. It\u2019d be good to design additional experiments to showcase shortcomings, if any, of this proposed approach. For example, the authors used ten thousands transitions as initial demonstrations. It\u2019d be interesting to see how the proposed approach would fare when different amounts of demonstrations are available.\n\n==============\n\nRecommendation\n\nOverall the approach proposed is interesting and promising. I\u2019d recommend a weak accept. The authors are strongly encouraged to further improve the experiments. \n\n==============\n\nMinor Comments\n\nTable 1 is confusing to read. Especially in the top half, \u201cDemo\u201d rewards are juxtaposed with other numbers in a completely different units (number of episodes).\n\nThere are some typos. In section 2, it\u2019s not clear what determination is meant in \u201cdetermination of DAGGER\u201d. In Section 1 \u201cdifficult consistent\u201d -> \u201cdifficult to be consistent\u201d. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069217, "tmdate": 1606915784244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2859/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review"}}}, {"id": "U2thcg5eBr", "original": null, "number": 4, "cdate": 1603909253575, "ddate": null, "tcdate": 1603909253575, "tmdate": 1605023932773, "tddate": null, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "invitation": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review", "content": {"title": " ", "review": "This paper is introducing a learning method which combines both Imitation Learning and Reinforcement Learning, such that an autonomous learner can leverage prerecorded expert knowledge. In comparison to previous work, this model has an expert cost function which gives priority to the expert behavior, not only using the expert demos (like in DQfD), but also with a model trained with those demos using  behavioral cloning, such that it could be evaluated in states that were not visited during the demonstrations. Additionally, new executions of the learner that have high performance are included in the buffer for training the policy imitating the expert, since they can also be considered new better demonstrations.\n\nThe paper presents an interesting idea with potential, although there are some aspects to consider mainly about the presentation of the paper, in order to improve its content, as listed below. However, the aspects that are not clear in the current version of the paper need to be fixed before considering acceptance, since there are clear open questions.\n\nSection 2 Related work needs some improvement:\n- First sentence of Section 2 could be improved for it to be more meaningful \"Learning from demonstration (LfD) is a class of decision-making methods by learning from demonstration and \u00a0imitation \u00a0learning is a subset of LfD\", additionally LfD are not exactly decision-making methods.\n- It is stated that BC received extensive attention \"due to its great performance\", I'd say rather \"due to its simplicity\", since it is simply the application of supervised learning, there is extensive literature arguing that it does not always perform well, therefore several other methods have been proposed.\n- \"Behavioral cloning is an end-to-end learning method\" is not a right statement.\n- Authors mention about DAgger that \"... address the shortcoming of limited state-action space covered by demonstration in traditional imitation learning\". This is an indirect result, but actually it is intended to cope with the covariate shift problem.\n- in \"...adversarial learning method and has yielded impressive results...\", impressive could be replaced by an objective word.\n\nRegarding section 4:- In \"As the transition quality of historical demonstrations is usually much higher than that associated with stochastic policies...\" stochastic policies are not necessarily something meaningless, I guess the authors meant a completely random policy.\n- In the line 20 of Algorithm 1. how the \"get the best episode score\" is computed? it is just taken from the final cumulative reward of each episode compared to the previous history? if that's the case how to deal with lucky episodes in which the conditions are simply easier for the agent, and not necessarily that the current policy is indeed better.\n- It's not so clear why the self learning process is executed, my interpretation is that authors use it to update the BC policy towards \\pi(s), such that suboptimal behaviors in the demonstrations don't keep pulling the learning policy.\n- lines 8 and 18 have different notations for the same operation,\u00a0 authors could use only one of them to be more consistent.\nRegarding the experiments section- There are missing details of the experiments that do not allow for reproducing the procedure, how many repetitions were run for each case in Fig 2 and 3?, and also for the ablation study? It is not clear whether the results of Fig 2 and 3 are obtained from the same experiments that gave the results in Table 1, at least they do not seem to match. Why testing with the 2 variants of the CartPole environment instead of another kind of problem? since the difference between them is very simple\n\n- Why is there no data in Table 1 for the cases of imperfect demonstrations for the CartPole-V0 environment?\n- Why the learning curves of DDQN for the CartPole-V0 and CartPole-V1 cases are not similar at least during the first 200 episodes, looking at the mean and variance, both environments are not close to reach the maximum time steps, therefore in that region they should behave exactly the same.\n- In Table 1, it is not so clear what average stop episode means, if it is the episode number in which the stop condition was reached, why are there negative numbers for the acrobot? if they are rewards obtained in the last episode, why are rewards higher than the possible to obtain in CartPole-V0.\n- It is mentioned\u00a0 \"...but the DDQN and DQfD fail to meet the experimental termination condition with imperfect demonstration\", However this is completely wrong, DDQN do not use demonstrations.\n\n- It would be interesting to additionally mention how to define the size of the margin for the expert cost function, because this may depend on the kind of reward functions/range of value functions.\n \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2859/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2859/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Q Learning from Dynamic Demonstration with Behavioral Cloning", "authorids": ["~Xiaoshuang_Li1", "junchen@kth.se", "x.wang@ia.ac.cn", "~Fei-Yue_Wang2"], "authors": ["Xiaoshuang Li", "Junchen Jin", "Xiao Wang", "Fei-Yue Wang"], "keywords": [], "abstract": "    Although Deep Reinforcement Learning (DRL) has proven its capability to learn optimal policies by directly interacting with simulation environments, how to combine DRL with supervised learning and leverage additional knowledge to assist the DRL agent effectively still remains difficult. This study proposes a novel approach integrating deep Q learning from dynamic demonstrations with a behavioral cloning model (DQfDD-BC), which includes a supervised learning technique of instructing a DRL model to enhance its performance. Specifically, the DQfDD-BC model leverages historical demonstrations to pre-train a supervised BC model and consistently update it by learning the dynamically updated demonstrations. Then the DQfDD-BC model manages the sample complexity by exploiting both the historical and generated demonstrations. An expert loss function is designed to compare actions generated by the DRL model with those obtained from the BC model to provide advantageous guidance for policy improvements. Experimental results in several OpenAI Gym environments show that the proposed approach adapts to different performance levels of demonstrations, and meanwhile, accelerates the learning processes. As illustrated in an ablation study, the dynamic demonstration and expert loss mechanisms with the utilization of a BC model contribute to improving the learning convergence performance compared with the origin DQfD model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|deep_q_learning_from_dynamic_demonstration_with_behavioral_cloning", "pdf": "/pdf/1f61489d65f2e690f6362b0e31d5c6b732b5225d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=U2Ygl1NSRA", "_bibtex": "@misc{\nli2021deep,\ntitle={Deep Q Learning from Dynamic Demonstration with Behavioral Cloning},\nauthor={Xiaoshuang Li and Junchen Jin and Xiao Wang and Fei-Yue Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=hLElJeJKxzY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hLElJeJKxzY", "replyto": "hLElJeJKxzY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2859/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069217, "tmdate": 1606915784244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2859/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2859/-/Official_Review"}}}], "count": 13}