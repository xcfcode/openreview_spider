{"notes": [{"id": "rJgD2ySFDr", "original": "B1eoabkYDH", "number": 1956, "cdate": 1569439663465, "ddate": null, "tcdate": 1569439663465, "tmdate": 1577168224831, "tddate": null, "forum": "rJgD2ySFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AuYsZoMBV", "original": null, "number": 1, "cdate": 1576798736811, "ddate": null, "tcdate": 1576798736811, "tmdate": 1576800899533, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Decision", "content": {"decision": "Reject", "comment": "There was some support for this paper, but it was on the borderline and significant concerns were raised. It did not compare to the exiting related literature on communications, compression, and coding. There were significant issues with clarity.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725908, "tmdate": 1576800277912, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Decision"}}}, {"id": "H1gA7XOEhS", "original": null, "number": 3, "cdate": 1574368037648, "ddate": null, "tcdate": 1574368037648, "tmdate": 1574368037648, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "\nPaper Summary:\n\nThe paper proposes to use ML methods, specifically neural networks, to learn source and/or channel coding systems, either jointly or separately. Specifically, they investigate these systems under the bandwidth-limited channel.  They investigate their models applied to the task of transferring images across the channel.\n\nPros:\n\n- The paper is in a difficult area, and needs to communicate ideas about communications and information as well as various deep learning models. The initial exposition does this well.\n\n- The paper discusses ML-based communications models applied to the bandwidth limited channel and performs experiments to investigate joint vs. separate coding. \n\nTwo main things of concern:\n\n- This area (communications, compression, coding) is highly developed, yet there are no comparisons to practical, state of the art techniques in this paper. The experiments investigate using the authors' models to send images across a noisy channel. So shouldn't we see a comparison to an equivalent non-ML-based system that currently accomplishes this task, if only for perspective? \n\n- The paper contains too much exposition and the structure makes it difficult to read.  The authors' work seems to mostly be focused in sections 4 and 6, but prior work is summarized in section 5.  The authors want to communicate a lot of ideas which the audience might not be familiar with, and this is a difficult task. However, the paper could be reorganized such that the ideas are clearer. Rate distortion is delegated to an appendix, but two pages are spent on basic communications. Overall it could be much more focused. \n\nSome smaller concerns:\n\n- Pg. 3 \"hypothesis spaces can be searched increasingly quickly in an automated fashion, allowing researchers to search...\" \nI think this statement may be over-reaching. The authors also frequently use the term \"flexible function approximation\". Neural networks do have interesting approximation properties. But this is not a complete picture of deep learning, and the situation is much more nuanced than this. Where is the role of optimization and data? In order to transmit data with the author's algorithms, do we need to go out and collect a large body of examples in the specific domain, like CelebA? Because you don't need to do that to e.g. compress any image with JPEG, code the data using LDPC to send across a channel. Is this comparing apples and oranges?\n\n- Pg. 6 \"Note, that a simple additive white Gaussian noise.... LDPC. However, in more general scenarios they do not perform as well and can be beaten by neural network architectures.....\"\n\nI think the claims in this paragraph need to be toned down a bit. LDPC does not perform well in regards to what? Can be beaten under what conditions? Decode efficiently with regard to what block length? I don't think the picture is as clear as painted here.\n\n- Pg. 4 The statements here regarding the bandwidth-limited channel appear to be the focus of the author's work. This section should be expanded and explained. Reading the first paragraph then going to the second paragraph (\"To summarize...\"), there's sort of a disconnect. How do we know these two things are equivalent? What exactly is novel that was introduced?\n\nSmall typos:\n- Pg. 2 final paragraph, some  norm is used for the distortion, but this is not defined (either on pg. 2 or in App. B)\n- Pg. 6 white is misspelled \"withe\"\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575870802298, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Reviewers"], "noninvitees": [], "tcdate": 1570237729851, "tmdate": 1575870802312, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review"}}}, {"id": "r1g4BlWssB", "original": null, "number": 3, "cdate": 1573748795694, "ddate": null, "tcdate": 1573748795694, "tmdate": 1573748795694, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "B1g4UQY3Kr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment", "content": {"title": "Personal response  AnonReviewer3", "comment": "Thank you for your review. In the following, we would like to address both of your questions.\n\nQuestion 1: Why does joint coding outperform separate coding specifically in the ML setting?\n\nThis is indeed very interesting and has sparked various discussions among ourselves as well. \nThere is of course classical research that illuminates why solving the joint problem may be possible in some scenarios where solving the communication problem separately is not possible (under more realistic assumptions). \n\nMore specifically, in the ML context when separate coding is performed, it is understood that the channel coder receives the distribution of source embeddings. However, to be source agnostic, this distribution needs to be a generic (i.e. source data independent) distribution. For example, this could be a standard Gaussian as is used in a basic VAE. This, however, induces a bottleneck: The source coder needs to match the aforementioned generic prior distribution such that the channel coder receives the correct input. At the same time, if the source coder was to perfectly match this prior, there would be no mutual information I(source data; source embedding), and thus nothing would be learned. Joint coding does not suffer from this trade-off problem. We believe this is why it outperforms separate coding in our experiments.\n\nQuestions 2a:  Relevance of the bandwidth-limited channel.\n\nWe hope in our main rebuttal we could point out why modelling with the bandwidth limited channel has such a central role in modelling communication, and why introducing learning to coding is a relevant contribution. \n\nQuestion 2b: How does our approach extend to other domains (non-vision)? \n\nConcerning the dataset we used, we are currently running an experiment on imagenet to diversify our claim. \nWe do believe, however, that our findings extend far beyond image datasets to video, language, audio and even beyond perceptual tasks. We believe that there is evidence that the architecture of the neural encoders and decoders that are employed will determine the success in these domain. Domain specific architectures are, however, not the focus of this work. Farsad et al. (2018) for example considers a language application.\n\n\nWe thank the reviewer for any further feedback and are happy to discuss more if desired. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgD2ySFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1956/Authors|ICLR.cc/2020/Conference/Paper1956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148422, "tmdate": 1576860557305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment"}}}, {"id": "BJgDWQejsS", "original": null, "number": 1, "cdate": 1573745407213, "ddate": null, "tcdate": 1573745407213, "tmdate": 1573747744988, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment", "content": {"title": "Main rebuttal (1)", "comment": "We would like to thank the reviewers for their review and start the discussion of our paper.\nIn the main rebuttal we would like to clarify novelty, the connection to other work, and why bandwidth-limited information transfer is a relevant problem. \n\n# NOVELTY\n\nIn our paper, we assess the research question: How can we best model communication when the level of information transfer varies? This classic problem of information theory is highly relevant in many communication settings, as elaborated in the next section.\n\nSpecifically, in this work we attempt to understand what the best modelling choices are for this problem when using flexible function approximators to encode and decode messages. We make three fundamental observations about the model class that should be used.\n\nFor best message reconstruction, we observe:\n  1. Bandwidth-limited communication should be modelled as a joint coding problem. \n  2. Flexible approximate prior distributions should be provided for decoders to marginalize over possible message encodings.\n\nWhen sampling from the communication model (e.g. when there is little transmitted information), we observe:\n  3. Directly modelling the marginalization over missing information with an auxiliary latent variable decoder results in more reasonable (in distribution) decoded samples.\n\nThis assessment itself, and the resulting observations, are novel work to the best of our knowledge. Additionally, we propose a novel model for the bandwidth-limited channel (BWLC),as well as the novel concept of the auxiliary latent variable decoder.\n\n# OTHER WORK\n\nOur work is distinguished from other work in that it is the first to investigate the bandwidth-limited channel with the tools of machine learning.\nRelated work in this field focuses on the assessment of joint coding vs. separate coding (see last paragraph section 5) and channel coding (2nd paragraph section 5) with learned function approximators. \n\n\n# APPLICATIONS\n\nGeneral\nThe BWLC model is applicable to many communication problems.\nTypical examples involve communication over radio, telephone lines or WiFi. All three can be modelled as a BWLC with white noise. By extension, any radio signal, phone conversation, or internet traffic, such as video streaming, can be seen as a relevant application. \n\nReinforcement learning\nIt is also possible to integrate communication algorithms such as those proposed here into multi-agent reinforcement learning systems to emulate more realistic communication between agents. \n\nRepresentation learning\nAdditionally, the bandwidth-limited channel orders the latent representation according to its importance for reconstruction. Thus, our approach could be useful for representation learning by allowing for straightforward dimensionality reduction. There may be a connection to other dimensionality reduction methods such as PCA that could be explored in follow up work.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgD2ySFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1956/Authors|ICLR.cc/2020/Conference/Paper1956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148422, "tmdate": 1576860557305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment"}}}, {"id": "r1ginQgssH", "original": null, "number": 2, "cdate": 1573745586992, "ddate": null, "tcdate": 1573745586992, "tmdate": 1573745586992, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "ryeWu1pp9S", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment", "content": {"title": "Personal response to AnonReviewer2", "comment": "Thank you for starting this discussion, I hope we could clarify the main points of novelty in the paper and draw the connection to existing work sufficiently in our main rebuttal (1)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Program_Chairs", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgD2ySFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1956/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1956/Authors|ICLR.cc/2020/Conference/Paper1956/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148422, "tmdate": 1576860557305, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Authors", "ICLR.cc/2020/Conference/Paper1956/Reviewers", "ICLR.cc/2020/Conference/Paper1956/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Comment"}}}, {"id": "B1g4UQY3Kr", "original": null, "number": 1, "cdate": 1571750732419, "ddate": null, "tcdate": 1571750732419, "tmdate": 1572972401848, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on transmitting messages reliably by learning joint coding with the bandwidth-limited channel. The authors justify joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators. Their experiments show the advantage of their design decisions via improved distoration and FID scores.\n\nPros:\n\n1. This paper is clearly written and well-structured in logic. For example, the authors use Figures 1 and 2 assist readers to catch the difference between joint communication system and separate communication system.\n\n2. This paper gives a reliazation of joint source-channel coding, especially to give auxilary latent variable decoders.\n\n3. This paper has been verified in both Gaussian channel and bandwidth-limited channel. The empirical results show the advantage of joint coding.\n\nCons:\n\n1. Intuitively, you Section 4.3 should be better than Section 4.2. However, I don't see any difference or major items to justify this kind of benefits. Could you please explain why techniques in Section 4.3 can outperform these in Section 4.2.\n\n2. Although the authors verified their work on CelebA, it seems that the proposed method has very limited applications. If possible, the authors should do more datasets to verify their proposed method, which will be more useful to boarder readers."}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575870802298, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Reviewers"], "noninvitees": [], "tcdate": 1570237729851, "tmdate": 1575870802312, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review"}}}, {"id": "ryeWu1pp9S", "original": null, "number": 2, "cdate": 1572880233187, "ddate": null, "tcdate": 1572880233187, "tmdate": 1572972401813, "tddate": null, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "invitation": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper is out of my research area. I could understand that the paper studies the message transformation with bandwidth-limited channels. It seems naturally the message transformation could be represented as a autoencoder model. The paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example VAE, etc. Technically, what's new of this paper? Was it the auxiliary variable decoders? Is it that this class of algorithms/models firstly applied to this problem domain? To be honest the paper mentioned most of the terminologies in ML and seems that the paper wanted to connect to them, for example, ELBO, VAE, GAN, re-parameterization, etc. The paper provides experimental results on the designed model for bandwidth-limited channel."}, "signatures": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1956/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["mail.karen.ullrich@gmail.com", "fviola@google.com", "danilor@google.com"], "title": "Neural Communication Systems with Bandwidth-limited Channel", "authors": ["Karen Ullrich", "Fabio Viola", "Danilo J. Rezende"], "pdf": "/pdf/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "TL;DR": "We learn neural joint coding with bandwidth-limited channel models. ", "abstract": "Reliably transmitting messages despite information loss due to a noisy channel is a core problem of information theory.  One of the most important aspects of real world communication is that it may happen at varying levels of information transfer. The bandwidth-limited channel models this phenomenon.  In this study we consider learning joint coding with the bandwidth-limited channel.  Although, classical results suggest that it is asymptotically optimal to separate the sub-tasks of compression (source coding) and error correction (channel coding), it is well known that for finite block-length problems, and when there are restrictions to the computational complexity of coding, this optimality may not be achieved. Thus, we empirically compare the performance of joint and separate systems, and conclude that joint systems outperform their separate counterparts when coding is performed by flexible learnable function approximators such as neural networks.  Specifically, we cast the joint communication problem as a variational learning problem. To facilitate this, we introduce a differentiable and computationally efficient version of this channel.  We show that our design compensates for the loss of information by two mechanisms: (i) missing information is modelled by a prior model incorporated in the channel model, and (ii) sampling from the joint model is improved by auxiliary latent variables in the decoder. Experimental results justify the validity of our design decisions through improved distortion and FID scores.", "code": "https://www.dropbox.com/s/tnznqx4u80cpjr6/iclr2020_code.zip?dl=0", "keywords": ["variational inference", "joint coding", "bandwidth-limited channel", "deep learning", "representation learning", "compression"], "paperhash": "ullrich|neural_communication_systems_with_bandwidthlimited_channel", "original_pdf": "/attachment/0a2ba3450223349e26969cced649b9ac4f4cdb8b.pdf", "_bibtex": "@misc{\nullrich2020neural,\ntitle={Neural Communication Systems with Bandwidth-limited Channel},\nauthor={Karen Ullrich and Fabio Viola and Danilo J. Rezende},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgD2ySFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgD2ySFDr", "replyto": "rJgD2ySFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1956/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575870802298, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1956/Reviewers"], "noninvitees": [], "tcdate": 1570237729851, "tmdate": 1575870802312, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1956/-/Official_Review"}}}], "count": 8}