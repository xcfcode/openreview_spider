{"notes": [{"id": "OEgDatKuz2O", "original": "qHf9U3j5qE", "number": 1200, "cdate": 1601308134531, "ddate": null, "tcdate": 1601308134531, "tmdate": 1614985630187, "tddate": null, "forum": "OEgDatKuz2O", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yvpkM6pA4BZ", "original": null, "number": 1, "cdate": 1610040530288, "ddate": null, "tcdate": 1610040530288, "tmdate": 1610474139753, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes an EM type of approach for domain adaptation under covariate shift. The approach well motivated and developed and experimentally evaluated on synthetic data.\n\nPro:\n- The EM type of framework is simple and natural and  promising direction for DA, which should be explored and analyzed further.\n\nCon:\n- The presentation is highly overselling the results. Both in terms of the generality of the findings and in terms references to privacy preserving properties. Both would need a solid formal analysis which this submission does not provide.\n- Several reviewers have stated that, while the authors promised updates to their manuscript during the author response phase, no such updated submission has been made.\n- The work bases their approach by referring to a well known theoretical DA bound by Ben-David et al (2010). The theorem is not stated correctly. The most important component in that work is to restrict the models to a class of bounded capacity.\n- The claim of the authors of \"solving the problem\" under covariate shift are overstated. It is reasonable to expect that the authors provide a more thorough analysis of the limitations or their approach, that is, clearly state the conditions under which it would succeed and fail. Below are some references on lower bounds of DA under covariate shift.\n- Given that the theoretical analysis is limited, a more thorough experimental exploration would be expected.\n\nRefs on difficulty of DA learning under covariate shift and bounded d_H distance:\n\nShai Ben-David, Ruth Urner:\nOn the Hardness of Domain Adaptation and the Utility of Unlabeled Target Samples. ALT 2012: 139-153\n\nShai Ben-David, Tyler Lu, Teresa Luu, D\u00e1vid P\u00e1l:\nImpossibility Theorems for Domain Adaptation. AISTATS 2010: 129-136 "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040530275, "tmdate": 1610474139738, "id": "ICLR.cc/2021/Conference/Paper1200/-/Decision"}}}, {"id": "tQzg5ouEnKX", "original": null, "number": 1, "cdate": 1603698080213, "ddate": null, "tcdate": 1603698080213, "tmdate": 1606792644876, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review", "content": {"title": "Official Blind Review2", "review": "In this paper, the authors propose generative domain adaptation approach called EMTL. The key idea is to model a mediator distribution which can approximate the true target joint distribution. Specifically, the authors apply an E-M strategy to infer the model parameters. Experimental studies are done on both synthetic and real-world datasets. \n\nThe paper is well-organized and easy to follow. My major concern is on the significance of the paper, which is not significantly novel especially compared with the recent DNN-based domain adaptation methods. Moreover, there are some technical flaws, which need to be further clarified. Here are the detailed comments:\n\n(1)\tOne motivation of the paper is the sensitivity of the source data. Due to security or privacy issues, source data may not be accessible. While it is a practical and nice point of motivation, the paper misses one important research line on federated learning that is specially proposed for privacy issues. It is necessary to discuss with some federated learning related works. \n\n(2)\tRegarding the access of the source data, the proposed method still requires the source data to do source density estimation. In this sense, I am not convinced by the claim on the privacy preservation. From Algorithm 1, it can be clearly seen that D^s is still used for the initialization of \\theta_s. \n\n(3)\tThe authors highlight no access of source data in the adaptation phase. Could you elaborate on what is the adaptation phase? Based on my understanding, the whole Algorithm 1 is for adaptation, but it still needs source data. Is it better to claim that the proposed method only requires source model parameters trained previously? \n\n(4)\tThe related work section can be further improved, by discussing more on both subspace-based and deep-learning based domain adaptation methods. \n\n(5)\tThe key idea is based on Theorem 1, and aims to build a mediator distribution to approximate the target joint distribution. Most of existing domain adaptation methods share the same idea although they are not generative models, please highlight the main advantage of proposed generative model over existing subspace-based and deep-learning based methods. \n\n(6)\tRegarding Eq. (5), why \\theta_s is used in the subscription of \\mathbb{E}? It should be \\thetha_m^(0), right? Moreover, how to obtain y_i for each target data point? \n\n(7)\tFor Eq. (5), does it only hold for the first iteration where the source parameters are used as the initialization? For the following iterations, are you still using p^s(y_i = j | x_i^t)? or using p^m(y|x)? If the latter is used, does it mean y_i is updated in each iteration? \n\n(8)\tThe proposed synthetic dataset is very na\u00efve. It is more convincing to test on more complex datasets with higher dimensionality data.  For the real-world datasets, there are a lot of benchmark datasets for domain adaptation, e.g., office 31, office-caltech 10, and office-home etc. It is more convincing to test on these well-known datasets. More importantly, please compare with more state-of-the-art baselines, on both subspace-based and deep learning based. Even on the reported datasets, the improvements of EMTL over SA and DANN (these 2 are not state-of-the-art) are marginal. \n\nUpdate: Thanks for the authors' response. After reading the response and the other reviewers' comments, I think the paper needs to be further improved, and thus I will keep my score.\n\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124311, "tmdate": 1606915808533, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1200/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review"}}}, {"id": "w7zf12ZF9vZ", "original": null, "number": 4, "cdate": 1603958116965, "ddate": null, "tcdate": 1603958116965, "tmdate": 1606698757441, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review", "content": {"title": "Nice idea but privacy properties, conceptual intuition, and relation to other methods, is unclear", "review": "Update after author response: I appreciate the clarifications, but given the lack of comparisons or discussion to related prior methods (at least Liang et al 2020 or some alternative equivalent), I cannot recommend acceptance at this point. The authors did not submit a paper revision as well. I think the idea seems promising, so don't take this as a critique of the research direction.\n\n#########################################################################\n\nSummary:\n\nThis paper tackles the problem of unsupervised domain adaptation, where there may be privacy constraints on the source, so we have access to a source model but not the source data. Their approach first learns a generative model p(x, y | theta_s) for the source. They use this to pseudolabel every target example p(y^t_i | x^t_i, theta_s), and then fit a new generative model to these examples. They iterate this process multiple times to get their final model p(x, y | theta), from which they can predict p(y | x, theta).\n\n#########################################################################\n\nReasons for score:\n\nI think this is a cool idea, but the paper in its current form seems incomplete. One of the goals is to preserve privacy of the source dataset, but it seems unlikely that a generative model p(x, y | theta_s) learned on the source preserves privacy. In particular, we could effectively \u201cgenerate\u201d the source and get access to private information. Is there any prior work that suggests that this is not the case? This seems to be a key point, since barring privacy concerns their method doesn\u2019t seem to do better than baselines. The conceptual explanation for the method seems unclear and incomplete (more below). The method could also be better tied into /compared with the existing privacy preserving domain adaptation literature, and the self-training literature. Overall, this is a promising and interesting idea, and with more work would be good to publish.\n\n#########################################################################\n\nPros:\n\n- I like the idea of using a generative model for x and y, and repeatedly applying that by pseudolabeling the target and self-training.\n\n- Their synthetic experiments show some promise, and on some of the UCI datasets they seem to do well.\n\n\n#########################################################################\n\nCons:\n\n- No explanation given for why a generative model p(x, y | theta_s) learned on the source preserves privacy, when having a generative model would allow us to generate samples and potentially get sensitive information. This seems to be a key point of the paper, since experimental results are not better than baselines, but the argument is that those don\u2019t preserve privacy.\n\n- There are many other related alternatives. One prominent alternative is self-training or pseudolabeling based approaches, which train a classifier on the source, and use it to pseudolabel the target, then training a regularized classifier on the pseudolabeled data. This, and related methods such as entropy minimization, has been used in the context of domain adaptation (Shu et al, Kumar et al), also in the context of privacy preserving domain adaptation (Liang et al). These methods are highly scalable as well, and all the above papers use multi-layer neural networks and high dimensional datasets. Would be good to compare with at least some existing baseline.\n\n- The conceptual / mathematical explanation seems unclear and incomplete. If there is only covariate shift, then P_m and P_t should be identical. This is because P_t and P_m have the same distribution over x. P_m has the same y | x distribution as P_s, but in covariate shift this is the same as P_t. So it\u2019s unclear what P_m is doing. Section 5.1 argues that the EM training objective enforces P_m to have similar x distribution to P_t and similar y | x distribution to P_s. However, there isn\u2019t a proper explanation of why this objective is a good thing. There seems to be an attempt to explain this in Section 4.2, but in the case of no covariate shift, the Bayes classifier on the source = Bayes classifier on the target, so it suffices to simply train a model on the source.\n\n#########################################################################\n\nQuestions and things to improve:\n\nPlease address questions and comments listed in cons.\n\n#########################################################################\n\nReferences mentioned:\n\nA DIRT-T Approach to Unsupervised Domain Adaptation. Rui Shu, Hung H. Bui, Hirokazu Narui, Stefano Ermon. ICLR 2018.\n\nUnderstanding Self-Training for Gradual Domain Adaptation. Ananya Kumar, Tengyu Ma, Percy Liang. ICML 2020.\n\nDo We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. Jian Liang, Dapeng Hu, Jiashi Feng. ICML 2020.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124311, "tmdate": 1606915808533, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1200/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review"}}}, {"id": "97XgRYd7qC", "original": null, "number": 2, "cdate": 1603814651229, "ddate": null, "tcdate": 1603814651229, "tmdate": 1606668247004, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review", "content": {"title": "An interesting motivation, but more work would be needed.", "review": "This paper proposes a novel method for Unsupervised Domain Adaptation (UDA) when the source domain's privacy should be preserved. The authors propose EMTL, which is a generative method using multivariate densities using RNADE (Uria et al., 2013) and a mediator joint density function bridging both source and target domains. EMTL achieves comparable performances to those of DANN (Ganin et al., 2016) on a single dataset.\n\n**Pros**\n\n- Unique motivation for UDA and privacy-preserving.\n- Well formulated method using RNADE and a mediator density function. In the adaptation phase, the source domain data can be deleted.\n\n**Cons**\n\n- There is a closely related paper for privacy-preserving UDA (Song et al., 2020) before the deadline of ICLR 2021. The method by Song et al. utilized a framework of federated learning and encryption. Thus the approaches are different from each other, but the motivation for privacy-preserving is close. The authors should compare them quantitatively.\n\nSong et al. Privacy-Preserving Unsupervised Domain Adaptation in Federated Setting. IEEE Access, Vol. 8, pp.143233-143240, 2020.\n\n- Although the adaptation phase does not require the source domain data, a probabilistic function $p^m(y|x)$ should be available. The reviewer just concerns if model inversion attacks, such as (Fredrikson et al., 2015), violate the source domain's privacy.\n\nFredrikson et al. Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures. In CCS, 2015.\n\n- It is reasonable to compare ETML to DANN since both methods have conceptually similar characteristics: matching the data distribution and learning the posterior probabilities of the label given a sample. However, as the authors referred to in the main text, several methods have similar characteristics and much better performance than DANN. It is good to know if ETML is complementary to those methods through further experiments.\n- Experiments on a single real dataset are difficult to convince about the generality of UDA performance. Additional experiments on other datasets such as visual ones can strengthen the generality.\n\n**Overall rating**\n\nThe reviewer is leaning toward rejection, although the motivation is clear. The rating can be upgraded if the authors can solve the cons above.\n\n**Additional comment after rebuttal**\n\nHappy to hear that the authors plan to upgrade their draft. Since the submitted paper is not updated, the reviewer keeps the first rating but also looks forward to read a revised version in another conference or journal.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124311, "tmdate": 1606915808533, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1200/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review"}}}, {"id": "hSmZ-PufiD", "original": null, "number": 3, "cdate": 1603890571462, "ddate": null, "tcdate": 1603890571462, "tmdate": 1606312889805, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review", "content": {"title": "Review #1", "review": "--------Updates after rebuttal-----------\n\nSince the author did not propose an updated paper and new experiments. I keep my original score.\n\n---------------------------------------------------\n\n\n\nSummary:\nThis paper proposed a generative domain adaptation (DA) approach under covariate shift. Different from previous domain discriminator methods, this paper introduced a mediator distribution and adopted an autoregressive approach (RNADE) to estimate the distribution density. Empirical results on simple datasets (UCI and Amazon) verified its practical benefits.\n\n------------------------------------------------------\nOverall review \n\nPros:\n\n[1] As far as I know, this is the first paper that used the autoregressive approach in DA. \n\n[2] The proposed adaptation algorithm does not require accessing the source data at the adaptation phase, which has some practical potential.\n\n[3] The high-level idea seems logical and correct (But some technical details seem problematic.)\n\nCons:\n\n[1] The motivation of the proposed approach is unclear: it seems a simple plug-in approach with RNADE in DA. A thorough analysis is lacking.\n\n[2] The empirical significance of the paper is rather limited: the paper did not effectively show its practical utility.\n\n[3]  Some technical details are difficult to follow or flawful. \n\nBased on these reasons, I recommend rejection.\n\n--------------------------------------------------\nDetailed explanations\n\n[1] Motivation\n\nI am rather confused about the motivation of the proposed approach. As for the generative model, the particular reason to use RNADE is unclear. Is the discriminator unable to solve the source-target separation issue? An alternative approach is to train a model on source only and apply the unlabelled target for fine-tuning. (see recent paper [1]). Discussion on the benefits of these settings is highly expected. \n\n[2] Experiments\n\nSince it is an empirical paper, I am most concerned about the empirical results.\n\n[a] The current results are rather limited. The author only evaluated on UCI and Amazon review dataset. Both are simple datasets and linear models can achieve good results.\n\n[b] The compared baselines are NOT SOTA. DANN is the standard baseline.\n\n[3] Technical details\n\n[a] The RNADE is a high time complexity approach for high dimensional data. I would like to see an empirical and theoretical discussion on the high-dimensional dataset (such as the image)\n\n[b] The notation in the EM algorithm is rather confusing and difficult to follow. \nBesides, Eq (7) is the log-MLE approach, then it can be naturally decomposed in three terms. Eq (9) is not correct, $p(x)$ should be a continuous function (not discrete). Using the empirical counterpart to estimate the KL divergence is problematic in the high dimensional dataset. \n\n[c] Sec 5.3 \u201cAs we will show in Section 6, by setting a large $\\eta$ and doing more iterations, EMTL will reduce the weight on the Q function and allow us to escape from covariate shift constraints\u201d. This discovery is really important and interesting. I think it deserves a better justification.\n\n--------------------------------------------\nSuggestions \n\nI suggest extensive empirical results for showing the effectiveness of the proposed approach.\n\n\n[1] Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. ICML 2020\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OEgDatKuz2O", "replyto": "OEgDatKuz2O", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124311, "tmdate": 1606915808533, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1200/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Review"}}}, {"id": "72d16qiwr4", "original": null, "number": 7, "cdate": 1606238571597, "ddate": null, "tcdate": 1606238571597, "tmdate": 1606238571597, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "w7zf12ZF9vZ", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment", "content": {"title": "Answer to reviewer #4", "comment": "- We apology for having put too much focus on privacy and thank you for pointing out the logical flaws. We updated the abstract. As kindly mentioned by other reviewer, our goal is the same as the one in [1], i.e., \"tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems.\"  As a side effect of not accessing the source data during the adaptation phase, we may have put some light on privacy theory, misleading the reader on the general direction of this research. Privacy is not our prime focus in this paper, which remains DA. We rewrote part of the introduction in order to make this really clear.\n\n- We thank you for the recommending these papers. Although, their relatedness to our work, we unfortunately missed them during our paper survey phase.  We  have extended our related work to include them. We are also working on adding these SOTA algorithms in our experiments and conducting a more comprehensive comparison: 1) we are adding these algorithms on UCI and amazon experiments. 2) image experiments listed in these papers are added as well. We will update the experimental results as soon as possible. \n\n- We thank you for pointing out the unclear and incomplete part in mathematical explanation. We have modified and re-arranged the paper so that $p^m$ and also the EMTL concept are much more clear.  $p^m(x,y)$ aims to minimize $E_{x \\sim p^t(x)} D_{KL}(p^s(y|x), p(y|x;\\theta_m))$ and maximize  $E_{x \\sim p^t(x)} \\log p(x;\\theta_m)$ (same to minimize $D_{KL}(p^t(x),p^m(x))$). Thus, in the paper we mentioned that  $p^m(x,y)$ bridges the gap between source and target. The hyper parameter $\\eta$ serves to balance  $E_{x \\sim p^t(x)} D_{KL}(p^s(y|x), p(y|x;\\theta_m))$ and $E_{x \\sim p^t(x)} \\log p(x;\\theta_m)$ and actually its role is similar to $\\lambda$ in DANN (i.e., the hyper parameter controlling the domain regularizer [3]). In the case where $\\eta=0$, EMTL is exactly a source classifier. This will be formally proved in the updated paper. This $\\eta$ parameter leverages the trade-off between those two terms and hence between two different regimes.\n\n### Reference\n[1]: Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. Jian Liang, Dapeng Hu, Jiashi Feng. ICML 2020.\n\n[2] Ben-David, Shai, et al. \"A theory of learning from different domains.\" Machine learning 79.1-2 (2010): 151-175.\n\n[3] Ganin, Yaroslav, et al. \"Domain-adversarial training of neural networks.\" The Journal of Machine Learning Research 17.1 (2016): 2096-2030.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OEgDatKuz2O", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1200/Authors|ICLR.cc/2021/Conference/Paper1200/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862485, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment"}}}, {"id": "qUS3g9msTbZ", "original": null, "number": 4, "cdate": 1605709691219, "ddate": null, "tcdate": 1605709691219, "tmdate": 1605800321965, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "tQzg5ouEnKX", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment", "content": {"title": "Answer to reviewer #2", "comment": "We thank you very much for your thoughtful review. Below are detailed explanations of the points you raised.\n\n- 1-3: We apology for having put too much focus on privacy and thank you for pointing out the logical flaws. We updated the abstract. As kindly mentioned by other reviewer, our goal is the same as the one in [1], i.e., \"*tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems.*\"[1]  As a side effect of not accessing the source data during the adaptation phase, we may have put some light on privacy theory, misleading the reader on the general direction of this research. Privacy is not our prime focus in this paper. We rewrote part of the introduction in order to make this really clear.\n\n- 4: We updated the related work section, we added:\n    - Liang, Jian, et al. \"Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation.\" ICML (2020).\n    - Yang, Shiqi, et al. \"Unsupervised Domain Adaptation without Source Data by Casting a BAIT.\"\n    - Peng, Xingchao, et al. \"Federated adversarial domain adaptation. (ICLR2020).\n    - Song, et al. Privacy-Preserving Unsupervised Domain Adaptation in Federated Setting. IEEE Access (2020).\n    -  Rui, Shu, et al. A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR (2018).\n    - Kumar, et al. Understanding Self-Training for Gradual Domain Adaptation. ICML(2020)\n\n- 5:\nThe advantage of generative model is in allowing to explicitly minimize the right hand side of theorem 1. This is the biggest difference between EMTL and other current solutions. Actually, most of them, e.g., DANN, SHOT and DIRT-T, are from the representation side rather than the distribution side to minimize the rhs of the theorem 1.  However, DANN needs to access both source and target data at the same time, SHOT and DIRT-T use on \"clustering\" assumptions to further constrain the representation learning during adaptation phase, which is a very great heuristic but not an explicitly way. \n\n- 6-7:\nWe use several RNADE models in order to estimate the density of the datasets regarding of the labels: 1) $p^s(x|y=0)$ consists in one RNADE model with parameter $\\omega_{s0}$ and 2) $p^s(x|y=1)$ is another RNADE model with parameter $\\omega_{s1}$. As we do not have access to target dataset labels, we learn a single RNADE model to have an estimation of $p^t(x)$. The goal of our transfer learning method is to infer the true distribution $p^t(x,y)$ in order to accurately classify the target dataset. We introduce the mediator distribution $p^m(x,y)$, composed of 2 RNADE models (one for each label), as the surrogate of true $p^t(x, y)$. We summarize the parameters we used to make them clearer:\n    - $\\theta_s = [\\omega_{s0}, \\omega_{s1}, \\tau_{s0}]$ is the parameter for $p^s(x,y)$. It is learned by using source data and is fixed during the EM steps.  It includes three parts:\n        - $\\omega_{s0}$ is the RNADE parameters of $p^s(x|y=0)$, we learned it by using source data $x|y=0$;\n         - $\\omega_{s1}$ is the  RNADE parameters of $p^s(x|y=1)$, we learned it by using source data $x|y=1$;\n         - $\\tau_{s0}$ is the ratio of class 0 in source data;\n    - $\\omega_{t}$ is the RNADE parameter of $p^t(x)$, we learned it by using full unlabeled target data $x$;\n    - $\\theta_m =  [\\omega_{m0}, \\omega_{m1}, \\tau_{m0}]$ is the parameter for $p^m(x,y)$. This is unknown and is the one we want to learn. It also includes three parts (just as $\\theta_s$):\n        - $\\omega_{m0}$ is the RNADE parameters of $p^m(x|y=0)$;\n        - $\\omega_{m1}$ is the  RNADE parameters of $p^s(x|y=1)$;\n        - $\\tau_{m0}$ is the ratio of class 0;\n\n Eq. (5) is indeed only valid for the first iteration. At $k+1$-th iteration, $\\theta_m^{(k)}$ is used in order to compute $\\theta_m^{(k+1)}$. The $y_i$ in Eq. (5) serves as the latent variable as regular EM did. The whole section is rewritten in a much clearer way in order to avoid confusion for the reader, all the previous points are now explicitly and clearly stated.\n- 8: We are working on more benchmark datasets as well as adding more SOTA DA algorithms and we will update the experimental results as soon as possible.\n\n### Reference\n[1]: Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. Jian Liang, Dapeng Hu, Jiashi Feng. ICML 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OEgDatKuz2O", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1200/Authors|ICLR.cc/2021/Conference/Paper1200/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862485, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment"}}}, {"id": "NE3nA3OJYn5", "original": null, "number": 6, "cdate": 1605799726571, "ddate": null, "tcdate": 1605799726571, "tmdate": 1605799726571, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "EggB1lZX0Jt", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment", "content": {"title": "Feedback", "comment": "Thanks for your quick response!\n1. Thanks for your explanation. Maybe my review is not clear, the `discriminator`  in my review is referred to domain discriminator, not the classifier. I am wondering about the difference between domain discriminator and your approach.\n\n2. Now I have a better understanding of the difference between the SHOT and EMTL. Thanks!\n\n3. Yes. I think the biggest concern in the proposed approach is the vanilla approach in estimating KL divergence for the high dimensional dataset.\n\n4. The covariate assumption is difficult to achieve for the deep learning-based approach. e.g. considering the source domain SVHN and target domain MNIST, the underlying label generation distribution can be really different.\n\n5. I am looking forward to your additional experiments.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OEgDatKuz2O", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1200/Authors|ICLR.cc/2021/Conference/Paper1200/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862485, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment"}}}, {"id": "9-2cEET7_I", "original": null, "number": 5, "cdate": 1605714407574, "ddate": null, "tcdate": 1605714407574, "tmdate": 1605714407574, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "97XgRYd7qC", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment", "content": {"title": "Answer to reviewer #3 ", "comment": "1. We apology for having put too much focus on privacy and thank you for pointing out the logical flaws. We updated the abstract. As kindly mentioned by other reviewer, our goal is the same as the one in [1], i.e., \"tackles a practical setting where only a trained source model is available and investigates how we can effectively utilize such a model without source data to solve UDA problems.\" As a side effect of not accessing the source data during the adaptation phase, we may have put some light on privicy theory, misleading the reader on the general direction of this research. Privacy is not our prime focus in this paper, which remains DA. We rewrote part of the introduction in order to make this really clear.\n\n2. We are currently performing experiments in order to better justify our approach. We will update the paper really soon with extensive results.\n\n\n\n[1] Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. Jian Liang, Dapeng Hu, Jiashi Feng. ICML 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OEgDatKuz2O", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1200/Authors|ICLR.cc/2021/Conference/Paper1200/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862485, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment"}}}, {"id": "EggB1lZX0Jt", "original": null, "number": 3, "cdate": 1605706541288, "ddate": null, "tcdate": 1605706541288, "tmdate": 1605714176155, "tddate": null, "forum": "OEgDatKuz2O", "replyto": "hSmZ-PufiD", "invitation": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment", "content": {"title": "Answer to reviewer #1", "comment": "1. Discriminator cannot fully replace the density estimator in EMTL. The density function $p(x,y;\\theta)$ is tunable thus allows us to optimize $D_{\\text{KL}}(p^s(y|x), p(y|x;\\theta))$, $\\log p(x;\\theta)$ and $D_{\\text{KL}}(p^t(x), p(x;\\theta))$ at the same time. Discriminator (let's say a neural network $f(x)$) can be used as approximator for $p(y|x)$, but that is not enough. Discriminator does not have capability to optimize $D_{\\text{KL}}(p^t(x), p(x))$, thus we further need another module like MMD as a regularizer. For example, DANN includes both classifier (discriminator) and adversarial item (MMD module).  Thus the density estimator is crucial in our algorithm in order to accurately perform the computations described in section 5.1 and 5.2.  However RNADE is not the only possible choice, any density estimator may be used as an alternative. In our work we choose RNADE because it is a powerful, yet simple, density estimator in moderate-dimensional problem. We will better clarify our approach regarding density estimation in the paper.\n2. Both SHOT and EMTL solve the same problem, i.e., the source data is unaccessible in adaptation phase. But SHOT and EMTL tackle the problem from two different directions: SHOT fixes the classifier and tunes the feature extractor in target. Whereas EMTL fixes the feature part but tunes the classifier. SHOT includes a global diverse loss in the objective function to encourage the target model to generate a diversity results. However, this assumption may be violated in a lot of real applications where the target class distributions can be imbalanced and unknown. As a contrast, EMTL doesn't have this issue.\n3. We did choose to use a na\u00efve empirical approximation for the KL-divergence in Eq.(9). As correctly noted, this empirical estimator is indeed not well suited in high dimension setting. Closed-form expression or better estimators exist for GMM, but cannot be straightforwardly applied to RNADE. We leave the extension to tighter estimator and/or upper bound for future work.\n4. The hyperparameter $\\eta$ leverage the trade-off between $D_{\\text{KL}}(p^s(y|x), p(y|x))$ and $D_{\\text{KL}}(p^s(x), p(x)) $:\n    - When setting $\\eta$ as 0, optimizing objective function results to a $p(x,y)$ which has $p(y|x)=p^s(y|x)$. In covariate shift case where we assume $p^s(y|x)=p^t(y|x)$, thus the learned $p(y|x)$ is $p^t(y|x)$.\n    - When setting a big $\\eta$, the first term becomes negligible and EMTL focuses more on reducing the distance between $p(x)$ and $p^t(x)$. As \"$p(y|x)=p^s(y|x)$\" constraint is weak thus EMTL can escape from covariate shift setting and fit to more general cases.\nIn summary, EMTL allows us to balance well between these two settings. It is interesting to notice that in most of our experiments, the optimal eta is located precisely in the \"middle\" of these two worlds. We clarified this section and added the etas used in the experimental section to put more light on this important property of EMTL.\n5. We are currently performing experiments in order to better justify our approach. We will update the paper really soon with extensive results."}, "signatures": ["ICLR.cc/2021/Conference/Paper1200/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EMTL: A Generative Domain Adaptation Approach", "authorids": ["~Jianfeng_Zhang2", "~Illyyne_Saffar1", "~Aladin_Virmaux1", "~Bal\u00e1zs_K\u00e9gl2"], "authors": ["Jianfeng Zhang", "Illyyne Saffar", "Aladin Virmaux", "Bal\u00e1zs K\u00e9gl"], "keywords": ["unsupervised domain adaptation", "EM", "generative model", "density estimation", "deep learning", "transfer learning"], "abstract": "We propose an unsupervised domain adaptation approach based on generative models. We show that when the source probability density function can be learned, one-step Expectation\u2013Maximization iteration plus an additional marginal density function constraint will produce a proper mediator probability density function to bridge the gap between the source and target domains. The breakthrough is based on modern generative models (autoregressive mixture density nets) that are competitive to discriminative models on moderate-dimensional classification problems. By decoupling the source density estimation from the adaption steps, we can design a domain adaptation approach where the source data is locked away after being processed only once, opening the door to transfer when data security or privacy concerns impede the use of traditional domain adaptation. We demonstrate that our approach can achieve state-of-the-art performance on synthetic and real data sets, without accessing the source data at the adaptation phase.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|emtl_a_generative_domain_adaptation_approach", "pdf": "/pdf/adcbbf7838a45db95b3e9df078a686547ab5f7e2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=B5erNvhMs", "_bibtex": "@misc{\nzhang2021emtl,\ntitle={{\\{}EMTL{\\}}: A Generative Domain Adaptation Approach},\nauthor={Jianfeng Zhang and Illyyne Saffar and Aladin Virmaux and Bal{\\'a}zs K{\\'e}gl},\nyear={2021},\nurl={https://openreview.net/forum?id=OEgDatKuz2O}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OEgDatKuz2O", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1200/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1200/Authors|ICLR.cc/2021/Conference/Paper1200/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1200/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862485, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1200/-/Official_Comment"}}}], "count": 11}