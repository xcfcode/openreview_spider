{"notes": [{"id": "Vd7lCMvtLqg", "original": "q2pqOghtwLi", "number": 1353, "cdate": 1601308151069, "ddate": null, "tcdate": 1601308151069, "tmdate": 1615441847167, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "E6x0CDtFbll", "original": null, "number": 1, "cdate": 1610040415483, "ddate": null, "tcdate": 1610040415483, "tmdate": 1610474013535, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes a method to cope with large vocabulary sizes. The idea is to find a small number of anchor words and to express every other word as a sparse nonnegative linear combination of them. They give an end-to-end method for training, and give a statistical interpretation of their algorithm as a Bayesian nonparametric prior (in particular an Indian restaurant process). They give extensions that allow them to deduce the optimal number of anchors which allows them to avoid needing to tune this hyperparameter. Finally they give a variety of experiments, particularly in language and recommendation tasks. The results on language are particularly impressive, and in the author response period, at the behest of a reviewer, they were able to extend the experiments to the Amazon Review dataset which contains 233M reviews on 43.5 M items by 15.2 M users. \n\nThis paper is a nice combination of a simple but powerful idea, and a range of experiments demonstrating its utility. Other papers have proposed related ideas, but here the main novelty is in (1) using a small number of anchors that can incorporate domain knowledge and (2) using a sparse linear transformation to express other words in this basis. One reviewer did not find the Bayesian nonparametric interpretation to be fruitful, since it does not lead to techniques for handling growing datasets (e.g. if the ideal number of anchors changes over time). "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040415469, "tmdate": 1610474013519, "id": "ICLR.cc/2021/Conference/Paper1353/-/Decision"}}}, {"id": "QsB9YxscPbf", "original": null, "number": 6, "cdate": 1606198838949, "ddate": null, "tcdate": 1606198838949, "tmdate": 1606199024520, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "AUkrQXKVb2f", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment", "content": {"title": "Scaling up to Amazon Product reviews - 43.5M items and 15.2M users", "comment": "[R2 large vocab size] We have performed additional experiments with a much larger Amazon Review dataset (https://nijianmo.github.io/amazon/index.html) which is the largest public recommendation systems benchmark so far containing 233M reviews spanning 43.5M items and 15.2M users. We first experiment on a commonly used subset of the data, Amazon Electronics, to ensure our results match published baselines (Wan et al., 2020).\n\n\t\t\tUser |A|\tItem |A| \tMSE\t\t  nnz params (M)\n\tMF\t  9.84M\t   0.76M\t\t1.524\t\t170\n\tANT\t 20\t\t  8\t\t\t**1.422**\t25.8\n\tANT\t 8\t\t   3\t\t\t1.529\t\t7.10\n\tANT\t 5\t\t   3\t\t\t1.591\t\t**3.89**\n\nNext, we scale our experiment to the entire dataset:\n\n\t\t\tUser |A|\tItem |A| \tMSE\t\t  nnz params (M)\n\tMF\t  43.5M\t   15.2M\t\t1.164\t\t939\n\tANT\t 15\t\t  10\t\t   **1.099**\t201\n\tANT\t 8\t\t   8\t\t\t1.167\t\t**95.9**\n\nFor both, we find that ANT compresses embeddings by **25x** on Amazon Electronics while maintaining performance, and **10x** on the full Amazon reviews dataset. **We added these results in section 4.4 product recommendation.**\n\nWan et al., Addressing Marketing Bias in Product Recommendations. WSDM 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vd7lCMvtLqg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1353/Authors|ICLR.cc/2021/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860680, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment"}}}, {"id": "AZ3rPYWZPB", "original": null, "number": 3, "cdate": 1605839918749, "ddate": null, "tcdate": 1605839918749, "tmdate": 1606083724925, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "p_vAr3LMMHS", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your constructive comments, feedback, and for finding the method to be ingenious. We answer your questions here:\n\n[R4 domain knowledge] We do not use any domain knowledge in the experiments reported in the main paper. Experiments with domain knowledge are presented in Appendix, e.g. Table 10, 11 and we show further improvements using WordNet and co-occurrence statistics to impose relationships between the anchor and non-anchor words, particularly on compression. We agree with the reviewer that there can be more elaborate manners to incorporate domain knowledge - we simply attempted to give several proposals as a proof-of-concept that achieves promising results. These more elaborate methods are ripe areas for future research.\n\n[R4 non-zero rows] That is a good question. We didn\u2019t do any specific tuning for rare words/items and consider it as a feature, not a bug. It shows the method adaptively eliminates only undesired words. Empirically we found all words in WikiText-103 to have non-zero rows when doing little compression (38.4 perplexity with 6.5M parameters). For more compression (54.2 perplexity with 0.4 parameters), 179K words (out of 267K total) had zero rows. **ANT clearly differentiates between useful words to model and rare words that it can ignore for compression:**\n\nZero rows: Anarky, Perl, Voorhis, Gaud\u00ed, Lat, Bottomley, Nescopeck, Tzara, LeMond, Doby, Gulfton, Mileena, Kuznetsova, FN, Wallenberg, Ahalya, Betances, Cabral, Braham, Coatbridge, rajah, Finkelstein, Wilders, Swaminarayan, Youkilis, Robben, Satin, Whorf, Ralphie, BN, Betelgeuse, Vithoba, FIU, Astatine, Hextall, Massino\n\nNon-zero rows: out, about, than, game, between, later, three, most, while, new, On, made, film, such, season, where, before, years, only, 2, up, He, they, after, would, time, into, It\n\nWe also noticed that certain rare words that might be predictive are assigned non-zero rows in T, such as:\nsociologists, losers, finder, deadlines, indestructible, causeways, captions, outsourced, rotors, refrigerated, reconsiders, glacially, heartening, unchallenging, roughest\n\nWe believe the objective function learns to make all entries 0 in a row only if including a few doesn\u2019t increase the log-likelihood much, i.e. an all-zero vector can behave like the embedding of a UNK token and having a UNK token is not much worse for such rare words.\n\nWe observed a similar trend in MovieLens rating prediction. Only 2673 out of 59047 movies had an entire zero row, **of which 84% only had 1 rating (i.e. very rare movies).** Furthermore, all user transformations had non-zero rows - on average each user transformation had 3 (out of 8) non-zero transformations wrt user anchor embeddings. **We added a discussion on this in section 4.5 under \u20184) Zero transformations learned\u2019.**\n\n[R4 lambda] The regularization parameter \\lambda is selected via grid search on a held-out validation set."}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vd7lCMvtLqg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1353/Authors|ICLR.cc/2021/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860680, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment"}}}, {"id": "AKrKPKQf4bc", "original": null, "number": 5, "cdate": 1605841161787, "ddate": null, "tcdate": 1605841161787, "tmdate": 1606082758918, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "AUkrQXKVb2f", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for your constructive comments and feedback. We answer your questions here:\n\n[R2 Bayesian] We provide a statistical interpretation as a Bayesian nonparametric prior since it shows us a way to automatically learn the optimal number of anchors. Empirically, in Figure 4, we plot the value of eq (5) across values of |A| after a comprehensive hyperparameter sweep on ANT across 1000 settings. In comparison, nbANT optimizes |A| and reaches a good value of eq (5) from just **one**run, which shows that **the Bayesian interpretation provides successful principles for algorithm design to further reduce training efficiency.**\n\nFurthermore, it is not true that Bayesian inference must always be associated with a posterior distribution - in fact, several papers before us have applied Small Variance Asymptotics to obtain approximate point estimates for hierarchical Bayesian models (Broderick et al., 2013a; Jiang et al., 2012; Roychowdhury et al., 2013). In our case, the quantities of interest in Bayesian inference are the anchor matrix A and transformation matrix Z, which are very high-dimensional matrices. Modeling the posterior distribution is useful for scalar quantities, e.g. to obtain confidence intervals, but not so much for high-dimensional latent variables (Zaheer et al., 2016, Tristan et al., 2015). Furthermore, we do only care about a point estimate for |A| (and A and Z) so we do not actually need a posterior distribution.\n\nZaheer et al., Exponential Stochastic Cellular Automata for Massively Parallel Inference. AISTATS 2016\n\nTristan et al., Efficient Training of LDA on a GPU by Mean-for-Mode Estimation. ICML 2015\n\n[R2 online] Our Algorithm 2 in the appendix describes the procedure for the model to grow/contract in terms of the number of anchors, |A|, during training. We agree that online settings are important and **we convert Algorithm 2 to an online version**inspired by the paper you mentioned. Instead of looping through all mini-batches for all epochs, we treat each batch as a new set of data coming in an online setting and train on that batch until convergence, modify |A| as in Algorithm 2, before moving onto the next batch. In this significantly more challenging online setting, nbANT is able to learn well and **achieve an MSE of 0.875 with 1.25M non zero parameters.** Initially, the clusters grew steadily from 10 up to 26 as online batches were seen. As even more batches were seen, the number of anchors decreased steadily from 26 to 10 and oscillated between 8 and 10. This means initially some connections/groupings between online batches were not clear, but with more data, natural clusters merged together. Interestingly this online version of nbANT settled on a similar range of final user (8) and item (8) anchors as compared to the non-online version (see Table 3), which confirms the robustness of nbANT in finding relevant anchors automatically for growing model capacity. **We added these results under \u2018nbANT\u2019 in section 4.3 movie recommendation and in Appendix K.3.**\n\n[R2 large vocab size] Thank you for suggesting this experiment. To the best of our knowledge, we could not find public datasets with more than 100M users. If you have one in mind, please let us know. In the meantime, we are currently experimenting with a much larger Amazon Review dataset (https://nijianmo.github.io/amazon/index.html; Ni et al., 2019) which is the largest public recommendation system benchmark so far containing 233M reviews spanning 43.5M items and 15.2M users. Even for a dataset of this size, there are no recommender system baselines on the entire dataset, they all take a subset of product categories (e.g. Amazon clothing). We will update this comment and the paper with new results by the weekend.\n\nNi et al., Justifying recommendations using distantly-labeled reviews and fined-grained aspects. EMNLP 2019\n\n[R2 AUC] We agree that AUC scores are very useful for evaluating ranking tasks, but these are only applicable for MovieLens if explicitly converted to a ranking task. We performed the rating prediction task as measured using MSE which has been the precedent for this dataset: recent works using MovieLens for recommender systems have all studied rating prediction as measured using MSE (Ginart et al., 2019, Strub et al., 2017).\n\nStrub et al., Hybrid Recommender System based on Autoencoders. 2017\n\nSaadati et al., 2019. Movie Recommender Systems: Implementation and Performance Evaluation. 2019\n\nGinart et al., Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vd7lCMvtLqg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1353/Authors|ICLR.cc/2021/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860680, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment"}}}, {"id": "zWMCvZfZ8a8", "original": null, "number": 4, "cdate": 1605840030811, "ddate": null, "tcdate": 1605840030811, "tmdate": 1605841282722, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "1riDEfR6CT", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for your constructive comments and feedback. We answer your questions here:\n\n[R3 baselines 1] MixedDim Embeddings by Ginart et al., (2019) are specifically designed for recommender systems and we did compare to MixedDim in our MovieLens experiment in Table 3, where we showed improvements. MixedDim also requires a significant preprocessing time to count the frequency of users and items that scales poorly to very large numbers of users and items. Compositional Embeddings by Shi et al., (2020) is a very similar paper from the same authors as Ginart et al., (2019) which we compared to.\n\n[R3 baselines 2] SNRM seems to be orthogonal to our focus. It is designed to yield sparse embeddings for documents, whereas we operate at sparsifying the word embedding table. At token level SNRM still requires the full embedding table (as described on page 4 of their paper) and thus has no parameter size reduction with respect to vocabulary size. In fact, it is possible to use ANT in combination with SNRM which we believe can inspire future work in combining local (word/object-level) and global (document-level) compression.\n\nThanks for pointing out about SOLAR. We were not aware of the paper and it seems to not be published anywhere and was only released on arxiv 2 weeks before the ICLR deadline.\n\n[R3 AG news] AG News is the smallest dataset in our experiments and large models with full embedding matrices are likely to overfit and generalize poorly. ANT with low-rank anchor embeddings and sparse transformations seems to provide a regularization effect that learns better word representations and generalizes better. We did not observe this for the larger datasets for text classification (DBPedia, Sogou-News, and Yelp-review, see Table 10 in the Appendix), language modeling (PTB and WikiText-103, see Table 2), and recommender systems (MovieLens in Table 3), where there was a clear trade-off between performance and compression."}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Vd7lCMvtLqg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1353/Authors|ICLR.cc/2021/Conference/Paper1353/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860680, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Comment"}}}, {"id": "p_vAr3LMMHS", "original": null, "number": 1, "cdate": 1603911932682, "ddate": null, "tcdate": 1603911932682, "tmdate": 1605024466112, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review", "content": {"title": "An ingenious two-step method on representation learning", "review": "In this paper, the authors proposed a method to learn efficient representations of discrete tokens. They took a two step approach: in step 1, they learn \"full fledged\" embeddings for a subset of anchor tokens. In step 2, they learn a sparse matrix that is used to relate all tokens to the set of chosen anchors. This two-step approach reduced the overall number of parameters. The sparse matrix T can also encode domain knowledge (e.g. knowledge graphs). In the experiment section, the authors showed that their approach has good performance on several language tasks, with far fewer parameters.\n\nIn general the paper is well written and the flow is easy to follow. I find the main idea plausible and ingenious. For language tasks and word embeddings, anchoring method has been shown to be effective in several tasks already (e.g. [1] http://papers.nips.cc/paper/8152-the-global-anchor-method-for-quantifying-linguistic-shifts-and-domain-adaptation). The authors took two steps forward: 1) instead of in [1] where the entire vocab is used for anchoring purposes, the authors used a subset of tokens which reduces the amount of parameters. 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. Although this paper is probably related to other strains of research (e.g. leaning manifolds for IR/NLP where anchoring is also a key concept, which the authors could have admittedly surveyed more), I particularly liked the fact that the two-step procedure decomposes two tasks that are often mixed together for embedding tasks: learning representation vs learning relations.\n\nWhile the authors claimed that they can further impose domain knowledge in the learning process (which I think this is at least a good attempt), this part in general feels a bit less convincing. To be specific, there can be a variety of knowledge (like related, is a subset of, analogy, etc.). It is not clear how the distinction of different types of knowledge can be incorporated. What the authors proposed is lumping them into the notion of \"positive pair\" and relax constraints on them. This may or may not suffice (for the purpose of adding domain knowledge), but on paper, there is a chance that some finer structures of the domain knowledge may get lost. It's not clear how much gain (especially the experiment section, for fair comparison purposes where other methods do know use domain knowledge in particular) is from incorporating domain knowledge; an ablation study might help.\n\nAnother question is about training. It's not obvious to me how to guarantee that for every row of T, there is at least 1 non-zero element. Is some specific tuning needed for rows corresponding to rare words? How the regularization strength $\\lambda$ on T is selected?\n\nThe reduction of parameters is while keeping task performance is illustrated quite well in the experiment section. Their method does not reduce the theoretical complexity (still linear w.r.t. vocab size, as T must have at least one element per row), but in practice the reduction (which mostly comes from savings of dimensionality) is quite obvious.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120642, "tmdate": 1606915776941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1353/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review"}}}, {"id": "1riDEfR6CT", "original": null, "number": 2, "cdate": 1603950885473, "ddate": null, "tcdate": 1603950885473, "tmdate": 1605024466045, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review", "content": {"title": "A neat idea to pose sparse embedding model as a sparse linear combination of dense latent vectors. Nice interpretation as a Bayesian prior. Good empirical evidence on multiple NLP and Information Retrieval tasks. Might need comparison against some other sparse models like Compostional Embeddings, SNRM, SOLAR.", "review": "This paper proposes ANT to solve the problem of learning Sparse embeddings instead of dense counterparts for tasks like Text Classification, Language Modeling and Recommendation Systems. When the vocabulary size |V| runs into several 100Ks or millions, it is impractical to store one dense vector per label. Hence the paper proposes to only store a few anchor/latent vectors (the matrix is A with |A|<<|V|). All label vectors are expressed as linear combinations of a 'few' anchor vectors. To train this end-to-end, we need a transformation matrix T such that T*A = E (E is V\\times d embedding matrix). T has to be structured, i.e., each row of T has to be sparse and positive only (although negative weights are also fine, I'm not sure if weight redundancy is that important). \n\nThis pipeline is trained end to end using YOGI optimizer for regular gradient updates and 'proximal gradient descent' for T which does soft thresholding with a lower bound of 0 (accomplishing both sparsity and positivity part).\n\nThis design admits multiple ways of initializing the anchors A. And the authors perform experiments with both frequent token vectors and random anchor vectors (both have their merits, random seems to be a robust choice).\n\nThe authors provide a statistical interpretation of their approach using a generative formulation to the embedding vectors in terms of the latent vectors (using a Indian Buffet Process membership matrix Z).\n\nThe experiments span two major domains, NLP and Information Retrieval. Across multiple NLP datasets, ANT outperforms Sparse-Coding (Chen et. al. 2016)  and Post-Sparse-Hash (Guo et.al. 2017). On the IR task with MovieLens dataset, the primary comparison is against SLIMMING (Liu et.al. 2017). While gains are substantial on the NLP tasks, they seem minimal on the MovieLens task.\n\nI've listed most pros above. The cons are here:\n1. The idea seems a little similar to Compositional Embeddings (Shi et. al. 2020, Ginart et. al.2019). It might warrant a discussion or comparison.\n2. There are other sparse embedding methods like SNRM (Zamani et.al. 2018) and SOLAR-Sparse Orthogonal ...(Medini et.al. 2020) which might be comparison candidates at-least for IR tasks.\n3.  The precision in table 1 for ANT anomalously increases when |A| is reduced. ANy explanation as to why this happens? The information bottleneck is supposed to reduce precision right?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120642, "tmdate": 1606915776941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1353/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review"}}}, {"id": "AUkrQXKVb2f", "original": null, "number": 3, "cdate": 1604263801733, "ddate": null, "tcdate": 1604263801733, "tmdate": 1605024465973, "tddate": null, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "invitation": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review", "content": {"title": "This paper proposes a practical solution to cut the embedding storage. But the Bayesian interpretation is not persuasive and there are still missing pieces in the experiments.", "review": "This paper introduces a row-rank approximation of embeddings using \u201canchors\u201d. It also proposes a probabilistic interpretation of their method as a non-parametric Bayesian dictionary learning model, which can be inferred by optimizing the small-variance asymptotic objective.\n\nWhat I agree with the authors are:\ni) Using properly chosen basis vectors may greatly reduce the memory cost for embeddings, especially for huge vocabulary sizes (e.g. over 100 million).\nii) The initialization for basis vectors is extremely important and should be updated through training.\niii) Experimental results look reasonable in this paper.\n\nWhat I feel confused about are:\ni) Why interpret this method in a Bayesian non-parametric way? To be more specific:\ni.1) The final objective function (5) does not involve Bayesian posterior inference. If you want a point estimation of sparse representation + learnable anchors, you don\u2019t need a Bayesian model.\ni.2) Bayesian non-parametric is useful because it can automatically learn the model size. In your case, |A|. You mention this point in Figure 3, but there is no online learning result showing that your model has the capacity to grow the model through training. One example is \u201cTruly Nonparametric Online Variational Inference for Hierarchical Dirichlet Processes\u201d by M. Bryant and E. Sudderth.\n\nii) The vocabulary size in your experiment is decent, but not very big. Normally in a recommendation system, the vocabulary size can be the number of users, which is at least 100M. Normally the embedding size is around 16 to 64 in real systems. The proposed method could be a huge gain in storing such a huge embedding table. But I cannot see an experiment at this vocabulary level. Even rough results at this level could make this paper much stronger.\n\niii) This is a minor point, but AUC results in MovieLens besides MSE can reflect the ranking quality in recommendations.\n\nOverall, this paper proposes a practical solution to cut embedding storage. But the Bayesian interpretation is not persuasive and there are still missing pieces in the experiments.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1353/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1353/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies", "authorids": ["~Paul_Pu_Liang1", "~Manzil_Zaheer1", "~Yuan_Wang1", "~Amr_Ahmed1"], "authors": ["Paul Pu Liang", "Manzil Zaheer", "Yuan Wang", "Amr Ahmed"], "keywords": ["sparse embeddings", "large vocabularies", "text classification", "language modeling", "recommendation systems"], "abstract": "Learning continuous representations of discrete objects such as text, users, movies, and URLs lies at the heart of many applications including language and user modeling. When using discrete objects as input to neural networks, we often ignore the underlying structures (e.g., natural groupings and similarities) and embed the objects independently into individual vectors. As a result, existing methods do not scale to large vocabulary sizes. In this paper, we design a simple and efficient embedding algorithm that learns a small set of anchor embeddings and a sparse transformation matrix. We call our method Anchor & Transform (ANT) as the embeddings of discrete objects are a sparse linear combination of the anchors, weighted according to the transformation matrix. ANT is scalable, flexible, and end-to-end trainable. We further provide a statistical interpretation of our algorithm as a Bayesian nonparametric prior for embeddings that encourages sparsity and leverages natural groupings among objects. By deriving an approximate inference algorithm based on Small Variance Asymptotics, we obtain a natural extension that automatically learns the optimal number of anchors instead of having to tune it as a hyperparameter. On text classification, language modeling, and movie recommendation benchmarks, we show that ANT is particularly suitable for large vocabulary sizes and demonstrates stronger performance with fewer parameters (up to 40x compression) as compared to existing compression baselines.", "one-sentence_summary": "End-to-end learning of sparse embeddings for large vocabularies with a Bayesian nonparametric interpretation that results in up to 40x smaller embedding tables.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liang|anchor_transform_learning_sparse_embeddings_for_large_vocabularies", "supplementary_material": "/attachment/0fd2f819e697c9b02203864a5bf5e0d96d41ac5e.zip", "pdf": "/pdf/22fba9165ecde0ee1ac4f19e7d5c5d99fd4c5146.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nliang2021anchor,\ntitle={Anchor {\\&} Transform: Learning Sparse Embeddings for Large Vocabularies},\nauthor={Paul Pu Liang and Manzil Zaheer and Yuan Wang and Amr Ahmed},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Vd7lCMvtLqg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Vd7lCMvtLqg", "replyto": "Vd7lCMvtLqg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1353/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120642, "tmdate": 1606915776941, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1353/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1353/-/Official_Review"}}}], "count": 9}