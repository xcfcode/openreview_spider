{"notes": [{"id": "BylD9eSYPS", "original": "rJxZAAgtPH", "number": 2475, "cdate": 1569439887229, "ddate": null, "tcdate": 1569439887229, "tmdate": 1577168251975, "tddate": null, "forum": "BylD9eSYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QwmqusbVHJ", "original": null, "number": 1, "cdate": 1576798749888, "ddate": null, "tcdate": 1576798749888, "tmdate": 1576800885980, "tddate": null, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "invitation": "ICLR.cc/2020/Conference/Paper2475/-/Decision", "content": {"decision": "Reject", "comment": "The paper discusses a simple but apparently effective clustering technique to improve exploration. There are no theoretical results, hence the reader relies fully on the experiments to evaluate the method. Unfortunately, an in-dept analysis of the results is missing making it hard to properly evaluate the strength and weaknesses. Furthermore, the authors have not provided any rebuttal to the reviewers' concerns.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724729, "tmdate": 1576800276426, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2475/-/Decision"}}}, {"id": "rkgyvJ3_uS", "original": null, "number": 1, "cdate": 1570451286964, "ddate": null, "tcdate": 1570451286964, "tmdate": 1572972333519, "tddate": null, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "invitation": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a clear approach to improve the exploration strategy in reinforcement learning, which is named clustered reinforcement learning. The approach tries to push the agent to explore more states with high novelty and quality.  It is done by adding a bonus reward shown in Eq. (3) to the reward function. The author first cluster states into clusters using the k-means algorithm. The bonus reward will return a high value for a state if the corresponding cluster has a high average reward. When the total reward in a cluster is smaller than a certain threshold, the bonus reward will consider the number of states explored. In the experiments, the authors test different models on two MuJoCo tasks and five Atari games. TRPO, TRPO-Hash, VIME are selected as baselines to compare with. Results show that the proposed bonus reward reaches faster convergence and the highest return in both MuJoCo tasks. In those five Atari games, the proposed method achieves the highest or second-highest average returns.\n\nAlthough the paper is generally easy to follow and the motivations of the equations are clear,  the analysis of the results is missing and thus the paper provides very limited insights on the behavior of the proposed method. As a result, my opinion on this paper leans to a rejection. As ICLR recommends paper length to be 8 pages, the authors can and should use the remaining space to give more details. For example, the authors can show the mean reward and number of states in all clusters to see whether the agent is efficiently exploring different clusters. And as the number of clusters K is an important hyper-parameter, the reader will also be curious about the resultant performance of the method with different K.\n\nSome questions:\n1) In Eq. (3), you have two hyper-parameters in the bonus reward, and for MuJoCo and Atari games, you are using different settings for the first coefficient. How do you choose the hyper-parameter settings? Do you perform grid search with another environment or a set of environments to determine the hyper-parameters?\n\n3) In the algorithm, the method has to learn new cluster assignments in each iteration. Does it significantly slow down the training of the agent? \n\n4) In the experiments, the authors compare with other methods on only five Atari games. However, there should be more environments available.  What is the reason for choosing these five games? Unless it is difficult to gather the scores on more environments, I believe the authors shall provide results with more Atari environments.\n\n5) The conclusion (Section 6) is extremely short and it claims that \"CRL can outperform other SOTA methods in most cases\". However, for the five Atari games, the only game that CRL achieves the best performance among seven methods is Venture, according to Table 1. \nSuch a claim will confuse readers as it is not in line with the results.\n\nAll in all, I believe this paper can be significantly improved if more details and analyses are provided.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575381701988, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2475/Reviewers"], "noninvitees": [], "tcdate": 1570237722308, "tmdate": 1575381702000, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review"}}}, {"id": "ryl70K8hYB", "original": null, "number": 2, "cdate": 1571740107042, "ddate": null, "tcdate": 1571740107042, "tmdate": 1572972333482, "tddate": null, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "invitation": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposed a new reinforcement learning framework named clustered reinforcement learning. The proposed method employs the clustering method to explore the novelty and quality in the neighboring area. Some suggestions are as below:\n\n1. The method adopts the k-means for clustering. How about other clustering methods, like Spectral Clustering and other recent deep clustering methods? It's expected to give the experiment comparison results with other clustering methods. Is the method senstive to the used clustering method. \n2. Usually in the clustering tasks, how to decide the cluster number K is a crucial problem for many applications. And for this method, the clustering is employed in the exploration stage to cluster neighboring areas, how to determine the value of K. Are there any specifical information could be used? Moreover, could the clustering part be jointly trained in the framework? It' required to investigate the influence of K theoretically and experimentally.\n3. As the author claimed, the novelty and quality are both important for exploration in RL, the key question is thus how to balance and utilize them carefully in the exploration stage. The contribution of this paper is about the state clustering for exploration in RL which can further reflect the novelty and quality. How to utilize and balance the novelty and quality is still unsolved in this paper. \n\nIn summary, I ack that the idea is effective but seems straightforward. It would be better to present some theoretical analysis. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575381701988, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2475/Reviewers"], "noninvitees": [], "tcdate": 1570237722308, "tmdate": 1575381702000, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review"}}}, {"id": "SklciHXTKB", "original": null, "number": 3, "cdate": 1571792290125, "ddate": null, "tcdate": 1571792290125, "tmdate": 1572972333446, "tddate": null, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "invitation": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed a clustering based algorithm to improve the exploration performance in reinforcement learning. Similar to the count based approaches, the novelty of a new state was computed based on the statistics of the corresponding clusters. This exploration bonus was then combined with the TRPO algorithm to obtain the policy. The experimental results showed some improvement, compare with its competitors.\n\nAlthough the proposed method is somewhat similar to the earlier hash based approaches, I think it is still interesting by using the clustering, instead of computing the hash code with neural networks. On the other hand, the motivation and explanation of this method are not well presented. I also have some concern regarding the fairness of the comparison in experiments. The English usage could be improved as well. My detailed comments and questions are as follows.\n1. The new proposal for the exploration bonus is provided in Equation (3). The denominator there is essentially the count, which is consistent with previous count based approaches (though not with the square root). For the numerator, I am a bit confused about the choice, as if \"N\" is small, the accumulated \"R\" could be small as well, which may offset the bonus based on count. I also didn't understand the author's claim that \"...it is highly possible that all states in cluster \\phi (s) have zero reward\", just below Equation (3). Unless the authors provide more details, I am not convinced that the enumerator could be a good choice.\n2. Given the proposed bonus, I am wondering how sensitive could it be to the choice of hyperparameters, especially w.r.t \\eta. The authors may need to provide more ablation studies on their effect.\n3. Another concern is regarding the scalability of the proposed method. Algorithm 1 implies that k-means needs to be conducted in every iteration, which could be very slow. So how about the running time of the proposed method, when compared with baselines?\n4. In the experiments, the authors claimed that the code for TRPO-Hash is provided by its authors. However, the scores for TRPO-Hash were much worse than the numbers in the TRPO-Hash paper (see their Table 1). Do you have any explanation? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2475/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2475/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575381701988, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2475/Reviewers"], "noninvitees": [], "tcdate": 1570237722308, "tmdate": 1575381702000, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2475/-/Official_Review"}}}, {"id": "HJx70oQouH", "original": null, "number": 1, "cdate": 1570614219306, "ddate": null, "tcdate": 1570614219306, "tmdate": 1570614219306, "tddate": null, "forum": "BylD9eSYPS", "replyto": "BylD9eSYPS", "invitation": "ICLR.cc/2020/Conference/Paper2475/-/Public_Comment", "content": {"comment": "Good idea, simple yet effective. However, I have some questions.\n1) as much as you've claimed that the choice of *clustering algorithm* is NOT the main contribution of your paper, is it possible to add additional experiments to compare k-means with other cluster methods?\n2) I'm curious about your eq.(3). It's so heuristic. Can you write some proofs or sth else?\n3) the last question, the last line in alg. 1.  The n-step reward will be computed as Vs = ri + b(si) + \\gamma*(Vsi+1), however, as far as I'm concerned, the bias form b(si) should not be the part of Vs. The intuitive idea maybe put bias b(si) after computed the n-step reward. So, there exists a proof or sth else?\n\nThanks.\n\nSorry for the typo, sending on my phone.", "title": "Some questions."}, "signatures": ["~Tianchi_Huang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Tianchi_Huang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["max@lamda.nju.edu.cn", "zhaosy@lamda.nju.edu.cn", "zhaohengyin@gmail.com", "liwujun@nju.edu.cn"], "title": "Clustered Reinforcement Learning", "authors": ["Xiao Ma", "Shen-Yi Zhao", "Zhao-Heng Yin", "Wu-Jun Li"], "pdf": "/pdf/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "abstract": "Exploration strategy design is one of the challenging problems in reinforcement learning~(RL), especially when the environment contains a large state space or sparse rewards. During exploration, the agent tries to discover novel areas or high reward~(quality) areas. In most existing methods, the novelty and quality in the neighboring area of the current state are not well utilized to guide the exploration of the agent. To tackle this problem, we propose a novel RL framework, called \\underline{c}lustered \\underline{r}einforcement \\underline{l}earning~(CRL), for efficient exploration in RL. CRL adopts clustering to divide the collected states into several clusters, based on which a bonus reward reflecting both novelty and quality in the neighboring area~(cluster) of the current state is given to the agent. Experiments on several continuous control tasks and several Atari-2600 games show that CRL can outperform other state-of-the-art methods to achieve the best performance in most cases.", "keywords": [], "paperhash": "ma|clustered_reinforcement_learning", "original_pdf": "/attachment/7e4855a1b50cbfe66596851d91a63b45744c3059.pdf", "_bibtex": "@misc{\nma2020clustered,\ntitle={Clustered Reinforcement Learning},\nauthor={Xiao Ma and Shen-Yi Zhao and Zhao-Heng Yin and Wu-Jun Li},\nyear={2020},\nurl={https://openreview.net/forum?id=BylD9eSYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylD9eSYPS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504179756, "tmdate": 1576860579917, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2475/Authors", "ICLR.cc/2020/Conference/Paper2475/Reviewers", "ICLR.cc/2020/Conference/Paper2475/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2475/-/Public_Comment"}}}], "count": 6}