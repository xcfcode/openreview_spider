{"notes": [{"id": "BJgkbyHKDS", "original": "S1xrlPidDH", "number": 1528, "cdate": 1569439479062, "ddate": null, "tcdate": 1569439479062, "tmdate": 1577168258517, "tddate": null, "forum": "BJgkbyHKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7C_BgXicy", "original": null, "number": 1, "cdate": 1576798725668, "ddate": null, "tcdate": 1576798725668, "tmdate": 1576800910828, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies the empirical performance of invertible generative models for compressive sensing, denoising and in painting. One issue in using generative models in this area has been that they hit an error floor in reconstruction due to model collapse etc i.e. one can not achieve zero error in reconstruction. The reviewers raised some concerns about novelty of the approach and thoroughness of the empirical studies. The authors response suggests that they are not claiming novelty w.r.t. to the approach but rather their use in compressive techniques. My own understanding is that this error floor is a major problem and removing its effect is a good contribution even without any novelty in the techniques. However,  I do agree that a more thorough empirical study would be more convincing. While I can not recommend acceptance given the scores I do think this paper has potential and recommend the authors to resubmit to a future venue after a through revision.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713750, "tmdate": 1576800263430, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Decision"}}}, {"id": "SJeePN_AFB", "original": null, "number": 1, "cdate": 1571877976041, "ddate": null, "tcdate": 1571877976041, "tmdate": 1574407892872, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Update: I have read the other reviews and the author response and have not changed my evaluation.\n\nRecent work has shown that GANs can be effective for use as priors in inverse problems for images such as compressed sensing, denoising, and inpainting. A drawback is that GANs may have the problem of inexact reconstruction, and strongly reflect the biases in the training set yielding poor performance on out-of-distribution data. This paper shows that the exact inverses available to normalizing flow models and their broad assignment of likelihoods allows for better reconstructions especially on out-of-domain data.\n\nAs far as I know, this is the first work to use normalizing flows for inpainting and compressed sensing. The approach and application is very natural, although it\u2019s a bit surprising that using the likelihood as a prior term directly did not work very well. The results of this work show that invertible generative models have utility for inverse image problems even when the quality of raw samples is substantially below GANs. In my opinion the main advantage in this method is not on having low reconstruction error on observed pixels, which becomes less of a problem for more powerful GAN models, but rather the good performance on out of domain data which is somewhat surprising. The authors are reasonably thorough, testing their model on a variety of problem settings and perform ablation studies on hyperparameters.\n\nAs additional baselines for compressed sensing and denoising, it would be good to compare to the Deep Image Prior since there is effectively no out-of-distribution input for this untrained model and it performs well with moderate image corruption. Additional discussion about the two could be useful, as for the Deep Image Prior a similar patter is observed where denoising requires explicit regularization (early stopping or gradient noise for DIP) but image completion and compressed sensing do not. Also, there have been many improvements to DCGAN over the years that might ameliorate the problems that were observed in reconstruction, but I don\u2019t fault the authors much for this as it can be difficult training models like StyleGAN even at 64x64 sizes.\n\nIt might also be interesting to know whether the good performance on out-of-distribution inputs is due to the exact invertibility or the log-likelihood objective, although I would guess that it is the latter. On way to test this would be training the GLOW model with an adversarial objective instead of NLL as done in [1].\n\nMinor Comments:\n\nFigure 4 would probably be better with a logarithmic scaling for # of measurements\n\nI did not understand the comment about sublevel sets of the data misfit term being inverse images of cylinders, maybe this could use some elaboration.\n\n[1] https://arxiv.org/abs/1705.08868\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575681350441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Reviewers"], "noninvitees": [], "tcdate": 1570237736066, "tmdate": 1575681350455, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review"}}}, {"id": "HyxURBHiiH", "original": null, "number": 4, "cdate": 1573766606453, "ddate": null, "tcdate": 1573766606453, "tmdate": 1573766606453, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "SJeePN_AFB", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment", "content": {"title": "Comparison to Unlearned Methods and Other Clarifications", "comment": "Thank you for your thorough reading of the paper.  We agree that a comparison to Deep Image Prior is quite interesting.  This has actually been done in another paper submitted to this conference [https://openreview.net/pdf?id=rkegcC4YvS].  Technically, they compare the DCGAN to the Deep Decoder, which is an underparameterized Deep Image Prior with simpler architecture and comparable performance.  Figure 1 (lower panels) in that paper demonstrate that the Deep Decoder underperforms the DCGAN when there are few enough measurements (m<500 in the same problem size as our experiments).  Also that figure shows the Deep Decoder gives consistently lower PSNRs than we report for INNs across the entire range of undersampling ratios.  In the case of significantly under sampled measurements, the Deep Decoder is over 5 dB worse in PSNR.  We will add a remark to this effect in the camera ready, if accepted.\n\nIt is a great suggestion to determine if out-of-distribution performance is due to invertibility or log-likelihood optimization.  A good argument can be made on both sides.  We are attempting to do this in time for the camera-ready.  Getting adversarial training to converge is challenging, but we will try and will add a remark to the camera ready.\n\n\nWe have clarified the comment about the sublevel sets.  The remark was intended to say that because of invertibility of the model G, there are no local minima (aside from global minima) of the data misfit term in z-space (||A G(z) - y||^2).  This ensures that the latent optimization we propose has a favorable landscape for convergence."}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgkbyHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1528/Authors|ICLR.cc/2020/Conference/Paper1528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154686, "tmdate": 1576860543937, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment"}}}, {"id": "rkgAISBisB", "original": null, "number": 3, "cdate": 1573766485736, "ddate": null, "tcdate": 1573766485736, "tmdate": 1573766485736, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "rkxWorWy5H", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment", "content": {"title": "Strong empirical results under a principled new framework for inversion", "comment": "Thank you for the thoughtful comments. \n\nNovelty of the paper:  The primary novelty of this paper is the proof of concept that invertible neural networks (INNs), out of the box, are surprisingly effective image priors for inverse problems, especially on out-of-distribution images.  This behavior was not known before this paper and can not be found in the previous literature either on invertible neural networks or on the literature in signal recovery.  There was one paper that uses INNs to directly learn a specific forward map, returning the inverse for free, but this method would need to be retrained for every variation of every inverse problem.   As a result of our paper, a practitioner who is building an image prior for a given distribution class aught to carefully consider the option of training an invertible net on their desired signal class.  (Naturally, they should also consider other methods too in order to see what works best for their problem).  Without this work, it is likely one might not think to give INNs a try because they are a substantially different architecture than everything else in the literature.  Other novelties of the paper are: in denoising, we introduce a formulation that directly optimizes image likelihood (demonstrating the strength of INNs in density estimation), and in compressed sensing we introduce formulation (2), which surprisingly works best with no direct likelihood penalization (gamma=0).  \n\nComparison to other methods: The purpose of this paper is to point out the promise of an entirely different framework for generative image priors for inverse problems.  We specifically did not put bells and whistles on the Invertible Neural Networks we trained because we wanted to show how much better the out-of-the-box performance of INNs was compared to GANs. Much followup work has happened with GANs in an attempt to lower their representation error.  These include Image Adaptive GANs and Latent Convolutional Models, which both use ideas from untrained neural networks (such as optimizing the weights of a neural network at inversion time).  Similar ideas could also be used for our Invertible Neural Network models, which will similarly make their level of performance even greater. \n\nClarity of paper:  If accepted, at the camera ready, we will clarify any sentences that the reviewer finds unclear.  We will also seek additional eyes before the camera ready in order to identify which sentences need additional clarity.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgkbyHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1528/Authors|ICLR.cc/2020/Conference/Paper1528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154686, "tmdate": 1576860543937, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment"}}}, {"id": "rygi7BBjjS", "original": null, "number": 2, "cdate": 1573766435034, "ddate": null, "tcdate": 1573766435034, "tmdate": 1573766435034, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "Bye3icUIqB", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment", "content": {"title": "Strong empirical results under a principled new framework for inversion", "comment": "Thank you for the thoughtful comments. \n\nWorthiness of being a full paper:  We argue that this work is worthy of being a full paper because it shows impressive empirical results for a principled signal recovery paradigm that address the central challenge facing generative models as image prior.  That central challenge is representation error, and our principled solution is invertible neural networks which can be trained by directly optimizing likelihood of test images.  The out-of-distribution performance we report is particularly important:  if one were to train a GAN for MRI images, it will be impossible to ensure all possible pathologies are in the training set, and thus images must generalize beyond their training data, which invertible nets do.  Given the substantial costs of using INNs, this paper provides a significant contribution to the field by demonstrating feasibility of an approach that might initially appear unlikely to work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgkbyHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1528/Authors|ICLR.cc/2020/Conference/Paper1528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154686, "tmdate": 1576860543937, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment"}}}, {"id": "HylG0NrosH", "original": null, "number": 1, "cdate": 1573766346165, "ddate": null, "tcdate": 1573766346165, "tmdate": 1573766346165, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "SJeomuT_cB", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment", "content": {"title": "Thoughts on Representation Error and Other Metrics", "comment": "Thank you for the careful reading of the paper. We agree that it is indeed quite interesting to see where are the sources of error in the DCGAN prior.  We did not include this in the paper because it is already included in the Bora et al. paper.  In Section 6.3 of their paper, they show that the dominant source of error is representation error (as opposed to measurement error or optimization error).  We have added a remark in Section 3.2 to this effect in the paper so that other readers can be aware of this observation.  \n\nIf the paper is accepted, in the camera ready supplemental material, we intend to include plots of recovery performance as measured by SSIM for the denoising and compressed sensing problems.  We already have presented the results in the MSE metric in the supplemental materials."}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgkbyHKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1528/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1528/Authors|ICLR.cc/2020/Conference/Paper1528/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154686, "tmdate": 1576860543937, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Authors", "ICLR.cc/2020/Conference/Paper1528/Reviewers", "ICLR.cc/2020/Conference/Paper1528/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Comment"}}}, {"id": "rkxWorWy5H", "original": null, "number": 2, "cdate": 1571915160985, "ddate": null, "tcdate": 1571915160985, "tmdate": 1572972456772, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes to employ the likelihood of the latent representation of images as the optimization target in the Glow (Kingma and Dhariwal, 2018) framework. The authors argue that to optimize the ''proxy for image likelihood'' has two advantages: First, the landscapes of the surface are more smooth; Second, a latent sample point in the regions that have a low likelihood is able to generate desired outcomes. In the experimental analysis, the authors compare their proposed method with several baselines and show prior performance.\n\nThis paper has three major flaws and should be clearly rejected. \nFirst, the novelty of this paper is trivial, in my opinion, the Eq. 2 is the only contribution of this paper.\nSecond, the experimental results are not convincing, almost all the methods proposed after 2015 have better performance compared to these baseline methods.\nThird, there are a lot of claims in this paper have been made without clarification, I have huge troubles in understanding certain sentences.\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575681350441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Reviewers"], "noninvitees": [], "tcdate": 1570237736066, "tmdate": 1575681350455, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review"}}}, {"id": "Bye3icUIqB", "original": null, "number": 3, "cdate": 1572395683856, "ddate": null, "tcdate": 1572395683856, "tmdate": 1572972456730, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors extend the invertible generative model of Kingma et. al. to image inverse problems. Specifically, they use Generator trained within the Glow framework as an image prior for de-noising, inpainting and compressed sensing tasks.\nDuring training, a heuristic adjustment to the objective is made allowing optimization of latent variable norm instead of image log likelihood. This seemed critical for convergence of image inverse tasks. The use of Glow prior was shown to be beneficial for all inverse tasks. Experiments were limited to face images from celebA database. While the proposal demonstrates improved empirical performance, it seems to be the only contribution of this paper. Taking an existing model and applying it to a problem where similar extensions have been tried (GAN etc) does not seem quite worthy of a full paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575681350441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Reviewers"], "noninvitees": [], "tcdate": 1570237736066, "tmdate": 1575681350455, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review"}}}, {"id": "SJeomuT_cB", "original": null, "number": 4, "cdate": 1572554786832, "ddate": null, "tcdate": 1572554786832, "tmdate": 1572972456685, "tddate": null, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "invitation": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper investigates the performance of invertible generative models for solving inverse problems. They argue that their most significant benefit over GAN priors is the lack of representation error that (1) enables invertible models to perform well on out-of-distribution data and (2) results in a model that does not saturate with increased number of measurements (as observed with GANs). They use a pre-trained Glow invertible network for the generator and solve a proxy for the maximum likelihood formulation of the problem, where the likelihood of an image is replaced by the likelihood of its latent representation. They demonstrate results on problems such as denoising, inpainting and compressed sensing. In all these applications, the invertible network consistently outperforms DCGAN across all noise levels/number of measurements. Furthermore, they demonstrate visually reasonable results on natural images significantly different from those in the training dataset.\n\nThe idea of using invertible networks for estimating a specific forward process is not new, as the authors also pointed out. The contribution of this paper is that they use a pre-trained invertible model as a prior in various tasks not known in training time and support their technique with experimental results and therefore I would recommend accepting this paper. \n\nSince one of the main arguments in the paper is how the lack of representation error benefits the Glow prior compared to DCGAN prior, it would be interesting to see the representation error quantitatively for the DCGAN results and how it contributes to the total error.  Moreover, demonstrating the comparison results in other metrics than PSNR (MSE, SSIM) would be interesting and more comprehensive."}, "signatures": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1528/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Invertible generative models for  inverse problems: mitigating representation error and dataset bias", "authors": ["Muhammad Asim", "Ali Ahmed", "Paul Hand"], "authorids": ["msee16001@itu.edu.pk", "ali.ahmed@itu.edu.pk", "p.hand@northeastern.edu"], "keywords": ["Invertible generative models", "inverse problems", "generative prior", "Glow", "compressed sensing", "denoising", "inpainting."], "TL;DR": "Invertible generative neural networks provide effective natural image priors for inverse problems, outperforming GAN and Lasso priors in Compressive Sensing Problems, while exhibiting strong out-of-distribution performance.", "abstract": "Trained generative models have shown remarkable performance as priors for inverse problems in imaging.  For example, Generative Adversarial Network priors permit recovery of test images from 5-10x fewer measurements than sparsity priors.  Unfortunately, these models may be unable to represent any particular image because of architectural choices, mode collapse, and bias in the training dataset. In this paper, we demonstrate that invertible neural networks, which have zero representation error by design, can be effective natural signal priors at inverse problems such as denoising, compressive sensing, and inpainting.   Our formulation is an empirical risk minimization that does not directly optimize the likelihood of images, as one would expect.  Instead we optimize the likelihood of the latent representation of images as a proxy, as this is empirically easier.\nFor compressive sensing, our formulation can yield higher accuracy than sparsity priors across almost all undersampling ratios.  For the same accuracy on test images, they can use 10-20x fewer measurements.  We demonstrate that invertible priors can yield better reconstructions than sparsity priors for images that have rare features of variation within the biased training set, including out-of-distribution natural images.  ", "pdf": "/pdf/90ee21b332b4fcc686df54cb6683bfeb6877f8ab.pdf", "code": "https://drive.google.com/file/d/1oqm_fnh3l7NP0Dycxq744mbH_-SU-KIf/view", "paperhash": "asim|invertible_generative_models_for_inverse_problems_mitigating_representation_error_and_dataset_bias", "original_pdf": "/attachment/6d913ff294e371d5b8a989fa767650026f6bb667.pdf", "_bibtex": "@misc{\nasim2020invertible,\ntitle={Invertible generative models for  inverse problems: mitigating representation error and dataset bias},\nauthor={Muhammad Asim and Ali Ahmed and Paul Hand},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgkbyHKDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgkbyHKDS", "replyto": "BJgkbyHKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575681350441, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1528/Reviewers"], "noninvitees": [], "tcdate": 1570237736066, "tmdate": 1575681350455, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1528/-/Official_Review"}}}], "count": 10}