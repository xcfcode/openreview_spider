{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396658389, "tcdate": 1486396658389, "number": 1, "id": "BkcgazIul", "invitation": "ICLR.cc/2017/conference/-/paper527/acceptance", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The proposed method doesn't have enough novelty to be accepted to ICLR."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396659013, "id": "ICLR.cc/2017/conference/-/paper527/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396659013}}}, {"tddate": null, "tmdate": 1481925525275, "tcdate": 1481917096160, "number": 7, "id": "SJl2MabEe", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "SkK4cjZ4g", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "writers": ["~Dawei_Li1"], "content": {"title": "Novelty & Technical Differences & Best Performance", "comment": "The reviewer missed the key points in our paper. There exist clear and significant differences between our work and previous works. \n\nWe are not \"replacing a pooling layer to a convolution layer of stride 2\" which was in fact proposed in another paper \"\u201cSTRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET\u201d. Instead, our method merges various types of non-tensor layers (i.e., pooling, LRN etc.) and their bottom convolution layer to a new convolution layer (i.e., the \"rebirth\" layer).  This is how we achieve significantly improved efficiency and accelerated model execution.   \n\nWe view this process as \"rebirth\" of a new layer  instead of \"a new *type* of layer\" as an analogy to help readers' catch up the idea and understand the merit of work. \n\nIn addition, from the reference (https://arxiv.org/abs/1605.02346), we did not find the similar operation as our algorithm which optimizes multiple layers with a new layer. As indicted in Section 4 (tasks and datasets section in reference), \u201cFirst, a shallow CNN which consists of six layers each followed by a rectified linear unit [32] and Batch Normalization [33]. The first four layers include max pooling with stride 2, leading to an effective stride of 16,\u201d  the method in reference relied on traditional network structure without applying the optimization schemes as ours.\n\n\nRegarding the novelty, to the best of our knowledge, this is *first* one that leverages merging of different types of layers (including  *non-tensor* layers) for acceleration of CNN models on mobile devices.  This is also the *best* method (see Table below) reported in accelerating the CNN on mobile devices.  It has technical novelty, and truly works pretty well in reality.    \n\nIf anyone has questions, we are very happy to do competitions on mobile devices.\n\nGiving all the above considerations, we are very disappointed to see the reviewer viewing our work as \"misleading and adding noise to the field. \" "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "tmdate": 1481912096784, "tcdate": 1481864169285, "number": 1, "id": "Sk-lElZNg", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "SJGyNcdMx", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "writers": ["~Dawei_Li1"], "content": {"title": "Architecture in Detail and Open Source", "comment": "(1) It is not always clear in the write-up how the depth (number of feature maps) of the merged layers relate to the depth of the layers before merging. It would be helpful to exhaustively list the depths of the layers so that readers can exactly reproduce the experiments.\n\nAns: The depth of feature maps for a rebirth layer is depending on that of the layers before merging. We enforce this to ensure the shape of its top layer remain unchanged and maintain the knowledge (i.e, the weights of its top layer are preserved).  We have listed the details of the merging in our revision and we also plan to release the entire model architectures designed in our paper (in caffe\u2019s prototxt format). \n\n(2) I also see no link to open-source code reproducing the experiments. Do you intend to provide it?\n\nAns: We plan to release the code after final decision. Please refer to our paper for model details. We have already attached the structure details in the revision. Please have a check. This is our first step for open-source. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "tmdate": 1481910833304, "tcdate": 1481910833304, "number": 3, "id": "SkK4cjZ4g", "invitation": "ICLR.cc/2017/conference/-/paper527/official/review", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "One of the main idea of this paper is to replace pooling layers with convolutions of stride 2 and retraining the model. Authors merge this into a new layer and brand it as a new type of layer. This is very misleading and adding noise to the field. And using strided convolutions rather than pooling is not actually novel (e.g. https://arxiv.org/abs/1605.02346).\nWhile the speed-up obtained are good, the lack of novelty and the rebranding attempt make this paper not a good fit for ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553965, "id": "ICLR.cc/2017/conference/-/paper527/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper527/AnonReviewer3", "ICLR.cc/2017/conference/paper527/AnonReviewer1", "ICLR.cc/2017/conference/paper527/AnonReviewer2"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553965}}}, {"tddate": null, "tmdate": 1481883787897, "tcdate": 1481883604166, "number": 6, "id": "r13AySbNl", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "B1fGNaBmx", "signatures": ["~Deguang_Kong1"], "readers": ["everyone"], "writers": ["~Deguang_Kong1"], "content": {"title": "Technical Novelty and Details & State-of-the-art Method & Very Competitive Empirical Results", "comment": "We are significantly improving the quality of paper.\n\nWe update our paper and rebuttals by emphasizing technical contribution,  illustrating technical details, eliminating  ambiguity and providing very strong experimental comparisons against state-of-the-art methods (including recent ICLR'16 and ICLR'17 submitted paper). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "tmdate": 1481882744830, "tcdate": 1481865279885, "number": 2, "id": "SkuBOeWNe", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "r111uFkXl", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "writers": ["~Dawei_Li1"], "content": {"title": "Replacing vs. DeepRebirth", "comment": "We only find one paper \u201cSTRIVING FOR SIMPLICITY: THE ALL CONVOLUTIONAL NET\u201d that replaces pooling layer with strided convolution layer. The key differences between deep rebirth and this work are: \n\n1.  We focus on accelerating the overall model execution speed instead of replacing a pooling layer with a convolution layer which may even slow down the execution.\n2.  Our approach is a generic method for model acceleration, which is not limited in optimizing pooling layer but also can be applied in other types of non-tensor layers such as LRN, BatchNorm. \n\nWe have added the comparison in our revision (in related work section)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "tmdate": 1481882498748, "tcdate": 1481868654202, "number": 3, "id": "ryIOSbZVl", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "S1xHuO_7g", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "writers": ["~Dawei_Li1"], "content": {"title": "Model Design Details", "comment": "(1) I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot.\n\nAns: Well, we name our approach as \u201cdeep-rebirth\u201d because in our proposed speed optimization pipeline, we have replaced multiple deep network layers which run much slower compared to the newly generated and faster layer (e.g., convolution layer in our experiment) like a rebirth.\n\n(2) There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them. It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes.\n\nAns: As far as we know, the operator fusion without retraining can only be applied to some specific structure, i.e.,  batch normalization or mean normalization after convolution. This kind of operator fusion can\u2019t be applied to other kinds of layers, such as pooling, LRN, etc. Our pipeline is much more general. \n\nThe deep learning framework such as Theano and XLA performs the operator fusion to reduce the memory. However, the execution time of these models may not be accelerated due to the use of same architectures (i.e., performing exactly the same set of floating-point operations).  If we view them as the baseline, their execution time would be much \u201chigher\u201d than deep-rebirth. In contrast, our method re-trained the rebirth layers and significantly speeds up the execution time.\n\n(3) Batchnorm can be folded into convolution layers without retraining by scaling the weights. Were they folded into the baseline figures reported in Table 7?\n\nAns: We agree that batchnorm can be folded into convolution layer without retraining by scaling the weights. The results listed in Table 7 also include other layers\u2019  optimization, e.g., pooling, not limited to batchnorm. \n\n(4) At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments. Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper. Not a requirement obviously.\n\nAns:  We have listed the details in our revision. We plan to release the code after final decision. Our first step for open-source would be the release of the model architectures (in caffe\u2019s prototxt format) used in our paper.\n\n(5) Retraining: how much time (epochs) does the retraining take? Did you consider using any form of distillation?\n\nAns: Each step of our optimization pipeline takes around 2 epochs (in section 4.1.1). For GoogleNet, it takes 9 steps and 18 epochs. We regard this training time (or rebirth time) as a one-time cost and once trained it can be deployed to any mobile devices. \n\nOur \u201crebirth\u201d method can be considered as a special form of distillation that transfers the knowledge from the cumbersome substructure of multiple layers to the rebirthed accelerated substructure. Different from traditional distillation method, our approach adopts layer-wise optimization and maintains the knowledge of the rest layers. Also, our method utilizes a softmax at the end of our CNN to optimize classification accuracy. \n\n(6) Interesting set of experiments. This paper needs a lot of improvements to be suitable for publication.\n\nAns: Thanks for your appreciation of our paper. We are constantly improving the quality of our paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "tmdate": 1481882180882, "tcdate": 1481877073029, "number": 4, "id": "SJYUUXZ4x", "invitation": "ICLR.cc/2017/conference/-/paper527/public/comment", "forum": "SkwSJ99ex", "replyto": "B1O_F2eVl", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "writers": ["~Dawei_Li1"], "content": {"title": "Technical Novelty & Model Applicability & Performance Comparisons ", "comment": "(1) There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n\nAns: We agree that there are many different kinds of model compression techniques. However, these algorithms mainly focus on reducing the model size instead of improving the running speed, especially, the acceleration of the execution time of deep model on mobile devices is not evident. We already compared with several state-of-the-art works in the paper. As listed in Table 5, in terms of running speed, runtime memory, energy, our approach performs significant better than GoogleNet-Tucker (ICLR'16), \u00a0Squeezenet (https://openreview.net/forum?id=S1xh5sYgx) and others.\u00a0The details of comparison are listed as follows:\n\n-----------------------|-----DeepRebirth-----|----SqueezeNet--|--Tucker (ICLR'16)*----|--GoogleNet---|\nAcc\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  |\t      86.5%\t  |\t      80.3%\t |     \t        85.7%          |\t        88.9%      |\t\nExecution time    |         65.34 ms      |       75.34 ms        |\t     342.5 ms        |       424.7 ms    |\nEnergy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  |\t        226 mJ      |          288 mJ        |             902 mJ         |          984 mJ    |\nMemory size\u00a0\u00a0\u00a0    |         14.8 MB       |           36.5 MB      |           35.8 MB         |         33.2 MB    |\nParameter size   |       11.99 MB       |         4.72 MB         |            14.38 MB      |        26.72 MB   |\n*The number is based on our implementation of the algorithm in ICLR'16 paper. \n\n(2) The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n\nAns: Our proposed approach can be applied to current general deep neural network structures that consist of different layers, e.g., convolution, pooling, LRN, etc. The reason why we used GoogleNet, Resnet and Alexnet is due to their popularity, as evident in current model compression work, e.g,  Song Han. et al. [ICLR\u201916],  Yong-Deok Kim et al. [ICLR\u201916].\n\n(3) Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nAns:  As far as we know, only mean variance normalization layer and batch normalization layer can all be absorbed without retraining, however, the operation can be only applied to two-layers and needs specific structure \u2013 normalization layer right after convolution. \n\nTo address these limitations, we proposed a general approach that can be applied to pooling, LRN, batchnorm and convolution, etc. \n \n(4) BTW, the DNN low-rank approximation technique was first proposed in speech recognition.\n\nAns: Thanks for the comment.  We have already included this reference in revision. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287536120, "id": "ICLR.cc/2017/conference/-/paper527/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287536120}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481878623303, "tcdate": 1478299454713, "number": 527, "id": "SkwSJ99ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkwSJ99ex", "signatures": ["~Dawei_Li1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481849200086, "tcdate": 1481849200079, "number": 2, "id": "B1O_F2eVl", "invitation": "ICLR.cc/2017/conference/-/paper527/official/review", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes to reduce model size and evaluation time of deep CNN models on mobile devices by converting multiple layers into single layer and then retraining the converted model. The paper showed that the computation time can be reduced by 3x to 5x with only 0.4% accuracy loss on a specific model.\n\nReducing model sizes and speeding up model evaluation are important in many applications. I have several concerns:\n\n1. There are many techniques that can reduce model sizes. For example, it has been shown by several groups that using the teacher-student approach, people can achieve the same and sometimes even better accuracy than the teacher (big model) using a much smaller model. However, this paper does not compare any one of them.\n2. The technique proposed in this paper is limited in its applicability since it's designed specifically for the models discussed in the paper. \n3. Replacing several layers with single layer is a relatively standard procedure. For example, the mean variance normalization layer and batch normalization layer can all be absorbed without retraining or losing accuracy.\n\nBTW, the DNN low-rank approximation technique was first proposed in speech recognition. e.g., \n\nXue, J., Li, J. and Gong, Y., 2013, August. Restructuring of deep neural network acoustic models with singular value decomposition. In INTERSPEECH (pp. 2365-2369).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553965, "id": "ICLR.cc/2017/conference/-/paper527/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper527/AnonReviewer3", "ICLR.cc/2017/conference/paper527/AnonReviewer1", "ICLR.cc/2017/conference/paper527/AnonReviewer2"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553965}}}, {"tddate": null, "tmdate": 1481308215948, "tcdate": 1481308215941, "number": 1, "id": "S1xHuO_7g", "invitation": "ICLR.cc/2017/conference/-/paper527/official/review", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "content": {"title": "Interesting analysis, flawed paper, no answer to reviewer questions", "rating": "4: Ok but not good enough - rejection", "review": "This paper looks at the idea of fusing multiple layers (typically a convolution and a LRN or pooling layer) into a single convolution via retraining of just that layer, and shows that simpler, faster models can be constructed that way at minimal loss in accuracy. This idea is fine. Several issues:\n- The paper introduces the concept of a 'Deeprebirth layer', and for a while it seems like it's going to be some new architecture. Mid-way, we discover that 1) it's just a convolution 2) it's actually a different kind of convolution depending on whether one fuses serial or parallel pooling layers. I understand the desire to give a name to the technique, but in this case naming the layer itself, when it's actually multiple things, non of which are new architecturally, confuses the argument a lot.\n- There are ways to perform this kind of operator fusion without retraining, and some deep learning framework such as Theano and the upcoming TensorFlow XLA implement them. It would have been nice to have a baseline that implements it, especially since most of the additional energy cost from non-fused operators comes from the extra intermediate memory writes that operator fusion removes.\n- Batchnorm can be folded into convolution layers without retraining by scaling the weights. Were they folded into the baseline figures reported in Table 7?\n- At the time of writing, the authors have not provided the details that would make this research reproducible, in particular how the depth of the fused layers relates to the depth of the original layers in each of the experiments.\n- Retraining: how much time (epochs) does the retraining take? Did you consider using any form of distillation?\nInteresting set of experiments. This paper needs a lot of improvements to be suitable for publication.\n- Open-sourcing: having the implementation be open-source always enhances the usefulness of such paper. Not a requirement obviously.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553965, "id": "ICLR.cc/2017/conference/-/paper527/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper527/AnonReviewer3", "ICLR.cc/2017/conference/paper527/AnonReviewer1", "ICLR.cc/2017/conference/paper527/AnonReviewer2"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553965}}}, {"tddate": null, "tmdate": 1481131017625, "tcdate": 1481131017618, "number": 2, "id": "B1fGNaBmx", "invitation": "ICLR.cc/2017/conference/-/paper527/official/comment", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "content": {"title": "Please respond to pre-review questions.", "comment": "I would like to submit final review by end of week.\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287535985, "id": "ICLR.cc/2017/conference/-/paper527/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers", "ICLR.cc/2017/conference/paper527/areachairs"], "cdate": 1485287535985}}}, {"tddate": null, "tmdate": 1480722390562, "tcdate": 1480722390558, "number": 2, "id": "r111uFkXl", "invitation": "ICLR.cc/2017/conference/-/paper527/pre-review/question", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer2"], "content": {"title": "Related work", "question": "Replacing pooling layers with strided convolutions doesn't seem like a new idea, yet I don't see mention or comparison to other work studying removing pooling layers?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959230889, "id": "ICLR.cc/2017/conference/-/paper527/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper527/AnonReviewer3", "ICLR.cc/2017/conference/paper527/AnonReviewer2"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959230889}}}, {"tddate": null, "tmdate": 1480266714103, "tcdate": 1480266714096, "number": 1, "id": "SJGyNcdMx", "invitation": "ICLR.cc/2017/conference/-/paper527/pre-review/question", "forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "signatures": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper527/AnonReviewer3"], "content": {"title": "Depth of merged layers?", "question": "It is not always clear in the write-up how the depth (number of feature maps) of the merged layers relate to the depth of the layers before merging. In the case of branch merging, you suggest that the final depth is the sum total of the merged layers' depth. In streamline merging, it's not clear what happens. Later in the write-up, you say: ' For layer inception 3a/3x3 reduce, we reduce the number of output feature maps from 96 to 48.' It would be helpful to exhaustively list the depths of the layers so that readers can exactly reproduce the experiments.\nI also see no link to open-source code reproducing the experiments. Do you intend to provide it?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DeepRebirth: A General Approach for Accelerating Deep Neural Network Execution on Mobile Devices", "abstract": "Deploying deep neural networks on mobile devices is a challenging task due to computation complexity and memory intensity. Existing works solve this problem by reducing model size using weight approximation methods based on dimension reduction (i.e., SVD, Tucker decomposition and Quantization). However, the execution speed of these compressed models are still far below the real-time processing requirement of mobile services. To address this limitation, we propose a novel acceleration framework: DeepRebirth by exploring the deep learning model parameter sparsity through merging the parameter-free layers with their neighbor convolution layers to a single dense layer. The design of DeepRebirth is motivated by the key observation: some layers (i.e., normalization and pooling) in deep learning models actually consume a large portion of computational time even few learned parameters are involved, and acceleration of these layers has the potential to improve the processing speed significantly. Essentially, the functionality of several merged layers is replaced by the new dense layer \u2013 rebirth layer in DeepRebirth. In order to preserve the same functionality, the rebirth layer model parameters are re-trained to be functionality equivalent to the original several merged layers. The extensive experiments performed on ImageNet using several popular mobile devices demonstrate that DeepRebirth is not only providing huge speed-up in model deployment and significant memory saving but also maintaining the model accuracy, i.e., 3x-5x speed-up and energy saving on GoogLeNet with only 0.4% accuracy drop on top-5 categorization in ImageNet. Further, by combining with other model compression techniques, DeepRebirth offers an average of 65ms model forwarding time on each image using Samsung Galaxy S6 with only 2.4% accuracy drop. In addition, 2.5x run-time memory saving is achieved with rebirth layers.", "pdf": "/pdf/3bca0d7ff08404194f79153f620f8e620475d1b4.pdf", "paperhash": "li|deeprebirth_a_general_approach_for_accelerating_deep_neural_network_execution_on_mobile_devices", "keywords": [], "conflicts": ["lehigh.edu", "samsung.com"], "authors": ["Dawei Li", "Xiaolong Wang", "Deguang Kong", "Mooi Choo Chuah"], "authorids": ["dal312@lehigh.edu", "visionxiaolong@gmail.com", "doogkong@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959230889, "id": "ICLR.cc/2017/conference/-/paper527/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper527/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper527/AnonReviewer3", "ICLR.cc/2017/conference/paper527/AnonReviewer2"], "reply": {"forum": "SkwSJ99ex", "replyto": "SkwSJ99ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper527/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959230889}}}], "count": 14}