{"notes": [{"id": "HyxnZh0ct7", "original": "S1xHc_n5tQ", "number": 1211, "cdate": 1538087940245, "ddate": null, "tcdate": 1538087940245, "tmdate": 1563978175279, "tddate": null, "forum": "HyxnZh0ct7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 28, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1eEpO66S4", "original": null, "number": 22, "cdate": 1550862524338, "ddate": null, "tcdate": 1550862524338, "tmdate": 1554484583485, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Camera-ready.", "comment": "The ICLR'19 camera-ready version of the paper has been uploaded.\nCode available at https://github.com/bertinetto/r2d2"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "ByeHwtpR4E", "original": null, "number": 21, "cdate": 1549879645025, "ddate": null, "tcdate": 1549879645025, "tmdate": 1549994133575, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "BkxDPnDZMV", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Thank you for your contribution", "comment": "We agree on the importance of making the research in ML (or any field) accessible and reproducible - we are glad that initiatives such as the reproducibility challenge exist.\n\nWe are also glad that the authors were able to reproduce our main findings, despite not having had access to our implementation and using a different framework (we used PyTorch and we are working on finalizing the code release).\n\nIn response to specific details of the report:\n- We agree that the details of stride and padding amounts are missing, and we will update the paper accordingly. This should also resolve the difference in feature dimension between our paper and the replication.\n- We believe that our sentence \u201cTraining is stopped when the error on the meta-validation set does not decrease meaningfully for 20,000 episodes\u201d has been misinterpreted, as the authors say: \u201cwe opted to meta-train for a fixed 20k iterations\u201d.\nWhat we meant is that we performed early-stopping if error does not decrease for a period of 20k episodes, not that we train for 20k episodes in total.\nClearly, in this way the total number of training epoch varies, but we observed that generally it stops around 60k-80k episodes. We will make this point more clear in the camera ready. This also means that our results were obtained with longer training than in the replication.\n- Re the sentence \u201cdifferent neural architectures should be taken into consideration when comparing results\u201d and direct comparison with MAML in general.\nThis comment refers to the fact that we did not report results on a 32-channels embedding in our experiments, which is instead what MAML uses.\nHowever, we believe that our experiments already show that performance is not simply the result of a trivial increase in capacity.\nTo demonstrate that, we reported both a) results of our method on a 64-channels embedding and b) results of three representative baselines (protonets, MAML and GNN) with our embeddings (the * in our tables)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "BkxDPnDZMV", "original": null, "number": 8, "cdate": 1546906719039, "ddate": null, "tcdate": 1546906719039, "tmdate": 1546906719039, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "We have carried out a  reproducibility analysis of this interesting paper on meta-learning. Some parameters and training methodologies, which would be required for full reproducibility, are not present in the manuscript at the time of writing:\n- stride of the convolutional filters\n- padding of the convolutional filters\n- a clear stopping criterion (<-> \"the error on the meta-validation set does not decrease meaningfully for 20k episodes\"),\n\nHowever, making reasonable assumptions, we were able to reproduce the most important part of the paper (R2D2) in TensorFlow and achieve similar results. We did not reproduce the LRD2 part of the paper, as we wanted to focus on the truly differentiable closed-form solver (R2D2). Most importantly, we were able to reproduce the increase in performance of the proposed method (with the given architecture) over some reproduced baseline results, which supports the conclusions in the original paper.\n\nThe different neural network architectures should be taken into consideration when comparing results. For example the MAML baseline of Finn et al. (2017) uses four convolutional blocks with [32, 32, 32, 32] filters, whereas this paper's four blocks employ a [96, 192,384, 512] scheme. Because of this we implemented R2D2 with both the architecture mentioned in the paper and the MAML baseline architecture. In our reproducibility report we show that when using the exact same baseline architecture as MAML, and standard training procedure, the improvement in performance of the proposed method is not clear.\n\nOur full reproducibility report is available at: https://github.com/reproducibility-challenge/iclr_2019/blob/c53e6c1ea8d0e158f66b7d70681fa6ecde6a4f2b/papers/LCAX-HyxnZh0ct7/LCAX.pdf\nOur codebase: https://github.com/ArnoutDevos/r2d2", "title": "ICLR 2019 Reproducibility Challenge key findings"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "H1eyie3leV", "original": null, "number": 1, "cdate": 1544761494564, "ddate": null, "tcdate": 1544761494564, "tmdate": 1545354519183, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Meta_Review", "content": {"metareview": "The reviewers disagree strongly on this paper. Reviewer 2 was the most positive, believing it to be an interesting contribution with strong results. Reviewer 3 however, was underwhelmed by the results. Reviewer 1 does not believe that the contribution is sufficiently novel, seeing it as too close to existing multi-task learning approaches.\n\nAfter considering all of the discussion so far, I have to agree with reviewer 2 on their assessment. Much of the meta learning literature involves changing the base learner *for a fixed architecture* and seeing how it affects performance. There is a temptation to chase performance by changing the architecture, adding new regularizers, etc., and while this is important for practical reasons, it does not help to shed light on the underlying fundamentals. This is best done by considering carefully controlled and well understood experimental settings. Even still, the performance is quite good relative to popular base learners.\n\nRegarding novelty, I agree it is a simple change to the base learner, using a technique that has been tried before in other settings (linear regression as opposed to classification), however its use in a meta learning setup is novel in my opinion, and the new experimental comparison regression on top of pre-trained CNN features helps to demonstrate the utility of its use in the meta-learning settings.\n\nWhile the novelty can certainly be debated, I want to highlight two reasons why I am opting to accept this paper: 1) simple and effective ideas are often some of the most impactful. 2) sometimes taking ideas from one area (e.g., multi-task learning) and demonstrating that they can be effective in other settings (e.g., meta-learning) can itself be a valuable contribution. I believe that the meta-learning community would benefit from reading this paper.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A closed form solver for the base learner is new in the meta-learning literature, and the experiments are sufficiently carried out to show its effectiveness."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1211/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352923601, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352923601}}}, {"id": "BkgTUolegV", "original": null, "number": 20, "cdate": 1544715093104, "ddate": null, "tcdate": 1544715093104, "tmdate": 1544718411149, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SJeA_NNyxE", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "We were responding to the claim that these papers had small novelty and thus our own as well, which we disagree with", "comment": "1) We wrote: \u201c\u201c[multi-task learning] is different to our work, and in general to all of the previous literature on meta-learning applied to few-shot classification (e.g. Finn et al. 2017, Ravi & Larochelle 2017, Vinyals et al. 2016, etc). Notably, these methods and ours take into account adaptation *already during the training process*, which requires back-propagating errors through the very fine-tuning process.\u201d\u201d\n\n2) R1 answered with: \u201c\u201cMerely because some other paper also had small novelty and got accepted in the past I can not see why this paper should also get accepted\u201d\u201d\n\n3) We then observed that *R1 did not refute any of our point of rebuttal* (long answer in this thread) and seems to be dismissive of the above papers, which are widely accepted by the community.\n\n> \u201c\u201c However, using a multi-task technique in meta-learning setting cannot be treated as a novel or original contribution.\u201d\u201d\nAgain, it is not what we do - we amply addressed this point both on OpenReview (last two answers to the reviewer) and in the paper.\n\nWe would like to repeat that if this were true, the baseline experiment we described (applying ridge regression in the manner that the reviewer refers to as standard) would not have been possible, since our method and the baseline would then be the same (which they are not -- both in methodology and results)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "SJeA_NNyxE", "original": null, "number": 7, "cdate": 1544664182159, "ddate": null, "tcdate": 1544664182159, "tmdate": 1544664182159, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "r1l23D05y4", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "The 3 meta-learning papers developed new techniques and/or models for meta-learning (which have never been proposed or used in multi-task learning), while this paper applies existing multi-task learning technique in the meta-learning setting. The contributions in these two cases are very different. I think it is misleading to indicating that the contribution of this paper is as novel as the 3 meta-learning papers.\n\nIt is fine to apply multi-task learning technique to the meta-learning problem. To some extent, meta-learning can be explained as a generalization of multi-task learning in the way that meta-learning applies to any set of tasks sampled from certain *task distribution*, while the set of tasks in multi-task learning are fixed. They both need knowledge transfer between different tasks. However, using a multi-task technique in meta-learning setting cannot be treated as a novel or original contribution. ", "title": "It is misleading to indicate that the paper is as novel as the 3 meta-learning papers"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "r1l23D05y4", "original": null, "number": 18, "cdate": 1544378291927, "ddate": null, "tcdate": 1544378291927, "tmdate": 1544378291927, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SyxWU3iYkE", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "3 papers with hundreds of citations (Finn et al., Ravi & Larochelle, Vinyals et al.) cannot be dismissed as \u201csome other paper also had small novelty\u201d", "comment": "The reviewer has not refuted any of the points we made above. Namely:\n\n- That meta-learning approaches (like ours) back-propagate errors through the fine-tuning process, a major departure from standard multi-task/transfer learning.\n- That *not doing so* incurs a large performance penalty, as demonstrated by our experiments.\n\nWe invite the reviewer to address these points, rather than just reiterate a subjective judgment over the value of meta-learning. While we respect this opinion, our paper cannot be rejected based solely on the reviewer\u2019s opinion that meta-learning papers are not novel in general (compared to multi-task learning).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "H1llepity4", "original": null, "number": 17, "cdate": 1544301799639, "ddate": null, "tcdate": 1544301799639, "tmdate": 1544301799639, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "Syg19STh6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "I respectfully disagree with Reviewer #2.", "comment": "Merely combining ridge regression (trivial, and nothing novel) inside meta-learning is not sufficentlynovel in my opinion. \n\nIn essence we agree to disagree. I request the AC to make a decision based on both our inputs. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "SyxWU3iYkE", "original": null, "number": 16, "cdate": 1544301640915, "ddate": null, "tcdate": 1544301640915, "tmdate": 1544301640915, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "H1e3St5u67", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "I still feel the novelty is very small (I'm reviewer #1)", "comment": "I disagree with Reviewer #2 and the authors about the novelty. The delta from just simple multi-task learning approach of eg Caruana 93 is extremely small -- the same algorithms are trivially extended to deal with meta-learning. The mere fact of using closed form ridge regression in this setting does not feel like sufficient contribution to warrant an ICLR paper to this reviewer. Merely because some other paper also had small novelty and got accepted in the past I can not see why this paper should also get accepted with minimal novel contributions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "HyeglRQ_JE", "original": null, "number": 15, "cdate": 1544203752502, "ddate": null, "tcdate": 1544203752502, "tmdate": 1544203752502, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "BJg6awLwk4", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Thanks", "comment": "Thank you for pointing us to this interesting paper! We agree that methods with limited inductive bias such as protonets are attractive, and there is indeed a good case for their performance scaling better with computation and data.\nWe are looking forward to try out the proposed testbed. One possible advantage of using our  R2-D2 with the deeper architectures of their setup is that we can concatenate activations from multiple layers together without increasing the computational burden of the base-learner thanks to the Woodbury identity."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "BJg6awLwk4", "original": null, "number": 6, "cdate": 1544148932756, "ddate": null, "tcdate": 1544148932756, "tmdate": 1544149183866, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "rygq4xgwkN", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "I understand your points. Overall, I like the idea of using closed-form base learner which also demonstrate good performance when the backbone network is shallow. However, as a practioner, I may not adopt the proposed method for now.\n\nIn my opinion, meta-learning is about learning a data-driven inductive bias for few-shot learning. Closed-form regression itself introduces a strong inductive bias which is not learned. Therefore, it is interesting to investigate whether the inductive bias of closed-form regression is needed when the backbone network gets deeper.\n\nAs shown in the Figure 3 of https://openreview.net/pdf?id=HkxLXnAcFQ , the performance gap between different meta-learning methods diminishes as the backbone gets deeper. One intersting point in the figure is that ProtoNet typically outperforms other methods when the network is deeper.", "title": "what happens when the backbone network gets deeper"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "rygq4xgwkN", "original": null, "number": 13, "cdate": 1544122418133, "ddate": null, "tcdate": 1544122418133, "tmdate": 1544122418133, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "r1e-G4YL1N", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "These improvements can be used in any few-shot learning methods. We outperform prototypical networks in an apples-to-apples comparison.", "comment": "We thank the anonymous commenter for pointing out a GitHub repo with improvements. We note that neither data augmentation nor the optimizer schedule are mentioned at all in the associated published paper.\n\nAdditionally, the mentioned improvements are not specific to prototypical networks (or to any method for that matter), and can also be applied to ours.  As such, we fail to see how this says anything about the merits of our proposal.\nIn our experiments, we compare against prototypical networks using the same setup of the original paper (Adam optimizer, halving LR every 20 epochs; no data augmentation).\nIn this fair comparison, we outperform it.\n\nWe would gain no knowledge by showing that \u201cproto-nets with data augmentation and optimizer improvements\u201d (as suggested) beats \u201cR2D2 with no data augmentation\u201d, or that \u201cMAML with a ResNet base\u201d beats \u201cR2D2 with 4 layers\u201d. These are apples-to-oranges comparisons which make any scientific conclusion very hard to draw.\n\nInstead, a proper comparison is to take the innovation of each paper -- the prototype layer in proto-nets, and the ridge regression layer in R2D2 -- and compare them, with everything else fixed. This includes data augmentation, as well as network model and initialization.\n\nCarefully controlled comparisons are a core part of the scientific method, and ignoring them will lead to unsubstantiated conclusions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "r1e-G4YL1N", "original": null, "number": 5, "cdate": 1544094729345, "ddate": null, "tcdate": 1544094729345, "tmdate": 1544094729345, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "BkljQHC-T7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "Using closed-form base learner is an interesting idea. However, the results are underwhelming. \n\nAs shown in https://github.com/gidariss/FewShotWithoutForgetting , Prototypical Networks can be quite powerful with some modifications. The modifications include:\n1. add data augmentation\n2. use SGD with momentum optimizer\n3. scale the output of the euclidean distance to a suitable range\n\nUsing a 4-Conv backbone with 64 channels, Protypical Networks are able achieve remarkable results in MiniImagenet:  1-shot: 53.30% +/- 0.79 5-shot: 70.33% +/- 0.65\n\nEven without data augmentation, in my experiments, Protypical Networks can still get 5-shot accuracy around 68.8%.\n\nConsidering this, the proposed method has not demonstrated superior empirical results than Protypical Network yet.", "title": "interesting idea but underwhelming results"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "r1gGLd-507", "original": null, "number": 9, "cdate": 1543276618355, "ddate": null, "tcdate": 1543276618355, "tmdate": 1543938721660, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "To all: Appendix now includes runtime analysis, 1-vs-all experiment and extended discussion", "comment": "We would like to thank both reviewers and anonymous commenters for their feedback and participation.\nIn light of the discussion, the Appendix of the paper has been updated:\n\n* Section B offers a runtime analysis which reveals that R2-D2 is several times faster than MAML and almost as fast as a simple (fixed) metric learning method such as prototypical networks, while still allowing per-episode adaptation.\n* Section A reports the accuracy of the 1-vs-all variant of LR-D2 (as suggested by AnonReviewer2), which is comparable with the one of R2-D2.\n* Finally, Section C extends the discussion sparked here on OpenReview about a) the nature of our contribution b) the disambiguation with the multi-task learning paradigm ."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "BkljQHC-T7", "original": null, "number": 2, "cdate": 1541690659324, "ddate": null, "tcdate": 1541690659324, "tmdate": 1543851966391, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "r1xPm1Kah7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Our proposal demonstrates results competitive with SNAIL despite using a much simpler architecture (SNAIL uses ResNet, we just use 4 conv layers).", "comment": "We thank the reviewer for the comments and questions.\n\n> \u201cWhy one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W?\u201d\nIn the case of logistic regression, the scaling and shifting is not needed, and we have \\hat{Y}=X\u2019W. This is because logistic regression is a classification algorithm, and directly outputs class scores. These scores are fed to the (cross-entropy) loss L.\n\nHowever, ridge regression is a regression algorithm, and its regression targets are one-hot encoded labels, which is only an approximation of the discrete problem (classification). This means that an extra calibration step is needed (eq. 6), to allow the network to tune the regressed outputs into classification scores for the cross-entropy loss L.\n\n> \u201cThe empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL\u201d\nOur method actually outperforms SNAIL on an apples-to-apples comparison, with the same number of layers. We would like to draw the reviewer\u2019s attention to the last paragraph of the \u201cMulti-class classification\u201d subsection (page 8).\n\nThe result mentioned by the reviewer uses a ResNet, while we use a 4-layer CNN to remain comparable to prior work. SNAIL with a 4-layer CNN ([11] Appendix B) performs much worse than our method (7.4% to 10.0% accuracy improvement).\n\nEven disregarding the great difference in architecture capacity, our proposal's performance coincides with SNAIL on miniImageNet 5way-5shot and it is comparable on 3 out of 4 Omniglot setups. We would have liked to establish a comparison also on CIFAR, but unfortunately the official code for SNAIL hasn\u2019t been released.\n\nBorrowing the words of AnonReviewer2: \u201cNotably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced.\u201d\n\nWe hope that this addresses the two concerns raised by the reviewer. We will be happy to answer any other question about the paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "Syg19STh6Q", "original": null, "number": 7, "cdate": 1542407559156, "ddate": null, "tcdate": 1542407559156, "tmdate": 1542407559156, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SklLz3csp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Re-using existing components in clever ways for new problems should be encouraged!", "comment": "I respectfully disagree with the argument regarding lack of novelty. Indeed, the authors did not invent the meta-learning framework, and they did not invent ridge regression. Yet the two of them had not been combined before in this way, and this combination is evidently beneficial. It does seem like a natural idea, but if it was so obvious, how come it wasn't done before?\n\nIt may be tempting to create complicated models to solve a problem, yielding \"more novel\" solutions. But this seems wrong if the same problem can be solved in a simpler way! I feel strongly that re-using existing components in clever ways that yield good results on new problems is an important contribution and should be encouraged."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "SklLz3csp7", "original": null, "number": 4, "cdate": 1542331406491, "ddate": null, "tcdate": 1542331406491, "tmdate": 1542331870396, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SyedD2rXaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "Thanks for your reply!\n\n> Clearly, the overall training framework is not novel and it is common in the few-shot learning literature. \n\nThanks for the clarification! But if the \"essential difference\" (asked in my first post and answered in your previous reply) is not the contribution, it is hardly to tell the essential novelty of this method.\n\n> We strongly disagree with the statement. This is exactly the nature of the contribution of most approaches for few-shot classification.\n\nI do not agree with this statement. Simply replacing the base learner and following the standard meta learning/few-shot learning  scheme sounds not novel to me. The claimed adaptation capability comes from the standard meta-learning scheme, while the claimed efficiency comes from the closed-form solver. Both are well known and common for years. \n\nYes MAML can be explained to be using SGD as base learner (but there are other more intuitive explanations), but they redesigned the learning procedure specifically for SGD, since SGD is a dynamic optimization algorithm rather than a model. Other meta-learning methods either proposes new algorithm or new model structure specifically for few-shot learning. BTW, I do not agree that \"some papers propose their methods in the similar way, so our paper also presents contribution of similar novelty\".\n\n>Our contribution is to use closed-form solvers such as ridge regression to tackle few-shot classification, which is novel in the literature and it is a non-trivial endeavor.\n\nUsing closed-form solver for sure can converge faster than using deep neural networks or doing second order optimization (like MAML). But this is an advantage of the existing closed-form solvers. In addition, as mentioned in your reply and paper, the fine-tuning still needs to backpropagate the error from the closed form solver to the pre-trained deep CNN. Together they still compose a deep model whose last layer is the closed-form solver, and each epoch of the fine tuning might need heavy computation (**This has been also pointed out by Reviewer 1**). Then the advantage of using shallow model is not clear: you can always find a good trade-off between fine tuning a large/small backbone model and a complex/simple base learner. Besides, logistic regression does not have a closed-form solver so the title is somehow misleading.\n\nOverall, I agree that using closed-form solver of a shallow model might have some practical value, especially in the case when you use a very powerful pre-trained CNN as the backbone model. However, I am not convinced that this is a novel contribution. \n", "title": "Using a classical linear model as base learner is not novel to me"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "H1e3St5u67", "original": null, "number": 6, "cdate": 1542134084261, "ddate": null, "tcdate": 1542134084261, "tmdate": 1542134084261, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SJlghKO937", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "This is a comment on a different technique than what we propose", "comment": "We thank the reviewer for the comment.\nHowever, we believe that the low score originates from a misunderstanding of our proposal.\nBelow, we try to bring some clarity by disambiguating between what the reviewer refers to and our method.\nIf our interpretation of what the reviewer refers to as \u201centirely common\u201d is incorrect, it would be great to be provided with at least one reference, so that we can continue the conversation on the same ground.\n\n> \u201cnovel contribution?\u201d , \u201ctraining multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common.\u201d\n\u201cIt is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last layer\u201d\n\nWe understand that the reviewer is hinting at the common multi-task scenario with a shared network and task-specific layers (e.g. Caruana 1993). He/she also refers to basic transfer learning approaches in which a CNN is first pre-trained on one dataset/task and then adapted to a different dataset/task by simply adapting the final layer(s) (e.g. Yosinski et al. \u201cHow transferable are features in deep neural Networks?\u201d - NIPS 2014; Chu et al. \u201cBest Practices for Fine-tuning Visual Classifiers to New Domains\u201d - ECCVw 2016).\n\nIf so, then this is significantly different to our work, and in general to all of the previous literature on meta-learning applied to few-shot classification (e.g. Finn et al. 2017, Ravi & Larochelle 2017, Vinyals et al. 2016, etc).\nNotably, these methods and ours take into account adaptation *already during the training process*, which requires back-propagating errors through the very fine-tuning process.\n\nWithin this setup, our main contribution is to propose an adaptation procedure based on closed-form regressors, which have the important characteristic of allowing different models for different episodes while still being fast because of 1) their convergence in one (R2-D2) or few (LR-D2) steps, 2) the use of the Woodbury identity, which is particularly convenient in the few-shot data regime, and 3) back-propagation through the closed-form regressor can be made efficient.\n\nTo better illustrate our point, we conducted a baseline experiment.\nFirst, we pre-trained the same 4-layers CNN architecture, but for a standard classification problem, using the same training samples as our method. We simply added a final fully-connected layer (with 64 outputs, like the number of classes in the training splits) and used the cross-entropy loss.\nThen, we used the convolutional part of this trained network as a feature extractor and fed its activation to our ridge-regression layer to produce a per-episode set of weights.\nOn miniImagenet, the drop in performance w.r.t. our proposed R2-D2 is very significant: 13.8% and 11.6% accuracy for the 1 and 5 shot problems respectively.\nResults are consistent on CIFAR, though less drastic: 11.5% and 5.9%.\n\nThis confirms that simply using a \u201cshared feature representation and task specific final layer\u201d as commented by the reviewer is not what we are doing and it is not a good strategy to obtain results competitive with the state-of-the-art in few-shot classification.\nInstead, it is necessary to enforce the generality of the underlying features during training explicitly, which we do by back-propagating through the fine-tuning procedure (the closed-form regressors).\n\nWe would like to conclude remarking that, probably, the source of confusion arises from the overlap that exists in general between the few-shot learning and the transfer/multi-task learning sub-communities.\nWe realize that the two have developed fairly separately while trying to solve very related problems, and unfortunately the similarities/differences are not acknowledged enough in few-shot classification papers, including our own. We intend to alleviate this problem in our related work section, and invite the reviewer to suggest more relevant works from this area.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "SyedD2rXaQ", "original": null, "number": 5, "cdate": 1541786719761, "ddate": null, "tcdate": 1541786719761, "tmdate": 1541786719761, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "Byg2xy8MpX", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "There is ample precedent in the few-shot learning literature for proposing new base learners as the main contribution.", "comment": "Thank you. \n\n> \u201cI understand that the main novelty here is to apply fine tuning on the test set (of tasks sampled for training) in meta-learning, instead of on the training data of a single supervised learning task (as we normally did in supervised learning).\u201d\n\nSorry but this is not claimed in the paper or in the answer above. Clearly, the overall training framework is not novel and it is common in the few-shot learning literature. In fact, we specifically wrote: \u201cOur training procedure (and indeed, all meta-learning methods for few-shot learning, such as MAML, SNAIL, etc) ...\u201d.\n\nThe point of our previous comment was simply to clarify why different episodes correspond to different sets of parameters.\n\n\n> \u201c\u201cchanging the model of base learners cannot be recognized as a novelty\u201d\nWe strongly disagree with the statement. This is exactly the nature of the contribution of most approaches for few-shot classification. For example, both MAML and prototypical networks use the same algorithm (SGD) in the external loop, while they vastly differ for the method used in the inner loop (SGD and nearest neighbour respectively).\n\nOur contribution is to use closed-form solvers such as ridge regression to tackle few-shot classification, which is novel in the literature and it is a non-trivial endeavor.\nAs stated by AR2: \u201c[it] strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods [e.g. prototypical networks]]) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged.\u201d\n\nBesides offering a trade-off with respect to existing techniques, our proposal also presents a significant practical value in terms of performance, as outlined in our experimental section.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "Byg2xy8MpX", "original": null, "number": 3, "cdate": 1541721843523, "ddate": null, "tcdate": 1541721843523, "tmdate": 1541721843523, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "rkevXdCb6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "Thanks a lot for your reply and explanation! I understand that the main novelty here is to apply fine tuning on the test set (of tasks sampled for training) in meta-learning, instead of on the training data of a single supervised learning task (as we normally did in supervised learning). However, I agree with AnonReviewer1: I do not think this work presents very original contributions. It applies the existing fine-tuning technique by following standard meta-learning setting, as many other meta-learning methods already did.\n\nFine tuning is an existing technique that can be generally applied to different learning settings. The basic idea is to update a pre-trained model and continue to train it on new training instances. In supervised learning, each training instance is a data point, and the learning goal is to minimize the training error on each data point. In meta-learning, each training instance is an (n-way k-shot) classification task, and the learning goal is to minimize the validation/test error on the test set of each training task. Therefore, fine tuning in meta-learning should be applied to the test sets of training tasks (as this paper does). In fact, in meta-learning, any training happening on task-shared part (e.g., meta-learner or shared pre-trained model) should minimize the error/loss on the test sets of training tasks. However, these are all well-known facts, derived from the very early optimization formulation of \"learning to learn\" (although meta-learning becomes very popular topic very recently). So they are not the contributions of this paper.\n\nIn addition, as the authors mentioned, many existing meta-learning methods use the same idea, the only difference here is that the base learner for each task changes to ridge/logistic regression model. But changing the model of base learners cannot be recognized as a novelty. Therefore, I think this is a successful application of existing technique, it re-explains how to do fine-tuning in meta-learning setting, but is not novel to me. ", "title": "Fine tuning on the test set of training tasks is not novel"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "Hkgvq5AZpm", "original": null, "number": 4, "cdate": 1541692047126, "ddate": null, "tcdate": 1541692047126, "tmdate": 1541692047126, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "Syegwm5yaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": ".", "comment": "Thank you, this is a really nice paper. The bi-level optimization point of view is very insightful. Although their framework is very general, they seem to specialize it in the experiments using gradient descent for the inner loop, which is different from our closed-form solutions."}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "rkevXdCb6m", "original": null, "number": 3, "cdate": 1541691423070, "ddate": null, "tcdate": 1541691423070, "tmdate": 1541691423070, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "r1ggxa85nm", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "It's the procedure to generate the pre-trained convnet", "comment": "> \u201cI am confused about whether the proposed method is the same as \u2026 multiple models (e.g., logistic regression) for different tasks based on shared input features provided by a pre-trained model (e.g., CNN)\u201d\n\nThank you for participating in the discussion. This describes well only the behavior at test-time -- when facing a new task, a new regressor is learned based on pre-trained features (hence, different tasks will have different parameters). However, this leaves out a crucial detail: where does this pre-trained CNN come from?\n\nThe standard approach is to use a CNN that was pre-trained on ImageNet or another task. However, there is no guarantee that the CNN features will transfer well to unknown tasks. In the case of few-shot learning, with only 1 or 5 training samples, fine-tuning will result in extreme over-fitting.\n\nOur training procedure (and indeed, all meta-learning methods for few-shot learning, such as MAML, SNAIL, etc) train the CNN features specifically to perform well on new, unseen tasks. \u201cPerforming well on unseen tasks\u201d is formalized as achieving a low error after fine-tuning. This means that we have to back-propagate errors through the fine-tuning procedure, which can be SGD (MAML) or a ridge/logistic regression solver (ours). The end result is a CNN that is especially trained to be fine-tuned later under the same conditions; this differs substantially from standard pre-training.\n\nThere is a nice, informal introduction to this (admittedly subtle!) distinction, that was written by the authors of MAML:\nhttps://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "B1lS7rnxT7", "original": null, "number": 1, "cdate": 1541616925225, "ddate": null, "tcdate": 1541616925225, "tmdate": 1541616925225, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "SkeX8K6thm", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "content": {"title": "Response to AR2", "comment": "We thank the reviewer for the insightful comments and analysis.\n\n> \u201cOne-vs-all classifiers\u201d for LR-D2\nThis is a great suggestion, and we are not quite sure how we missed it. We will update the results for 5-way classification incorporating this method.\n\n> \u201cablation where for the LR-D2 variant SGD was used ... instead of Newton\u2019s method\u201d\nWe previously did exactly this experiment, although for the R2-D2 (ridge regression) variant. We did not include it due to space constraints. It is equivalent to MAML, which also uses SGD, but adapting only the classification layer for new tasks (instead of adapting all parameters).\n\nWe tested this variant on miniImageNet with 5 classes, with the lowest-capacity CNN (which is the most favorable model for MAML/SGD). It yields 45.4\u00b11.6% accuracy for 1-shot classification and 61.7\u00b11.0% for 5-shot classification. Comparing it to Table 1, there\u2019s a drop in performance compared to our closed form solver (3.5% and 4.4% less accuracy, respectively), and also compared to the original MAML (3.3% and 1.4% respectively).\n\nAlthough we expect the conclusions for logistic regression (LR-D2) to be similar, we will extend the experiment to this case and report the results.\n\n> \u201cNeither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example\u201d\nWe agree, and will amend the text. Their interest may lie more in their technical novelty.\n\n> Suggestions on multinomial term and sentence grammar\nThese do improve the readability of the text and will be corrected.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604625, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxnZh0ct7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1211/Authors|ICLR.cc/2019/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604625}}}, {"id": "Syegwm5yaQ", "original": null, "number": 2, "cdate": 1541542743787, "ddate": null, "tcdate": 1541542743787, "tmdate": 1541542743787, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "r1ggxa85nm", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "IMO, shared parameters are optimized for Base test-set (Figure 1) instead of Base training-set, which is different than multi-task learning setup. ( I think AnonReviewer1 also raised similar issues...)\n\nAnd, I think authors missed a reference, which is very relevant.\nhttps://arxiv.org/abs/1806.04910", "title": "shared parameters are optimized for Base test-set"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}, {"id": "r1xPm1Kah7", "original": null, "number": 3, "cdate": 1541406494547, "ddate": null, "tcdate": 1541406494547, "tmdate": 1541533329009, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "content": {"title": "results are not very promising", "review": "This paper proposes a new meta-learning method based on closed-form solutions for task specific classifiers such as ridge regression and logistic regression (iterative). The idea of the paper is quite interesting, comparing to the existing metric learning based methods and optimization based methods. \n\nI have two concerns on this paper. \nFirst, the motivation and the rationale of the proposed approach is not clear. In particular, why one can simply treat \\hat{Y} as a scaled and shifted version of X\u2019W?\n\nSecond, the empirical performance of the proposed approach is not very promising and it does not outperform the comparison methods, e.g., SNAIL.  It is not clear what is the advantage. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "cdate": 1542234280142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897115, "tmdate": 1552335897115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlghKO937", "original": null, "number": 2, "cdate": 1541208488090, "ddate": null, "tcdate": 1541208488090, "tmdate": 1541533328803, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "content": {"title": "Not clear what is novel here", "review": "Summary: The paper proposes an algorithm for meta-learning which amounts to fixing the features (ie all hidden layers of a deep NN), and treating each task  as having its own final layer which could be a ridge regression or a logistic regression. The paper also proposes to separate the data for each task into a training set used to optimize the last, task specific layer, and a validation set used to optimize all previous layers and hyper parameters. \n\nNovelty: This reviewer is unsure what the paper claims as a novel contribution. In particular training multi-task neural nets with shared feature representation and task specific final layer is probably 20-30 years old by now and entirely common. It is also common freeze the feature representation learned from the first set of tasks, and to simply use it for new tasks by modifying the last (few) layer(s) which would according to this paper qualify as meta-learning since the new task can be learned with very few new examples. \n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "cdate": 1542234280142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897115, "tmdate": 1552335897115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkeX8K6thm", "original": null, "number": 1, "cdate": 1541163339103, "ddate": null, "tcdate": 1541163339103, "tmdate": 1541533328588, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "content": {"title": "A good idea that achieves good results", "review": "This paper proposes a meta-learning approach for the problem of few-shot classification. Their method, based on parametrizing the learner for each task by a closed-form solver, strikes an interesting compromise between not performing any adaptation for each new task (as is the case in pure metric learning methods) and performing an expensive iterative procedure, such as MAML or Meta-Learner LSTM where there is no guarantee that after taking the few steps prescribed by the respective algorithms the learner has converged. For this reason, I find that leveraging existing solvers that admit closed-form solutions is an attractive and natural choice. \n\nSpecifically, they propose ridge regression as their closed-form solver (R2-D2 variant). This is easily incorporated into the meta-learning loop with any hyperparameters of this solver being meta-learned, along with the embedding weights as is usually done. The use of the Woodbury equation allows to rewrite the closed form solution in a way that scales with the number of examples instead of the dimensionality of the features; therefore taking advantage of the fact that we are operating in a few-shot setting. While regression may seem to be a strange choice for eventually solving a classification task, it is used as far as I understand due to the availability of this widely-known closed-form solution. They treat the one-hot encoded labels of the support set as the regression targets, and additionally calibrate the output of the network (via a transformation by a scale and bias) in order to make it appropriate for classification. Based on the loss of ridge regression on the support set of a task, a parameter matrix is learned for that task that maps from the embedding dimensionality to the number of classes. This matrix can then be used directly to multiply the embedded (via the fixed for the purposes of the episode embedding function) query points, and for each query point, the entry with the maximum value in the corresponding row of the resulting matrix will constitute the predicted class label.\n\nThey also experimented with a logistic regression variant (LR-D2) that does not admit a closed-form solution but can be solved efficiently via Newton\u2019s Method under the form of Iteratively Reweighted Least Squares. When using this variant they restrict to tackling the case of binary-classification.\n\nA question that comes to mind about the LR-D2 variant: while I understand that a single logistic regression classifier is only capable of binary classification, there seems to be a straightforward extension to the case of multiple classes, where one classifier per class is learned, leading to a total of N one-vs-all classifiers (where N is the way of the episode). I\u2019m curious how this would compare in terms of performance against the ridge regression variant which is naturally multi-class. This would allow to directly apply this variant in the common setting and would enable for example still oversampling classes at meta-training time as is done usually.\n\nI would also be curious to see an ablation where for the LR-D2 variant SGD was used as the optimizer instead of Newton\u2019s method. That variant may require more steps (similar to MAML), but I\u2019m curious in practice how this performs.\n\nA few other minor comments:\n- In the related work section, the authors write: \u201cOn the other side of the spectrum, methods that optimize standard iterative learning algorithms, [...] are accurate but slow.\u201d Note however that neither MAML nor MetaLearner LSTM have been showed to be as effective as Prototypical Networks for example. So I wouldn\u2019t really present this as a trade-off between accuracy and speed.\n- I find the term multinomial classification strange. Why not use multi-class classification?\n- In page 8, there is a sentence that is not entirely grammatically correct: \u2018Interestingly, increasing the capacity of the other method it is not particularly helpful\u2019.\n\nOverall, I think this is good work. The idea is natural and attractive. The writing is clear and comprehensive. I enjoyed how the explanation of meta learning and the usual episodic framework was presented. I found the related work section thorough and accurate too. The experiments are thorough as well, with appropriate ablations to account for different numbers of parameters used between different methods being compared. This approach is evidently effective for few-shot learning, as demonstrated on the common two benchmarks as well as on a newly-introduced variant of cifar that is tailored to few-shot classification. Notably, the ridge regression variant can reach results competitive with SNAIL that uses significantly more weights and is shown to suffer when its capacity is reduced. Interestingly, other models such as MAML actually suffer when given additional capacity, potentially due to overfitting.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1211/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Official_Review", "cdate": 1542234280142, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1211/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897115, "tmdate": 1552335897115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1211/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1ggxa85nm", "original": null, "number": 1, "cdate": 1541201128458, "ddate": null, "tcdate": 1541201128458, "tmdate": 1541201128458, "tddate": null, "forum": "HyxnZh0ct7", "replyto": "HyxnZh0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "content": {"comment": "After reading this paper, I am confused about whether the proposed method is the same as a widely used technique, i.e., training multiple models (e.g., logistic regression) for different tasks based on shared input features provided by a pre-trained model (e.g., CNN), which can be fine-tuned. Although a minor difference here is that the tasks are sampled from a distribution of tasks rather than a fixed set (which follows a standard meta-learning setting), the used technique already exists and is well-known.\n\nSince the proposed method is claimed to be a meta-learning approach that can quickly adapt to novel tasks, the training algorithm or the meta-learner should do something different for different tasks (i.e., adaptive to each specific task). However, The CNN remains the same for different tasks, and the closed-form solvers do not have any hyper-parameters changed with the task. I am not sure if it can be recognized as a meta-learning method. It might be more suitable to be categorized in multi-task learning, where models for different tasks share the same feature extractor (the CNN here).\n\nPlease correct me if I am wrong in the understanding of the essential idea of this paper. Thanks a lot!", "title": "What is the essential difference compared to training multiple models that share a pre-trained ConvNet (fine-tuning is allowed) providing input features?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1211/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-learning with differentiable closed-form solvers", "abstract": "Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures.\nMost work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent.\nNonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently.\nIn this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning.\nThe main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data.\nThis requires back-propagating errors through the solver steps.\nWhile normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage.\nWe propose both closed-form and iterative solvers, based on ridge regression and logistic regression components.\nOur methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.", "keywords": ["few-shot learning", "one-shot learning", "meta-learning", "deep learning", "ridge regression", "classification"], "authorids": ["luca@robots.ox.ac.uk", "joao@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "vedaldi@robots.ox.ac.uk"], "authors": ["Luca Bertinetto", "Joao F. Henriques", "Philip Torr", "Andrea Vedaldi"], "TL;DR": "We propose a meta-learning approach for few-shot classification that achieves strong performance at high-speed by back-propagating through the solution of fast solvers, such as ridge regression or logistic regression.", "pdf": "/pdf/1f623df4f10a6bba5340200358b11ba3d03aec04.pdf", "paperhash": "bertinetto|metalearning_with_differentiable_closedform_solvers", "_bibtex": "@inproceedings{\nbertinetto2018metalearning,\ntitle={Meta-learning with differentiable closed-form solvers},\nauthor={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxnZh0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1211/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311652453, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HyxnZh0ct7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1211/Authors", "ICLR.cc/2019/Conference/Paper1211/Reviewers", "ICLR.cc/2019/Conference/Paper1211/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311652453}}}], "count": 29}