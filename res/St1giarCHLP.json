{"notes": [{"id": "St1giarCHLP", "original": "vXjql7NHn1", "number": 1080, "cdate": 1601308121503, "ddate": null, "tcdate": 1601308121503, "tmdate": 1611607644211, "tddate": null, "forum": "St1giarCHLP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "bnOKzwzNM37", "original": null, "number": 1, "cdate": 1610040469858, "ddate": null, "tcdate": 1610040469858, "tmdate": 1610474073773, "tddate": null, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This work provides additional insights into a class of generative models that is rapidly gaining traction, and extends it by potentially providing a faster sampling mechanism, as well as a way to meaningfully interpolate between samples (an ability which adversarial models, currently the most popular class of generative models, also have). The revised manuscript includes an extension to discrete data, which could potentially amplify the impact of this work. The authors have also run additional experiments in response to the reviewers' comments.\n\nReviewer 1 raised several concerns about the choice of language (i.e. referring to the proposed model as a diffusion model, and the precise meaning of 'implicit' in the context of generative models). This is a fair point, as the authors introduce changes that affect the Markovian nature of the \"diffusion\" process, and a diffusion process is supposed to be Markovian by definition.\n\nHowever, I think there is something to be said for the authors' argument of using the word 'diffusion' to clearly link this work to the prior work on which it is based. Given that technically speaking, the original DDPM work already 'abuses' the term to refer to a discrete-time process, it is difficult to argue compellingly that 'diffusion' should not feature in the name of the proposed model. Referring to 'non-Markovian diffusion processes' however seems more problematic, as this is a direct contradiction. If the authors wish to use this phrase, adding a few sentences to the introduction that justify this use would be helpful, and personally I feel this would be sufficient to address the issue (I noted that Section 4.1 already acknowledges that the forward process is no longer a diffusion). Plenty of work in our field abuses notation and this is justified simply with the phrase \"with (slight) abuse of notation...\"; I don't think this would be any different.\n\nReviewer 1 is technically correct that 'stochastic' is an absolute adjective, i.e. something can only be stochastic or deterministic, there is nothing in between, and there are no degrees/levels of stochasticity or determinism. In practice however, it is quite often used in a comparative sense, and I believe I have in fact been guilty of this myself! I do not feel that it causes any ambiguity in this case. Indeed, the phrase 'degree of stochasticity' seems to be in relatively common use in literature. While there may be more correct terms to use, I subscribe to the descriptivist view on language, and I do not think the comparative use of 'stochastic' is a major issue here. The alternatives I can think of seem potentially more cumbersome (e.g. I wager that 'more/less entropic' would be more poorly understood than 'more/less stochastic'). Still, I recommend that the authors consider potential alternatives in the future, to avoid any confusion.\n\nOverall, I think the reviewers' major concerns have been addressed in the revised manuscript. Given that all reviewers consider the idea worthwhile, I will join them in recommending acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "St1giarCHLP", "replyto": "St1giarCHLP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040469845, "tmdate": 1610474073756, "id": "ICLR.cc/2021/Conference/Paper1080/-/Decision"}}}, {"id": "R-Q7ndlC_LD", "original": null, "number": 10, "cdate": 1606079879411, "ddate": null, "tcdate": 1606079879411, "tmdate": 1606079879411, "tddate": null, "forum": "St1giarCHLP", "replyto": "fbp54zwj721", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Authors,\n\nThank you for responding to my comments and clarifying my understanding. I am glad that you have made the changes you did to the paper. I believe these changes have slightly improved the work. I initially reviewed this paper at a 7 and I do not feel these new changes warrant me to further raise this score, so I will stay at a 7. This is a good paper and I hope it is accepted. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "-OEVebC7R1n", "original": null, "number": 1, "cdate": 1603821860787, "ddate": null, "tcdate": 1603821860787, "tmdate": 1606066297223, "tddate": null, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review", "content": {"title": "An interesting approach but difficult to disentangle the explicit contributions. ", "review": "#### DESCRIPTION \nThis paper consider tweaks to denoising diffusion models, exploring non-Markovian inference models, as well as shorter and possibly deterministic generative trajectories. \n\n#### DISCUSSION\n\nI'm having difficulty placing this work in the correct context, and disentangling exactly what the contributions are. From my understanding of denoising diffusion models (as presented in Sohl-Dickstein et al (2015) and Ho et al (2020)), the following hold:\n\n(i) At test time, the variance of each step of the generative process can be set to a different value than at training time, effectively evaluating a different model than the one that was trained. This is exploited by Ho et al (2020) to remove the weights in the variational lower bound at training time (they effectively set sigma such that the weights are 1 at training time).   \n\n(ii) At test time, it is possible to select a subset of the timesteps used at training time, and generate a corresponding schedule for those selected timesteps, possibly leading to more efficient sample generation at the expense of sample quality.\n\n(iii) Any inference process which satisfies the Markov property (as in Sohl-Dickstein et al or Ho et al) admits the factorization given in eq. (6). \n\nNow, I would have thought the first port of call would be to compare the proposed method in this paper to (i) and (ii). Moreover, while I *think* the choice of q(xt-1 | xt, x0) in eq. (7) indeed means that q(xt | xt-1, x0) =\\= q(xt | xt-1) (i.e. the process which starts at data and ends at noise is indeed non-Markovian), the lack of the comparisons with (i) and (ii) makes it unclear whether this step actually needs to be introduced, and the fact that the parameter sigma appears in both the mean and variance of q(xt | xt-1, x0) ties the mean and variance in a somewhat opaque way. \n\nI'm also a little confused by the choice of language in the paper. Traditionally, a diffusion process describes a continuous-time stochastic process which satisfies the Markov property. 'Denoising diffusion models' stretch the term diffusion to describe a finite discretization of this diffusion process, but explicitly retain the Markov property, and have a clear interpretation as a generative models which iteratively denoises a white noise sample. If you further make the inference process non-Markovian, so that you have a non-continuous, non-Markovian process, does it make sense to persist in using the word diffusion to describe the model? \n\nThis may be a nitpick, but I'm also not sure about the sense in which this model can be described as 'implicit'. The term implicit is typically used to describe generative models which specify the data-generating procedure mechanistically as a series of prescribed operations (possibly stochastic) applied to some given inputs, so that the resulting output distribution is defined 'implicitly' by these generative steps, and *crucially* marginalizing over any associated latent variables is intractable. The 'implicit' model described in this paper is formulated as the deterministic limit of a standard latent variable generative model fit using amortized stochastic variational inference. Recent work has examined normalizing flows as the deterministic limit of a Markovian generative model (e.g. \"Stochastic Normalizing Flows\", Wu et al (2020), \"SurVAE Flows\", Nielsen et al 2020) and indeed, the sentence \"The resulting model becomes an implicit probabilistic model, where samples are generated from latent variables with a fixed procedure.\" from this paper could equally describe a standard normalizing flow, which is certainly not an implicit model. \n\nAs far as the experiments are concerned:\n\n- It seems the takeaway message from section 5.1 is that the proposed model better adapts to evaluation with shorter trajectories when eta is small, and that the original DDPM doesn't fare well with shorter trajectories. The sample quality gets worse the shorter the trajectory (which is expected), but the best results are still given by evaluating the full DDPM model with 1000 steps.   \n\n- In section 5.2, I suppose it's expected that samples generated using different length trajectories from the same starting point are similar given that the generative process likely corresponds to some ODE discretization (as you've mentioned in the conclusion), but it's good to have it tested.   \n\n- In section 5.3, I don't really understand the purpose of interpolations for demonstrating the capabilities of generative models. What exactly do they demonstrate? \n\n#### EXTRA NOTES\n\nWhy are 'joint' and 'marginals' repeatedly placed in quotation marks? \n\n\"The magnitude of \u03c3 controls the how stochastic the forward process is.\", \"...the stochasticity of the process\", \"...compared to its less stochastic counterparts\" What does it mean for something to be more or less 'stochastic' than something else? How can 'stochasticity' be used as a quantitative measure?\n\n\"Fenchel Inception Distance\" -> \"Frechet Inception Distance\"\n\n#### CONCLUSION \nOverall, I think fundamental idea of improving the long sampling trajectories of DDPMs is certainly interesting. However, I'm finding it difficult to understand the extent to which the proposed non-Markovian inference process is really necessary i.e. I'd like to see baseline comparison with points (i) and (ii) above. Right now, I'm not able to disentangle which components of the proposed method are actually necessary.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127480, "tmdate": 1606915792012, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1080/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review"}}}, {"id": "eRb_TjIRmP", "original": null, "number": 9, "cdate": 1606066284103, "ddate": null, "tcdate": 1606066284103, "tmdate": 1606066284103, "tddate": null, "forum": "St1giarCHLP", "replyto": "4QmfFJVMCrx", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "Response", "comment": "Re use of language ('diffusion', 'implicit', 'stochastic' etc.): I think it's important to choose language carefully, because clear communication of an idea can often be as important as the idea itself. \n\nDespite your method's clear influences, since a diffusion typically refers to a continuous-time Markov process, my question is whether the term diffusion is really the best way to describe a discrete-time non-Markovian process. \n\n\"Regarding modeling assumptions, we copy this quote verbatim from [Mohamed and Lakshminarayanan, 2016]\" \nOK, but consider the following from the remainder of the paragraph from which that quote is taken: \n\"When the function G is well-defined, such as when the function is invertible, or has dimensions m = d with easily characterised roots, we recover the familiar rule for transformations of probability distributions. We are interested in developing more general and flexible implicit generative models where the function G is a non- linear function with d > m, specified by deep networks. The integral (2) is intractable in this case: we will be unable to determine the set {G\u03b8 (z) \u2264 x}, the integral will often be unknown even when the integration regions are known and, the derivative is high-dimensional and difficult to compute. Intractability is also a challenge for prescribed models, but the lack of a likelihood term significantly reduces the tools available for learning. In implicit models, this difficulty motivates the need for methods that side-step the intractability of the likelihood (2), or are likelihood-free.\"\nIntractability of the likelihood is one of the defining factors of an implicit model, evidenced by the fact that the terms implicit and likelihood-free are often used interchangeably, and the fact that the above paper exists to deal with learning in implicit generative models *because* the likelihood is intractable. Your model has a tractable lower bound on the likelihood which you use for training, and only becomes deterministic at test time in the limit of a scalar hyperparameter. I also do not understand how a normalizing flow could be described as an implicit model. \n\n\"how stochastic\", \"vary the... stochasticity\", \"less stochastic\"\nI repeat: 'stochasticity' is *not* a quantitative measure. What you're trying to describe is the entropy of the process, which *is* a quantitative measure. Saying one stochastic process is more stochastic than another doesn't have meaning. \n\nRe contributions: I think the fundamental idea of the paper, namely improving on the long sampling time of DDPMs using some model tweaking, is definitely worthwhile. I'm inclined to believe that the proposed tweaks on the DDPM model are justified, and the experimental results certainly suggest an improvement in wall-clock time, even at the expense of some quality. \n\nUltimately, it seems as if the paper has been pitched as a more efficient DDPM-style approach, in terms of language, theory, and empirical evaluation. If the goal is to \"make diffusion type models much faster and easier to apply in real-world scenarios\", then the gold standard comparison is a GAN (qualitative differences in model class notwithstanding), but there's no comparison there. Overall, I feel comfortable raising my score to a 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "4QmfFJVMCrx", "original": null, "number": 7, "cdate": 1605833784487, "ddate": null, "tcdate": 1605833784487, "tmdate": 1605833812252, "tddate": null, "forum": "St1giarCHLP", "replyto": "bXEmBU5Og7f", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "cont'd", "comment": "**Q: \u201cthe best results are still given by evaluating the full DDPM model with 1000 steps.\u201d**\n\n**A**: First, the goal of our paper is to drastically reduce sampling time while retaining good sample quality, not to optimizeFID scores with as much compute as possible. Second, the DDPM model uses different variances for different datasets to achieve the best score; we mentioned this in the experiment section that CIFAR10 used a **larger variance than LSUN**, and even included the [exact line of code from Ho et al.\u2019s implementation](https://github.com/hojonathanho/diffusion/blob/master/scripts/run_cifar.py#L136) in our first version. From our experiments, if the LSUN variance is used for CIFAR10 then worse FID scores will be observed. In contrast, we do not tune this extra hyperparameter and still achieve good performance.\n\n**Q: Why do the interpolation experiments at all?**\n\n**A**: It reveals an interesting property of DDIM that GANs also have, so there is the potential to use them for applications such as image manipulation. DDPMs, on the other hand, do not have this property. \n\n\n**Q: Why are 'joint' and 'marginals' repeatedly placed in quotation marks?**\n\n**A**: In the update we have removed the quotation marks and commented on how this may be a slight abuse of notation.\n\n\n**Q: How can 'stochasticity' be used as a quantitative measure?**\n\n**A**: We use the term \u2018stochastic\u2019 to refer to larger $\\sigma_t$ values, since we can choose a different $\\sigma$ for each $t$, we simplify the case by using a scalar coefficient $\\eta$ that measures how much noise we inject at each step. If $\\eta = 0$, then the process is deterministic (i.e., least \u201cstochastic\u201d), if $\\eta = 1$, then we inject more noise at each step. Our empirical results (Figure 3) support these claims.\n\nThank you for the review."}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "bXEmBU5Og7f", "original": null, "number": 6, "cdate": 1605833762817, "ddate": null, "tcdate": 1605833762817, "tmdate": 1605833762817, "tddate": null, "forum": "St1giarCHLP", "replyto": "-OEVebC7R1n", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "We make diffusion type models much faster and easier to apply in real-world scenarios, whereas existing methods cannot.", "comment": "We start by clarifying that our main contribution is not to outperform DDPMs under 1000 steps; as we argue in the introduction, these many steps cost too much compute, so as it stands, DDPMs are unlikely to be used in any practical, latency-sensitive scenarios. \n\nOur contributions are twofold:\n- On a practical level, we propose methods that are computationally more efficient by 10x to 50x, while retaining the sample quality of the original case (see Figure 5). We show in the paper and in the following response that a compute-limited variant of DDPM does not perform nearly as well, even with the modifications suggested by the reviewer. \n- We are the first to introduce non-Markovian forward processes that generalize the Markovian ones from the literature, which enable a large family of generative models, including categorical ones for discrete data (Appendix A in revision). \n\n**Q: Compare with two possible modifications to DDPM: (i) set variance of each step of the generative process differently; (ii) select a subset of timesteps and generate corresponding schedule for selected timesteps.**\n\n**A**: **We explicitly considered (ii) in the last 2 rows of Table 1 (original version and revised version)**, where we used fewer timesteps for DDPM (from T=0 to 100) to achieve comparable runtimes / accelerate sampling; the fact that sampling can be made faster is not explicitly mentioned in (Ho et al. 2020), but we count it as a DDPM baseline nonetheless. We find that our approach generates better samples than DDPM in the accelerated cases.\n\nFor (i), we did an additional experiment where we set the variance in the DDPM generator to zero, where we keep the mean function the same as in (Ho et al. 2020). **This results in images that are entirely gray, corresponding to samples that contain all zero values.** This is not unexpected, because it represents a case where the generation process is a poor approximation to the reverse of the Markovian inference process.\n\n**Q: What components of DDIM are necessary for good sampling quality in a shorter time?**\n\n**A**: Comparisons with (ii) show that having a smaller variance in the sampling process is necessary. Comparisons with (i) show that simply having a smaller variance in DDPM is not enough; we must also revisit the mean function accordingly, which only comes from our generalized non-Markovian forward process analysis.\n\n\n**Q: Why is the \u201cnon-Markovian\u201d point of view useful?**\n\n**A**: As we have shown with the comparisons of (ii) and (i), without our contributions (non-Markovian) inference process, the corresponding generative process fails to recover high-quality samples in a shorter time. In Appendix A of the revision, we also show how the \u201cnon-Markovian\u201d view can be generalized to discrete inference distribution as well.\n\n**Q: Why call the method \u201cdiffusion\u201d?**\n\n**A**: In the abstract of (Sohl-Dickstein et al., 2015), they mentioned \u201cwe then learn a reverse diffusion process\u201d, but the forward process in their case is also just a discretization of a diffusion process. We keep the term \u201cdiffusion\u201d to highlight the connections to existing works that work under discretized time.\n\n**Q: \u201cThis may be a nitpick, but I'm also not sure about the sense in which this model can be described as 'implicit'.\u201d (\u201cThe term implicit \u2026 and crucially marginalizing over any associated latent variables is intractable.\u201d)**\n\n**A**: We follow the definition from a line of work on \u201cimplicit generative models\u201d. \n\nRegarding modeling assumptions, we copy this quote **verbatim** from [Mohamed and Lakshminarayanan, 2016]:\n\"Implicit generative models use a latent variable $z$ and transform it using a deterministic function $G_\\theta$ that maps from $R^m \\to R^d$ using parameters $\\theta$.\"\n\nFor our deterministic sampling process, our latent variable is $x_T$, our process is a deterministic function that maps from $R^d \\to R^d$, so it is an implicit generative model in this sense. It differs from DDPM in the sense that the DDPM mapping from $x_T$ to $x_0$ is not deterministic.\n\nRegarding tractability, we copy this quote (also **verbatim**) from [Diggle and Gratton, 1984]:\n\"In principle, a unique log-likelihood function underlies any such implicit statistical model (but not vice versa) and **if this can be written down explicitly**, inference can proceed along conventional lines.\"\n\nThe fact that likelihood can be tractable does not make a model less implicit; by this argument, normalizing flows are also implicit models (just with tractable log-likelihood functions). In fact, Flow-GAN [Grover et al., 2018] trains a flow with adversarial training, which can be advantageous for some tasks other than density estimation.\n\n[Mohamed and Lakshminarayanan, 2016], Learning in Implicit Generative Models\n[Diggle and Gratton, 1984], Monte Carlo Methods of Inference for Implicit Statistical Models\n[Grover et al., 2018] Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "re7j-liM6vI", "original": null, "number": 5, "cdate": 1605833034515, "ddate": null, "tcdate": 1605833034515, "tmdate": 1605833034515, "tddate": null, "forum": "St1giarCHLP", "replyto": "H_F-YtuXKxI", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "There is no need to train a different epsilon_t", "comment": "Thank you for the review. In Appendix A of the revision, we show how our idea can be generalized to discrete / multinomial inference distribution as well, which resulting in an objective that does not minimize MSE but rather minimizes classification error.\n\n**Q: Is there a need to train a different epsilon_t?**\n\n**A**: In our experiments, we trained using the same maximum T (1000) and then chose different sampling processes $S$ accordingly.\n\nIf it is known a-priori that we will at most sample in S steps as well as which $\\alpha_t$ we will choose, then having more $\\epsilon_t$ than S seems unnecessary. However, if we train with more steps, then we have greater flexibility in trading-off computational costs for better sample quality / FID scores. Moreover, some schedules for $\\alpha_t$ might work better than others, so training with more steps than what is needed for sampling seems adequate (the model size does not increase much even with more steps, due to the positional embeddings that condition the step).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "fbp54zwj721", "original": null, "number": 4, "cdate": 1605832894808, "ddate": null, "tcdate": 1605832894808, "tmdate": 1605832894808, "tddate": null, "forum": "St1giarCHLP", "replyto": "32Iem2FenH_", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment", "content": {"title": "DDIM uses the same model for all experiments, and can generalize to discrete cases", "comment": "Thank you for the review.\n\n**Q: Were all the results presented generated from the same DDIM?**\n\n**A**: Yes, we use the same model for all the experiments within each dataset. In particular, all the models are trained with $L_{simple}$ objective in (Ho et al., 2020); apart from CelebA 64x64 (where we do not have pre-trained checkpoints), all other experiments use the checkpoints from (Ho et al., 2020). We emphasize this point in the revision.\n\n**Q: How would different training procedures affect performance?**\n\n**A**: As we mentioned in Theorem 1, an advantage of the non-Markovian perspective is that it shows the equivalence of using any weights $\\gamma_t$ at different time $t$ (at least in principle, if the parameters are not shared). In practice, using the same weight across different objectives may be advantageous because the Adam optimizer keeps track of the gradient at each step to better condition the problem, and similar weights could be more stable simply from an optimization perspective. \n\n**Q: Do you think these same insights could be applied to discrete diffusion models proposed in the original work on non-equilibrium thermodynamics?**\n\n**A**: While our focus on the paper is not exactly for discrete models, we believe that discrete diffusion models could also benefit from such a view. In Appendix A of our revision, we discuss in detail how we can derive the variational objective for categorical data with **multinomial forward processes**, under various levels of stochasticity. \n\nAnalogous to how the objective for the Gaussian case minimizes MSE, **the objective for the multinomial case minimizes multi-class classification error** (with the target being the one-hot vector observation). Similar to the Gaussian case, we can use an objective with the same weights for each step, and choose different sampling processes.\n\n**Q**: What would the lower bounds to the likelihood with non-Markovian procedures become?\n\n**A**: In the derivation of Theorem 1, we showed that the variational lower bounds to the log-likelihood are inversely correlated with the $\\sigma$ parameters, so the bounds can only become worse as we decrease $\\sigma$. Intuitively, this is because unlike DDPM, the forward (inference) process is no longer a good approximation to the posterior of the backward (generation) process. Nevertheless, we show in the revised Section 5.4 that we can find latent $x_T$ that reconstruct $x_0$ in the test set relatively well.\n\n**Q: Abstract can be more descriptive.**\n\n**A**: We have updated the abstract to reflect that the non-Markovian aspect is what makes it possible to derive the deterministic sampling method.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "St1giarCHLP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1080/Authors|ICLR.cc/2021/Conference/Paper1080/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863906, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Comment"}}}, {"id": "H_F-YtuXKxI", "original": null, "number": 2, "cdate": 1603896030275, "ddate": null, "tcdate": 1603896030275, "tmdate": 1605024535677, "tddate": null, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review", "content": {"title": "Very good, clean, paper", "review": "Summary : This paper develops a variant (DDIM) of an existing method (DDPM) with the goal of accelerating it greatly while still maintaining performance. The authors are working in the context of a denoising process that runs in the reverse direction to a sequence of steps that each add a small amount of Gaussian noise to the original data. The proposal is to introduce an auxiliary function that breaks the Markov assumption by leaking some information in a controlled way about the training points x0, and then use this auxiliary function as scaffolding to train the actual Markov chain of denoising functions.\n\nThis paper builds on other works in the recent literature and proposes something useful and novel. They propose something relatively down-to-earth and then they methodically analyze the consequences and derive all the mathematical formulas that follow. I feel that the dose of mathematical content is just appropriate for what they set out to do. Less would be too vague, more would be excessive.\n\nThe direction that they are proposing is relevant, contrary to certain other papers that involve a lot of correct equations but don't take us anywhere interesting. In my mind, this is a very good clean paper. \n\nI appreciated the authors taking the time to mention the connection with ODEs, and I thought they would mention Neural ODEs in section 7 as a nod to the recent surge of interest in ODEs in the context of Deep Learning sparked by that paper.\n\nThe only bad things that I could possibly say about this paper would involve comparing it to certain other wildly creative papers, and to say that it's not as innovative or throught-provoking as those papers. And that's not a fair thing to say.\n\nI would like to ask the authors if, with their framework, there is a need to train a completely different epsilon_t for every t=1..T ? I understand these models to be U-Nets, as mentioned in the appendix. I presume that the thing that makes this reasonable is the fact that S < T, and only S different models need to be trained?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127480, "tmdate": 1606915792012, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1080/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review"}}}, {"id": "32Iem2FenH_", "original": null, "number": 3, "cdate": 1603903785524, "ddate": null, "tcdate": 1603903785524, "tmdate": 1605024535615, "tddate": null, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "invitation": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review", "content": {"title": "Keen insights, good results", "review": "Summary:\n\nThis paper proposes a change to the recently popular diffusion models, motivated by increasing the speed of sampling. This is accomplished by changing the \u201cforward\u201d process which adds noise to the data. In the original diffusion models, this forward process is a Markov process whose marginals and conditionals can be computed efficiently in closed form. This paper proposes to replace this Markov forward process with a non-markovian process that is designed to have the same marginals. The generative model, in this case, changes such that to predict the next step in the process, the model must first predict the \u201cclean\u201d sample at the end of the chain which is then used to give an estimate for the next step in the chain. \n\nIntriguingly, the objective for training this new generative model is identical to training a standard diffusion model. Thus, the models differ only at sampling time. Under the new interpretation, we can sample from a family of models after training. This family can be tuned to increase the speed of sampling, at the cost of some sample quality. Inside this family, there also exists an implicit generative model which can be sampled from in a more deterministic fashion than the other members -- hence the name of the paper. \n\nThe authors present a number of image generation experiments and study the impact of the sampling parameters on sample quality. They find that in settings where the efficiency of sampling is greater than the original diffusion model, the proposed approach achieves higher sample quality -- although never as high as the original diffusion model. \n\nBecause the model can be sampled in a deterministic fashion, the authors note that fixing the latent variables during sampling results in consistent samples. This allows a much more controlled generation compared to the original diffusion model. \n\n\nStrong areas:\n\nI quite enjoyed this paper. These diffusion models have gained considerable attention and this work addresses what is likely their largest issue -- the slow speed of sampling. Most interesting to me is these new ideas can be applied to the original diffusion models without retraining. \n\nI find that this work provides more insight into how these diffusion models work and which choices in the original presentation are important for their performance and which (like the inference process) can be changed. These are important insights and should be impactful for further research on this class of models. \n\n\nWeaknesses:\n\nWhile this is a strong paper, there are a few issues in my opinion. I found some of the experimental details difficult to understand. Were all the results presented generated from the same DDIM? Or were the models trained separately for each sampling parameter set? This should be made more clear. \n\nIn section 3.2 you claim some equivalence between the original DDPM objective and the variational bound in your model. This looks like re-weighting the various terms in the objective. In the DDPM work, they discuss the impact of this re-weighting. I would have liked to see a similar discussion here. They found it improved sample quality but decreased likelihood. I would have loved a comparison of the various training procedures but I did not find one. Along the same line, you present no likelihood results. You should be able to generate a lower-bound on likelihood using your model (except for the eta=0 model I believe). I am curious how the various sampling schemes impact likelihood. It is not necessary for the proposed approach to be appealing but would definitely complete the picture. \n\n\nNit-picky issues:\n\nI think the abstract could be a bit more descriptive. You should add some information saying what differs between your approach and standard diffusion models. In reading the abstract, it was not clear to me that using a non-markovian forward process would make sampling faster. This became clear as I read the paper. You could add a little more information to make that clear in the abstract.\n\n\nMy recommendation:\n\nI think this paper was clearly written and proposed a strong contribution to the field of generative modeling. I think the insights presented here give us more understanding of diffusion models and while also improving their sampling speed. I will recommend accepting this paper. \n\nSome questions:\n\nDo you think these same insights could be applied to discrete diffusion models proposed in the original work on non-equilibrium thermodynamics? \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1080/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1080/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Denoising Diffusion Implicit Models", "authorids": ["~Jiaming_Song1", "chenlin@stanford.edu", "~Stefano_Ermon1"], "authors": ["Jiaming Song", "Chenlin Meng", "Stefano Ermon"], "keywords": ["generative models", "variational autoencoders", "denoising score matching", "variational inference"], "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process. We generalize DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective. These non-Markovian processes can correspond to generative processes that are deterministic, giving rise to implicit models that produce high quality samples much faster. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, perform semantically meaningful image interpolation directly in the latent space, and reconstruct observations with very low error. ", "one-sentence_summary": "We show and justify a GAN-like iterative generative model with relatively fast sampling, high sample quality and without any adversarial training.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "song|denoising_diffusion_implicit_models", "pdf": "/pdf/875efe63ea0caad6ec5128262b975180e8fa3f7d.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nsong2021denoising,\ntitle={Denoising Diffusion Implicit Models},\nauthor={Jiaming Song and Chenlin Meng and Stefano Ermon},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=St1giarCHLP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "St1giarCHLP", "replyto": "St1giarCHLP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1080/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127480, "tmdate": 1606915792012, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1080/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1080/-/Official_Review"}}}], "count": 11}