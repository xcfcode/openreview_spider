{"notes": [{"id": "HygDIcyFTm", "original": null, "number": 6, "cdate": 1542154830898, "ddate": null, "tcdate": 1542154830898, "tmdate": 1591080280887, "tddate": null, "forum": "rygkk305YQ", "replyto": "Hye08Kwcnm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 1)", "comment": "We thank the reviewer for the great feedback. Below are the itemized responses regarding each comment. We will also incorporate these into the revised version.\n\nDue to the \"max 5000 characters\" limit, we break down our response into multiple comments.\n\n\nRe:\nThough empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. \n\nAns:\n- Identifying reliable objective quality metrics for TTS, and indeed generative models more generally, is an open problem. This is especially true for high level style concepts such as accent, or labels related to emotional valence or arousal. It is common practice across papers on generating real data to demonstrate the ability to independently control attributes qualitatively, using generated samples, providing quantitative evaluation on few attributes that are easy to estimate due to similar difficulties, e.g. Glow (Kingma et al., 2018). Qualitative evaluation similar to what we provide has been the standard in other recent work on controllable TTS models such as GST (Wang et al., 2018) and VAE-Loop (Akuzawa et al., 2018). \n- In addition, it is difficult to provide comprehensive quantitative evaluation because (1) the set of unlabeled attributes we are able to measure is limited to those for which reliable systems exist to estimate them from the speech signal, as in e.g. pitch tracking. It is unclear how to convincingly measure the level of excitement, deepness, or roughness of a voice for style control. (2) Also, estimation of attributes such as pitch can be prone to error. For example, pitch estimation of rough voices is not accurate because the harmonic patterns are not as clear as normal voices (See the bottom row in Figure 16). Even in cases where it is straightforward to report statistics to show the attribute we claim to be affected does indeed vary, simple metrics alone do not show the whole picture since they do not verify that other attributes remain the same. However, such *independent* control of an individual attribute is easily demonstrated qualitatively. To circumvent the difficulty, we provide multiple samples on the demo page to demonstrate that control over each attribute is invariant to other decoder input. \n\n\nRe:\npage 5 following \"It clearly presents the samples (in fact, all the samples) drawn from components in group one were noisy, while the samples drawn from the other components were clean\"\n\nAns:\n- Quantitative evaluation on the ability to control noise level is presented in Figure 5, Figure 13, and Table 1. \n-- In Figure 5, we plot the WADA-SNR with respect to different values for the noise level dimension, which demonstrates the ability to control noise level through that dimension. In Figure 13, we plot the distribution of the noise level dimension for each mixture component, showing two groups of components that are well separated in this dimension. We can conclude that the group of mixture components on the right in Figure 13 generate clean speech because they have high probability of generating samples that correspond to high SNRs. In contrast, the group on the left in Figure 13 generates noisy speech.\n-- In Table 1, we report WADA-SNR when conditioning on the mean of a clean component, verifying that the clean component can consistently generate clean speech.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "rygkk305YQ", "original": "B1g1HvhqKQ", "number": 948, "cdate": 1538087894902, "ddate": null, "tcdate": 1538087894902, "tmdate": 1545890687601, "tddate": null, "forum": "rygkk305YQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 29, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJgOnIpzxE", "original": null, "number": 1, "cdate": 1544898223638, "ddate": null, "tcdate": 1544898223638, "tmdate": 1545354493757, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Meta_Review", "content": {"metareview": "This is an ambitious paper tackling the important and timely problem of controlling non-annotated attributes in generated speech. \n\nThe reviewers had mixed opinions about the results. R1 asks for more convincing exposition of results but, nevertheless, acknowledging that it is difficult to evaluate TTS systems systematically. Besides, R2 and R3 find the results good. \n\nJudging from the reviews and previous work, this paper does not seem to be very novel, although it certainly has intriguing new elements. Furthermore, it constitutes a mature piece of work.  \n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "Paper attacks significant problem"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper948/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353022961, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": "rygkk305YQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353022961}}}, {"id": "S1e8NIZJeN", "original": null, "number": 26, "cdate": 1544652334254, "ddate": null, "tcdate": 1544652334254, "tmdate": 1544652334254, "tddate": null, "forum": "rygkk305YQ", "replyto": "HJefYZCX1V", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re: Details about latent encoders", "comment": "We thank you for your interest. Below are the itemized responses to your questions.\n\n1. We treat a mel spectrogram as a T-by-1 image with 80 channels (dimensionality of a mel spectrum), where T corresponds to the number of frames. Convolution is therefore effectively one-dimensional across time.\n2. We use stride=1 and zero-paddings such that the width and height remain the same after each convolution layer.\n3. We did not add a skip connection from the input to the bidirectional LSTM (BLSTM) input -- i.e., the input to the first layer BLSTM is the output from the last layer convolutional layer, which has depth of 512 at each time step.\n4. The mean pooling layer is quite simple. The last layer of BLSTM outputs one 512-dimensional vector at each time step, i.e. the output for the full sequence is a T x 512 matrix containing T frames. The mean pooling layer averages these vectors (per-dimension) across time to produce a single 512-dimensional vector summary of the spectrogram.  We hope this clarifies things."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "HJefYZCX1V", "original": null, "number": 5, "cdate": 1543917946102, "ddate": null, "tcdate": 1543917946102, "tmdate": 1543917946102, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "content": {"comment": "Great papers but the details in regard to the latent encoders were a bit vague. \n\n\"For each encoder, a mel spectrogram is first passed through two convolutional layers, which contains\n512 filters with shape 3 \u00d7 1. The output of these convolutional layers is then fed to a stack of\ntwo bidirectional LSTM layers with 256 cells at each direction. A mean pooling layer is used\nto summarize the LSTM outputs across time, followed by a linear projection layer to predict the\nposterior mean and log variance.\"\n\n1. Are the convolutional layers 2D convolutions? \n2. What are the paddings and strides for the convolutions?\n3. If the convolutional layers are 2D, were the mel channels (80) and the filter channels (512) collapsed together for the input of the LSTM?\n4. Mean pooling layer details are vague and unrepeatable.\n\nThank you.", "title": "Details about latent encoders vague"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311714494, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rygkk305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311714494}}}, {"id": "r1xNW0zmCm", "original": null, "number": 22, "cdate": 1542823419595, "ddate": null, "tcdate": 1542823419595, "tmdate": 1543265911554, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "A new revision uploaded", "comment": "We thank all reviewers for the thoughtful comments and constructive suggestions, which were extremely helpful to us in improving the paper. We have uploaded a new revision to address these comments. Major changes are listed below:\n\n- List novel contribution of this work in the introduction (Section 1).\n\n- Expand discussion of related work.\n\n- Added naturalness MOS scores of ground truth audio samples in every table for comparison.\n\n- Added discussion about the dimensionality of latent space in Appendix C.\n\n- Added discussion about the potential for posterior collapse in Appendix D.\n\n- Added objective quantitative evaluations for disentangled attribute control for models trained on noisy multi-speaker English corpus (Appendix F.3) and single-speaker audiobook corpus (Appendix G.1).\n\n- Fixed typo in the q(y_l | X) derivation (Appendix A.1).\n\n- Added more details describing the experiment setup (Appendix C).\n\n- Revised synthesizer neural network descriptions (Appendix B.1).\n\n- Moved the speaker similarity evaluation for crowd-sourced audiobook corpus to the main text (Section 4.4.2).\n\n## EDIT (2018-11-26)\n- Moved objective quantitative noise-level control evaluations from Appendix F.3 to Section 4.2.2, and changed the tables to plots."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "S1gIVoyFCQ", "original": null, "number": 24, "cdate": 1543203630056, "ddate": null, "tcdate": 1543203630056, "tmdate": 1543203630056, "tddate": null, "forum": "rygkk305YQ", "replyto": "HyeevXg4AX", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re:conditioning site", "comment": "We thank the reviewer for clarification. The encoder submodule in the synthesizer aims to encode a sequence of text one-hot vectors into a sequence of text encodings, which are taken as input along with other attribute encodings by the decoder submodule. By conditioning the latent vectors at the encoder output, we modularize the encoder in the synthesizer to focus on only encoding the text, and designate the decoder to integrate information of text, latent attributes, and observed attributes from the three inputs in order to predict spectrograms. We did not experiment with injecting the information elsewhere in the synthesizer."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "Byx-PFs1p7", "original": null, "number": 3, "cdate": 1541548376832, "ddate": null, "tcdate": 1541548376832, "tmdate": 1543042886399, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "content": {"title": " A structure to the z-space to create predictabilily in the features of the generated speech is implemented  with good results", "review": "The authors describe the conditioned GAN model to generate speaker conditioned Mel spectra. They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio. In a way this is like a partially conditioned model that has \"extra\" degrees of freedom. It looks that the \"latent\" variables are just concaneted to the \"original\" set of z-values (altough with particular conditions to maximize independence). The conditioning of the z-space has originality in it and may provide interesting to the audience. Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal.\n\nAlso, I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude. Very often this leads to suboptimal generation, and the remedy is to use the time domain like in ( https://arxiv.org/ftp/arxiv/papers/1810/1810.05319.pdf).  However, in this case the audio samples show a pretty nice generation of sound.  However, it is not really end to end.\n\nThe manuscript has some curious decisios in its concepts - I do not see the architecture really hiearchial, nor end to end. I would prefer modifications on the paper that concentrate on the truly novel features. \n\nThe paper is clear, well written and done with high ambition, from data set utilization  to novel architetures to human quality panels. Results are good and interesting.\n\nNEW:\nThe authors have addressed the concerns I had with the manuscript.\n\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "cdate": 1542234340315, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygkk305YQ", "replyto": "rygkk305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335839423, "tmdate": 1552335839423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyeevXg4AX", "original": null, "number": 4, "cdate": 1542878040314, "ddate": null, "tcdate": 1542878040314, "tmdate": 1542878040314, "tddate": null, "forum": "rygkk305YQ", "replyto": "HyxBrtvX0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "content": {"comment": "My apologies, it looks like wording was not clear enough. By 'choice' I meant choice of condition site: text embeddings vs. text encoder outputs vs. somewhere inside of the decoder. One could add or concatenate the variational encoder's outputs to either of those.\n\nWould you please elaborate on your choice of condition site?", "title": "Re: "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311714494, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rygkk305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311714494}}}, {"id": "HyxBrtvX0Q", "original": null, "number": 23, "cdate": 1542842684768, "ddate": null, "tcdate": 1542842684768, "tmdate": 1542842684768, "tddate": null, "forum": "rygkk305YQ", "replyto": "BJlI-2NgCm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re: \"Thanks and additional questions to Paper948 Authors\"", "comment": "We thank the reviewer for the comment. Below are our itemized responses.\n\nRe: Would you please elaborate on your choice?\nAns: We experimented with concatenating and adding, but did not observe significant difference based on subjective evaluation.\n\nRe: Also there's a couple of minor misprints...\nAns: We thank the reviewer for pointing out these typos. These typos were fixed in the latest revision."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "Hyx_FQuzCX", "original": null, "number": 18, "cdate": 1542779776174, "ddate": null, "tcdate": 1542779776174, "tmdate": 1542780362178, "tddate": null, "forum": "rygkk305YQ", "replyto": "HJg_pOMnT7", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re:\"Re: Response to AnonReviewer1 (Part1)\" (Part 1)", "comment": "Dear AnonReviewer1,\n\n\nRe:\nI am well aware of the issues you are facing in speech synthesis making quantitative measurements. In exactly the same way as you judge that your method is better than others you can judge whether the speech you generate is (a) more/less excited (b) deep/shallow, (c) rough/smooth.\n\nAns:\nWe are not sure if the reviewer meant to suggest running a subjective evaluation of these attributes. We agree that such an evaluation could be informative, however high quality subjective evaluations are tricky to set up, especially to evaluate somewhat subjective speech styles. Reliably measuring such attributes would require preparing and calibrating appropriate and unambiguous rater instructions (to ensure that they are indeed measuring the attribute of interest), which can be quite involved for brand new tasks. In addition, we would like to reemphasize that we are not aware of, any previous study which uses such qualitative evaluations of speech styles. Given this context we feel that developing such a qualitative evaluation rubric for different aspects of speech style should be left as future work.  However, as a starting point to evaluate the proposed model's effect on different underlying speech attributes, we prepared quantitative metrics that can be measured objectively as mentioned in our previous response. \n\nTo objectively evaluate our model\u2019s ability to control individual attributes, we compute the average duration and F0 (fundamental frequency) (in voiced frames) using YIN (De Cheveigne & Kawahara, 2002) as measures of speaking rate and pitch variation, respectively. Specifically, we randomly draw 10 samples of seed $z_l$ from the prior, deterministically set the target dimension to $mu-3sigma$, $mu$, and $mu+3sigma$ to construct modified $z^*_l$, where $mu$ and $sigma$ are the mean and standard deviation, respectively, of the marginal distribution of the target dimension. We then synthesize a set of the same 25 text sequences for each of the 30 resulting values of $z^*_l$. For each value of the target dimension, we compute an average metric over 250 synthesized utterances (10 seed $z_l$ * 25 texts). Results of the model trained on single-speaker audiobook corpus (Section 4.3) are shown below.\n\nPitch dimension ($mu-3sigma$, $mu$, $mu+3sigma$)\nDuration (seconds):\t 3.5\t\t3.5\t\t3.5\nF0 (Hz):                           178.3\t187.1\t204.4\nSpeed dimension ($mu-3sigma$, $mu$, $mu+3sigma$)\nDuration (seconds):\t 3.0\t\t3.5\t\t4.0\nF0 (Hz):\t\t                 197.1\t189.7\t182.5\n\nFor the \"pitch\" dimension, we can see that the measured $F_0$ varies substantially, while the speech remains constant.  Similarly, as the value in the \"speaking rate\" dimension varies, the measured duration varies substantially.  However, there is also a smaller inverse effect on the pitch, i.e. the pitch slightly increases with the speaking rate, which is consistent with natural speaking behavior (Apple et al., 1979; Black, 1961). These results are an indication that manipulating individual dimensions primarily controls the corresponding attribute.\n\nIn addition, to validate our statement in Sec. 4.1 that \u201cthe several components which modeled US female speakers (3, 5, and 6) actually modeled groups of speakers with distinct characteristics, e.g. different F0 ranges,\u201d we draw 20 latent $z_l$ from each of these three component and decode the same set of 25 text sequences for each $z_l$. The table below shows the average pitch for each component, which corroborates our statement on they capture different pitch ranges.\n\nComponent 3: 169.1 \u00b1 10.1\nComponent 5: 214.8 \u00b1 8.4\nComponent 6: 192.2 \u00b1 21.4\n\nWe will add these quantitative results to the appendix in the next revision of the text.\n\nReferences:\nDe Cheveign\u00e9, Alain, and Hideki Kawahara. \"YIN, a fundamental frequency estimator for speech and music.\" The Journal of the Acoustical Society of America 111.4 (2002): 1917-1930.\nApple, William, Lynn A. Streeter, and Robert M. Krauss. \"Effects of pitch and speech rate on personal attributions.\" Journal of Personality and Social Psychology 37.5 (1979): 715.\nBlack, John W. \"Relationships among fundamental frequency, vocal sound pressure, and rate of speaking.\" Language and Speech 4.4 (1961): 196-199."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "SyeDKHuMAX", "original": null, "number": 21, "cdate": 1542780286583, "ddate": null, "tcdate": 1542780286583, "tmdate": 1542780286583, "tddate": null, "forum": "rygkk305YQ", "replyto": "Skekrnz2pm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re:\"Re: Response to AnonReviewer1 (Part 3)\"", "comment": "Dear AnonReviewer1,\n\nRe:\nif you include results in the paper I interpret them as important. I understand when the results in the appendix just corroborate findings discussed in the main body of the paper - different data sets with same conclusions. However, when the results in the appendix present something different, even if that suggests that a chosen speaker similarity test is inappropriate, should not they be discussed in the main body or left out altogether. \n\nAns:\nWe tried to strictly follow the initial submission page limits, but we agree that this is an interesting result which is not covered in the main text. We will move it into the main text in the next revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "H1xbSHOMR7", "original": null, "number": 20, "cdate": 1542780217111, "ddate": null, "tcdate": 1542780217111, "tmdate": 1542780217111, "tddate": null, "forum": "rygkk305YQ", "replyto": "S1gZVqMnpX", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re:\"Re: Response to AnonReviewer1 (Part 2)\"", "comment": "Dear AnonReviewer1,\n\nRe:\nIt always aids in understanding if you explicitly give an example for variables that you set. For instance, \"y_o is an observed variable (i.e. the speaker ID of an utterance)\". Is it a scalar, integer, or one-hot encoding?\n\nAns:\nWe implement it with an one-hot encoding, and will update the text to make this more explicit."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "SJxw0VdGAQ", "original": null, "number": 19, "cdate": 1542780110620, "ddate": null, "tcdate": 1542780110620, "tmdate": 1542780110620, "tddate": null, "forum": "rygkk305YQ", "replyto": "Hyx_FQuzCX", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Re:\"Re: Response to AnonReviewer1 (Part1)\" (Part 2)", "comment": "\nRe:\nI am always nervous seeing magic numbers like 13 being that special dimension that is responsible for something. When you re-train your model from another random initialization what happens to the magic number. What Figure 5 looks like for all other dimensions other than 13.\n\nAns:\nThe number 13 simply reflects the index of the noise dimension that we found in one training run.  We explain in Section 4.2.2 that this noise-level dimension can be automatically identified by running a per-dimension linear discriminative analysis. Over several different training runs with different random initialization, we found that the same procedure was always able to identify the noise-level dimension (although of course it was not always dimension 13).\n\nWe ran an additional experiment varying all other dimensions while keeping the \"noise dimension\" (dimension 13 for this training run) constant, and estimate the SNR of the resulting signal in the same way as Figure 5.  We find that the estimated SNR is consistently higher than 16 dB when dim 13 is set to a \"clean\" value, and below 4 dB when it is set to a noise value. \nThe full results are shown in below. Note that the dynamic range used for traversing dimension 13 is set to [-1, 1] in Figure 5, which roughly spans 2.5 standard deviations for dimension 13. Therefore, when traversing the other dimensions, we set the target dimension to $mu - 1.2sigma$, $mu$, $mu + 1.2sigma$. The SNRs remain nearly constant as the other dimensions vary. The small degree of variation, e.g. in dimensions 2 and 4, occur because some of those dimensions control attributes which directly affect the synthesized noise, such as type of noise (musical/white noise) and initial background noise offset, and therefore also affect the estimated SNR. We will add these results to the appendix of the final revision.\n\nSNR (dimension 13 set to a clean value):\nDim\tval=mu-1.2sigma\tval=mu    \tval=mu + 1.2sigma\n0\t17.00\t\t\t17.53\t\t17.75\n1\t17.75\t\t\t17.86\t\t17.88\n2\t18.87\t\t\t17.88\t\t16.16\n3\t17.80\t\t\t17.79\t\t17.70\n4\t16.49\t\t\t17.15\t\t18.54\n5\t17.64\t\t\t17.84\t\t18.01\n6\t18.04\t\t\t17.83\t\t17.51\n7\t17.34\t\t\t17.84\t\t17.55\n8\t18.08\t\t\t17.82\t\t17.38\n9\t17.72\t\t\t17.85\t\t17.61\n10\t18.28\t\t\t17.75\t\t17.49\n11\t17.70\t\t\t17.71\t\t17.55\n12\t18.40\t\t\t17.64\t\t16.95\n14\t17.63\t\t\t17.80\t\t17.90\n15\t18.10\t\t\t17.27\t\t17.36\n\nSNR (dimension 13 set to a noisy value):\nDim\tval=mu-1.2sigma\tval=mu    \tval=mu + 1.2sigma\n0      1.69 \t\t\t2.60 \t\t3.17\n1      1.91 \t\t\t2.03 \t\t1.96\n2      2.85 \t\t\t2.28 \t\t1.41\n3      1.88 \t\t\t1.91 \t\t1.70\n4      1.93 \t\t\t1.82 \t\t1.97\n5      1.05 \t\t\t1.80 \t\t2.05\n6      1.99 \t\t\t1.77 \t\t1.94\n7      1.94 \t\t\t1.43 \t\t0.01\n8      1.95 \t\t\t1.65 \t\t1.09\n9      1.73 \t\t\t1.86 \t\t2.00\n10    2.41 \t\t\t2.24 \t\t1.89\n11    2.22 \t\t\t2.04 \t\t1.40\n12    3.17 \t\t\t2.25 \t\t1.59\n14    1.66 \t\t\t1.81 \t\t2.15\n15    2.28 \t\t\t1.70 \t\t1.81"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "BJlI-2NgCm", "original": null, "number": 3, "cdate": 1542634494453, "ddate": null, "tcdate": 1542634494453, "tmdate": 1542641418627, "tddate": null, "forum": "rygkk305YQ", "replyto": "SJgpPspnaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "content": {"comment": "Thank you for such a thorough and valuable response!\n\nConcerning my first question: if I understood correctly you condition a submodule of the Tacotron decoder on output of the variational encoder. The Tacotron GST paper [1] contains a brief discussion of condition sites:\n\"We experimented with different combinations of conditioning sites, and found that replicating the style embedding and simply adding it to every text encoder state performed the best.\"\n\nWould you please elaborate on your choice?\n\nAlso there's a couple of minor misprints:\n1) In Appendix C, you denote variance with a $\\sigma$, which is usually used for standard deviation. Did you mean $\\sigma^2$?\n2) In Appendix A.1, there is an equality sign between eq. 11 and eq. 12, despite absence of a denominator present in eq. 7. It is obvious that values could be easily renormalized to probabilities since the distribution is discrete, but still, an equality sign might not be the most appropriate.\n\n[1] Wang et al., \"Style Tokens: Unsupervised Style Modeling, Control and Transfer in\nEnd-to-End Speech Synthesis\"", "title": "Thanks and additional questions to Paper948 Authors"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311714494, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rygkk305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311714494}}}, {"id": "rJg8bqCa67", "original": null, "number": 17, "cdate": 1542478334042, "ddate": null, "tcdate": 1542478334042, "tmdate": 1542478334042, "tddate": null, "forum": "rygkk305YQ", "replyto": "rJeZUzdsa7", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to anonymous reviewer", "comment": "We thank the reviewer for the great feedback. Below are the itemized responses regarding each comment. We will also incorporate these into the revised version.\n\n\nRe:\nthe paper never specifies the choice of distribution for p(X | y, z).  When generating samples, do you sample from p(X | y, z) or simply output the mean?\n\nAns:\nWe model the distribution as $p(X | y, z) = \\prod p(x_t | x_{1:t-1}, y, z)$, where $p(x_t | x_{1:t-1}, y, z)$ is a fixed-variance isotropic Gaussian distribution, whose mean is predicted by the auto-regressive decoder at step $t$. This is implied by the fact that we minimize the mean squared error of the spectrogram reconstruction as mentioned in the second paragraph in Appendix B.1. We simply output the mean when generating samples. We will update the text to further clarify this. \n\n\nRe:\nFurthermore, it would be nice to discuss whether posterior collapse was an issue, as my intuition tells me a VAE with an autoregressive decoder such as this would run into this issue, though this again will depend on the choice of p(X | y, z).\n\nAns:\nIn our experiments, we observed that the latent variable $z_l$ is always used (i.e., the KL-divergence of $z_l$ never drops to zero), without applying any tricks such as KL-annealing. Previous studies report posterior-collapse of directly conditioned latent variables when using strong models (e.g. auto-regressive model) to parameterize the conditional distribution of text (Bowman et al., 2015, Kim et al., 2018). The reason that the posterior-collapse does not occur in our experiments is likely because of the complexity of the distribution of speech -- it is very high dimensional and contains a lot of detail that is not fully explained by the previous frames. As a result, even though we use an auto-regressive decoder, reconstruction performance can still significantly improve by utilizing the information from $z_l$, and such improvement overpowers the increase in KL-divergence.\nA discussion about the posterior-collapse issue of the discrete latent variable $y_l$ can be found in our response to another comment below. We will also update the text to more explicitly address this issue.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "SJxl-ip2a7", "original": null, "number": 14, "cdate": 1542408952208, "ddate": null, "tcdate": 1542408952208, "tmdate": 1542477996731, "tddate": null, "forum": "rygkk305YQ", "replyto": "BkxEWrFKam", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to anonymous reviewer (Part 1)", "comment": "We thank the reviewer for the great feedback. Below are the itemized responses regarding each comment. We will also incorporate these into the revised version.\n\nDue to the \"max 5000 characters\" limit, we break down our response into multiple comments.\n\n\nRe: \nIt follows from the quote that attibute representations are passed through recurrent layers \u2026 How would one pass a fixed vector through recurrent layers? Moreover, why output of these recurrent layers is concatenated with decoder input to predict next frame? Should not it be concatenated with decoder output? What exactly word 'decoder' signifies in this section?\n\nAns:\n- We thank the reviewer for pointing out the confusion about \u201cdecoder\u201d. The definition of \u201cdecoder\u201d in this work follows the convention in auto-regressive sequence-to-sequence modeling literature (Sutskever et al., 2014, Bahdanau et al., 2015), which parameterizes $p(X | z_l, z_o, text_encodings)$. The text_encoding is computed using an encoder network that takes text ($y_t$) as input. The conditional distribution $p( X | z_l, z_o, y_t)$ which generates the output spectrogram (usually termed \u201cdecoder\u201d in the variational autoencoder literature (Kingma et al., 2014)) is in fact parameterized by both the text encoder and the auto-regressive decoder as $Decoder(z_l, z_o, TextEncoder(y_t) )$ in our work.\n- The quoted text describes the computation involved at each step of the auto-regressive decoder. In our model, the decoder input at each timestep is a concatenated vector composed of four vectors: $z_l$, $z_o$ (or $y_o$), bottlenecked previous output frame, and attention-aggregated text encoding. This decoder input is passed to two layers of LSTM units. The output from the final-layer LSTM unit is concatenated with the decoder input, and passed to a linear layer to predict the mel spectrum of the current frame and the binary end-of-sentence token.\n- At each decoder step, the same $z_l$ and $z_o$ are used for concatenating with the other two vectors. In other words, $z_l$ and $z_o$ are used for global conditioning across all frames in the output mel-spectrum sequence.\n\n\nRe:\nIn the \u201cRelated work\u201d section the first referenced paper [Dilokthanakul et al., 2016] contains a discussion. Some quotes from section 3.4 ('The Over-regularisation Problem'): \n1) 'The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (<references>)' \n2) 'As we show in the experimental section below, this problem of over-regularisation is also prevalent in the assignment of the GMVAE clusters and manifests itself in large degenerate clusters'. \nIn this same section Dilokthanakul et al. briefly (and with references) describe possible approaches (such as KL-term annealing) to mitigating these problems. One of the approaches is discussed in section 4.1.\nUnfortunately, the paper does not touch this topic. It is unclear whether authors of GMVAE-Tacotron follow any of these approaches or succeed entirely without doing so.\n\nAns:\n- There are two latent variables in our graphical model as shown in Figure 1(left): latent attribute class $y_l$ (discrete) and latent attribute representation $z_l$ (continuous). We will discuss the posterior-collapse problem for each of them separately.\n\n- The continuous latent variable $z_l$ is used to directly condition the generation of $X$, along with two other observed variables, $y_o$ and $y_t$. In our experiments, we observed that the latent variable $z_l$ is always used (i.e., the KL-divergence of $z_l$ never drops to zero), without applying any tricks such as KL-annealing. Previous studies report posterior-collapse of directly conditioned latent variables when using strong models (e.g. auto-regressive model) to parameterize the conditional distribution of text (Bowman et al., 2015, Kim et al., 2018). This phenomenon arises from the competition between (1) increasing reconstruction performance by utilizing information provided by the latent variable and (2) decreasing the KL-divergence by having an uninformative latent variable. Auto-regressive models are more likely to converge to the second case during training because the improvement in reconstruction from utilizing the latent variable can be smaller than the increase in KL-divergence. However, this does not always happen, because the amount of improvement resulted from utilizing the information provided by the latent variable depends on the type of data. The reason that the posterior-collapse does not occur in our experiments is likely a consequence of the complexity of the speech sequence distribution, compared to text, such that even when we have an auto-regressive model, reconstruction performance can still be improved significantly by utilizing the information from $z_l$, and such improvement overpowers the increase in KL-divergence."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "SJgpPspnaQ", "original": null, "number": 16, "cdate": 1542409061337, "ddate": null, "tcdate": 1542409061337, "tmdate": 1542409061337, "tddate": null, "forum": "rygkk305YQ", "replyto": "Bklg8iTn6X", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to anonymous reviewer (Part 3)", "comment": "Re:\nNumber of Monte Carlo samples for estimating the objective is set to 1 with no further discussion.\n\nAns:\nThis follows the common practice in the VAE literature (Kingma et al., 2014) when training a model with a batch size that is large enough.  We use a batch size of 256 in all experiments. To estimate the approximated posterior over latent attribute classes, q(y_l | X), using Eq (5), we experimented with using 1 or 10 samples of z_l drawn from q(z_l | X), and did not see much effect on training loss or disentanglement performance.\n\n\nReferences:\nSutskever, Ilya, Oriol Vinyals, and Quoc V. Le. \"Sequence to sequence learning with neural networks.\" Advances in neural information processing systems. 2014.\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. \"Neural machine translation by jointly learning to align and translate.\" arXiv preprint arXiv:1409.0473 (2014).\nKingma, Diederik P., and Max Welling. \"Auto-encoding variational bayes.\" ICLR (2014).\nDilokthanakul, Nat, et al. \"Deep unsupervised clustering with gaussian mixture variational autoencoders.\" arXiv preprint arXiv:1611.02648 (2016).\nBowman, Samuel R., et al. \"Generating sentences from a continuous space.\" arXiv preprint arXiv:1511.06349 (2015).\nKim, Yoon, et al. \"Semi-Amortized Variational Autoencoders.\" arXiv preprint arXiv:1802.02550 (2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "Bklg8iTn6X", "original": null, "number": 15, "cdate": 1542409032134, "ddate": null, "tcdate": 1542409032134, "tmdate": 1542409032134, "tddate": null, "forum": "rygkk305YQ", "replyto": "SJxl-ip2a7", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to anonymous reviewer (Part 2)", "comment": "\nAns (continued):\n- The discrete latent variable $y_l$ indexes the mixture components in the space of latent attribute representation $z_l$. We did not observe the phenomenon of degenerate clusters mentioned in Dilokthanakul et al. (2016) when training our model using the hyperparameters listed in Table 6. Below we identify the difference between our GMVAE and that in Dilokthanakul et al. (2016), which we will refer to as Dil-GMVAE, and explain why our formulation is less likely to have the posterior-collapse issue. \n\n- *Difference between GMVAE and Dil-GMVAE*: We first unify the notation system as it is different in the two papers. Let $X$ be the observed data we want to generate (speech), let $y$ be the discrete latent variable that denotes the mixture components, and let $z$ be the continuous latent variable that is conditionally sampled based on $y$ to directly condition the generation of $X$. \nIn our model, the conditional distribution $p(z | y)$ is a diagonal-covariance Gaussian, $N(z; \\mu_y, \\sigma^2_y)$, parameterized by a mean and a covariance vector. In contrast, in Dil-GMVAE, the conditional distribution of $z$ given $y$ is much more flexible, because it is parameterized using neural networks as $p(z | y) = \\int_\\epsilon N( z; NN_{\\mu}(y, \\epsilon), NN_{\\sigma^2}(y, \\epsilon) ) N(\\epsilon; 0, I) d\\epsilon$, where $NN_{\\mu}$ and $NN_{\\sigma^2}$ are neural networks that take $y$ and an auxiliary noise variable $\\epsilon$ as input to predict the mean and variance of $p(z | y, \\epsilon)$. The conditional distribution of each component in Dil-GMVAE can be seen as a mixture of infinitely many diagonal-covariance Gaussian distributions, which can model much more complex distributions, as shown in Figure 2(d) in Dilokthanakul et al. (2016). That is to say, Dil-GMVAE can be regarded as having a much stronger stochastic decoder that maps y to z compared with our GMVAE. \n\n- *why our formulation is less likely to have the posterior-collapse issue*: Suppose the decoder that maps $z$ to $X$ can benefit from having a very complex, non-Gaussian marginal distribution of $z$, denoted as $p^*(z)$. To obtain such a marginal distribution of $z$ in Dil-GMVAE, the $y$->$z$ decoder can choose between (1) having the same $p(z | y) = p^*(z)$ for all $y$, or (2) having $p(z | y)$ model a different distribution for each $y$, and $\\sum_y p(z | y) p(y) = p^*(z)$. As noted in Dilokthanakul et al. (2016), the KL-divergence term for $y$ in the ELBO prefers degenerate clusters that basically model the same distribution for each cluster. As a result, the first option would be preferred with respect to the ELBO objective compared to the second one, because it does not compromise the expressiveness of $z$ while minimizing the KL-divergence on $y$. In contrast, our GMVAE reduces to a single Gaussian when $p(z | y)$ is the same for all $y$, and hence there is a trade-off between the expressiveness of $p(z)$ and the KL-divergence on $y$.\n\n- In addition, we now explain the connection between posterior-collapse and hyperparameters of the conditional distribution $p(z | y)$ in our work. In our GMVAE model, posterior-collapse of $y$ is equivalent to having the same conditional mean and variance for each mixture component. In the ELBO derived from our model, there are two terms that are relevant to $p(z | y)$, which are (1) the expected KL-divergence on $z$: $E_{q(y | X)} [ KL(q(z | X) || p(z | y)) ]$ and (2) the KL-divergence on $y$: $KL(q(y | X) || p(y))$. The second term encourages a uniform posterior $q(y | X)$, which effectively pulls the conditional distribution for each component to be close to each other, and promotes posterior collapse. In contrast, the first term pulls each $p(z | y)$ to be close to $q(z | X)$ with a force proportional to the posterior of that component, $q(y | X)$. In one extreme, where the posterior $q(y | X)$ is close to uniform, each $p(z | y)$ is also pushed toward the same distribution, $q(z | X)$, which promotes posterior collapse. In the other extreme, where the posterior $q(y | X)$ is close to one-hot, only the conditional distribution of the assigned component $p(z | y)$ is pushed toward $q(y | X)$, and as long as different X are assigned to different components this term is anti-collapse. Therefore, we can see that the effect of the first term on posterior-collapse depends on the entropy of $q(y | X)$, which is controlled by the scale of the variance when the means are not collapsed. This variance is similar to the temperature parameter used in softmax: the smaller the variance is, the more spiky the posterior distribution over y is. This is why we set the initial variance of each component to a smaller value at the beginning of training, which helps avoid posterior collapse.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "HkeAzv1tpm", "original": null, "number": 4, "cdate": 1542154006359, "ddate": null, "tcdate": 1542154006359, "tmdate": 1542296805094, "tddate": null, "forum": "rygkk305YQ", "replyto": "HyxdG2D5h7", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 1)", "comment": "We thank the reviewer for the great feedback. Below are the itemized responses regarding each comment. We will also incorporate these into the revised version.\n\nDue to the \"max 5000 characters\" limit, we break down our response into multiple comments.\n\n\nRe:\nThis work is closely related to Akuzawa et al. (2018). The difference is not properly discussed. \n\nAns:\n- There are two major differences from the latent space modeling perspectives, both of which have large effects on the interpretability of the latent representation and on the ability to control individual attributes. In addition, there is a difference in terms of the adopted neural network architecture, which we think is of less importance.  We will update the text to better describe these differences.\n-- First, we model latent attributes using a *mixture distribution*, which allows automatic discovery of latent attribute clusters. This design makes the resulting latent space much more interpretable, as data-driven clustering leads to meaningful structure. Specifically, we can 1) quickly analyze clusters to get a sense of what they correspond to (similar to GST), as in multispeaker experiments (Section 4.1), and 2) easily analyze inter-cluster variance to identify most distinctive attributes, which e.g. allows us to easily identify the noise dimension (Section 4.2.2). We demonstrate how identifying this structure can be easily used to train a model on noisy found-data that can consistently synthesize clean speech, which has not been done before to the best of our knowledge. Such interpretability cannot be achieved using an isotropic Gaussian prior as in Akuzawa et al., (2018). \n-- Second, we describe an extension of the proposed framework to additionally model speaker attributes with a second mixture distribution, where each speaker corresponds to one mixture component. This formulation learns disentangled speaker and latent attribute representations, which can be applied to one-shot learning to imitating the voice of speakers previously unseen during training (Section 4.4 and Appendix G.2). This functionality was not provided in Akuzawa et al. (2018) either. We will include this discussion in the next version.\n-- Third, regarding the neural network architecture, while Akuzawa et al. (2018) uses the VoiceLoop (Taigman et al., 2018) architecture for the synthesizer, our proposed model is based on Tacotron 2 (Shen et al., 2018). However, we would like to emphasize that such a choice is not the major difference in terms of the ability to control latent attributes, and our approach can also be applied to the VoiceLoop architecture.\n- Asides from the three differences mentioned above, we would also like to point out that our model synthesizes speech with higher quality than the samples presented on the demo page from Akuzawa et al. (2018) (https://akuzeee.github.io/VAELoopDemo/).\n\n\nRe:\nii) In the abstract, \u201cend-to-end text-to-speech\u201d is an unfortunate claim, because the proposed system has two separately trained component: 1) a text to mel-spectrogram model based on Tacotron and, 2) a WaveRNN for waveform synthesis. In ASR, it\u2019s absolutely fine to claim a spectrogram to text model as a end-to-end system, because the wave to spectrogram step is trivial. In TTS, waveform synthesis is a very crucial step and largely determines the final naturalness results. \n\nAns:\nWe agree with the reviewer and will delete the term \u201cend-to-end\u201d to reflect that vocoding from Mel-spectrogram is done with a separate WaveRNN module. In addition, we would like to restate that the focus of this paper is the ability to control latent attributes, which is achieved by the GMVAE-Tacotron module, and having the WaveRNN vocoder only improves the audio fidelity, similar to the observation made in GST (Wang et al., 2018). For example, we observed that the WADA-SNR metric has no difference when using Griffin-Lim or WaveRNN for vocoding.\n\n\nRe:\niii) In experiment, one need include the MOS score of ground truth for comparison or debiasing. \n\nAns:\nWe are now evaluating the MOS score of ground truth, and will reply once the evaluation is finished.\n\nEDIT: We have obtained the ground truth MOS scores. Please see the comment below."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "Sklk9EMiaQ", "original": null, "number": 10, "cdate": 1542296711272, "ddate": null, "tcdate": 1542296711272, "tmdate": 1542296711272, "tddate": null, "forum": "rygkk305YQ", "replyto": "HkeAzv1tpm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "MOS of ground truth", "comment": "Below are the MOS scores of the ground truth, which we will add to the corresponding tables in the paper text:\n- Multi-speaker English corpus (top-line for Table 1): 4.48 \u00b1 0.04\n- Single-speaker audiobook corpus (Table 2): 4.67 \u00b1 0.04\n- Crowd-sourced audiobook corpus (Table 5):\n  - seen clean (SC): 4.60 \u00b1 0.07\n  - seen noisy (SN): 4.45 \u00b1 0.08\n  - unseen clean (UC): 4.54 \u00b1 0.08\n  - unseen noisy (UN): 4.34 \u00b1 0.08\nNote that the ground truth MOS score of the multi-speaker English corpus serves as the very top-line for models trained on Noisy multi-speaker English corpus (Section 4.2), because those audio samples are clean and represent the best case scenario compared to the data used for training models shown in Table 1, which had noised mixed in.\nThe proposed model achieved comparable MOS to the ground truth on the single-speaker audiobook corpus. On the multi-speaker audiobook corpus, the MOS scores of the proposed model are 0.14 - 0.42 behind those of the ground truth. For the model trained on the noisy multi-speaker English corpus, the MOS score is 0.23 lower than the very top-line.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "BkxEWrFKam", "original": null, "number": 1, "cdate": 1542194428128, "ddate": null, "tcdate": 1542194428128, "tmdate": 1542194428128, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "content": {"comment": "Authors describe a way to learn disentagled inner representations for prosody using variational autoencoder approach. This allows for fine-grained control of synthesized speech parameters such as tempo, accent, etc.\n\nPresented results are stunning, the writing is mostly clear. My concerns are:\n\n1. Quote (B.1): \"To condition the output on additional attribute representations, the decoder is extended to consume z_l and z_o (or y_o) after passing them through a stack of two uni-directional LSTM layers with 1024 units. The output from the stacked LSTM is concatenated with the decoder input, and linearly projected to predict the mel spectrum of the current frame, as well as an end-of-sentence token.\"\nIt follows from the quote that attibute representations are passed through recurrent layers, while the next subsection (B.2) describes a mean pooling component for summarizing outputs across time in latent and observed encoders, so encoders' outputs are fixed-dimension vectors. How would one pass a fixed vector through recurrent layers? Moreover, why output of these recurrent layers is concatenated with decoder input to predict next frame? Should not it be concatenated with decoder output? What exactly word 'decoder' signifies in this section?\n\nA more careful and detailed description of modifications to Tacotron 2 architecture is desirable.\n\n2. Learning a good latent disentangled representation using VAE is known to be not an easy task (https://openreview.net/references/pdf?id=Sy2fzU9gl)\nActually, in the last paragraph of 'Related work' section the first referenced paper [Dilokthanakul et al., 2016] contains a discussion. Some quotes from section 3.4 ('The Over-regularisation Problem'):\n1) 'The possible overpowering effect of the regularisation term on VAE training has been described numerous times in the VAE literature (<references>)'\n2) 'As we show in the experimental section below, this problem of over-regularisation is also prevalent in the assignment of the GMVAE clusters and manifests itself in large degenerate clusters'.\nIn this same section Dilokthanakul et al. briefly (and with references) describe possible approaches (such as KL-term annealing) to mitigating these problems. One of the approaches is discussed in section 4.1.\n\nUnfortunately, the paper does not touch this topic. It is unclear whether authors of GMVAE-Tacotron follow any of these approaches or succeed entirely without doing so.\n\n3. Number of Monte Carlo samples for estimating the objective is set to 1 with no further discussion.", "title": "Great result but some important details are missing"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311714494, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rygkk305YQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311714494}}}, {"id": "rklBfjyYTm", "original": null, "number": 9, "cdate": 1542155021365, "ddate": null, "tcdate": 1542155021365, "tmdate": 1542155021365, "tddate": null, "forum": "rygkk305YQ", "replyto": "BJg-gjyFam", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 4)", "comment": "\nRe:\nWhat are different modes using the proposed model? \n\nAns:\n- We are not sure if the reviewer meant \u201cwhat are the attributes presented in the generated samples when conditioning on the mode of each mixture component?\u201d If we are interpreting it wrong, we would like to ask the reviewer for clarification so we can address the question correctly.\n- The mode (mean) of each mixture component presents the average latent attributes when conditioned for generation. We demonstrated in Figure 3 examples of conditioning on two different modes for generation.\n\n\nRe:\nFor example a reliable numerical evidence is needed on page 4 following \"We also found...\", page 5 following \"We discovered....\", page 6 following \"Figure 7(b)...\". \n\nAns:\nWe are looking into metrics to quantitatively evaluate the effect of varying different latent dimensions on average pitch and speech duration now, and will update this thread once the results are available.  But see our response above for caveats about the reliability of this approach.\n\n\nReferences:\nKingma, Diederik P., and Prafulla Dhariwal. \"Glow: Generative flow with invertible 1x1 convolutions.\" arXiv preprint arXiv:1807.03039 (2018).\nWang, Yuxuan, et al. \"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\" in International Conference on Machine Learning (ICML), 2018.\nAkuzawa, Kei, Yusuke Iwasawa, and Yutaka Matsuo. \"Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder.\" in Interspeech, 2018.\nGlorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" in International Conference on Artificial Intelligence and Statistics (AISTATS), 2010.\nJia, Ye, et al. \"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis.\" arXiv preprint arXiv:1806.04558 (2018).\nSkerry-Ryan, R. J., et al. \"Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron.\" in International Conference on Machine Learning (ICML), 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "BJg-gjyFam", "original": null, "number": 8, "cdate": 1542154985230, "ddate": null, "tcdate": 1542154985230, "tmdate": 1542154985230, "tddate": null, "forum": "rygkk305YQ", "replyto": "B1l-39yFpX", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 3)", "comment": "\nRe:\nWhy both negative results are in the appendix?\n\nAns:\n- We are not certain which results the reviewer was referring to here. There are results in Table 8 and Table 9 where the proposed model does not have the best performance. If this comment refers to something else, please clarify so we can better address the concern.\n- First of all, we would like to point out that 1) These results are not really negative as explained in the accompanying text, and 2) We were not trying to hide anything. These results were not put it in the main text simply because we felt that they were less critical to validating the main contributions of the paper compared to those in the main text, and we did not have sufficient space to describe extensive experiments.\n- Here we will discuss each experiment in greater detail.\n-- For the speaker similarity test in Table 9 in Appendix G.2, our proposed model is compared with ground truth or other models on three sets: seen clean speakers (SC), seen noisy speakers (SN), and unseen clean speakers (UC). \n--- On the UC set, we compare with d-vector systems (Jia et al., 2018), which is a Tacotron-based system that utilizes a separately trained speaker verification system to extract speaker embeddings. To draw an analogy to our framework, the d-vector model can be regarded as training the q(z_o | X) module separately using a speaker verification task. In Table 9, we compare the proposed model to two d-vector systems, one of which utilizes a speaker verification system trained on the same training set as our proposed model, which contains ~1k speakers, and the other utilizes a speaker verification system trained on a much larger set that contains ~18k speakers. Our proposed model outperforms the former significantly (2.79 vs 2.23), which is the system that uses exactly the same amount of resources as our proposed model and hence is a fair comparison. The proposed model is worse than the latter d-vector-based model: d-vector (large). However we emphasize that this is not a fair comparison, as that d-vector system utilizes a speaker representation extractor *trained on 20 times more speakers than the proposed model*. Furthermore, we point out that our proposed model is complementary to d-vector systems. Incorporating the high quality speaker transfer from the d-vector model with the strong controllability in the GMVAE is something we intend to pursue in future work.\n--- On the SN set, we explain in text that \u201csimilarity of the acoustic conditions between the paired utterances biased the speaker similarity ratings.\u201d In other words, even when both utterances come from the same speaker, we found that the raters still tended to assign a lower similarity score to the pair with different acoustic conditions than the pair with the matching acoustic conditions. We verified this phenomenon by showing that the human rated speaker similarity score for a speaker that has recordings with significant variation in acoustic conditions (Ground truth w/ channel variation) is much lower (3.30), compared to the ground truth similarity score of the clean speakers (4.33) reported in Jia et al, 2018. The baseline without denoising achieves the highest MOS score (3.83), because it reproduces the noise of the noisy speakers, while the baseline+denoise model and our proposed model have lower MOS scores because the acoustic conditions are changed comparing to the reference noisy utterances. This result implies that this subjective similarity test is not reliable in the presence of acoustic condition variation, and requires additional work to design speaker similarity test that is unbiased to these nuisance factors.\n-- The parallel style transfer experiment in Appendix F.1 evaluates the ability of the model to reconstruct the reference utterance frame-by-frame, which is highly correlated with the latent space capacity as noted in Skerry-Ryan et al, 2018. We showed in the table that increasing the latent space dimensionality of z_l in our proposed model from 16 to 32 greatly reduces the MCD and FFE gap to the GST model which performs best, and explained in text that the GST model still has more degrees of freedom in the latent space and a larger number of parameters than our proposed model. We can expect the model to outperform GST on these two metrics if we further increase the dimensionality of z_l; however, this would likely reduce interpretability and generalization of the latent attribute control.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "B1l-39yFpX", "original": null, "number": 7, "cdate": 1542154920965, "ddate": null, "tcdate": 1542154920965, "tmdate": 1542154920965, "tddate": null, "forum": "rygkk305YQ", "replyto": "HygDIcyFTm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 2)", "comment": "\nRe:\nThe model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. \nAns:\n- Throughout the paper, we use y_* to denote discrete class variables, and z_* to denote continuous variables. We attempted to explain y_l clearly In Section 2.1 of the text and in Figure 1, e.g. it says \u201cy_l is a K-way categorical discrete variable\u201d that identifies the discrete cluster that the latent attribute representation z_l is sampled from. If the reviewer has any specific comments about how this wording is confusing or suggestions for how it can be clarified, we would be more than happy to incorporate them.\nBased on this notation system, we use y_o (the observed class) to denote the categorical variable that represents speaker IDs. The cardinality of y_o is the number of speakers in the training set. We will update the text in the paper to make this more explicit.\n- Regarding computation of these two variables, since the observed class y_o is an observed variable (i.e. the speaker ID of an utterance), we do not need to compute it. In contrast, the latent class y_l is a latent variable, and we compute the posterior of y_l given an utterance using Eq. (2), whose detailed derivation can be found in Appendix A.1.\n- Regarding initialization, each of the K possible values of y_l is associated with a diagonal-covariance Gaussian distribution which models the conditional distribution of z_l for that mixture component. Let D be the dimensionality of z_l. We implement this using two K-by-D lookup tables, one to store the mean of each mixture component, and the other for the corresponding log variance. Both the mean and the log variance table are trainable. The variances are initialized to a constant value listed in Table 6. Initial values of the mean is set with Xavier Initialization (Glorot and Bengio, 2010). For the extension proposed in Section 2.3, each value of y_o is also associated with a diagonal-covariance Gaussian that models the conditional distribution of z_o for a speaker. Likewise, the mean table is initialized using Xavier initialization and the initial value of the variance is set to a constant listed in Table 6. We will update the text to clarify this.\n- Regarding setting, we are not certain if the reviewer meant \u201chow to generate samples conditioning on some values of y_l/y_o.\u201d Please clarify if we misinterpreted. If our interpretation is correct, for the proposed model shown in Figure 1 (left), we sample z_l from the conditional distribution of the y_l-th mixture component, which is diagonal-covariance Gaussian whose mean and log variance are specified in two lookup tables. For the extended proposed model shown in Figure 1 (right), we sample z_l in the same way as mentioned above, and similarly sample z_o from the conditional distribution of the y_o-th mixture components, which is also diagonal-covariance Gaussian whose mean and log variance are specified in another two lookup tables.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "H1xiUwyFpQ", "original": null, "number": 5, "cdate": 1542154067286, "ddate": null, "tcdate": 1542154067286, "tmdate": 1542154067286, "tddate": null, "forum": "rygkk305YQ", "replyto": "HkeAzv1tpm", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 2)", "comment": "\nRe:\niv) Did you try different values of D other than 16? When you check the meaning of different dimensions, how many dimensions are meaningful? How many are meaningless, or even just dummy?\n\nAns: \n- As shown in Table 8, we found that increasing the dimensionality of z from 16 to 32 improves reconstruction quality; however, it also increases the difficulty of interpreting each dimension. On the other hand, reducing the dimensionality too much would result in insufficient modeling capacity for latent attributes, however we have not carefully explored this lower bound.\n- Empirically, we found 16-dimensional z_l to be appropriate for capturing the salient attributes one would like to control in the four datasets we experimented with. When checking the meaning of each dimensions, we find the majority of the dimensions to be interpretable, and the number of dummy dimensions which did not seem to affect the model output varied across datasets, as each of them inherently has variation across a different number of unlabeled attributes. For example, the model trained on the multi-speaker English corpus (Section 4.1) has are four dummy dimensions of z_l that do not affect the output. On the other hand, for the model trained on the crowd-sourced audio book corpus (Section 4.4), which contains considerably more variation in style and prosody, we only found one dummy dimension of z_l.\n\n\nRe:\nModerate Novelty\n\nAns:\n- With respect to the novelty of this paper, although we build off of strong previous work, we believe the proposed model is a significant advance compared to the existing state of the art in controllable neural TTS. We propose a novel and more principled probabilistic hierarchical generative model which significantly improves 1) disentangled attribute control compared to e.g. the GST model of Wang et al. (2018), and 2) interpretability as well as quality compared to e.g. Akuzawa et al. (2018). Another advantage compared to the GST model is that the proposed model has a straightforward sampling procedure, making it easy to generate varied samples for the same text.\n- Using two mixture distributions to factor the latent encoding and separately model speaker attributes and latent attributes is another novel aspect of this work. This formulation enables the inference model to disentangle speaker and latent attribute representations by encoding them into separate latent variables, which can be applied to condition the generation on the speaker attribute and latent attribute inferred from different reference utterances. This functionality was not provided in any previous work.\n- In addition, this work is the first that we know of to successfully train a high-quality controllable text-to-speech system on real found data containing significant variation in audio quality, acoustic conditions, as well as prosody and style. Previous results on this dataset focused only on speaker modeling (Ping, et al 2017, Nachmani, et al 2018, Arik, et al 2018, Jia, et al 2018), and did not address explicit control of prosody and background noise. Our proposed model is capable of inferring the speaker attribute representation from a noisy utterance of a previously unseen speaker, and using it to synthesize high-quality clean speech that approximates the voice of that unseen speaker. To the best of our knowledge, no other work has accomplished this.\n- We will edit the abstract and introduction to better highlight the novelty of our contribution in the next revision of the text. \n\n\nReferences:\nAkuzawa, Kei, Yusuke Iwasawa, and Yutaka Matsuo. \"Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder.\" in Interspeech, 2018.\nTaigman, Yaniv, et al. \"VoiceLoop: Voice Fitting and Synthesis via a Phonological Loop.\" in International Conference on Learning Representations (ICLR), 2018.\nShen, Jonathan, et al. \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.\" International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\nWang, Yuxuan, et al. \"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\" in International Conference on Machine Learning (ICML), 2018.\nPing, Wei, et al. \"Deep voice 3: 2000-speaker neural text-to-speech.\" arXiv preprint arXiv:1710.07654 (2017).\nNachmani, Eliya, et al. \"Fitting New Speakers Based on a Short Untranscribed Sample.\" arXiv preprint arXiv:1802.06984 (2018).\nArik, Sercan O., et al. \"Neural Voice Cloning with a Few Samples.\" arXiv preprint arXiv:1802.06006 (2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "HJgljrkKpX", "original": null, "number": 3, "cdate": 1542153623624, "ddate": null, "tcdate": 1542153623624, "tmdate": 1542153623624, "tddate": null, "forum": "rygkk305YQ", "replyto": "BkeBJHkt6X", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 2)", "comment": "Re:\nThe manuscript has some curious decisios in its concepts - I do not see the architecture really hiearchial, nor end to end. I would prefer modifications on the paper that concentrate on the truly novel features. \n\nAns:\n- As illustrated in Figure 1, the VAE prior distribution is a hierarchical model -- i.e. a Gaussian mixture model, where one first samples a mixture component y_l, and then samples a continuous vector z_l conditioning on that mixture component. By introducing such a structured prior, our model provides better interpretability compared to the model which uses an isotropic Gaussian prior for latent attributes (Akuzawa et al, 2018). For example, our model can automatically discover clusters of latent attributes (Section 4.1), and we easily analyze inter-cluster variance with linear discriminative analysis to identify the most distinctive attributes, e.g. as in identifying the noise dimension (Section 4.2.2). Designing such a hierarchical structure to model latent attributes for interpretability is our main contribution, which we feel is clear from the text. The sequence-to-sequence speech synthesizer (bottom block in Figure 8) itself is based on standard architecture in the literature (Wang et al., 2017 and Shen et al., 2018), and is indeed not especially hierarchical.\n- Regarding \u201cend-to-end\u201d, as mentioned in the response to the previous question, we agree with the reviewer and will delete the term from the text.\n\n\nReferences:\nShen, Jonathan, et al. \"Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.\" International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\nKalchbrenner, Nal, et al. \"Efficient Neural Audio Synthesis.\"  in International Conference on Machine Learning (ICML), 2018.\nWang, Yuxuan, et al. \"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis.\" in International Conference on Machine Learning (ICML), 2018.\nAkuzawa, Kei, Yusuke Iwasawa, and Yutaka Matsuo. \"Expressive Speech Synthesis via Modeling Expressions with Variational Autoencoder.\" in Interspeech, 2018.\nWang, Yuxuan, et al. \"Tacotron: Towards end-to-end speech synthesis.\" in Interspeech, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "BkeBJHkt6X", "original": null, "number": 1, "cdate": 1542153437107, "ddate": null, "tcdate": 1542153437107, "tmdate": 1542153437107, "tddate": null, "forum": "rygkk305YQ", "replyto": "Byx-PFs1p7", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 1)", "comment": "We thank the reviewer for the great feedback. Below are the itemized responses regarding each comment. We will also incorporate these into the revised version.\n\nDue to the \"max 5000 characters\" limit, we break down our response into multiple comments.\n\n\nRe: \nThe authors describe the conditioned GAN model to generate speaker conditioned Mel spectra. They augment the z-space corresponding to the identification with latent variables that allow a richer set of produced audio. In a way this is like a partially conditioned model that has \"extra\" degrees of freedom. It looks that the \"latent\" variables are just concaneted to the \"original\" set of z-values (altough with particular conditions to maximize independence). The conditioning of the z-space has originality in it and may provide interesting to the audience. Ultimately one coud think about z-space direction being totally mapped to specific features of the produced signal.\n\nAns: \n- The proposed model is not a conditional GAN, but is in fact a conditional VAE which uses a Gaussian mixture prior for latent attribute representations (z_l), and a Gaussian mixture prior for speaker attribute representations (z_o) where each speaker corresponds to one mixture component. While the speaker identity (y_o) is given, the latent attribute representations and speaker attribute representations (z_l and z_o) are both continuous latent variables, the posteriors of which are estimated from an utterance by neural networks, denoted in Figure 8 as latent encoder and observed encoder, respectively.\n- We are not sure if the reviewer is referring to z_o as the \u201coriginal set of z-values\u201d and z_l as the \u201clatent variables.\u201d As mentioned above, during training, both z_o and z_l used for generating the target speech X are latent variables inferred directly from the spectrogram, as shown in Eq. (4). This is different from the typical speaker embedding-based method, which uses a fixed embedding learned for each speaker stored in an embedding table. From the perspective of modeling speakers, our models has the advantage that it is capable of inferring the z_o for an unseen speaker and imitate the voice of that unseen speaker, while the speaker embedding method cannot.\n\n\nRe: \nAlso, I am curious to know how the Mel spectra are used to produce the actual sound wave - as the phase information is not present if utilizing only the spectral amplitude\u2026. However, in this case the audio samples show a pretty nice generation of sound. However, it is not really end to end.\n\nAns:\n- To synthesize waveforms from Mel spectrograms, we closely follow the Tacotron 2 (Shen et al, 2018) framework, which trains a neural network-based vocoder to predict waveforms conditioning on the Mel-spectrograms generated by GMVAE-Tacotron. In this work, we use WaveRNN (Kalchbrenner et al, 2018) as the neural vocoder, as described in Section 2.4. We will update the text to further clarify this.\n- We agree with the reviewer about the wording and will delete the term \u201cend-to-end\u201d to reflect the use of neural vocoder. In addition, we would like to restate that the focus of this paper is the ability to control latent attributes, which is achieved by the GMVAE-Tacotron module, and having the WaveRNN vocoder only improves the audio fidelity, similar to the observation made in GST (Wang et al., 2018). For example, we observed that the WADA-SNR results have no difference when using Griffin-Lim or WaveRNN for vocoding.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609125, "tddate": null, "super": null, "final": null, "reply": {"forum": "rygkk305YQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper948/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper948/Authors|ICLR.cc/2019/Conference/Paper948/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers", "ICLR.cc/2019/Conference/Paper948/Authors", "ICLR.cc/2019/Conference/Paper948/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609125}}}, {"id": "HyxdG2D5h7", "original": null, "number": 2, "cdate": 1541205008108, "ddate": null, "tcdate": 1541205008108, "tmdate": 1541533553208, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "content": {"title": "A good work", "review": "This paper proposes a two layer latent variable model to obtain disentangled latent representation, thus facilitates fine-grained control over various attributes including noise level, speaker rate etc.\n\nDetailed comments:\n\ni) This work is closely related to Akuzawa et al. (2018). The difference is not properly discussed. \n\nii) In the abstract, \u201cend-to-end text-to-speech\u201d is an unfortunate claim, because the proposed system has two separately trained component: 1) a text to mel-spectrogram model based on Tacotron and, 2) a WaveRNN for waveform synthesis.  In ASR, it\u2019s absolutely fine to claim a spectrogram to text model as a end-to-end system, because the wave to spectrogram step is trivial.  In TTS, waveform synthesis is a very crucial step and largely determines the final naturalness results. \n\niii) In experiment, one need include the MOS score of ground truth for comparison or debiasing.\n\niv) Did you try different values of D other than 16? When you check the meaning of different dimensions, how many dimensions are meaningful? How many are meaningless, or even just dummy?\n\nIn summary,  \npros:\n- A good work with impressive results.\ncons:\n- Related work need to be properly discussed. \n- Doesn\u2019t include the MOS of ground truth.\n- Moderate novelty. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "cdate": 1542234340315, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygkk305YQ", "replyto": "rygkk305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335839423, "tmdate": 1552335839423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hye08Kwcnm", "original": null, "number": 1, "cdate": 1541204310001, "ddate": null, "tcdate": 1541204310001, "tmdate": 1541533552998, "tddate": null, "forum": "rygkk305YQ", "replyto": "rygkk305YQ", "invitation": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "content": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "review": "Quality: This submission claims to present a model that can control non-annotated attributes such as speaking style, accent, background noise, etc. Though empirical evidence in the form of numerical measurements is presented for some controllable attributes more evidence other than individual samples and authors claims is needed. For example a reliable numerical evidence is needed on page 4 following \"We also found...\", page 5 following \"We discovered....\", page 5 following \"It clearly presents...\", page 5 following \"Drawing samples...\" evidence is given only for 1 dimension, page 6 following \"Figure 7(b)...\". \n\nClarity: The model is simple though the exact form and nature of observed and latent class variables could be made more explicit. Including how they are computed/initialised/set. What are different modes using the proposed model? Why both negative results are in the appendix? \n\nOriginality: moderately\n\nSignificance: moderately\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper948/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Generative Modeling for Controllable Speech Synthesis", "abstract": "This paper proposes a neural end-to-end text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, it is capable of consistently synthesizing high-quality clean speech regardless of the quality of the training data for the target speaker.", "keywords": ["speech synthesis", "representation learning", "deep generative model", "sequence-to-sequence model"], "authorids": ["wnhsu@mit.edu", "ngyuzh@google.com", "ronw@google.com", "heigazen@google.com", "yonghui@google.com", "logpie@gmail.com", "yuancao@google.com", "jiaye@google.com", "zhifengc@google.com", "jonathanasdf@google.com", "drpng@google.com", "rpang@google.com"], "authors": ["Wei-Ning Hsu", "Yu Zhang", "Ron J. Weiss", "Heiga Zen", "Yonghui Wu", "Yuxuan Wang", "Yuan Cao", "Ye Jia", "Zhifeng Chen", "Jonathan Shen", "Patrick Nguyen", "Ruoming Pang"], "TL;DR": "Building a TTS model with Gaussian Mixture VAEs enables fine-grained control of speaking style, noise condition, and more.", "pdf": "/pdf/dc17849f3867fe07f78c874d2397170d7d3a6bd2.pdf", "paperhash": "hsu|hierarchical_generative_modeling_for_controllable_speech_synthesis", "_bibtex": "@inproceedings{\nhsu2018hierarchical,\ntitle={Hierarchical Generative Modeling for Controllable Speech Synthesis},\nauthor={Wei-Ning Hsu and Yu Zhang and Ron Weiss and Heiga Zen and Yonghui Wu and Yuan Cao and Yuxuan Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rygkk305YQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper948/Official_Review", "cdate": 1542234340315, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rygkk305YQ", "replyto": "rygkk305YQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper948/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335839423, "tmdate": 1552335839423, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper948/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 30}