{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396449504, "tcdate": 1486396449504, "number": 1, "id": "rJFXnf8_e", "invitation": "ICLR.cc/2017/conference/-/paper232/acceptance", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable. \n \n Originality, Significance:\n  The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.\n The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.\n That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396450021, "id": "ICLR.cc/2017/conference/-/paper232/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396450021}}}, {"tddate": null, "tmdate": 1484653334669, "tcdate": 1484653334669, "number": 7, "id": "r1kmXFj8e", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "writers": ["~Marc_Brockschmidt1"], "content": {"title": "General response", "comment": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. https://arxiv.org/abs/1602.03218\n[3] Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow.\n    Lifelong Perceptual Programming by Example. https://arxiv.org/abs/1611.02109"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1482211856497, "tcdate": 1482211856497, "number": 5, "id": "BkuzfSIVx", "invitation": "ICLR.cc/2017/conference/-/paper232/official/review", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer5"], "content": {"title": "very interesting but probably too derivative from earlier already published work", "rating": "5: Marginally below acceptance threshold", "review": "The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512654760, "id": "ICLR.cc/2017/conference/-/paper232/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer3", "ICLR.cc/2017/conference/paper232/AnonReviewer2", "ICLR.cc/2017/conference/paper232/AnonReviewer4", "ICLR.cc/2017/conference/paper232/AnonReviewer5"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512654760}}}, {"tddate": null, "tmdate": 1481997680993, "tcdate": 1481997649371, "number": 4, "id": "SytIag7Vx", "invitation": "ICLR.cc/2017/conference/-/paper232/official/review", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer4"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "The paper discusses a range of modelling choices for designing differentiable programming languages. Authors propose 4 recommendations that are then tested on a set of 13 algorithmic tasks for lists, such as \"length of the list\", \"return k-th element from the list\", etc. The solutions are learnt from input/output example pairs (5 for training, 25 for test).\n\nThe main difference between this work and differentiable architectures, like NTM, Neural GPU, NRAM, etc. is the fact that here the authors aim at automatically producing code that solves the given task.\n\nMy main concern are experiments - it would be nice to see a comparison to some of the neural networks mentioned in related work. Also, it would be useful to see how this model is doing on typical problems used by mentioned neural architectures (problems such as \"sorting\", \"merging\", \"adding\"). I'm wondering how this is going to generalize to other types of programs that can't be solved with prefix-loop-suffix structure.\n\nIt is also concerning that although  1) the tasks are simple, 2) the structure of the solution is very restricted and 3) model is using extensions doing most of the work, the proposed model still fails to find solutions (example: A+L model that has \u201cloop\u201d fails to solve \u201clist length\u201d task in 84% of the runs).\n\n\nPro:\n- generates code rather than black-box neural architecture\n- nice that it can learn from very few examples\n\nCons:\n- weak results, works only for very simple tasks, missing comparison to neural architectures", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512654760, "id": "ICLR.cc/2017/conference/-/paper232/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer3", "ICLR.cc/2017/conference/paper232/AnonReviewer2", "ICLR.cc/2017/conference/paper232/AnonReviewer4", "ICLR.cc/2017/conference/paper232/AnonReviewer5"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512654760}}}, {"tddate": null, "tmdate": 1481983158383, "tcdate": 1481983158383, "number": 3, "id": "r1C2NaGEg", "invitation": "ICLR.cc/2017/conference/-/paper232/official/review", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper presents design decisions of TerpreT [1] and experiments about learning simple loop programs and list manipulation tasks. The TerpreT line of work (is one of those which) bridges the gap between the programming languages (PL) and machine learning (ML) communities. Contrasted to the recent interest of the ML community for program induction, the focus here is on using the design of the programming language to reduce the search space. Namely, here, they used the structure of the control flow (if-then-else, foreach, zipWithi, and foldli \"templates\"), immutable data (no reuse of a \"neural\" memory), and types (they tried penalizing ill-typedness, and restricting the search only to well-typed programs, which works better). My bird eye view would be that this stands in between \"make everything continuous and perform gradient descent\" (ML) and \"discretize all the things and perform structured and heuristics-guided combinatorial search\" (PL).\n\nI liked that they have a relevant baseline (\\lambda^2), but I wished that they also included a fully neural network program synthesis baseline. Admittedly, it would not succeed except on the simplest tasks, but I think some of their experimental tasks are simple enough for \"non-generating code \" NNs to succeed on.\n\nI wished that TerpreT was available, and the code to reproduce these experiments too.\n\nI wonder if/how the (otherwise very interesting!) recommendations for the design of programming languages to perform gradient descent based-inductive programming would hold/perform on harder task than these loops. Even though these tasks are already interesting and challenging, I wonder how much of these tasks biased the search for good subset of constraints (e.g. those for structuring the control flow).\n\nOverall, I think that the paper is good enough to appear at ICLR, but I am no expert in program induction / synthesis.\n\nWriting:\n - The paper is at times hard to follow. For instance, the naming scheme of the model variants could be summarized in a table (with boolean information about the features it embeds).\n - Introduction: \"basis modern computing\" -> of\n - Page 3, training objective: \"minimize the cross-entropy between the distribution in the output register r_R^{(T)} and a point distribution with all probability mass on the correct output value\" -> if you want to cater to the ML community at large, I think that it is better to say that you treat the output of r_R^{(T)} as a classification problem with the correct output value (you can give details and say exactly which type of criterion/loss, cross-entropy, you use).\n\n\n[1] \"TerpreT: A Probabilistic Programming Language for Program Induction\", Gaunt et al. 2016", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512654760, "id": "ICLR.cc/2017/conference/-/paper232/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer3", "ICLR.cc/2017/conference/paper232/AnonReviewer2", "ICLR.cc/2017/conference/paper232/AnonReviewer4", "ICLR.cc/2017/conference/paper232/AnonReviewer5"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512654760}}}, {"tddate": null, "tmdate": 1481931891041, "tcdate": 1481931891041, "number": 2, "id": "BksdhxGVg", "invitation": "ICLR.cc/2017/conference/-/paper232/official/review", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer3"], "content": {"title": "Not sure if very interesting to the ICLR community", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i\u2019m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn\u2019t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512654760, "id": "ICLR.cc/2017/conference/-/paper232/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer3", "ICLR.cc/2017/conference/paper232/AnonReviewer2", "ICLR.cc/2017/conference/paper232/AnonReviewer4", "ICLR.cc/2017/conference/paper232/AnonReviewer5"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512654760}}}, {"tddate": null, "tmdate": 1481827952206, "tcdate": 1481827952200, "number": 6, "id": "ByuO8PeVe", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "B1nNqBjXl", "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "writers": ["~Marc_Brockschmidt1"], "content": {"title": "Re: Dealing with noise", "comment": "Thanks for the comment. We agree with the commenter that the above statement is incomplete. What we should have said is that dealing with noisy data is a more difficult problem and requires non-trivial changes to standard synthesis techniques (e.g., demonstrated by requiring dedicated papers). It may be the case that gradient-based optimization over program space is relatively more effective in this case, but to our knowledge this direction has not been explored yet. The point is just that having good ways of searching over program space as a gradient-based optimization problem may enable new ways of handling noisy data, as it also enables new models that mix program structure and neural network structure (like in the \"Lifelong Perceptual Programming by Example\" paper mentioned above). Thus, we believe it valuable to continue studying how to make gradient-based program search work better, even if it's currently not the best technique on benchmarks.\n\nAs discussed in the paper, a Lambda2 failure is reported as success rate \"0\". In all cases, failure is due to exceeding a time bound of 600s. This detail was not in the paper, but we will make it clear in the next revision.\nWe have now also run Lambda2 again with significantly longer timeouts. \"getIdx\" and \"findLastIdx\" are eventually solved (after 4166s resp. 5896s), but all other examples reported with a \"0\" success ratio (\"last2\" and the bigger get$N/dup$N examples) run into a timeout of 6000s."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1481826637473, "tcdate": 1481826637467, "number": 1, "id": "Byr8WPl4e", "invitation": "ICLR.cc/2017/conference/-/paper232/official/review", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer1"], "content": {"title": "Weak accept", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512654760, "id": "ICLR.cc/2017/conference/-/paper232/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer3", "ICLR.cc/2017/conference/paper232/AnonReviewer2", "ICLR.cc/2017/conference/paper232/AnonReviewer4", "ICLR.cc/2017/conference/paper232/AnonReviewer5"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512654760}}}, {"tddate": null, "tmdate": 1481718588660, "tcdate": 1481718588654, "number": 5, "id": "BkSHjh0Qe", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "ryMuLaLmx", "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "writers": ["~Marc_Brockschmidt1"], "content": {"title": "Re: Comparison with neural network models", "comment": "While we agree it would be nice to compare to a model like Neural Random Access Machines,  we have not done this yet for two reasons. Firstly, our problem setting is somewhat different, as we only consider very few (5) input-output examples, which might be problematic when training an RNN-based controller due to overfitting. Secondly, the NRAM paper notes that a range of tricks (most notably, curriculum learning and extensive hyperparameter searches) were required for good results. As we are not aware of a well-tested open source implementation of the NRAM model, doing these experiments would entail development, debugging and significant computational effort for not necessarily comparable results (due to implementation details), so we feel that the amount of work that it would take to provide these baseline experiments outweighs the benefit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1481503200181, "tcdate": 1481493044100, "number": 4, "id": "B1nNqBjXl", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "ByjI1Qm7l", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Dealing with noise", "comment": "The claim in point (1) seems somewhat incorrect: some end-user synthesizers in PL have a way to deal with noisy data (a standard problem as a user can mistype an example, etc). \n\nSee for instance:\nTransforming spreadsheet data types using examples. POPL 2016: 343-356\n\nThere are also entire papers dedicated to dealing with noise, e.g.:\nLearning programs from noisy data. POPL 2016: 761-774\n\nFor point 2, what is going on with lambda 2 for the 3 programs where it is stated to be 0.00? The other models also do not do well there.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1481197162306, "tcdate": 1481197162302, "number": 3, "id": "ryMuLaLmx", "invitation": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer3"], "content": {"title": "Comparison with neural network models", "question": "It would be interesting to know how well one of the neural network models does on the evaluated tasks, as additional baseline besides lambda squared. While they do not produce source code they can still probably solve the tasks, but to which extent ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481197162841, "id": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer5", "ICLR.cc/2017/conference/paper232/AnonReviewer3"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481197162841}}}, {"tddate": null, "tmdate": 1480957779222, "tcdate": 1480957779211, "number": 3, "id": "ByjI1Qm7l", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "B1BCBBJ7e", "signatures": ["~Marc_Brockschmidt1"], "readers": ["everyone"], "writers": ["~Marc_Brockschmidt1"], "content": {"title": "Re: Just an application of TerpreT?", "comment": "We've briefly touched on this question in the Introduction (2nd paragraph) and Conclusion (last paragraph). Fundamentally, we see two important points in reply to your question:\n\n(1) Synthesis techniques from the programming languages community are doing well on simple data types such as integers or lists of integers. They offer no natural extension points to work with noisy (i.e., sometimes incorrect) or perceptual data, which can easily be integrated in the gradient descent-based setting. We are excited about synthesizing systems that are part interpretable, but differentiable program and part traditional neural network. As example, our concurrent ICLR submission \"Lifelong Perceptual Programming By Example\" handles inputs in the form of pictures, and indeed, when developing the models for that paper, our modelling recommendations from this paper proved crucial to achieve meaningful results.\n\n(2) The machine learning community has only very recently started to investigate differentiable interpreters and gradient descent as methods to synthesize programs. They currently do not work as well as more mature, optimized search techniques, but as the experimental results in Fig. 1 and Tab. 2 show, there are examples on which our gradient descent-based methods do better than a specialized PL technique. The results also show that modelling choices can have dramatic impact on the performance of gradient descent-based methods. Overall, our conclusion in this paper is that such methods can be improved substantially, and that further investigation is needed to understand if (and in what cases) they can outperform search-based baselines.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1480717321565, "tcdate": 1480717321560, "number": 2, "id": "ByzfEukXl", "invitation": "ICLR.cc/2017/conference/-/paper232/public/comment", "forum": "HkpLeH9el", "replyto": "HJSpcihMe", "signatures": ["~John_Feser1"], "readers": ["everyone"], "writers": ["~John_Feser1"], "content": {"title": "Comparison with Graves et al 16", "comment": "The main differences between our memory model and Graves et. al. are in how memory is addressed and how it is allocated and freed.\n \nFor addressing, Graves et al. use content-based addressing and a \"link matrix\" to make it easier to iterate over memory in the order that it was written. These appear to be useful constructs but are outside of what we considered because we are focused on inducing source code rather than a neural network controller. It's not clear that these addressing schemes, particularly the content-based addressing, have a simple natural representation as source code. However, it would be interesting in future work to think about programming language constructs that could produce similar behavior.\n \nThe method of allocating and freeing memory in Graves et al. is interesting, but our argument is that re-using the same memory for multiple purposes is not generally desirable in differentiable memory models. If memory is mutable, then during learning we must store the state of each memory location at each timestep, which can quickly become expensive. If memory is immutable, then we only need to store one copy of the memory's value. Thus, while it may seem wasteful to just continue to allocate new memory, it's actually cheaper in terms of memory usage. We note that the recent paper \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" (Rae et al, 2016) leverages sparsity to address the same issue, at the cost of some additional complexity in the implementation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287673149, "id": "ICLR.cc/2017/conference/-/paper232/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkpLeH9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper232/reviewers", "ICLR.cc/2017/conference/paper232/areachairs"], "cdate": 1485287673149}}}, {"tddate": null, "tmdate": 1480705518973, "tcdate": 1480705484659, "number": 2, "id": "B1BCBBJ7e", "invitation": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer5"], "content": {"title": "Just an application of TerpreT?", "question": "Having just been introduced to TerpreT by this paper, it appears to me as if this paper might be pandering to the ICLR community just a bit by failing to mention your earlier claims (in \"TerpreT: A Probabilistic Programming Language for Program Induction\") that \"the overwhelming trend in the experiments is that the techniques from the programming languages community outperform the machine learning approaches by a significant margin,\" where, specifically, the gradient-based optimization in this paper is precisely this disparaged machine learning approach.  So, what's actually new here (\"we developed our models in TerpreT which hides [] technicalities\") and which is the reason you're pushing gradient descent learning here: a) because it's ICLR or b) because by taking into account the nuggets of advice given in the paper you actually find that the gradient methods now outperform the programming languages techniques previously favored?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481197162841, "id": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer5", "ICLR.cc/2017/conference/paper232/AnonReviewer3"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481197162841}}}, {"tddate": null, "tmdate": 1480534716937, "tcdate": 1480534716933, "number": 1, "id": "HJSpcihMe", "invitation": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "forum": "HkpLeH9el", "replyto": "HkpLeH9el", "signatures": ["ICLR.cc/2017/conference/paper232/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper232/AnonReviewer1"], "content": {"title": "Comparison with the memory model of Graves et al 16", "question": "It'd be interesting to see a comparison between the memory model proposed in this paper and the one used in \"Hybrid computing using a neural network with dynamic external memory\". More specifically, the way allocation and releasing works in Graves et al 16 seems more robust than what is done here."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481197162841, "id": "ICLR.cc/2017/conference/-/paper232/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper232/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper232/AnonReviewer1", "ICLR.cc/2017/conference/paper232/AnonReviewer5", "ICLR.cc/2017/conference/paper232/AnonReviewer3"], "reply": {"forum": "HkpLeH9el", "replyto": "HkpLeH9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper232/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481197162841}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478279252601, "tcdate": 1478279252594, "number": 232, "id": "HkpLeH9el", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkpLeH9el", "signatures": ["~John_Feser1"], "readers": ["everyone"], "content": {"title": "Neural Functional Programming", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "pdf": "/pdf/f8f084e85876380943390d08b632c235ea739dd9.pdf", "TL;DR": "A differentiable functional programming language for learning programs from input-output examples.", "paperhash": "feser|neural_functional_programming", "keywords": ["Supervised Learning"], "conflicts": ["microsoft.com"], "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": ["Byp_ccVte"], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 16}