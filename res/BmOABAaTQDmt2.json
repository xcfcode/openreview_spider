{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363968300000, "tcdate": 1363968300000, "number": 2, "id": "gL2tL3lwAfLw1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BmOABAaTQDmt2", "replyto": "BmOABAaTQDmt2", "signatures": ["Xavier Glorot, Antoine Bordes, Jason Weston, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their comments.\r\n\r\nIt is true that our model should be compared with (Jenatton et al., NIPS12). This model has been developed simultaneously as ours, that's why it has not been included in the first version.  We added this reference and their results (LFM model) in a revised version of our abstract (http://arxiv.org/abs/1301.3485v2). \r\n\r\nUnfortunately, SME is slightly outperformed by LFM on Kinships and Nations and equivalent on UMLS. Still, we believe that this work  would make an interesting presentation at ICLR.  First, together with LFM, SME is the only current method that can scale to large number of relation types (and both have been developed at the same time). The LFM paper actually displays an experiment on data with 5k relation types on which LFM and SME perform similarly. Second, contrary to all previous methods, SME models relation types as vectors, lying in the space as entities. From a conceptual viewpoint, this is powerful, since it models any relation types as a standard entity (and vice-versa). Hence, SME is the only method that could be applied on data for which any entity can also create relationships between other entities.\r\n\r\nWe now also compare our model with CANDECOMP-PARAFAC (CP), a standard tensor factorization method."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.", "pdf": "https://arxiv.org/abs/1301.3485", "paperhash": "glorot|a_semantic_matching_energy_function_for_learning_with_multirelational_data", "keywords": [], "conflicts": [], "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "authorids": ["xavier.glorot@gmail.com", "antoine.bordes@gmail.com", "jweston@google.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362379680000, "tcdate": 1362379680000, "number": 1, "id": "fjenfiFhEZfLM", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BmOABAaTQDmt2", "replyto": "BmOABAaTQDmt2", "signatures": ["anonymous reviewer cae2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "review": "The paper proposes two functions for assigning energies to triples of\r\nentities, represented as vectors.  One energy function essentially\r\nadds the vectors of the relations and the entities, while another\r\nenergy function computes a tensor product of the relation and both\r\nentities.  The new energy functions appear to beat other methods.\r\n\r\nThe main weakness is in the relative lack of novelty. The paper\r\nproposes a slightly different neural network architecture for\r\ncomputing energies of object triplets from the ones that existed\r\nbefore, but its advantage over these architectures hasn't been\r\ndemonstrated conclusively.  How does it compare to a simple tensor\r\nfactorization? (or even a factorization that computes an energy with a\r\n3-way inner product sum_i a_i R_i b_i? such a factorization embeds\r\nentities and relations in the same space) Without this comparison, the\r\nnew energy function is merely a 'new neural network architecture' that\r\nis not shown to outperform other architectures.  And indeed, the the\r\nperformance of a simple Tensor factorization method matches the\r\nresults of the more sophisticated factorization method that is\r\nproposed here, on the datasets from [6] that overlap with the datasets\r\nhere (namely, UML and kinship).\r\n\r\nIn general, new energy functions or architectures are worthwhile only\r\nwhen they reliably improve performance (like the recently introduced\r\nmaxout networks) or when they have other desirable properties, such as\r\ninterpretability or simplicity.  The energy function proposed here is\r\nmore complex than a simple tensor factorization method which appears\r\nto work just as well.\r\n\r\nPros \r\n - New energy function, method appears to work well\r\nCons\r\n - The architecture is not compared against simpler architectures,\r\n   and there is evidence that the simpler architectures achieve\r\n   identical performance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.", "pdf": "https://arxiv.org/abs/1301.3485", "paperhash": "glorot|a_semantic_matching_energy_function_for_learning_with_multirelational_data", "keywords": [], "conflicts": [], "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "authorids": ["xavier.glorot@gmail.com", "antoine.bordes@gmail.com", "jweston@google.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362123900000, "tcdate": 1362123900000, "number": 3, "id": "ibXkikDckabeu", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "BmOABAaTQDmt2", "replyto": "BmOABAaTQDmt2", "signatures": ["anonymous reviewer 428a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "review": "Semantic Matching Energy Function for Learning with Multi-Relational Data\r\n\r\nPaper Summary\r\n\r\nThis paper deals with learning an energy model over 3-way relationships. Each entity in the relation is associated a low dimensional representation and a neural network associate a real value to each representation triplet. The learning algorithm relies on an online ranking loss. Two models are proposed a linear model and a bilinear model.\r\n\r\nReview Summary\r\n\r\nThe paper is clear and reads well. Its use of the ranking loss function to this problem is an interesting proposition. It could give more details on the ranking loss and the training procedure. The experiments could also be more thorough. My main concern however is that references to a co-author own work have been omitted. This omission means that the authors pretend not to know that a model with better reported performance exists. This should be discouraged and I will recommend the rejection of the paper.\r\n\r\nReview\r\n\r\nThis paper is part of the recent effort of using distributed representation and various loss function for learning relational models. Papers focusing on this line of research include work from A. Bordes, J. Weston and Y. Bengio:\r\n- A Latent Factor Model for Highly Multi-relational Data (NIPS 2012).\r\n  Rodolphe Jenatton, Nicolas Le Roux, Antoine Bordes and Guillaume Obozinski.\r\n- Learning Structured Embeddings of Knowledge Bases (AAAI 2011). \r\n  Antoine Bordes, Jason Weston, Ronan Collobert and Yoshua Bengio.\r\n\r\nThe variations among this paper mainly involves  \r\n- model regularization (low rank, parameter tying...)\r\n- loss function\r\n\r\nRegarding regularization, your proposition, Jenatton et al, and RESCAL are highly related. Basically your bilinear model seems to introduce a rank constrain on the 3D tensor representing all the relation {R_k, forall k} as in RESCAL notation. Basically your bilinar model decomposes R_k = (E_{rel,k} W_l) (W_r E_{rel,k})^T while the NIPS2012 model decomposes R_k as a linear combination of rank one matrices shared accross relations. Like Jenatton et al, you brake the symetry of the left and right relations.\r\n\r\nRegarding loss, RESCAL uses MSE, Jenatton et al uses logistic loss and you use a ranking loss.\r\n\r\nThis differences result in different AUCs. Jenatton et al is always better, RESCAL and your model are close. Given that Jenatton et al and RESCAL precede your submission. I feel it is necessary to check one thing at a time, i.e. training a model parameterized like RESCAL / Jenatton et al / yours with all three losses. This would give the best combination. This could give an empirical advantage to your ideas (either parameterization or ranking loss) over Jenatton et al.\r\n\r\nGiven that your model is worse in terms of AUC compared to Jenatton et al. I feel that you should at least explain why and maybe highlight some other advantages of your approach. I am disappointed that you do not refer to Jenatton et al: you know about this paper (shared co-author), the results on the same data are better and you do not even mention it.\r\n\r\nTypos/Details\r\n\r\nIntro: unlike in previous work -> put citation here.\r\n2.2 (2) even if it remains low dimensional, nothing forces the dimension of ... -> barely understandable rephrase this sentence."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.", "pdf": "https://arxiv.org/abs/1301.3485", "paperhash": "glorot|a_semantic_matching_energy_function_for_learning_with_multirelational_data", "keywords": [], "conflicts": [], "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "authorids": ["xavier.glorot@gmail.com", "antoine.bordes@gmail.com", "jweston@google.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358337600000, "tcdate": 1358337600000, "number": 12, "id": "BmOABAaTQDmt2", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "BmOABAaTQDmt2", "signatures": ["xavier.glorot@gmail.com"], "readers": ["everyone"], "content": {"title": "A Semantic Matching Energy Function for Learning with Multi-relational\r\n    Data", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.", "pdf": "https://arxiv.org/abs/1301.3485", "paperhash": "glorot|a_semantic_matching_energy_function_for_learning_with_multirelational_data", "keywords": [], "conflicts": [], "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "authorids": ["xavier.glorot@gmail.com", "antoine.bordes@gmail.com", "jweston@google.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}