{"notes": [{"id": "SyxMWh09KX", "original": "r1gYibA5YQ", "number": 1148, "cdate": 1538087929686, "ddate": null, "tcdate": 1538087929686, "tmdate": 1545355406284, "tddate": null, "forum": "SyxMWh09KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification", "abstract": "Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Inspired by the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. Our experimental results reveal that, for few-shot text classification tasks, gradient-based meta-learning approaches ourperform popular transfer learning methods. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "gabriel@imagia.com", "hassan.chouaib@imagia.com", "thomas.vincent@imagia.com", "andrew.jesson@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Gabriel Chartrand", "Hassan Chouaib", "Thomas Vincent", "Andrew Jesson", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "Meta-learning task-agnostic representations with attention.", "pdf": "/pdf/b2e1422643e66521ec17675ea6c013fc7d58c0c0.pdf", "paperhash": "jiang|attentive_taskagnostic_metalearning_for_fewshot_text_classification", "_bibtex": "@misc{\njiang2019attentive,\ntitle={Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification},\nauthor={Xiang Jiang and Mohammad Havaei and Gabriel Chartrand and Hassan Chouaib and Thomas Vincent and Andrew Jesson and Nicolas Chapados and Stan Matwin},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxMWh09KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgYJ8R1eV", "original": null, "number": 1, "cdate": 1544705504813, "ddate": null, "tcdate": 1544705504813, "tmdate": 1545354507348, "tddate": null, "forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1148/Meta_Review", "content": {"metareview": "This paper describes an incorporation of attention into model agnostic meta learning. The reviewers found that the paper was rather confusing in its presentation of both the method and the tasks. While the results seemed interesting, it was difficult to frame them due to lack of clarity as to what the task is, and the relation between attention and MAML. It sounds like this paper needs a bit more work, and thus is not suitable for publication at this time.\n\nIt is disappointing that the reviews were so short, but as the authors did not challenge them, unfortunately the AC must decide on the basis of the first set of comments by reviewers.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting results but very unclear narrative"}, "signatures": ["ICLR.cc/2019/Conference/Paper1148/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1148/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification", "abstract": "Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Inspired by the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. Our experimental results reveal that, for few-shot text classification tasks, gradient-based meta-learning approaches ourperform popular transfer learning methods. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "gabriel@imagia.com", "hassan.chouaib@imagia.com", "thomas.vincent@imagia.com", "andrew.jesson@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Gabriel Chartrand", "Hassan Chouaib", "Thomas Vincent", "Andrew Jesson", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "Meta-learning task-agnostic representations with attention.", "pdf": "/pdf/b2e1422643e66521ec17675ea6c013fc7d58c0c0.pdf", "paperhash": "jiang|attentive_taskagnostic_metalearning_for_fewshot_text_classification", "_bibtex": "@misc{\njiang2019attentive,\ntitle={Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification},\nauthor={Xiang Jiang and Mohammad Havaei and Gabriel Chartrand and Hassan Chouaib and Thomas Vincent and Andrew Jesson and Nicolas Chapados and Stan Matwin},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxMWh09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1148/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352948980, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1148/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1148/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1148/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352948980}}}, {"id": "SygMnjw63X", "original": null, "number": 3, "cdate": 1541401514237, "ddate": null, "tcdate": 1541401514237, "tmdate": 1541533382256, "tddate": null, "forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "content": {"title": "Review", "review": "Summary of paper: For the few shot text classification task, train a model with MAML where only a subset of parameters (attention parameters in this case) are updated in the inner loop of MAML. The empirical results suggest that this improves over the MAML baseline.\n\nI found this paper confusingly written. The authors hop between a focus on meta-learning to a focus on attention, and it remains unclear to me how these are connected. The description of models is poor -- for example, the ablation mentioned in 4.5.3 is still confusing to me (if the attention parameters are not updated in the inner loop of MAML, then what is?). Furthermore, even basic choices of notation, like A with a bar underneath in a crowded table, seem poorly thought out.\n\nI find the focus on attention a bit bizarre. It's unclear to me how any experiments in the paper suggest that attention is a critical aspect of meta-learning in this model. The TAML baseline (without attention) underperforms the ATAML model (with attention), but all that means is that attention improves representational power, which is not surprising. Why is attention considered an important aspect of meta learning?\n\nTo me, the most interesting aspect of this work is the idea of not updating every parameter in the MAML inner loop. So far, I've seen all MAML works update all parameters. The experiments suggest that updating a small subset of parameters can improve results significantly in the 1-shot regime, but the gap between normal MAML and the subset MAML is much smaller in the 5-shot regime. This result suggests updating a subset of parameters can serve as a method to combat overfitting, as the 1-shot regime is much more data constrained than the 5-shot regime.\n\nIt's unfortunate that the authors do not dig further down this line of reasoning. When does the gap between MAML on all parameters and only on a subset of parameters become near-zero? Does the choice of the subset of parameters matter? For example, instead of updating the attention weights, what happens if the bottommost weights are updated? How would using pretrained parameters (e.g., language modeling pretraining) in meta-learning affect these results? In general, what can be learned about overfitting in MAML?\n\nTo conclude, the paper is not written well and has a distracting focus on attention. While it raises an interesting question about MAML and overfitting, it does not have the experiments needed to explore this topic well.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1148/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification", "abstract": "Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Inspired by the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. Our experimental results reveal that, for few-shot text classification tasks, gradient-based meta-learning approaches ourperform popular transfer learning methods. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "gabriel@imagia.com", "hassan.chouaib@imagia.com", "thomas.vincent@imagia.com", "andrew.jesson@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Gabriel Chartrand", "Hassan Chouaib", "Thomas Vincent", "Andrew Jesson", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "Meta-learning task-agnostic representations with attention.", "pdf": "/pdf/b2e1422643e66521ec17675ea6c013fc7d58c0c0.pdf", "paperhash": "jiang|attentive_taskagnostic_metalearning_for_fewshot_text_classification", "_bibtex": "@misc{\njiang2019attentive,\ntitle={Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification},\nauthor={Xiang Jiang and Mohammad Havaei and Gabriel Chartrand and Hassan Chouaib and Thomas Vincent and Andrew Jesson and Nicolas Chapados and Stan Matwin},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxMWh09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "cdate": 1542234295025, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1148/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883235, "tmdate": 1552335883235, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1148/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1xKIsQcnQ", "original": null, "number": 2, "cdate": 1541188432672, "ddate": null, "tcdate": 1541188432672, "tmdate": 1541533382051, "tddate": null, "forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "content": {"title": "Interesting approach for few-shot text classification", "review": "This paper presents a meta learning approach for few-shot text classification, where task-specific parameters are used to compute a context-dependent weighted sum of hidden representations for a word sequence and intermediate representations of words are obtained by applying shared model parameters. \n\nThe proposed meta learning architecture, namely ATAML, consistently outperforms baselines in terms of 1-shot classification tasks and these results demonstrate that the use of task-specific attention in ATAML has some positive impact on few-shot learning problems. The performance of ATAML on 5-shot classification, by contrast, is similar to its baseline, i.e., MAML. I couldn\u2019t find in the manuscript the reason (or explanation) why the performance gain of ATAML over MAML gets smaller if we provide more examples per class. It would be also interesting to check the performance of both algorithms on 10-shot classification.\n\nThis paper has limited its focus on meta learning for few-shot text classification according to the title and experimental setup, but the authors do not properly define the task itself.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1148/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification", "abstract": "Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Inspired by the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. Our experimental results reveal that, for few-shot text classification tasks, gradient-based meta-learning approaches ourperform popular transfer learning methods. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "gabriel@imagia.com", "hassan.chouaib@imagia.com", "thomas.vincent@imagia.com", "andrew.jesson@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Gabriel Chartrand", "Hassan Chouaib", "Thomas Vincent", "Andrew Jesson", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "Meta-learning task-agnostic representations with attention.", "pdf": "/pdf/b2e1422643e66521ec17675ea6c013fc7d58c0c0.pdf", "paperhash": "jiang|attentive_taskagnostic_metalearning_for_fewshot_text_classification", "_bibtex": "@misc{\njiang2019attentive,\ntitle={Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification},\nauthor={Xiang Jiang and Mohammad Havaei and Gabriel Chartrand and Hassan Chouaib and Thomas Vincent and Andrew Jesson and Nicolas Chapados and Stan Matwin},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxMWh09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "cdate": 1542234295025, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1148/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883235, "tmdate": 1552335883235, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1148/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkljaHMGhm", "original": null, "number": 1, "cdate": 1540658627025, "ddate": null, "tcdate": 1540658627025, "tmdate": 1541533381839, "tddate": null, "forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "content": {"title": "Attentive Task-Agnostic Meta-Learning for very-few-shot learning", "review": "The authors introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification.\nThe main idea is to learn task-independent representations, while other parameters, including the attention mechanism, are being fine-tuned for each specific task after pretraining. \nThe authors find that, for few-shot text classification tasks, their proposed approach outperforms several important baselines, e.g., random initialization and MAML, in certain settings. In particular, ATAML performs better than MAML for very few training examples, but in that setting, the gains are significant. \n\nComments:\n- I am unsure if I understand the contributions paragraph, i.e., I cannot count 3 contributions. I further believe the datasets are not a valid contribution, since they are just subsets of the original datasets.\n- Using a constant prediction threshold of 0.5 seems unnecessary. Why can't you just tune it?\n- 1-shot learning is maybe theoretically interesting, but how relevant is it in practice? ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1148/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification", "abstract": "Current deep learning based text classification methods are limited by their ability to achieve fast learning and generalization when the data is scarce. We address this problem by integrating a meta-learning procedure that uses the knowledge learned across many tasks as an inductive bias towards better natural language understanding. Inspired by the Model-Agnostic Meta-Learning framework (MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML) algorithm for text classification. The proposed ATAML is designed to encourage task-agnostic representation learning by way of task-agnostic parameterization and facilitate task-specific adaptation via attention mechanisms. We provide evidence to show that the attention mechanism in ATAML has a synergistic effect on learning performance. Our experimental results reveal that, for few-shot text classification tasks, gradient-based meta-learning approaches ourperform popular transfer learning methods. In comparisons with models trained from random initialization, pretrained models and meta trained MAML, our proposed ATAML method generalizes better on single-label and multi-label classification tasks in miniRCV1 and miniReuters-21578 datasets.", "keywords": ["meta-learning", "learning to learn", "few-shot learning"], "authorids": ["xiang.jiang@dal.ca", "mohammad@imagia.com", "gabriel@imagia.com", "hassan.chouaib@imagia.com", "thomas.vincent@imagia.com", "andrew.jesson@imagia.com", "nic@imagia.com", "stan@cs.dal.ca"], "authors": ["Xiang Jiang", "Mohammad Havaei", "Gabriel Chartrand", "Hassan Chouaib", "Thomas Vincent", "Andrew Jesson", "Nicolas Chapados", "Stan Matwin"], "TL;DR": "Meta-learning task-agnostic representations with attention.", "pdf": "/pdf/b2e1422643e66521ec17675ea6c013fc7d58c0c0.pdf", "paperhash": "jiang|attentive_taskagnostic_metalearning_for_fewshot_text_classification", "_bibtex": "@misc{\njiang2019attentive,\ntitle={Attentive Task-Agnostic Meta-Learning for Few-Shot Text Classification},\nauthor={Xiang Jiang and Mohammad Havaei and Gabriel Chartrand and Hassan Chouaib and Thomas Vincent and Andrew Jesson and Nicolas Chapados and Stan Matwin},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxMWh09KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1148/Official_Review", "cdate": 1542234295025, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxMWh09KX", "replyto": "SyxMWh09KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1148/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883235, "tmdate": 1552335883235, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1148/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}