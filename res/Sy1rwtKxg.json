{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396363001, "tcdate": 1486396363001, "number": 1, "id": "SyQRiG8ue", "invitation": "ICLR.cc/2017/conference/-/paper111/acceptance", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods. \n The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396363540, "id": "ICLR.cc/2017/conference/-/paper111/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396363540}}}, {"tddate": null, "tmdate": 1482271035678, "tcdate": 1482271035678, "number": 6, "id": "H1VSK7D4x", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "H1lZYRSEl", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "Addressing reviewer's comments", "comment": "Regarding the compact representation of M:\n==========================================\nYou are right that M has an O(n . f) representation, which is compact for datasets with n << f. Unfortunately, such a representation defeats the purpose of a parallel algorithm because computing M.w from this representation performs almost the same sequential computation as the sequential SGD (a dot product of an instance with a weight vector followed by a vector addition). By repeating the (sequential) work in the reducers we will not get parallel speedups. For speedups, we want the work in the reduction to be independent of n (we shouldn\u2019t have to revisit the training data) and perform only a small fraction of the cost of the learners (otherwise the Amdahl's law will limit our speedup).\n\nAlso, from Table 1, n > f for many of our datasets and n > f/10 for all our datasets. But your observation and the discussion above is relevant for other datasets and we will add them in the paper.\n\nWe do utilize the \"low-rank\" representation of M in other places in the paper. For instance, projecting N = M-I instead of M relies on this property. We also use this representation to efficiently compute M_A = M*A, the projected version of M. \n\nRegarding the complexity of SymSGD:\n===================================\nThis is a great point and we will add the discussion in the paper (one other reviewer also brought this up and we have a revision that addressed his/her concern).\nLet n be the number of instances in the training dataset, f be the total number of features, f\u2019 be the average number of non-zeros per instance, c be the number of classes, k be the dimension of the projected space, and p be the number of processors. Therefore, the local learner complexity for all processors per pass is O(n . f\u2019 . (k+c)) in time and O(p . f . (k+c)) in space. The reduction complexity for all processors is O(p . f . k . c) in time and O(p . f . (k+c)) in space.\n\nRegarding convergence rate analysis of SymSGD:\n====================================\nBy producing the same result as a sequential SGD in expectation, SymSGD enjoys the same theoretical convergence rates as SGD. We will be happy to formalize this in a theorem in the paper. Our empirical results confirm this fact."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "tmdate": 1482270754183, "tcdate": 1482270754183, "number": 5, "id": "SJqm_mD4g", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "HJaDM17Eg", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "Addressing reviewer's comments", "comment": "Regarding other popular formulations: \n====================================\nWe compared the maximum accuracy that Vowpal Wabbit can achieve using logistic and linear regression and table 1, columns 7 and 8 compare the results. As it can be seen, among the datasets we examined, the difference is very minimal and in some cases linear regression achieves even better accuracy. Therefore, we believe studying performance of linear regression is valuable.\n\nRegarding linear programming approaches: \n========================================\nOne of the best existing least square linear solver is Intel MKL implementation of LAPACK which uses Singular Value Decomposition method (gelsd). It currently only supports completely dense datasets \u2013 those in which every instance has a value for every feature. For our only completely dense dataset, epsilon, we evaluated the performance of MKL in comparison to our single-threaded baseline:\na.\tMKL using 16 cores takes around 20.64 seconds to converge and gives an accuracy of 89.62%\nb.\tOur baseline using 1 core takes around 17.66 seconds to converge to an accuracy of 89.71%\nClearly, even our SGD baseline approach is faster than MKL with 16 cores. Moreover, SymSGD using 16 cores is 9x faster than MKL using 16 cores.\nFor datasets that are not completely dense, (including \u201cdense\u201d datasets which still have some features missing per instance,) we have to manually densify the dataset which clearly increases the overhead of the whole computation and requires excessive amount of memory. An alternative would be to compute X^T . X which is an fxf sparse matrix and then use MKL PARDISO parallel sparse linear solver to compute the exact solution. There are two issues with this approach: (1) X^T . X needs to be full-rank since MKL can only find the answer when it is unique. It is very unlikely that X^T . X is full-rank since the features of a dataset can be linearly dependent. Also, even if X^T . X is full-rank, the unique answer can be overfitting and undesired. (2) Computing X^T . X requires O(f^2 . z) time where z is the average number of non-zeros across columns of X. This is also infeasible for datasets with large number of features. (For example, url has 3.2 million features and the corresponding fxf matrix requires an excessive amount of time.)\n\nRegarding the size of our datasets and their training time:\n===========================================================\nNot all of our datasets are small in size. For example, the training datasets for mnist8m, epsilon and url are 19GB, 11.8 GB and 1.5 GB, respectively. However, the size of a dataset by itself is not the only indication of the training time and number of classes has an important factor in the time since we need a model for each class. For example, our baseline takes 371 seconds on mnist8m with 10 classes, 124 seconds on sector with 105 classes and 59 seconds on url with 1 class until they converge to the maximum accuracy. Therefore, the training which takes minutes to end can take seconds to converge using our SymSGD parallel algorithm.\n\nRegarding Hogwild being faster:\n===============================\nAmong our 9 datasets which is combination of datasets with different sparsity, Hogwild is faster in only 2. But the important conclusion from our results is that Hogwild does not scale beyond one socket of processor while SymSGD keeps scaling (in most cases, by going from 8 to 16 threads, Hogwild\u2019s performance drops).\nRegarding using GPUs for dense datasets: SymSGD\u2019s parallelism is across iterations of the SGD loop and it is orthogonal to any other available source of parallelism within each iteration of SGD. For example, we may use multiple GPUs where each one computes a local model and a model combiner. This way we have the massive parallelism of GPU within an instance while across multiple GPUs we have the parallelism of SymSGD. However, note that there is not that much parallelism available even in a dense dataset. For example, mnist8m which is one of the biggest datasets in our study has on average ~200 non-zeros. Parallelism across 200 floating point operations can be tricky.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "tmdate": 1482270616691, "tcdate": 1480815729554, "number": 2, "id": "BJ5OEe-me", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "S1LllHCzx", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "You are correct, k is indeed small.", "comment": "You are correct, overhead of SymSGD per thread is proportional to k. However, what is surprising is that k across all of our benchmarks can be so small without loss of accuracy (k is between 7 to 15). For such a small k, we are able to hide most of the overhead by utilizing SIMD hardware within a thread. As it can be seen from Figure 2, the slowdown of SymSGD with one thread over the baseline which runs sequential SGD without computing the model combiner is less than 50%. Since SymSGD enables a parallel way to combine learned models, it enables multicore parallelism speed up.\nLike other algorithmic parameters, we did a parameter sweep to find the best performing k.\nWe made the changes in the last two paragraphs of Section 3.2 of the revision to discuss this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "tmdate": 1482184952225, "tcdate": 1482184952225, "number": 3, "id": "H1lZYRSEl", "invitation": "ICLR.cc/2017/conference/-/paper111/official/review", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer1"], "content": {"title": "Why we can have speedup is unclear", "rating": "4: Ok but not good enough - rejection", "review": "This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.\n\nI feel that there might be some fundamental misunderstanding on SGD.\n\n''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have\nthousands if not millions of features.\"\n\nI do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. \n\nWhy one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.\nI suggest authors to make the following changes to make this paper more clear and theoretically solid\n- provide computational complexity per step of the proposed algorithm\n- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694009, "id": "ICLR.cc/2017/conference/-/paper111/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper111/AnonReviewer3", "ICLR.cc/2017/conference/paper111/AnonReviewer2", "ICLR.cc/2017/conference/paper111/AnonReviewer1"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694009}}}, {"tddate": null, "tmdate": 1482002794661, "tcdate": 1481920098285, "number": 1, "id": "ryqwA6b4x", "invitation": "ICLR.cc/2017/conference/-/paper111/official/review", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "content": {"title": "review for Parallel Stochastic Gradient Descent with Sound Combiner", "rating": "6: Marginally above acceptance threshold", "review": "This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.\nComments\n1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.\n2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.\n3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.\n\nOverall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694009, "id": "ICLR.cc/2017/conference/-/paper111/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper111/AnonReviewer3", "ICLR.cc/2017/conference/paper111/AnonReviewer2", "ICLR.cc/2017/conference/paper111/AnonReviewer1"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694009}}}, {"tddate": null, "tmdate": 1482002731462, "tcdate": 1482002731462, "number": 2, "id": "S174bGQEg", "invitation": "ICLR.cc/2017/conference/-/paper111/official/comment", "forum": "Sy1rwtKxg", "replyto": "SkTXH7GEx", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "content": {"title": "clarification of comment2", "comment": "By linear regression I realy mean quadratic loss function, I have updated the review respectively."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723369, "id": "ICLR.cc/2017/conference/-/paper111/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723369}}}, {"tddate": null, "tmdate": 1481990756819, "tcdate": 1481990756819, "number": 2, "id": "HJaDM17Eg", "invitation": "ICLR.cc/2017/conference/-/paper111/official/review", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer2"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:\n\n- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. \n\n- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). \n\n- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694009, "id": "ICLR.cc/2017/conference/-/paper111/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper111/AnonReviewer3", "ICLR.cc/2017/conference/paper111/AnonReviewer2", "ICLR.cc/2017/conference/paper111/AnonReviewer1"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694009}}}, {"tddate": null, "tmdate": 1481942308876, "tcdate": 1481942308876, "number": 4, "id": "SkTXH7GEx", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "ryqwA6b4x", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "Addressing Reviewer's Comments", "comment": "We would like to thank you for your review and address comments 2 and 3 in here.\n\nRegarding comment 2: As stated in the paper, our technique is limited to learners which have a linear update rule for the weight vector. This includes linear or polynomial regression with squared loss function and L2 regularization. The only other approach to parallelize on a shared-memory system for such a problem is Hogwild which as our results show, does not scale across sockets in a multiple socket system. We are currently working on extending our approach to non-linear objective functions.\n\nRegarding comment 3: Utilizing SIMD requires regular accesses. Sequential SGD can benefit from SIMD only when the dataset is completely dense \u2013 every instance has values for every feature. In our collection of 9 datasets, epsilon is the only one with such 100% density \u2013 other datasets such as MNIST or ALOI are relatively dense but nowhere near 100% density required to use SIMD well.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480821895298, "tcdate": 1478231862619, "number": 111, "id": "Sy1rwtKxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sy1rwtKxg", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "content": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480821852920, "tcdate": 1480821852914, "number": 3, "id": "rJHP3ZbQl", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "BJBzD2yXg", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "Asynchronous SGD algorithms eventually converge, but they converge slower.", "comment": "We agree that as written our paper seems to claim that other algorithms do not converge. Our intention was to claim that these algorithms can potentially learn a different model than SGD after processing a certain number of instances, even if these algorithms eventually converge. Our algorithm produces the same answer as sequential SGD in expectation. We modified the sentence in our paper with this clarification. The uploaded revision includes this change. Our experiments (Fig 2) indicates that while HogWild learns models with equally good error rates after processing the same number of examples, AllReduce clearly falls behind."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "tmdate": 1480734476878, "tcdate": 1480734476872, "number": 2, "id": "BJBzD2yXg", "invitation": "ICLR.cc/2017/conference/-/paper111/pre-review/question", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer1"], "content": {"title": "convergence of asynchronous parallel algorithms", "question": "\"In parameter-server Li et al. (2014a), each thread (or machine) periodically sends its model deltas to a server that applies them to a global model. In ALLREDUCE Agarwal et al. (2014), threads periodically reach a barrier where they compute a weighted-average of the local models. Obviously, these approaches can produce a model that is potentially different from what a sequential SGD would have produced on these examples.\"\n\nThe convergence of asynchronous SGD has been theoretically proven. Why do you think it is \"potentially different from\" the sequential algorithm?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959456314, "id": "ICLR.cc/2017/conference/-/paper111/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper111/AnonReviewer3", "ICLR.cc/2017/conference/paper111/AnonReviewer1"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959456314}}}, {"tddate": null, "tmdate": 1480638446026, "tcdate": 1480638446020, "number": 1, "id": "S1LllHCzx", "invitation": "ICLR.cc/2017/conference/-/paper111/official/comment", "forum": "Sy1rwtKxg", "replyto": "BkBOj4Czx", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "content": {"title": "choice of k", "comment": "Perhaps I did not read careful enough, but how do you choose the size of k? Since per instance time of SymSGD is k times larger than the baseline, it seems k need to be small in order to have a speed gain over the baseline"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723369, "id": "ICLR.cc/2017/conference/-/paper111/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723369}}}, {"tddate": null, "tmdate": 1480637293096, "tcdate": 1480637293090, "number": 1, "id": "BkBOj4Czx", "invitation": "ICLR.cc/2017/conference/-/paper111/public/comment", "forum": "Sy1rwtKxg", "replyto": "ByFN2bazg", "signatures": ["~Saeed_Maleki1"], "readers": ["everyone"], "writers": ["~Saeed_Maleki1"], "content": {"title": "We do utilize the sparsity of a sparse dataset but the performance and correctness of SymSGD extends in dense and sparse datasets.", "comment": "We do utilize the sparsity of the dataset for gradient computation in the case of sparse datasets. However, our algorithm works for both sparse and dense datasets \u2013 the correctness and speedup extends in both cases. As demonstrated in our experiments, RCV1 for example is 99.84% sparse and Epsilon is 0% sparse. Our implementation ensures that when operating on a sparse dataset, the corresponding operations are proportional to the number of non-zero entries. Specifically, the gradient as equation (1) shows is (X_r \\cdot w_{i-1} \u2013 y_r)X_r^T where X_r is a sparse or a dense instance. Let\u2019s assume that X_r has O(z) number of non-zeros. Therefore, gradient computation involves a dot product computation of X_r \\cdot w_{i-1} which costs O(z) in time and a scalar-vector multiplication which also costs O(z) in time. This means that an iteration of our baseline algorithm takes O(z) amount of time. For the same instance, SymSGD takes O(z x k) amount of time where k is the size of projected space.\n\nWe have uploaded a revision of the paper with this clarification. The third paragraph of Section 3 discusses the complexity of sequential SGD and the last two paragraphs of Section 3.2 discuss the complexity of SymSGD."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287723494, "id": "ICLR.cc/2017/conference/-/paper111/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sy1rwtKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper111/reviewers", "ICLR.cc/2017/conference/paper111/areachairs"], "cdate": 1485287723494}}}, {"tddate": null, "tmdate": 1480559665233, "tcdate": 1480559665229, "number": 1, "id": "ByFN2bazg", "invitation": "ICLR.cc/2017/conference/-/paper111/pre-review/question", "forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "signatures": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper111/AnonReviewer3"], "content": {"title": "How is sparse algorithm implemented", "question": "Specifically, it is unclear from the paper whether does the gradient calculation utilizes the sparsity of the dataset.\nIt would be helpful to give a detailed description on how gradient is calculated, and time complexity analysis, given  N X M a sparse input data, with only Z entries\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Stochastic Gradient Descent with Sound Combiners", "abstract": "Stochastic gradient descent (SGD) is a well-known method for regression and classification tasks. However, it is an inherently sequential algorithm \u2014 at each step, the processing of the current example depends on the parameters learned from the previous examples. Prior approaches to parallelizing SGD, such as Hogwild! and AllReduce, do not honor these dependences across threads and thus can potentially suffer poor convergence rates and/or poor scalability. This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation. Each thread in this approach learns a local model and a probabilistic model combiner that allows the local models to be combined to produce the same result as what a sequential SGD would have produced, in expectation. This SymSGD approach is applicable to any linear learner whose update rule is linear. This paper evaluates SymSGD\u2019s accuracy and performance on 9 datasets on a shared-memory machine shows up-to 13\u00d7 speedup over our heavily optimized sequential baseline on 16 cores.", "pdf": "/pdf/c3d8427c4c7eada547f7650aaa1261353a36127e.pdf", "TL;DR": "This paper proposes SymSGD, a parallel SGD algorithm that retains the sequential semantics of SGD in expectation.", "paperhash": "maleki|parallel_stochastic_gradient_descent_with_sound_combiners", "keywords": [], "conflicts": ["microsoft.com", "illinois.edu", "ncsu.edu"], "authors": ["Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Yufei Ding"], "authorids": ["saemal@microsoft.com", "madanm@microsoft.com", "toddm@microsoft.com", "yding8@ncsu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959456314, "id": "ICLR.cc/2017/conference/-/paper111/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper111/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper111/AnonReviewer3", "ICLR.cc/2017/conference/paper111/AnonReviewer1"], "reply": {"forum": "Sy1rwtKxg", "replyto": "Sy1rwtKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper111/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959456314}}}], "count": 15}