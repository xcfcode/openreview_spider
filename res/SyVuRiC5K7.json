{"notes": [{"id": "SyVuRiC5K7", "original": "r1gNKqKqK7", "number": 911, "cdate": 1538087888410, "ddate": null, "tcdate": 1538087888410, "tmdate": 1549617651891, "tddate": null, "forum": "SyVuRiC5K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1lqM84kxE", "original": null, "number": 1, "cdate": 1544664594186, "ddate": null, "tcdate": 1544664594186, "tmdate": 1545354504723, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Meta_Review", "content": {"metareview": "As far as I know, this is the first paper to combine transductive learning with few-shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \\sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "A new transductive few-shot learning algorithm with strong empirical results"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper911/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353040602, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353040602}}}, {"id": "HkgdqB_y1V", "original": null, "number": 9, "cdate": 1543632271645, "ddate": null, "tcdate": 1543632271645, "tmdate": 1543862306618, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "r1lRhOusR7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "Hi\n\nIn section 3.2.4, it was written that cross-entropy loss is computed between F* and query labels, however, https://github.com/anonymisedsupplemental/TPN/blob/master/models.py#L145 the loss is computed between F* and the UNION of support labels and query labels. In fact, I changed the loss computation in your code to only use query labels and it resulted in poor accuracy similar to my earlier findings and decreasing alpha would match my previous results.\n\nThis is the main issue I've had when my implementation did not work even after some compatible initializations between tensorflow and pytorch.\n\nAlso for completeness, please add the relu after FC layer 1 in Figure 4 for graph construction.", "title": "Issue found: Disparity of CELoss in paper and the code"}, "signatures": ["~anon_ml_reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~anon_ml_reviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "rkg_emdoA7", "original": null, "number": 7, "cdate": 1543369455795, "ddate": null, "tcdate": 1543369455795, "tmdate": 1543862247536, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "rkxcWJf_Am", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "Hi\n\nThank you for the code! \n\nPlease see my comment below: https://openreview.net/forum?id=SyVuRiC5K7&noteId=HkgdqB_y1V\n\nOne suggestion; it'd be very helpful if you could add explicitly the pre-processing steps that was used to get the pickled data since I could not find it in \"Meta-Learning for Semi-Supervised Few-Shot Classification\" by Mengye Ren et. al.", "title": "Issue found"}, "signatures": ["~anon_ml_reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~anon_ml_reviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "ryek8HjfyN", "original": null, "number": 13, "cdate": 1543841095508, "ddate": null, "tcdate": 1543841095508, "tmdate": 1543841095508, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "HkgdqB_y1V", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Thanks for the feedback. New version will be revised.", "comment": "Thanks for the feedback. \n\nWe are going to revise the paper according to the useful suggestions regarding ce-loss and Figure4.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "r1lRhOusR7", "original": null, "number": 11, "cdate": 1543370934143, "ddate": null, "tcdate": 1543370934143, "tmdate": 1543370934143, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "rkg_emdoA7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Thanks for the reproduction. Pre-processing details.", "comment": "Thanks for the clarification of reproduction.\n\nFor the pre-processing step you mentioned, here we have two pieces of advices:\n1. We have the dataset flag '--pkl' which controls the usage of pkl data or original image data (our own preprocessing). We tested with our own preprocessing, the performance is similar to pkl data.\n2. For Mengye Ren's code, I think you can refer to https://github.com/renmengye/few-shot-ssl-public/blob/master/fewshot/data/mini_imagenet.py for more details.\n\nFor other code issues, we are glad to offer help in github issue."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "rkxcWJf_Am", "original": null, "number": 9, "cdate": 1543147265826, "ddate": null, "tcdate": 1543147265826, "tmdate": 1543147265826, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "rkxfh9RTTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Anonymous Code Link", "comment": "Thanks for the feedback.\n\nWe got the approval from program chairs to release an anonymous code link, as follow:\nhttps://github.com/anonymisedsupplemental/TPN\n\nWe would like to answer the related questions about our paper and code."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "r1xOHPbuAm", "original": null, "number": 8, "cdate": 1543145280267, "ddate": null, "tcdate": 1543145280267, "tmdate": 1543145280267, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "HJx6UQbfhX", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \"\n\n>>> In few-shot learning, episodic paradigm proposed by Matching Networks [1] is widely adopted by current researchers (we follow the same setting to make a fair comparison). In each episode, a small subset of N-way K-shot Q-query examples is sampled from the training set. Typically, for 1-shot experiments, N=5, K=1, Q=15 and for 5-shot experiments, N=5, K=5, Q=15. Thus, the number of training examples are Nx(K+Q) (80 for 1-shot and 100 for 5-shot). Constructing label propagation matrix W involves both support and query examples (80 or 100). So the dimension of W is either 80x80 or 100x100. Running label propagation on such small matrix is quite efficient.\n\n\"It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\"\n\n>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module. After we get the per-example feature representation f_{\\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\\phi}. The output of this module is a one-dimensional scalar. f and g are learned in an end-to-end way in our approach.\n\n\"solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \"\n\n>>> We want to answer this question from two aspects. On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5). In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100. On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data. \nOn miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps. This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.\n\n\n[1] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS. 2016.\n[2] Liang, De-Ming, and Yu-Feng Li. \"Lightweight Label Propagation for Large-Scale Network Data.\" IJCAI. 2018.\n[3] Fujiwara, Yasuhiro, and Go Irie. \"Efficient label propagation.\" ICML. 2014.\n[4] Weston, Jason. \"Large-Scale Semi-Supervised Learning.\""}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "Hye0RIZ_RX", "original": null, "number": 7, "cdate": 1543145174079, "ddate": null, "tcdate": 1543145174079, "tmdate": 1543145174079, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "S1x4ca-chQ", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"Some technical details are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\"\n\n>>> Thanks for pointing out the details. We want to clarify the few-shot setting. We follow the widely-used episodic paradigm proposed by Matching Networks [1]. In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples). The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset. This is very fast and efficient. \nIn deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k. In forward computation pass, the index position of the max (or top-k) values are stored. While in the back propagation pass, the gradient is computed only with respect to these saved positions. This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch. In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.\n\n\"Does episode training help label propagation? How about the results of label propagation without the episode training? \"\n\n>>> In our paper, the length scale parameter \\sigma is trained in an example-wise and episodic-wise way, as described in section 3.2.2 and Figure 4 of Appendix A. In order to investigate the benefit of episodic training, we combine the heuristic-based label propagation methods [2] with meta-learning to serve as a transductive baseline. Please refer to Table 1 and Table 2 line \"Label Propagation\". It can be seen that TPN outperforms naive label propagation with a large margin, thus verifying the effectiveness of episode training.\n\n\n[1] Vinyals, Oriol et al. \"Matching networks for one shot learning.\" NIPS. 2016.\n[2] Zhou, Denny et al. \"Learning with local and global consistency.\" NIPS. 2004."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "r1xNOL-uCX", "original": null, "number": 6, "cdate": 1543145067999, "ddate": null, "tcdate": 1543145067999, "tmdate": 1543145067999, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "Skx7vDii3X", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"\n\n>>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation. The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples. While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A). This leads to better generalization ability for test tasks. \nIn Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\"). \n\n\"(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.\"\n\n>>> At first, we want to clarify the few-shot network architecture setting. Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]). Our method belongs to the first one, which contains much fewer layers than the ResNet setting. Thus, it is more reasonable to compare TADAM with ResNet version of our method. To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:\n\nMethod                                  1-shot    5-shot\nSNAIL [4]                                 55.71     68.88\nadaResNet [5]                        56.88     71.94\nDiscriminative k-shot [6]     56.30     73.90\nTADAM [7]                              58.50     76.70\n--------------------------------------------------------\nOurs                                        59.46     75.65\n--------------------------------------------------------\n\nIt can be seen that we beat TADAM for 1-shot setting. For 5-shot, we outperform all other recent high-performance methods except for TADAM.\n\n>>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline. It combines label propagation method [8] with episodic meta-learning. The usage of transductive inference makes this baseline outperform most published state-of-the-art methods. Moreover, the performance of TPN over label propagation is not very small. For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training. The improvements are even larger for tieredImagenet with 4.68% and 2.87%. We believe in few-shot learning, this is a large improvement.\n\n\n[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.\n[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.\n[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.\n[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.\n[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.\n[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.\n[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.\n[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "S1lqdSZuRX", "original": null, "number": 5, "cdate": 1543144817530, "ddate": null, "tcdate": 1543144817530, "tmdate": 1543144817530, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Main response to reviewers", "comment": "We wish to thank the reviewers for their enlightening feedback! We would like to highlight the novelty and contribution of the proposed method. \n\n(1) The proposed TPN is not a direct combination of the original label propagation and few-shot learning, but a novel transductive meta-learning method when facing unevenly distributed data.\n\nThe main contribution of the proposed TPN is to propose a novel transductive meta-learning method when facing an uneven data distribution. Most of previous (label) propagation algorithms usually assume samples are distributed evenly in the data space. Unfortunately, in low-shot learning, the data is limited and unevenly distributed, which makes most of existing label propagation algorithms inapplicable. To clearly model the data distribution in low-shot learning settings, previous transductive methods adopt a fixed scheme to explore the correlations between data, i.e., compute the weights with a fixed \\sigma as shown in Eq (1). However, as pointed out in previous work [2][3] and in our experimental results, the performance of a transductive method is quite sensitive to the parameter \\sigma, and a fixed \\sigma will lead to suboptimal results. Our proposed TPN adaptively learns data correlation by calculating an optimal \\sigma on a per data basis. As shown in Figure 4 (Appendix A), the correlation among data pairs is optimized and updated in each episode according to data distribution of the neighborhood. In this way, a different model is specifically learned to uncover the correlation of each data pair, thereby largely ameliorating the uneven data distribution problem. \nExperimentally, TPN shows a big advantage over the direct combination (referred to as \"Label Propagation\" in Table 1 and Table 2).\n\n(2) To the best of our knowledge, we are the first to model transductive inference explicitly in the few-shot meta-learning. This transductive setting paves a new way to solve the limited data problem in few-shot learning. As shown in this paper, if one has the test data in whole or a batch manner, transductive inference significantly improves the performance without additional human annotations.\n\n(3) We advanced the state-of-the-art performance on the two most commonly-used benchmark datasets with large margins using the standard 4-layer ConvNets architecture.\n\n\n\n[1] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004.\n[2] Wang, Fei, and Changshui Zhang. \"Label propagation through linear neighborhoods.\" TKDE. 2008.\n[3] Xiaojin Z, Zoubin G. \"Learning from labeled and unlabeled data with label propagation.\" Technical Report. 2002."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "rkxfh9RTTQ", "original": null, "number": 6, "cdate": 1542478506410, "ddate": null, "tcdate": 1542478506410, "tmdate": 1542480386554, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "rJxOdw3sTX", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "Thank you for the clarifications! I have already used Snell's code for the embedding and admit baseline implementation is tricky. I have not used Snell's pre-processing as the github repository contains the pre-processing for omniglot only, instead used the RelationNet data pre-processing and loading for mini-imagenet for few corrections such as the normalization mean, std from imagenet https://github.com/floodsung/LearningToCompare_FSL\n\nI have double checked the hyper-parameters and accessed the values in debug mode extensively and wherever division happens, I added an epsilon of 1e-6 or 1e-8 including the element-wise division of f_phi(x_i) / (sigma_i + epsilon) and in the computation of D^{-1/2} where I previously used torch.rsqrt() and replaced it with 1.0 / (w.sum(1) + epsilon).sqrt(), (where w is the graph knn matrix with applied masked k_max=20 for each row and zero everywhere else). However, the issue persisted with alpha=0.99 and model does not learn. As said previously, changing alpha to 0.9 or even lower 0.6 helped learning a lot but the final accuracy for 5-way, 1-shot case remained around the previous result of 47.9%. \n\nI hope ICLR authorities take anonymous code release into the considerations as this is a major barrier for assessing the reproducibility.", "title": "Problem persisted"}, "signatures": ["~anon_ml_reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~anon_ml_reviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "rJxOdw3sTX", "original": null, "number": 4, "cdate": 1542338415944, "ddate": null, "tcdate": 1542338415944, "tmdate": 1542338415944, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "BJgohCDj6X", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Implementation details. Code will be released soon.", "comment": "Thanks for the comment and interest about our paper. \nAccording to the blind review policy, we can not release the code at this moment. We will release our code and the trained model as soon as the review process ends. Meanwhile, we have sent an email to the program chairs to check if it is allowed to release the code anonymously.  We will share the code upon approval.\n\nWe are sure about the reproducibility of the results shown in our paper. And in order to ensure the reproducibility, we ran the test procedure 10 times (each with 600 randomly generated episodes) and reported the average results to avoid accidentally high results. We are not sure if you have reproduced the result as outlined in [1]. If not, we sincerely hope you first try to reproduce the baseline method [1], so you may be closer to the right implementation. It took us quite a while to reproduce [1] even the code has been released. \nNevertheless, we would like to provide more details below which could be useful for you to reproduce the results of our paper.\n\n(1) Our implementation is based on Tensorflow 1.3+, and we also tested on Pytorch 0.4.0. There is only a slight accuracy difference.\n(2) The reason why your results only achieved 25% could be caused by value issues such as divided by zero. Sincerely hope you could double check your code and please make sure you have added an epsilon whenever you call a divide operation. \n(3) Our model is learned end-to-end from scratch, and no pretrain is needed. We did not see your code, but we reckon you did not use the validation set to decide the early stopping iteration, which is commonly used in few-shot learning, such as Prototypical networks. Please use this practice if it is the case.\n(4) The detailed hyperparameters are: alpha=0.99, k=20, query=15, lr=0.001 and halved every 10,000 episodes for at most 100,000 episodes. \n(5) Network architecture details: feature extraction module is exactly the same as Prototypical networks [1], graph construction module is described in Figure 4 of Appendix A. Note that BatchNorm is applied only in Conv layers. In Figure 4, there is no Relu activation after FC layer2. More training details: we use Tensorflow default initialization, BatchNorm with default parameters: decay=0.999 and epsilon=0.001. \n(6) As to preprocessing, for miniImagenet, we follow Prototypical networks [1] while for tieredImagnet we follow Ren et al. [2].\n\nWe have endeavored our best to 'guess' what mistakes you may have made, but there could be other issues that we are unable to enumerate. \nWe highly suggest that a basic starting point is to reproduce the results of Prototypical networks.  Below we provide a few good implementation codes of some related papers. \nPrototypical networks: \n\thttps://github.com/jakesnell/prototypical-networks\n\thttps://github.com/cyvius96/prototypical-network-pytorch\ntieredImagenet: \n\thttps://github.com/renmengye/few-shot-ssl-public\n\n\n[1] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.\n[2] Ren, Mengye, et al. \"Meta-learning for semi-supervised few-shot classification.\" ICLR. 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "BJgohCDj6X", "original": null, "number": 5, "cdate": 1542319795186, "ddate": null, "tcdate": 1542319795186, "tmdate": 1542328479607, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "Since the paper has not provided a reproducible code, based on my implementation in PyTorch 1.0.0.dev20181105, unfortunately I could not reproduce their results on Mini-Imagenet dataset for 5-way, 1-shot and 5-shot scenarios. Using the exact mentioned hyper-parameters, the model didn't learn much in the end-to-end manner and the test accuracy for 5-way, 1-shot was around 25% trained in 50,000 episodes and learning-rate is halved every 10,000 episodes. Instead I pretrained the emebedding in the train set and decreased alpha (label propagation) to 0.9 then it started learning better. \n\nThe best accuracy I could get for the 5-way, 1-shot case (trained with 5-way, 1-shot so no higher-shot) is with alpha=0.6 and it is 47.93% (+/- 1.14% as the 95% confidence interval) which is much lower than the claimed 53.75%. More precisely, I trained with batch-size=15 as described in RelationNet, k in knn=20, Xavier initialization of Conv layers, BatchNorm unit weight initialization and zero bias, zero-mean normal initialization with std 0.01 for Linear layers and unit bias, and then tested with 15 query examples where results were averaged over 600 randomly generated episodes from the test set.\n\nThe paper did not mention any pre-processing step, so I only resized images to 84 by 84 and normalized the mini-imagenet data using imagenet mean and std.", "title": "Reproducibility issues"}, "signatures": ["~anon_ml_reviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~anon_ml_reviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "Skx7vDii3X", "original": null, "number": 3, "cdate": 1541285723021, "ddate": null, "tcdate": 1541285723021, "tmdate": 1541533586242, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "content": {"title": "interesting empirically", "review": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.  Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. \n\nThe merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results.\n\nThe drawbacks  of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.  (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "cdate": 1542234348644, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335831228, "tmdate": 1552335831228, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1x4ca-chQ", "original": null, "number": 2, "cdate": 1541180812074, "ddate": null, "tcdate": 1541180812074, "tmdate": 1541533586025, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "content": {"title": "Novel idea, but important details and deeper analysis are missing", "review": "Summary\nThis paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.  The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. \n\nPros. \n-This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. \n-The empirical results show improvement over the baselines, which are expected. \n\nCons.\n-Some technical details  are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\n-Does episode training help label propagation? How about the results of label propagation without the episode training? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "cdate": 1542234348644, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335831228, "tmdate": 1552335831228, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJx6UQbfhX", "original": null, "number": 1, "cdate": 1540653908529, "ddate": null, "tcdate": 1540653908529, "tmdate": 1541533585814, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "content": {"title": "Transductive few-shot by meta-learning to propagate labels for . Solid work.", "review": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. \n\nThere is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. \n\nComments: \n-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \n-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\n-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper911/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Review", "cdate": 1542234348644, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335831228, "tmdate": 1552335831228, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1l4b54ijm", "original": null, "number": 3, "cdate": 1540209147991, "ddate": null, "tcdate": 1540209147991, "tmdate": 1540209147991, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SklAweC0qX", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Response to experiments and methods", "comment": "Thanks for the comments.\n\nFor each episode, we first utilize both the support set and query set to construct the graph structure. Then, label propagation is performed according to the graph information to get all query set labels. The performance gain comes from the fact that we share information among all query examples and learn to propagate labels. In contrast, inductive methods predict query examples one by one, which does not enjoy this benefit.\n\nAs to query set number experiments, please refer to Appendix B.2 for detailed information.\n\nFor distractor classes, this is not the main focus of our paper. However, in order to explore the extent of our method, we performed experiments in the presence of distractor classes (same setting as [1]). The results are shown below:\nModel                                  mini-5way1shot     mini-5way5shot    tiered-5way1shot    tiered-5way5shot\nSoft k-Mean [1]                     48.70+/-0.32            63.55+/-0.28           49.88+/-0.52            68.32+/-0.22\nSoft k-Mean+Cluster [1]      48.86+/-0.32            61.27+/-0.24           51.36+/-0.31            67.56+/-0.10\nMasked Soft k-Means [1]    49.04+/-0.31            62.96+/-0.14           51.38+/-0.38            69.08+/-0.25\nTPN-semi (Ours)                   50.43+/-0.84            64.95+/-0.73           53.45+/-0.93            69.93+/-0.80\n\nIt can be seen that our TPN-semi algorithm outperforms [1] in all cases, although our method is not specifically designed for the distractor-classes problem.\nWe believe with care design, the performance of our method will continue to increase. This will be the future work."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "SklAweC0qX", "original": null, "number": 4, "cdate": 1539395686321, "ddate": null, "tcdate": 1539395686321, "tmdate": 1539395686321, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "This paper tried to introduce transductive networks for few-shot learning. \nI want to know about the experiments here especially about the transductive process that was done for query set. I hope that you can make it clear what was specifically performed in the batch of query set to help you gain the performance?\nDo you also have any results if you increase the number of query set will affect your performance too? Because I believe this is the contribution that you can have as well from your work. \n\nOne more thing:\nI have a question in your results for semisupervised few-shot learning. \nI read the experiments in semisupervised few-shot learning protocol[1] that there are distractor classes in which I did not see this thing in your paper.\nI intuitively think that this method might be appropriate for the unlabeled data without many outliers/distractors.\nDo you also have the experiments about this before? It is  fine if you also show the drawback of this method, so the improvements can be proposed in the future to tackle that problem.\n\n\n\n\n[1] Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, Hugo\nLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification. International Conference on Learning Representations, 2018.", "title": "Experiments and Methods"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "Bkli_tYd5Q", "original": null, "number": 2, "cdate": 1538984307012, "ddate": null, "tcdate": 1538984307012, "tmdate": 1538984307012, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "ryekRHiecQ", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Response to the experiments", "comment": "Thanks for the comments.\nThe state-of-the-art performance on Omniglot is quite high (>99% except for 20way-1shot setting), which means this problem is nearly solved. Also, there is a tendency that recent high-quality papers do not report results on Omniglot, such as TADAM [1] (NIPS2018), Delta-encoder [2] (NIPS2018), LEO [3] (ICLR19 submission). For the 20way-1shot setting, we compare our TPN results with Relation Net and Prototypical Net as follows:\n\t\t\t\t\t20way-1shot\nPrototypical Net           96.00\nRelation Net                  97.60\nTPN\t\t\t\t        98.03\n\nAlthough zero-shot learning is not our focus, TPN can be easily adapted to zero-shot setting. The modification is similar to Prototypical network or Relation Network. First, a function g can be used to map class-level semantic feature into the same space of visual feature. Then, we can construct graph structure using both features and perform label propagation as in few-shot setting. \n\n[1] Oreshkin, Boris N., Alexandre Lacoste, and Pau Rodriguez. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS2018\n[2] Schwartz, Eli, et al. \"Delta-encoder: an effective sample synthesis method for few-shot object recognition.\" NIPS2018\n[3] Anonymous, \"Meta-Learning with Latent Embedding Optimization.\" ICLR2019 submission. "}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "ryekRHiecQ", "original": null, "number": 3, "cdate": 1538467271493, "ddate": null, "tcdate": 1538467271493, "tmdate": 1538467271493, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "It is interesting that this paper use a label propagation way to solve the low-data testing problem. However, the state-of-art few-shot(zero-shot) methods: Relation Net and Prototypical Net used both minImageNet, Omniglot for few-shot Testing and CUB-200 for zero-shot. So what's your results on Omniglot since you follow the idea of Prototypical Net. In addition, is it possible that your proposed TPN can deal with zero-shot problems since a general few-shot framework can Easily extend to cope with zero-shot problems?", "title": "About the experiments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "ryeo4LvecQ", "original": null, "number": 1, "cdate": 1538450995218, "ddate": null, "tcdate": 1538450995218, "tmdate": 1538450995218, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "H1xbcLqkcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "content": {"title": "Thank you for pointing out related work", "comment": "Thanks for pointing out the related work. We would like to include this reference to our manuscript in the next version. \n\nOur paper and the mentioned paper share the same idea of using metric learning and transduction. However, the target tasks are different. We focus on few-shot learning and meta-learning while the mentioned paper deals with unsupervised domain adaptation. This distinction leads to different algorithm designs: we learn to propagate labels while the mentioned paper proposes the transduction and adaptation steps."}, "signatures": ["ICLR.cc/2019/Conference/Paper911/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613716, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyVuRiC5K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper911/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper911/Authors|ICLR.cc/2019/Conference/Paper911/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613716}}}, {"id": "H1xbcLqkcQ", "original": null, "number": 2, "cdate": 1538397833319, "ddate": null, "tcdate": 1538397833319, "tmdate": 1538397833319, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": " The paper looks very interesting as transductive approaches are powerful for metric learning and semi-supervised learning. And, learning to transductive learn is an interesting direction. I would like to point a related work which authors probably missed which performs metric learning/transfer learning using transduction: https://papers.nips.cc/paper/6360-learning-transferrable-representations-for-unsupervised-domain-adaptation ", "title": "Pointer to a Related Work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}, {"id": "rygAusX19m", "original": null, "number": 1, "cdate": 1538370421994, "ddate": null, "tcdate": 1538370421994, "tmdate": 1538370421994, "tddate": null, "forum": "SyVuRiC5K7", "replyto": "SyVuRiC5K7", "invitation": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "content": {"comment": "This paper proposes a novel meta-learning framework, which aims to propagate labels from labeled instances to unlabeled test instances. This framework learns a graph construction module, exploiting the manifold structure in the data. The idea is reasonable and efficacy, and the experiments are comprehensive.", "title": "Reasonable and efficacy idea"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper911/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "TL;DR": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "pdf": "/pdf/d0e2ccbb91943186e71e62c25ca5f1bbc994e31c.pdf", "paperhash": "liu|learning_to_propagate_labels_transductive_propagation_network_for_fewshot_learning", "_bibtex": "@inproceedings{\nliu2018learning,\ntitle={{LEARNING} {TO} {PROPAGATE} {LABELS}: {TRANSDUCTIVE} {PROPAGATION} {NETWORK} {FOR} {FEW}-{SHOT} {LEARNING}},\nauthor={Yanbin Liu and Juho Lee and Minseop Park and Saehoon Kim and Eunho Yang and Sungju Hwang and Yi Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SyVuRiC5K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper911/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311723124, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyVuRiC5K7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper911/Authors", "ICLR.cc/2019/Conference/Paper911/Reviewers", "ICLR.cc/2019/Conference/Paper911/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311723124}}}], "count": 24}