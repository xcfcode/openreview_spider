{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488318227606, "tcdate": 1478287841656, "number": 364, "id": "B1ckMDqlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1ckMDqlg", "signatures": ["~Azalia_Mirhoseini1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396541698, "tcdate": 1486396541698, "number": 1, "id": "SkLF2M8_g", "invitation": "ICLR.cc/2017/conference/-/paper364/acceptance", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper uses mixtures of experts to increase the capacity of deep networks, and describes the implementation of such a model on a cluster of GPUs. The proposed mixture model achieves strong performances in language modeling and machine translation.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396542248, "id": "ICLR.cc/2017/conference/-/paper364/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396542248}}}, {"tddate": null, "tmdate": 1485204849839, "tcdate": 1485203756825, "number": 14, "id": "BJSEYk4Dg", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1wKm1kPx", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Of course it performs well on a single GPU.", "comment": "Efficient training on a single GPU is simpler than efficient training on multiple GPUs, since the network bandwidth issue goes away.  As we mention in section 3.1, our method scales gracefully with the number of devices.   This includes scaling down to a single device.\n\nThe one caveat is that the number of parameters is limited to what will fit in memory (about 1B parameters).  As can be seen in our experiments (sec. 5.1 and 5.2), increasing the number of parameters to 1 billion can dramatically improve quality.  Any of the models in our experiments which contain 1B parameters or fewer could have been trained on a single GPU.  Reducing the training cluster from n gpus to one GPU would have increased the training time by a factor of approximately n.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1484874660716, "tcdate": 1482744952845, "number": 3, "id": "B1ZFEvR4x", "invitation": "ICLR.cc/2017/conference/-/paper364/official/review", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper proposes a method for significantly increasing the number of parameters in a single layer while keeping computation in par with (or even less than) current SOTA models. The idea is based on using a large mixture of experts (MoE) (i.e. small networks), where only a few of them are adaptively activated via a gating network. While the idea seems intuitive, the main novelty in the paper is in designing the gating network which is encouraged to achieve two objectives: utilizing all available experts (aka importance), and distributing computation fairly across them (aka load). \nAdditionally, the paper introduces two techniques for increasing the batch-size passed to each expert, and hence maximizing parallelization in GPUs.\nExperiments applying the proposed approach on RNNs in language modelling task show that it can beat SOTA results with significantly less computation, which is a result of selectively using much more parameters. Results on machine translation show that a model with more than 30x number of parameters can beat SOTA while incurring half of the effective computation.\n\nI have the several comments on the paper:\n- I believe that the authors can do a better job in their presentation. The paper currently is at 11 pages (which is too long in my opinion), but I find that Section 3.2 (the crux of the paper) needs better motivation and intuitive explanation. For example, equation 8 deserves more description than currently devoted to it. Additional space can be easily regained by moving details in the experiments section (e.g. architecture and training details) to the appendix for the curious readers. Experiment section can be better organized by finishing on experiment completely before moving to the other one. There are also some glitches in the writing, e.g. the end of Section 3.1. \n- The paper is missing some important references in conditional computation (e.g. https://arxiv.org/pdf/1308.3432.pdf) which deal with very similar issues in deep learning.\n- One very important lesson from the conditional computation literature is that while we can in theory incur much less computation, in practice (especially with the current GPU architectures) the actual time does not match the theory. This can be due to inefficient branching in GPUs. It would be nice if the paper includes a discussion of how their model (and perhaps implementation) deal with this problem, and why it scales well in practice.\n- Table 1 and Table 3 contain repetitive information, and I think they should be combined in one (maybe moving Table 3 to appendix). One thing I do not understand is how does the number of ops/timestep relate to the training time. This also related to the pervious comment.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482744953452, "id": "ICLR.cc/2017/conference/-/paper364/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482744953452}}}, {"tddate": null, "tmdate": 1484874622657, "tcdate": 1484874622657, "number": 4, "id": "B1wKm1kPx", "invitation": "ICLR.cc/2017/conference/-/paper364/official/comment", "forum": "B1ckMDqlg", "replyto": "Byf6D0t8e", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "content": {"title": "Thanks for addressing my comments", "comment": "The paper is indeed improved. I will update my score accordingly. One thing to keep in mind is that most previous approaches where probably concerning single GPU case. It would be nice if you state clearly in the paper that your method is specifically designed for cluster of GPU. On that note, do you think you would be able to get it to perform well also on single GPU?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606479, "id": "ICLR.cc/2017/conference/-/paper364/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606479}}}, {"tddate": null, "tmdate": 1484546352881, "tcdate": 1484546352881, "number": 13, "id": "B1YVWy9Ig", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "H1EQ18hBx", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Thank you!", "comment": "I think you said it better than we did. \n\nWe rewrote our paper, clarifying the fact that this is the first demonstration of efficient massively-sparse conditional computation in neural networks, and enumerating the main challenges we encountered.\n\nWe also added language modeling experiments on a larger corpus (section 5.2), demonstrating computational efficiency and significant quality improvements out to 68 billion parameters :)\n\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1484545040997, "tcdate": 1484545040997, "number": 12, "id": "HyYzn0YIe", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "r10X7Es4g", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "addressing your comments", "comment": "Thank you for your helpful suggestions.  We updated our paper to address them as follows:\n\nDiscussion of relation to other work to effectively increase model capacity to enable the exploitation of very large datasets:\n\nWe rewrote the Introduction (Section 1) to clearly outline the key challenges to making efficient conditional computation work in practice. We cited a range of works on conditional computation and discussed their relation to our work (for example see Sections 1.1, 1.3, 2.1, and 4).\n \nSome of the related works we cited are as follows:\n \nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.\n\nDavid Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\n \nK. Cho and Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional\nComputation in Deep Learning. ArXiv e-prints, June 2014.\n\nPatrick Gallinari Ludovic Denoyer. Deep sequential neural network. arXiv preprint arXiv:1410.0510, 2014.\n\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\n\n A. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, and A. Courville. Dynamic Capacity Networks. ArXiv e-prints, November 2015.\n \n Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\nnetwork of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\n06194.\n \nDiscussion of computational efficiency:  \nWe added computational efficiency metrics (TFLOPS/GPU) to our language modeling experiments (see Tables 1, 7 and 8)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1484544766021, "tcdate": 1484544766021, "number": 11, "id": "SkIWsCt8e", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "Hy7i5WXNg", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "addressing your comments", "comment": "Thank you for your helpful suggestions.  We updated our paper to address them as follows:\n\nReal computational load and system behavior:\nWe added concrete computational efficiency metrics (TFLOPS/GPU) to our language modeling experiments.  (see Tables 1, 7 and 8).\n\nEffect of loss terms:\nWe added more detailed discussions related to our balancing losses as well as experimental tests to show their effectiveness (see Section 4 and Appendix A).\n \nFigure clarity: \nWe modified Figure 3 (which is now Figure 2).  Section 5.1  provides full explanations.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1484543929620, "tcdate": 1484543929620, "number": 10, "id": "Byf6D0t8e", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ZFEvR4x", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "addressing your comments", "comment": "Thank you for your helpful suggestions.  We updated our paper to address them as follows:\n\nDiscussion on how to deal with branching and why our approach performs well: \nWe rewrote the Introduction (Section 1)  and clearly outlined the key design considerations and challenges to making massively sparse conditional computation work in practice.  In the following sections (2, 3, and 4), we discussed our solutions to address all of these design considerations. Finally in Section 5, we showed how applying our approach to massive data sets can best realize the true potential of conditional computing.\n\nLength:  \nWe moved most of the less essential content to appendices, reducing the paper to 9 pages.\n\nBetter explanation of balancing losses:\nWe added a lot of additional explanation and motivation here, including experimental tests and citations to other authors who have encountered the same problem.  (See section 4 and appendix A).  We also  clarify in our new introduction that this issue is only one of several major challenges to implementing efficient conditional computation.\n\nRelated work: \nWe added citations to a range of works on conditional computing and discussed their relation to our work (for example, see Sections 1.1, 1.3, 2.1, and 4).  Citations include:\n\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.\n\nDavid Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.\n \nK. Cho and Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional\nComputation in Deep Learning. ArXiv e-prints, June 2014.\n\nPatrick Gallinari Ludovic Denoyer. Deep sequential neural network. arXiv preprint arXiv:1410.0510, 2014.\n\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\n\n A. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, and A. Courville. Dynamic Capacity Networks. ArXiv e-prints, November 2015.\n \n Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\nnetwork of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\n06194.\n\nExperiment tables and figures modification: \nWe removed the (former) Table 3 to appendix C, and modified the description of the figures and related experiments (see Section 5.1)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1484370438454, "tcdate": 1484370438454, "number": 9, "id": "H1CWMNvLg", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["~Azalia_Mirhoseini1"], "readers": ["everyone"], "writers": ["~Azalia_Mirhoseini1"], "content": {"title": "Rebuttal", "comment": "We thank the reviewers for their instructive feedback. We made a significant effort to improve our work both in terms of presentation and content to reflect the\nreviewers' suggestions. We believe our latest draft clarifies in detail our contributions with respect to the previous state of the field (e.g., conditional computation).\n\nThe main idea of our paper can be summarized as this:  Massively increasing the capacity of deep networks by employing efficient, general-purpose conditional computation.  This idea seems hugely promising and hugely obvious.  At first glance, it is utterly shocking that no one had successfully implemented it prior to us.  In practice, however, there are major challenges in achieving high performance and high quality.  We enumerate these challenges in the introduction section of our new draft.  Our paper discusses how other authors have attacked these challenges, as well as our particular solutions.   \n\nWhile some of our particular solutions (e.g., noisy-top-k gating, the particular batching schemes, the load-balancing loss, even the mixture-of-experts formalism) may not withstand the test of time, our main contribution, which is larger than these particulars, is to prove by example that efficient, general-purpose conditional computation in deep networks is possible and very beneficial. As such, this is likely a seminal paper in the field.\n\nOur apologies for not making this clearer in our first drafts. Please re-read our paper with this in mind, and consider updating your reviews accordingly, if appropriate.\n\nIn addition to the major changes described above, we have made several other improvements:\n - We added experimental tests of our balancing losses (see Appendix A).\n - We added computational efficiency metrics (TFLOPS/GPU) to our language modeling experiments (see Tables 1, 7 and 8).\n - We added a set of language modeling experiments on a 100 billion word corpus, using MoE models with up to 137 billion parameters. These demonstrate major quality improvements and good computational efficiency up to 68 billion parameters (see Section 5.2).\n - We added a set of experiments on learning a multilingual machine translation model, showing very large improvements over recently published results (see Table 5).\n - We moved some of the less important content to appendices, bringing the paper length down to 9 pages.\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1483656988556, "tcdate": 1483656988556, "number": 8, "id": "H1EQ18hBx", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "From an interested reader: I would give this at least a 9 rating.", "comment": "I read the paper and I feel the ratings are too low. The authors introduce a general-purpose mechanism to scale up neural networks significantly beyond their current size using sparsity of activation, i.e. by forcing the activation of most neurons in the net to be zero for any given training example.\n\nFirstly, I believe the sheer size of the models successfully trained in this paper warrant an 8 rating all by themselves.\n\nSecondly, we know historically that sparsity of parameters is among the most important modelling principles in machine learning, being used with great success in e.g. Lasso with the l1 penalty, in SVM with the hinge loss and in ConvNets by setting connections outside the receptive field to zero. This paper, in addition to sparsity of parameters (neurons in different experts are not connected) employs sparsity of activation, where the computation path is customized for each training example. It is, as far as I can tell, the first paper to implement this in a practical, scalable and general way for neural networks. If sparsity of activation turns out to be even a small fraction as important as sparsity of parameters, this paper will have a major impact.\n\nThirdly, I love the computational efficiency of the model presented. The authors achieve extreme sparsity yet fully utilize their GPUs. In particular, the authors design the network in such a way that there are very few connections between active and non-active units. If we have, say, a sparsely activated fully-connected network, most computation would be wasted on network connections that start on active units and end on non-active units.\n\nFourthly, the authors discuss and provide a practical and elegant strategy for large-scale cluster implementation, showcasing their technical sophistication.\n\nIt is perhaps unfortunate that current baseline datasets may not even be able to fully utilize the power of MoE or other to-be-designed networks following similar principles, but models like the one presented here are bound to only become more prominent in the future.\n\nI would rate this paper at least 9.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1482828523545, "tcdate": 1482828523545, "number": 3, "id": "ByXgssJHe", "invitation": "ICLR.cc/2017/conference/-/paper364/official/comment", "forum": "B1ckMDqlg", "replyto": "r1dWtP1Hg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "content": {"title": "Corrected review", "comment": "Sorry for that - I changed the review text accordingly. I agree that your results are very good. This, however, doesn't affect the other comments which I think can significantly improve your paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606479, "id": "ICLR.cc/2017/conference/-/paper364/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606479}}}, {"tddate": null, "tmdate": 1482811647605, "tcdate": 1482811647605, "number": 7, "id": "r1dWtP1Hg", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ZFEvR4x", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Machine translation results are better than SOTA.", "comment": "Our results on machine translation _are_ better than SOTA, measured both by perplexity and by BLEU score.  We should have made it clearer that BLEU score is a metric of similarity of the machine-generated translation and the human-generated translation, so higher is better.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1482811242960, "tcdate": 1482811242960, "number": 6, "id": "r1Quvv1re", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ZFEvR4x", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Agreed", "comment": "Yes.  Our paper does need a better discussion of different approaches to conditional computation, and their practical performance implications.   The strength of our method is that our computation time _does_ match the theory - i.e. that we are getting a significant fraction of the advertised TFLOPs of our gpus.  We will rewrite so as to include a better discussion of the issues and to include clearer performance measures.  \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1482654542209, "tcdate": 1482654542209, "number": 5, "id": "SJI87b6Ne", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "r10X7Es4g", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Which alternatives?", "comment": "Thanks for the review.   We'd love to add a comparison of MoE to alternative ways of increasing model capacity.  We are aware of the following alternatives:\n\n1.  Dense layers (conventional layers requiring O(1) computations per parameter per training example).  Expanding the number of parameters in the dense layers proportionally increases training time.\n\n2.  Embedding layers (such as the word-embedding layers in our language models).   These can be hugely sparse.  We can expand the capacity of an embedding layer by either increasing its dimensionality or its feature space.  Our belief is that MoE layers are more powerful than embedding layers, as evidenced by the experiments in our paper, where adding an MoE layer improved perplexity even in the presence of a large embedding layer.   We attribute this to the fact that MoE layers, similar to dense layers, can handle orders of magnitude more parameter-example-interactions than embedding layers.  The embedding layers tend to be limited by network bandwidth, as the parameters need to be sent over the network, whereas the MoE layers are limited only by GPU compute power, which tends to be much greater.\n\nOther than these, which alternatives do you think are most important to discuss?\n\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1482535717736, "tcdate": 1482535717736, "number": 2, "id": "r10X7Es4g", "invitation": "ICLR.cc/2017/conference/-/paper364/official/review", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer3"], "content": {"title": "Elegant use of MoE for expanding model capacity, but it would be very nice to discuss MoE alternatives in terms of computational efficiency and other factors.", "rating": "6: Marginally above acceptance threshold", "review": "Paper Strengths: \n-- Elegant use of MoE for expanding model capacity and enabling training large models necessary for exploiting  very large datasets in a computationally feasible manner\n\n-- The effective batch size for training the MoE drastically increased also\n\n-- Interesting experimental results on the effects of increasing the number of MoEs, which is expected.\n\n\nPaper Weaknesses:\n\n--- there are many different ways of increasing model capacity to enable the exploitation of very large datasets; it would be very nice to discuss  the use of MoE and other alternatives in terms of computational efficiency and other factors.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482744953452, "id": "ICLR.cc/2017/conference/-/paper364/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482744953452}}}, {"tddate": null, "tmdate": 1482001051147, "tcdate": 1482001051147, "number": 1, "id": "Hy7i5WXNg", "invitation": "ICLR.cc/2017/conference/-/paper364/official/review", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer2"], "content": {"title": "Nice use of MoE with good results", "rating": "7: Good paper, accept", "review": "This paper describes a method for greatly expanding network model size (in terms of number of stored parameters) in the context of a recurrent net, by applying a Mixture of Experts between recurrent net layers that is shared between all time steps.  By process features from all timesteps at the same time, the effective batch size to the MoE is increased by a factor of the number of steps in the model; thus even for sparsely assigned experts, each expert can be used on a large enough sub-batch of inputs to remain computationally efficient.  Another second technique that redistributes elements within a distributed model is also described, further increasing per-expert batch sizes.\n\nExperiments are performed on language modeling and machine translation tasks, showing significant gains by increasing the number of experts, compared to both SoA as well as explicitly computationally-matched baseline systems.\n\nAn area that falls a bit short is in presenting plots or statistics on the real computational load and system behavior.  While two loss terms were employed to balance the use of experts, these are not explored in the experiments section.  It would have been nice to see the effects of these more, along with the effects of increasing effective batch sizes, e.g. measurements of the losses over the course of training, compared to the counts/histogram distributions of per-expert batch sizes.\n\nOverall I think this is a well-described system that achieves good results, using a nifty placement for the MoE that can overcome what otherwise might be a disadvantage for sparse computation.\n\n\n\nSmall comment:\nI like Fig 3, but it's not entirely clear whether datapoints coincide between left and right plots.  The H-H line has 3 points on left but 5 on the right?  Also would be nice if the colors matched between corresponding lines.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482744953452, "id": "ICLR.cc/2017/conference/-/paper364/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482744953452}}}, {"tddate": null, "tmdate": 1481656128441, "tcdate": 1481656128435, "number": 4, "id": "H1dBwaame", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "H1Gvvb6mx", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Explanation of P(x, i)", "comment": "P(x, i) measures the probability that expert i is used for example x, which happens if expert i is among the top k experts for example x.   Let's denote the values that are being compared by F(x, i).  They each consist of a signal term and a noise term.  Expert i is used if F(x, i) is in the top k elements of {F(x, *)}.\n\n   F(x, i) = (x \u00b7 W_g)_i + StandardNormal() \u00b7 Softplus((x \u00b7 W_noise)_i)\n\nIdeally, we would compute the probability P(x, i) relative to a new random choice of noise for all the experts.  We can't find a closed form for this, so we fix the noise for the other experts at the chosen values, and compute the probability relative to a new random choice of noise on expert i.    The new  value of F(x, i) will be the top k elements of {F(x, *)}  if and only if it is greater than the k-th highest of the other elements.  This is what we mean by kth_excluding.  We are determining the threshold value that the new value of F(x, i) will need to exceed in order for expert i to be used.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1481656022970, "tcdate": 1481656022964, "number": 3, "id": "S1kyva6Xx", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "H1Gvvb6mx", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Motivation of Importance and Load", "comment": "Yes.  I can explain why each of these measures are necessary, and give examples where each measure is balanced while the other is unbalanced.\n\nThe \"Load\" measure is for performance reasons.  It measures the number of examples sent to an expert.  If we have a setup where the experts are evaluated in parallel on different GPUs, then performance is limited by the expert that gets the largest number of examples.  If, for example, one expert gets 10x as many examples as the others, the other GPUs sit 90% idle during training time, and throughput drops by a factor of 10.\n\nThe \"Importance\" measure is for quality reasons.  It measures the sum of the gate values for an expert, across a batch of examples.  When we do not assign an \"Importance\" loss, we have noticed that the network falls into a local minimum where many of the experts always get low (or zero) weights.  This is a self-perpetuating situation, since those experts receive little training, don't learn much, and so the gating network continues to avoid them.  This happens even without the sparsity (when k=n).   Assigning an \"Importance\" loss breaks this cycle and forces all of the experts to start learning something.\n\nHere is a case where Importance is balanced and Load is not balanced:\nn=4; k=2; batch_size=3; gating network output =  \n  Example 0:  (expert_0: 0.75, expert_3: 0.25)\n  Example 1:  (expert_1: 0.75, expert_3: 0.25)\n  Example 2:  (expert_2: 0.75, expert_3: 0.25)\nImportance: (expert_0: 0.75, expert_1: 0.75, expert_2: 0.75, expert_3: 0.75)\nLoad = (expert_0: 1, expert_1: 1, expert_2: 1, expert_3: 3)\n\nHere is a case where Load is balanced and Importance is not balanced:\nn=4; k=2; batch_size=4; gating network output =  \n  Example 0:  (expert_0: 0.99, expert_1: 0.01)\n  Example 1:  (expert_0: 0.99, expert_1: 0.01)\n  Example 2:  (expert_2: 0.99, expert_3: 0.01)\n  Example 3:  (expert_2: 0.99, expert_3: 0.01)\nImportance: (expert_0 :1.98, expert_1: 0.02, expert_2: 1.98, expert_3: 0.02)\nLoad = (expert_0: 2, expert_1: 2, expert_2: 2, expert_3: 2)\n\nWe have noticed both of these failure modes, so we include both losses.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1481607002367, "tcdate": 1481607002359, "number": 3, "id": "H1Gvvb6mx", "invitation": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer1"], "content": {"title": "Difference between Importance and Load", "question": "Can you explain the difference between Importance and Load as per-expert utilization measures?Why you think we need both terms in the loss function?\n\nCan you motivate the definition of P(x,i) and give an intuitive explanation? I'm not sure I understand what does kth_excluding term do here."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481607002989, "id": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481607002989}}}, {"tddate": null, "tmdate": 1481448198651, "tcdate": 1481448198643, "number": 2, "id": "r1kMjqqml", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "B1ChpDkml", "signatures": ["~Noam_Shazeer1"], "readers": ["everyone"], "writers": ["~Noam_Shazeer1"], "content": {"title": "Done.", "comment": "Thank you for your suggestion.  We re-ran our language modeling experiments and included among our baselines models where the MoE was replaced by feed-forward nets with one and four hidden layers. \n\nOur main reason for re-running the experiments was that we changed our gating network to allow for soft load balancing constraints. This scheme seems cleaner than the hard load-balancing scheme described in the appendix, in that it does not employ batchwise functions in the model - only in the losses.  \n\nWe also changed the architecture of the experts to have one hidden layer, similar to our translation experiments, and did a better job of tuning the dropout rate.  \n\nWe did not include a model with 30 billion parameters, as we did in the previous revision.  Our experiments suggest that this number of parameters is unnecessary for a 1-billion-word corpus.  Instead, we added additional models with 4 billion parameters and different amounts of computation.   If we have time before publication, we may add experiments with a larger corpus and a larger number of parameters so as to test the hypothesis that adding parameters helps until the number of parameters is on the order of the size of the training corpus.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1480804380736, "tcdate": 1480804380730, "number": 1, "id": "SJSXOTeXe", "invitation": "ICLR.cc/2017/conference/-/paper364/public/comment", "forum": "B1ckMDqlg", "replyto": "HyIrgUk7g", "signatures": ["~Azalia_Mirhoseini1"], "readers": ["everyone"], "writers": ["~Azalia_Mirhoseini1"], "content": {"title": "MoE enables training large models that are necessary for training very large datasets in a computationally feasible manner.", "comment": "Thank you for your question! For the very large datasets available in LM and MT, most models underfit because they do not have enough weights.  The obvious ways of adding more weights makes the training too slow per example so the model cannot be trained on all the data.  The main motivation for a MoE is that we can have hugely more weights without much increase in the computation time per example because nearly all of the weights are not touched on any particular example."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287606604, "id": "ICLR.cc/2017/conference/-/paper364/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1ckMDqlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper364/reviewers", "ICLR.cc/2017/conference/paper364/areachairs"], "cdate": 1485287606604}}}, {"tddate": null, "tmdate": 1480715702387, "tcdate": 1480715702381, "number": 2, "id": "B1ChpDkml", "invitation": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer2"], "content": {"title": "Multilayer mapping net baseline?", "question": "An additional baseline comparison, which I don't see yet, may be to replace the MoE with a simple 2- or so layer network with similar number of executed parameters or connections as the MoE (but fewer total parameters).  E.g., to use the k in the top-k experts (plus the gating net) to find the number of executed params/connections and create a slightly deeper, k layer net matching this number of connections (e.g., k layers each with the same 512x512 matrix size), that replaces the MoE layer.  The total number of stored params is fewer here, but the number of executed params and connections should be about the same, and it is a deeper net doing the mapping.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481607002989, "id": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481607002989}}}, {"tddate": null, "tmdate": 1480708157773, "tcdate": 1480708157768, "number": 1, "id": "HyIrgUk7g", "invitation": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "signatures": ["ICLR.cc/2017/conference/paper364/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper364/AnonReviewer3"], "content": {"title": "Use of huge Mixture-of-Experts ", "question": "Can you justify the use of Mixture-of-Experts in LM and MT? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": " Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "abstract": "The capacity of a neural network to absorb information is limited by its number of parameters.  Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation.  In practice, however, there are significant algorithmic and performance challenges.  In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters.  We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks.  A trainable gating network determines a sparse combination of these experts to use for each example.  We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.  We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers.  On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.", "pdf": "/pdf/e363a24f399de1c363914ca1d0231bc8521a1006.pdf", "paperhash": "shazeer|_outrageously_large_neural_networks_the_sparselygated_mixtureofexperts_layer", "keywords": ["Deep learning"], "conflicts": ["google.com"], "authors": ["Noam Shazeer", "*Azalia Mirhoseini", "*Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "authorids": ["noam@google.com", "azalia@google.com", "krzysztof.maziarz@student.uj.edu.pl", "andydavis@google.com", "qvl@google.com", "geoffhinton@google.com", "jeff@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481607002989, "id": "ICLR.cc/2017/conference/-/paper364/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper364/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper364/AnonReviewer3", "ICLR.cc/2017/conference/paper364/AnonReviewer2", "ICLR.cc/2017/conference/paper364/AnonReviewer1"], "reply": {"forum": "B1ckMDqlg", "replyto": "B1ckMDqlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper364/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481607002989}}}], "count": 24}