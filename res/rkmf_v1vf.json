{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124463113, "tcdate": 1518462987265, "number": 218, "cdate": 1518462987265, "id": "rkmf_v1vf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rkmf_v1vf", "signatures": ["~David_Pfau1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Minimally Redundant Laplacian Eigenmaps", "abstract": "Spectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.", "paperhash": "pfau|minimally_redundant_laplacian_eigenmaps", "keywords": ["spectral methods", "laplacian eigenmaps", "disentangling", "nonparametric", "nonlinear dimensionality reduction", "variational autoencoders"], "_bibtex": "@misc{\n  pfau2018minimally,\n  title={Minimally Redundant Laplacian Eigenmaps},\n  author={David Pfau and Christopher P. Burgess},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmf_v1vf}\n}", "authorids": ["pfau@google.com", "cpburgess@google.com"], "authors": ["David Pfau", "Christopher P. Burgess"], "TL;DR": "Many problems with classic manifold learning methods can be fixed by filtering out predictable dimensions of the embedding, and the resulting algorithm can also disentangle factors of variation.", "pdf": "/pdf/c23db4f9346a8f61a4b47cbefd2d3e2e9ce0878b.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582953601, "tcdate": 1520182546003, "number": 1, "cdate": 1520182546003, "id": "rJ9fHsKOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper218/Official_Review", "forum": "rkmf_v1vf", "replyto": "rkmf_v1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer2"], "content": {"title": "Authors of this paper demonstrated that the classic nonparametric manifold learning methods often suffer from the issues of collapsing embeddings, and argued that this collapse is due to the method by which embedding vectors are chosen. To fix this issue, minimally redundant laplacian eigenmaps were proposed.", "rating": "6: Marginally above acceptance threshold", "review": "Authors of this paper demonstrated that the classic nonparametric manifold learning methods often suffer from the issues of collapsing embeddings, and argued that this collapse is due to the method by which embedding vectors are chosen. To fix this issue, minimally redundant laplacian eigenmaps were proposed.\n\nAuthors stated that, due to the different changes to the latents cause different scales of changes in observation space, the nearest neighbors graph is more elongated along certain dimensions than others. \n1. It is not clear why selecting appropriate eigenvectors can overcome this issue. My concern is that the eigenvectors from Laplacian eigenmaps with incorrect nearest neighbor graph might be also incorrect. \n\n2. How about normalizing the scale of the observation data first and then performing Laplacian eigenmaps? \n\nFrom the description of selecting strategy in Section 2, it is not clear how to select unpredictable vectors. It seems that the method greedily selects one vector from the rest of vectors by setting the second smallest as the initial one. However, experimental results demonstrated certain combination of any two vectors from Laplacian eigenmap. More details are needed for better understanding. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimally Redundant Laplacian Eigenmaps", "abstract": "Spectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.", "paperhash": "pfau|minimally_redundant_laplacian_eigenmaps", "keywords": ["spectral methods", "laplacian eigenmaps", "disentangling", "nonparametric", "nonlinear dimensionality reduction", "variational autoencoders"], "_bibtex": "@misc{\n  pfau2018minimally,\n  title={Minimally Redundant Laplacian Eigenmaps},\n  author={David Pfau and Christopher P. Burgess},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmf_v1vf}\n}", "authorids": ["pfau@google.com", "cpburgess@google.com"], "authors": ["David Pfau", "Christopher P. Burgess"], "TL;DR": "Many problems with classic manifold learning methods can be fixed by filtering out predictable dimensions of the embedding, and the resulting algorithm can also disentangle factors of variation.", "pdf": "/pdf/c23db4f9346a8f61a4b47cbefd2d3e2e9ce0878b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582953383, "id": "ICLR.cc/2018/Workshop/-/Paper218/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper218/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper218/AnonReviewer1"], "reply": {"forum": "rkmf_v1vf", "replyto": "rkmf_v1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper218/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper218/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582953383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582888778, "tcdate": 1520528505600, "number": 2, "cdate": 1520528505600, "id": "ryGFh1yFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper218/Official_Review", "forum": "rkmf_v1vf", "replyto": "rkmf_v1vf", "signatures": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer1"], "content": {"title": "Minimally Redundant Laplacian Eigenmaps", "rating": "7: Good paper, accept", "review": "- The authors propose a new method called Minimally Redundant Laplacian Eigenmaps which starts from a (standard) spectral clustering with normalized graph Laplacian and then, instead of directly using the eigenvectors, consider a new criterion for selecting few eigenvectors for a low-dimensional embedding. A good motivation (scaling issues) is given for applying this modified procedure. The authors describe the new criterion and show an extensive experimental section with additional material in appendix. Overall, they make a nice and valuable contribution at this point.\n\n- On the other hand, a possible drawback of the proposed method might be that one does not known to which underlying objective the final solution is corresponding, after the two-steps procedure (because implicitly the final solution doesn't correspond to the original objective of step 1). \n\nIn order to obtain few components directly from the problem formulation in the first step, one would probably need to modify the objective function (or have a different variational principle) in step 1. The following recent work achieves few components directly from the problem formulation:\n\nFanuel M., Aspeel A., Delvenne J.C., Suykens J.A.K., Positive semi-definite embedding for dimensionality reduction and out-of-sample extensions, arXiv:1711.07271\n\n- In section 3 the authors choose k=200 and epsilon =0.3. Are the result sensitive with respect to these choices? (ideally it would be good if a model selection procedure could be given).\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimally Redundant Laplacian Eigenmaps", "abstract": "Spectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.", "paperhash": "pfau|minimally_redundant_laplacian_eigenmaps", "keywords": ["spectral methods", "laplacian eigenmaps", "disentangling", "nonparametric", "nonlinear dimensionality reduction", "variational autoencoders"], "_bibtex": "@misc{\n  pfau2018minimally,\n  title={Minimally Redundant Laplacian Eigenmaps},\n  author={David Pfau and Christopher P. Burgess},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmf_v1vf}\n}", "authorids": ["pfau@google.com", "cpburgess@google.com"], "authors": ["David Pfau", "Christopher P. Burgess"], "TL;DR": "Many problems with classic manifold learning methods can be fixed by filtering out predictable dimensions of the embedding, and the resulting algorithm can also disentangle factors of variation.", "pdf": "/pdf/c23db4f9346a8f61a4b47cbefd2d3e2e9ce0878b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582953383, "id": "ICLR.cc/2018/Workshop/-/Paper218/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper218/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper218/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper218/AnonReviewer1"], "reply": {"forum": "rkmf_v1vf", "replyto": "rkmf_v1vf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper218/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper218/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582953383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573561354, "tcdate": 1521573561354, "number": 81, "cdate": 1521573561017, "id": "By-6CR0FG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rkmf_v1vf", "replyto": "rkmf_v1vf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimally Redundant Laplacian Eigenmaps", "abstract": "Spectral algorithms for learning low-dimensional data manifolds have largely been supplanted by deep learning methods in recent years. One reason is that classic spectral manifold learning methods often learn collapsed embeddings that do not fill the embedding space. We show that this is a natural consequence of data where different latent dimensions have dramatically different scaling in observation space. We present a simple extension of Laplacian Eigenmaps to fix this problem based on choosing embedding vectors which are both orthogonal and \\textit{minimally redundant} to other dimensions of the embedding. In experiments on NORB and similarity-transformed faces we show that Minimally Redundant Laplacian Eigenmap (MR-LEM) significantly improves the quality of embedding vectors over Laplacian Eigenmaps, accurately recovers the latent topology of the data, and discovers many disentangled factors of variation of comparable quality to state-of-the-art deep learning methods.", "paperhash": "pfau|minimally_redundant_laplacian_eigenmaps", "keywords": ["spectral methods", "laplacian eigenmaps", "disentangling", "nonparametric", "nonlinear dimensionality reduction", "variational autoencoders"], "_bibtex": "@misc{\n  pfau2018minimally,\n  title={Minimally Redundant Laplacian Eigenmaps},\n  author={David Pfau and Christopher P. Burgess},\n  year={2018},\n  url={https://openreview.net/forum?id=rkmf_v1vf}\n}", "authorids": ["pfau@google.com", "cpburgess@google.com"], "authors": ["David Pfau", "Christopher P. Burgess"], "TL;DR": "Many problems with classic manifold learning methods can be fixed by filtering out predictable dimensions of the embedding, and the resulting algorithm can also disentangle factors of variation.", "pdf": "/pdf/c23db4f9346a8f61a4b47cbefd2d3e2e9ce0878b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}