{"notes": [{"id": "rygEokBKPS", "original": "rJlJApAdPr", "number": 1910, "cdate": 1569439643641, "ddate": null, "tcdate": 1569439643641, "tmdate": 1577168223906, "tddate": null, "forum": "rygEokBKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "F-82D30Ew", "original": null, "number": 1, "cdate": 1576798735679, "ddate": null, "tcdate": 1576798735679, "tmdate": 1576800900701, "tddate": null, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a new black-box adversarial attack based on tiling and evolution strategies. While the experimental results look promising, the main concern of the reviewers is the novelty of the proposed algorithm, and many things need to be improved in terms of clarity and experiments. The paper does not gather sufficient support from the reviewers even after author response. I encourage the authors to improve this paper and resubmit to future conference.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706556, "tmdate": 1576800254641, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Decision"}}}, {"id": "rkxAlx2iKS", "original": null, "number": 1, "cdate": 1571696630119, "ddate": null, "tcdate": 1571696630119, "tmdate": 1574490364936, "tddate": null, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposed a new query efficient black-box attack algorithm using better evolution strategies. The authors also add tiling trick to make the attack even more efficient. The experimental results show that the proposed method achieves state-of-the-art attack efficiency in black-box setting.\n\nThe paper indeed presented slightly better results than the current state-of-the-art black-box attacks. It is clearly written and easy to follow, however, the paper itself does not bring much insightful information. \n\nThe major components of the proposed method are two things: using better evolution strategies and using tiling trick. The tiling trick is not something new, it is introduced in (Ilyas et al., 2018) and also discussed in (Moon et al., 2019). The authors further empirically studied the best choice of tiling size. I appreciated that, but will not count it as a major contribution. In terms of better evolution strategies, the authors show that (1+1) and CMA-EA can achieve better attack result but it lacks intuition/explanations why these helps, what is the difference. It would be best if the authors could provide some theories to show the advantages of the proposed method, if not, at least the authors should give more intuition/explanation/demonstrative experiments to show the advantages.\n\nDetailed comments:\n- In section 3.2, is the form of the discretized problem a standard way to transform from continuous to discrete one? What is the intuition of using a and b? Have you considered using only one variable to do it?\n- In section 3.3.2 what do you mean by \u201cwith or without softmax, the optimum is at infinity\u201d? I hope the authors could further explain it.\n- In eq (2), do you mean  max_{\\tau} L(f(x + \\epsilon tanh(\\tau)), y) ?\n- In section 3.3.1, the authors said (1+1)-ES and CMA-ES can be seen as an instantiation of NES. Can the authors further elaborate on this?\n- Can the authors provide algorithm for DiagonalCMA?\n- It is better to put the evolution strategy algorithms in the main paper and discuss it. \n- Can the authors also comment/compare the results with the following relevant paper?\nLi, Yandong, et al. \"NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.\" ICML 2019.\nChen, Jinghui, Jinfeng Yi, and Quanquan Gu. \"A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks.\" arXiv preprint arXiv:1811.10828 (2018).\n\n-  In Table 1, why for Parsimonious and Bandit methods, # of tiles parts are missing? I think both of the baselines use tilting trick? And they should also run using the optimal tiling size? The result seems directly copied from the Parsimonious paper? It makes more sense to rerun it in your setting and environment cause the sampled data points may not be the same. Since CMA costs significantly more time, it makes a fair comparison to also report the attack time needed for each method.\n\n- In Table 3, why did not compare with Bandit and Parsimonious attacks? \n\n======================\nafter the rebuttal\n\nI thank the authors for their response but I still feel that there is a lot more to improve for this paper in terms of intuition and experiments. Therefore I decided to keep my score unchanged.  \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575706929073, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Reviewers"], "noninvitees": [], "tcdate": 1570237730524, "tmdate": 1575706929084, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review"}}}, {"id": "HylN_kKRKr", "original": null, "number": 3, "cdate": 1571880811623, "ddate": null, "tcdate": 1571880811623, "tmdate": 1574488149465, "tddate": null, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper proposed a DFO framework to generate black-box adversarial examples. By comparing with Parsimonious and Bandits, the proposed approach achieves lower query complexity and higher attack success rate (ASR).\n\nI have two main concerns about the current version:\n\n1)  Some important baselines might be missing. In addition to (Ilyas et al., 2018b) and (Moon et al., 2019), the methods built on zeroth-order optimization (namely, gradient estimation via function differences) were not compared. Examples include \n[1] There are No Bit Parts for Sign Bits in Black-Box Attacks\n[2] AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks\n[3] SIGNSGD VIA ZEROTH-ORDER ORACLE\n \n2) In addition to attack success rate and query complexity, it might be useful to compare different attacks in terms of $\\ell_p$ distortion, where $p \\neq \\infty$. This could provide a clearer picture on whether or not the query efficiency and the attack performance are at the cost of increasing the $\\ell_1$ and $\\ell_2$ distortion significantly.\n\n\n########### Post-feedback ##############\nThanks for the response and the additional experiments to address my first question.  However, I am not satisfied with the response \"But clearly our methods aim to reach the boundary of linf ball, so the distortion might be large\" to the second question.\n\nI am Okay with the design of $\\ell_\\infty$ attack. However, if the reduction in query complexity is at a large cost of perturbation power, e.g., measured by $\\ell_2$ norm, then it is better to demonstrate this tradeoff. Furthermore, if the $\\ell_2$ norm is constrained, will the proposed $\\ell_\\infty$ attack outperform the others? This is also not clear to me.\n\nThus, I decide to keep my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575706929073, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Reviewers"], "noninvitees": [], "tcdate": 1570237730524, "tmdate": 1575706929084, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review"}}}, {"id": "SJlZXsL2iH", "original": null, "number": 6, "cdate": 1573837592753, "ddate": null, "tcdate": 1573837592753, "tmdate": 1573837592753, "tddate": null, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment", "content": {"title": "Update of the paper", "comment": "We updated a version of the paper according to the remarks of all the three reviewers."}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygEokBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1910/Authors|ICLR.cc/2020/Conference/Paper1910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149109, "tmdate": 1576860557657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment"}}}, {"id": "BJxAAmW9oH", "original": null, "number": 5, "cdate": 1573684181965, "ddate": null, "tcdate": 1573684181965, "tmdate": 1573684181965, "tddate": null, "forum": "rygEokBKPS", "replyto": "HylN_kKRKr", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment", "content": {"title": "Comment about \"There are No Bit Parts for Sign Bits in Black-Box Attacks\"", "comment": "Compared to SignHunter, for epsilon=0.05, max 10000 queries, and ImageNet Inceptionv3, SignHunter reaches 2% failure rate with average of 578.6 queries, whereas we reach with continuous CMA and tile size of 30, 1.1% failure rate with average of 589 queries. We seem to have slightly better results than they have."}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygEokBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1910/Authors|ICLR.cc/2020/Conference/Paper1910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149109, "tmdate": 1576860557657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment"}}}, {"id": "SkeUlIkviH", "original": null, "number": 2, "cdate": 1573479917848, "ddate": null, "tcdate": 1573479917848, "tmdate": 1573683968315, "tddate": null, "forum": "rygEokBKPS", "replyto": "HylN_kKRKr", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment", "content": {"title": "Answer to reviewer 2", "comment": "We thank reviewer 2 for its comments on the paper.\n\n1. We thank reviewer 2 for pointing out these articles. We decided to compare to what is, up to our knowledge, the state of the art in black-box attacks (at least in published papers in Neurips 2019, 2018, ICML 2019, etc.), which is the Parsimonious attack [Moon et al., 2019]. Bandits is often taken as reference for black-box attacks [Ilyas et al., 2018], so we took it as a reference. We read the papers you provided to us. It comes out that [1] would be a good baseline to compare with too. Note that\u00a0this paper is  also submitted to ICLR (https://openreview.net/forum?id=SygW0TEFwH&noteId=SJx_zBx6tH) and we were not aware of the existence of this paper, so thank you. The results reported in [3] are not competitive to those obtained by Parsimonious attacks. The attack designed in [2] is an L2 one.\u00a0 It requires the training of an autoencoder, which is not fair for comparison with black-box attacks our algorithms belong to.\n\n\n2. The method we propose is for a Linf bounded problem, it is not usual to compare with other distortions. But clearly our methods aim to reach the boundary of linf ball, so the distortion might be large. That's why we also compare to Linf attacks too."}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygEokBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1910/Authors|ICLR.cc/2020/Conference/Paper1910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149109, "tmdate": 1576860557657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment"}}}, {"id": "rJxFN8ywsS", "original": null, "number": 3, "cdate": 1573479985073, "ddate": null, "tcdate": 1573479985073, "tmdate": 1573479985073, "tddate": null, "forum": "rygEokBKPS", "replyto": "SkgdbG96KH", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment", "content": {"title": "Answer to reviewer 1", "comment": "We thank reviewer 1 for its comment\n\nWe applied Adversarial training on CIFAR10 dataset, which is up to our knowledge the most efficient defense method so far.\n\nWe agree that many box-constraint handling methods exist. However here the point is not the handling of such constraints: we have that\u00a0 box constraints and optimal points are close to the frontiers. Based on an extensive set of experiments, using Nevergrad, CMA-ES and (1+1)-ES reveal themselves to be more competitive on this type of problems.\n\nThis type of fomulation is not new. In machine learning, this was used e.g. in Zoph et al for instance. We agree that our paper combines existing approaches, even though, up to our knowledge, these types of evolutionary strategies have never been used so far in this context.\nThere is a typo in Formulation (2), it is : max_{\\tau} L(f(x + \\epsilon tanh(\\tau)), y).\n\n\nAnswers to questions:\n\nP5: How are the original images to be attacked selected for Fig 2?\u00a0\n\nThe images are selected at random in ImageNet dataset\n\nP6:\u00a0 \"we highlight that neural neural networks are not robust to l\u221e tiled random noise. \" Isn't it the contribution of (Ilyas et al., 2018b)?\u00a0\n\nIlyas et al. introduced the tiling trick based on the observation that the gradient does not vary that much for two close points. Here we exhibit that convolutional neural nets are not robust to random tiled noise. This property helps in speeding up a subfamily of evolutionary algorithms based on pure random search in the first steps. This explains the good results obtained by (1+1)-ES and CMA-ES.\u00a0\u00a0\u00a0\n\nP7: What are the number of queries in Figure 3 and Table 1? Are they the number of queries spent until these algorithms found an adversarial example which is categorized to a wrong class for the first time?\n\nIn Figure 3, the number of queries are the number of queries spent until our algorithms find an adversarial example. In Table 3, we reported the mean and the median of these numbers of queries. We make it clearer in the updated version of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygEokBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1910/Authors|ICLR.cc/2020/Conference/Paper1910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149109, "tmdate": 1576860557657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment"}}}, {"id": "Bylcoryvir", "original": null, "number": 1, "cdate": 1573479841667, "ddate": null, "tcdate": 1573479841667, "tmdate": 1573479841667, "tddate": null, "forum": "rygEokBKPS", "replyto": "rkxAlx2iKS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment", "content": {"title": "Answer to reviewer 3", "comment": "We thank reviewer 3 for its comments.\n\nCMA uses a second order approximation of the shape of level sets. This is computationally expensive, but leads to an optimal use of a restricted budget.\nDiagonal CMA is a computationally faster version, thanks to a diagonal covariance (the reader might think of a diagonal Hessian matrix).\nThe (1+1) ES is even simpler; the covariance is proportional to the identity (corresponding to a Hessian matrix with all eigenvalues equal). It is therefore relevant for very low budget as it does not have to learn any matrix; on the other hand it is weaker for greater budget as the sampling does not match the shape of level sets. \n\n- In section 3.2, is the form of the discretized problem a standard way to transform from continuous to discrete one? What is the intuition of using a and b? Have you considered using only one variable to do it?\n\nWe designed this formulation ourselves, but it won\u2019t be surprising that it has already been used elsewhere. Using this formulation the solution to (3) is already in the corners of the Linf ball which is intuitively more likely to fool the network,\u00a0 We tried the two implementations (with one or two variables) and the results are very similar.\n\n- In section 3.3.2 what do you mean by \u201cwith or without softmax, the optimum is at infinity\u201d? I hope the authors could further explain it.\n\nSorry for this unclear statement. The optima of the ball constrained problem (1), would be close to the boundary or on the boundary of the Linf ball.\u00a0 In that case, the optimum of the continuous problem (2) will be at infty or \u201cclose\u201d to it. On the discrete case (3) it is easy to see that the optimum is when a_i or b_i -> infty. We reformulate this sentence accordingly\u00a0in the updated version of the paper.\n\n- In eq (2), do you mean  max_{\\tau} L(f(x + \\epsilon tanh(\\tau)), y) ?\n\nThank you for having spotted this typo. \n\n- In section 3.3.1, the authors said (1+1)-ES and CMA-ES can be seen as an instantiation of NES. Can the authors further elaborate on this?\n\nNES strategies are optimizations strategies based on Natural gradient [Ollivier et al., 2017, Wierstra et al., 2008]. It consists in iteratively updating a search distributions (the distribution of the optima). CMA-ES consists in updated the mean and the covariance of the distributions. (1+1)-ES updates the mean and add constraints on the covariance matrix is isotropic. The underlying optimisation is not the function, but its quantiles regarding the distributions. \n\n[Wierstra et al., 2008] https://arxiv.org/pdf/1106.4487.pdf\n[Ollivier et al., 2017] https://arxiv.org/pdf/1106.3708.pdf \n\n- Can the authors provide algorithm for DiagonalCMA?\n\nThe DiagonalCMA version is when the updates are only on diagonal coefficients of the covariance matrix, hence making a faster computation. We can provide an algorithm; it is a simple modification of CMA discussed in [Ross & Hansen, 2008] https://hal.inria.fr/inria-00270901/document. We will make it clearer in the paper.\n\n- It is better to put the evolution strategy algorithms in the main paper and discuss it. \n\nYes we can do so.\n\n- Can the authors also comment/compare the results with the following relevant paper?\nLi et al. \"NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.\"\nChen et al. \"A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks.\"\n\nFor both papers, the reported results are not competitive w.r.t the parsimonious attack. However, we are eager to include them in our benchmark if the reviewer wants so.\n\n-  In Table 1, why for Parsimonious and Bandit methods, # of tiles parts are missing? I think both of the baselines use tilting trick? And they should also run using the optimal tiling size? The result seems directly copied from the Parsimonious paper? It makes more sense to rerun it in your setting and environment cause the sampled data points may not be the same. Since CMA costs significantly more time, it makes a fair comparison to also report the attack time needed for each method.\n\n\nWe reported the results from the paper parsimonious, but we did not rerun the experiments in our proper setting, because they use the same architecture. However as suggested by the reviewer, we will re-run the experiments in our setting.\u00a0\n\nParsimonious attack divide progressively the image in tiles but do not use a proper tile size. Bandits, itself uses a tile size. In the updated version we make it clearer.\n\nCMA-ES takes indeed quite a lot of time, we will update the paper with the reported runtime - the diagonal one is much faster (but needs more evaluations).\n\n- In Table 3, why did not compare with Bandit and Parsimonious attacks? \n\nIn [Moon et al.,2019], they compare on a different architecture which is WideResNet 32x10, as we did not run the experiments in their setting, it is unclear that the results are the same.But we will give the corresponding results in the updated version of our pape.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygEokBKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1910/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1910/Authors|ICLR.cc/2020/Conference/Paper1910/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149109, "tmdate": 1576860557657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Authors", "ICLR.cc/2020/Conference/Paper1910/Reviewers", "ICLR.cc/2020/Conference/Paper1910/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Comment"}}}, {"id": "SkgdbG96KH", "original": null, "number": 2, "cdate": 1571820031987, "ddate": null, "tcdate": 1571820031987, "tmdate": 1572972407836, "tddate": null, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "invitation": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a black box adversarial attacks to deep neural networks. The proposed approaches consist of tiling technique proposed by Ilyas et al (2018) and derivative free approaches. The proposed approaches have been applied to targeted and untargeted adversarial attacks against modern neural network architectures such as VGG16, ResNet50, and InceptionV3 trained on ImageNet and CIFAR10 datasets. Experimental results show higher attack success rate with a smaller number of queries. \n\nThe experimental results look quite promising, i.e., revealing the vulnerability of the deep neural network against black-box adversarial attacks. A possible weakness in the experimental design is that the authors haven't apply any defense methodology to the classification models to be attacked. Yet the results are promising. \n\nFrom the viewpoint of technical soundness, the approach is a simple combination of the existing approaches. The tiling technique is used in Ilyas et al (2018) combined with a bandit approach. The current paper simply replaces the bandit with evolution strategies. The introduction of the evolution strategies is motivated by their good performance as a zeroth order optimization algorithm. \n\nA small novelty appears in a way to handle a bounded search space. The authors claim that many DFO algorithms are designed for unbounded real search space and need some constraint handling. The authors proposed two ways of transforming the bounded search space to the unbounded real search space. However, there must be existing approaches for this type fo constraint (rectangle constraint) in DFO settings. I can not list such approaches here as there are huge number of papers addressing the constraint of this type. There is not enough discussion in the paper why these two proposed approaches are promising. Formulation (2) makes the problem ill-posed and technically the optimal point may not exist. Formulation (3) with softmax representation makes the optimization problem noisy, hence it may annoy the optimizer. Nonetheless, I believe the combination of these constraint handling technique and evolutionary approaches are not new.\n\nSome minor comments / questions below:\n\nP5: How are the original images to be attacked selected for Fig 2? \n\nP6:  \"we highlight that neural neural networks are not robust to l\u221e tiled random noise. \" Isn't it the contribution of (Ilyas et al., 2018b)? \n\nP7: What are the number of queries in Figure 3 and Table 1? Are they the number of queries spent until these algorithms found an adversarial example which is categorized to a wrong class for the first time?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1910/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laurent.meunier1995@gmail.com", "jamal.atif@dauphine.fr", "oteytaud@fb.com"], "title": "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies", "authors": ["Laurent Meunier", "Jamal Atif", "Olivier Teytaud"], "pdf": "/pdf/2043564055208528c3261e68ec158d7bab5096ef.pdf", "TL;DR": "We propose a new black-box adversarial attack based on tiling and evolution strategies", "abstract": "We introduce a new black-box attack achieving state of the art performances. Our approach is based on a new objective function, borrowing ideas from $\\ell_\\infty$-white box attacks, and particularly designed to fit derivative-free optimization requirements. It only requires to have access to the logits of the classifier without any other information which is a more realistic scenario.  Not only we introduce a new objective function, we extend previous works on black box adversarial attacks to a larger spectrum of evolution strategies and other derivative-free optimization methods. We also highlight a new intriguing property that deep neural networks are not robust to single shot tiled attacks. Our models achieve, with a budget limited to $10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3 classifier  with $630$  queries to the network on average in the untargeted attacks setting, which is an improvement by $90$ queries of  the current state of the art. In the targeted setting, we are able to reach, with a limited budget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries on average, i.e. we need $800$ queries less than the current state of the art.", "keywords": ["adversarial examples", "black-box attacks", "derivative free optimization", "deep learning"], "paperhash": "meunier|yet_another_but_more_efficient_blackbox_adversarial_attack_tiling_and_evolution_strategies", "original_pdf": "/attachment/9dde76df3ae67bb8c4ab0b8d71504103aa39a89a.pdf", "_bibtex": "@misc{\nmeunier2020yet,\ntitle={Yet another but more efficient black-box adversarial attack: tiling and evolution strategies},\nauthor={Laurent Meunier and Jamal Atif and Olivier Teytaud},\nyear={2020},\nurl={https://openreview.net/forum?id=rygEokBKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygEokBKPS", "replyto": "rygEokBKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1910/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575706929073, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1910/Reviewers"], "noninvitees": [], "tcdate": 1570237730524, "tmdate": 1575706929084, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1910/-/Official_Review"}}}], "count": 10}