{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396426233, "tcdate": 1486396426233, "number": 1, "id": "SkzznGU_x", "invitation": "ICLR.cc/2017/conference/-/paper197/acceptance", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.\n \n Pros:\n - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself. \n - From an impact perspective, the reviewers found the approach clear and implementable. \n \n Cons:\n - Novelty criticisms are that the method is a \"compilation\" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are \"highly empirical\" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.\n - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396426832, "id": "ICLR.cc/2017/conference/-/paper197/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396426832}}}, {"tddate": null, "tmdate": 1485399669557, "tcdate": 1485399669557, "number": 7, "id": "HJpdIkwwl", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "S1iDqXoVl", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Followup on the latest draft", "comment": "We would just like to follow up and see if the latest version of our draft addresses your main concern regarding the empirical nature of the objective. In short, we updated the paper to connect our method with language modeling, which gives the method a good foundation. Additionally, we discuss and draw several connections with the papers you listed, giving our paper a theoretical justification.\n\nThe key novelty of our work is we develop pretraining for seq2seq, along with modifications in the network architecture and training algorithm to support it: monolingual language modeling losses and using residual connections to connect the pretrained components. In combination they achieve large improvements over strong baselines on competitive tasks. That said, we do agree that the main selling point of our paper is the empirical evaluation of the method, which we believe will be valuable for subsequent studies on this topic."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "tmdate": 1484847736265, "tcdate": 1482188462553, "number": 1, "id": "r1L2IyIVe", "invitation": "ICLR.cc/2017/conference/-/paper197/official/review", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["ICLR.cc/2017/conference/paper197/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper197/AnonReviewer3"], "content": {"title": "review", "rating": "6: Marginally above acceptance threshold", "review": "Authors propose the use of layer-wise language model-like pretraining for encoder-decoder models. This allows to leverage separate source and target corpora (in unsupervised manner) without necessity of large amounts of parallel training corpora. The idea is in principle fairly simple, and rely on initial optimising both encoder and decoder with LSTM tasked to perform language modelling. \n\nThe ideas are not new, and the paper is more like a successful compilation of several approaches that have been around for some time. The experimental validation, though, offers some interesting insights into importance of initialization, and the effectiveness of different initialisations approaches in enc-dec setting.\n\nThe regulariser you propose to use on page 3, looks like typical multi-task objective function, especially it is used in an alternating manner would be interesting to see whether similar performance might have been obtained starting with this objective, from random initialisation.\n\nYou should probably give credit for encoder-decoder like-RNN models published in 1990s.\n\nMinors:\nPg. 2, Sec 2.1 2nd paragraph: can be different sizes -> can be of different sizes", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483446858693, "id": "ICLR.cc/2017/conference/-/paper197/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper197/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper197/AnonReviewer3", "ICLR.cc/2017/conference/paper197/AnonReviewer1", "ICLR.cc/2017/conference/paper197/AnonReviewer4"], "reply": {"forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483446858693}}}, {"tddate": null, "tmdate": 1483493295437, "tcdate": 1483493295437, "number": 3, "id": "BJDnJCKBe", "invitation": "ICLR.cc/2017/conference/-/paper197/official/comment", "forum": "H1Gq5Q9el", "replyto": "ryGrtTKHg", "signatures": ["ICLR.cc/2017/conference/paper197/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper197/AnonReviewer4"], "content": {"title": "thanks", "comment": "I have changed my score to reflect the response.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690046, "id": "ICLR.cc/2017/conference/-/paper197/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper197/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper197/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690046}}}, {"tddate": null, "tmdate": 1483493146604, "tcdate": 1483446858101, "number": 3, "id": "HyfU5MFSg", "invitation": "ICLR.cc/2017/conference/-/paper197/official/review", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["ICLR.cc/2017/conference/paper197/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper197/AnonReviewer4"], "content": {"title": "good paper with strong experiments", "rating": "7: Good paper, accept", "review": "In this paper, the authors propose to pretrain the encoder/decoder of seq2seq models on a large amount of unlabeled data using a LM objective. They obtain improvements using this technique on machine translation and abstractive summarization.\n\nWhile the effectiveness of pretraining seq2seq models has been known among researchers and explored in a few papers (e.g. Zoph et al. 2016,  Dai and Le 2015), I believe this is the first paper to pretrain using a LM for both the encoder/decoder. The technique is simple, but the gains are large (e.g. +2.7 BLEU on NMT). In addition, the authors perform extensive ablation studies to analyze where the performance is coming from. Hence, I think this paper should be accepted.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483446858693, "id": "ICLR.cc/2017/conference/-/paper197/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper197/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper197/AnonReviewer3", "ICLR.cc/2017/conference/paper197/AnonReviewer1", "ICLR.cc/2017/conference/paper197/AnonReviewer4"], "reply": {"forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483446858693}}}, {"tddate": null, "tmdate": 1483491641593, "tcdate": 1483491641593, "number": 6, "id": "ryGrtTKHg", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "HyfU5MFSg", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Re: Reviewer4's review", "comment": "Thank you for the review. We would like to highlight some important differences between our work and the related works that were mentioned:\n\n* Dai and Le, 2015 focus on text classification, not harder seq2seq tasks.\n* It has been well documented in the machine learning literature that pretraining improves performance on tasks with very little data. Zoph et al. 2016, and a few other seq2seq works, demonstrate similar findings on resource poor language pairs.\n* However, the conventional wisdom is that pretraining is unnecessary, or even harmful, for tasks with medium or large datasets. We believe we are the first to show the contrary by achieving strong results on competitive tasks with medium sized datasets.\n\n\nWe were surprised to see the score of 6 given the positive review, because a paper with an averaged score of 6 may not eventually get accepted (taking into account last year\u2019s threshold and this year\u2019s increased number of submissions)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1482820314533, "tcdate": 1478273674536, "number": 197, "id": "H1Gq5Q9el", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1Gq5Q9el", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "content": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482774931412, "tcdate": 1482774931412, "number": 5, "id": "HysqtRC4g", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "r1L2IyIVe", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Re: Reviewer3's review", "comment": "Thank you for the review.\n\n* Even though our paper does look like a compilation of previous techniques, it is important to see that it really works. After years of research, it is extremely rare to see unsupervised learning help supervised learning, and thus our paper is an important achievement.\n* In recent literature, pretraining has been dismissed as being effective only on small datasets. We show that pretraining seq2seq models gives significant improvements even on challenging medium sized datasets.\n* Furthermore, as noted in the review, we\u2019re one of the only works that try to deeply understand pretraining, which we tackle with our comprehensive ablation study.\n* We in fact have an experiment with a randomly initialized model trained with the monolingual objective in our paper. Pretraining increases performance by 2.0 BLEU points over this baseline.\n* We have added additional references to papers published in the 80s and 90s, and have also corrected the typo in the latest revision of our paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "tmdate": 1482774701455, "tcdate": 1482774701455, "number": 4, "id": "r1rhd0CEl", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "S1iDqXoVl", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Re: Reviewer1's review", "comment": "Thank you for the review. The method of Chen et al., 2016 and our method are indeed similar in spirit, and the differences are in the details:\n\n* They do not apply their method to seq2seq. They experiment on toy datasets as a proof-of-concept. We demonstrate strong results on real world datasets.\n* If applied to seq2seq, we can interpret the first term of their objective as making the output sequence more likely, as scored by a pretrained language model. In our method, the language model is folded into the decoder. We believe that using the pretrained language model for scoring is less efficient than using all the pretrained weights.\n* They have an artificial objective to ensure output sequences depend on input sequences. We rely on labeled examples.\n\nWe believe that the connection between our algorithm and Chen et al\u2019s will strengthen our work and make our algorithm more theoretically justified. \n\nThe method of Dahl et al., 2012 and our method are also similar in that we both use unsupervised pretraining to help supervised learning, but they are different in many other ways:\n* They focus on feedfoward networks with DBNs, which are not a natural model for sequences. We focus on seq2seq learning, using easily trained language models.\n* Pretraining acoustic models is now considered unnecessary, probably because the reconstruction objective used by DBNs and autoencoders is too easy. We show that pretraining by next step prediction improves seq2seq models significantly on realistic and challenging datasets.\n* In addition to the differences in the algorithms, we perform comprehensive ablation studies to verify that pretraining helps generalization and optimization. They make this argument without substantial supporting evidence from their experiments.\n\nWe have rewritten the related works section in the latest revision of our paper to include discussions about these two interesting papers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "tmdate": 1482533475295, "tcdate": 1482533475295, "number": 2, "id": "S1iDqXoVl", "invitation": "ICLR.cc/2017/conference/-/paper197/official/review", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["ICLR.cc/2017/conference/paper197/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper197/AnonReviewer1"], "content": {"title": "the paper addresses a very important issue of exploiting non-parallel training data, but it should add detailed discussions on comparing with two pieces of prior arts detailed in the review below", "rating": "5: Marginally below acceptance threshold", "review": "strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483446858693, "id": "ICLR.cc/2017/conference/-/paper197/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper197/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper197/AnonReviewer3", "ICLR.cc/2017/conference/paper197/AnonReviewer1", "ICLR.cc/2017/conference/paper197/AnonReviewer4"], "reply": {"forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483446858693}}}, {"tddate": null, "tmdate": 1481088231880, "tcdate": 1481088231872, "number": 3, "id": "SJgg6GB7x", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "SJAhIUyXl", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Re: DNN pretraining", "comment": "(Dahl et al. 2012), which is the paper you link, uses layer-wise pretrained DBNs to initialize their DNNs. In contrast, we just pretrain language models on a ton of unlabeled data. It is no longer necessary to train using the layer-wise fashion since we can train large models end-to-end with modern techniques.\n\nHowever, their reasons for pretraining are actually similar to ours, so we can extend their reasoning:\n\n(1) Pretraining is a strong, data-dependent regularizer / prior.\n\nSay our unsupervised task uses the dataset D_U and the supervised task uses the dataset D_S. Then we can use Bayes Rule to compute the conditional probability of parameters (\\theta) after supervised training:\n\nlog p(\\theta | D_S) = log p(D_S | \\theta) + log p(\\theta | D_U) - log p(D_S)\n\nThe likelihood is standard. However, the prior here depends on the unsupervised pretraining, as opposed to something simple like a Gaussian prior (which corresponds to L2 regularization). Since our language models capture so much about language, this is an extremely informative prior, enabling a strong model to be trained.\n\nHowever, it is important that this prior is good. As seen in the ablation study for machine translation, if you pretrain on the parallel corpus (that is, the exact same data used for supervised training), the results are relatively poor compared with pretraining on large monolingual corpora. This is because the parallel corpus prior does not add much information to what is normally learned in supervised training, whereas a large monolingual corpora is required to learn the intricacies of language. This is a good thing -- it means that more unlabeled data gives a better prior, which matches our intuition.\n\nHaving the pretraining prior also makes learning on low-resource tasks much better. Without it, the parameter space is relatively unconstrained, and SGD will find parameters that do not generalize past D_S. Here, the pretraining prior is crucial for learning a good representation.\n \n(2) Pretraining is important for the optimization of networks that are very deep.\n\nFor their specific implementation, modern techniques make pretraining an unnecessary step. However, we still do not know how to effectively train RNNs that can capture very long term dependencies (over hundreds of steps). If initialized randomly, training an LSTM to effectively represent hundreds of inputs is incredibly difficult because the ill conditioned matrices lead to the vanishing gradient problem. Pretraining makes optimization easier from the very start -- the matrices are much better conditioned, which allows gradient to flow back much farther in time.\n\nThis will become an increasingly important going forward as more tasks will require reading long sequences (e.g. character-level NMT is becoming popular, but the input and output sequences are significantly longer than their word-level counterparts).\n\nHere's a great quote from (Dahl et al. 2012) that summarizes what I've said: \"The generative model learned during pre-training helps prevent overfitting, even when using models with very high capacity and can aid in the subsequent optimization of the recognition weights\"."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "tmdate": 1480709814122, "tcdate": 1480709814117, "number": 1, "id": "SJAhIUyXl", "invitation": "ICLR.cc/2017/conference/-/paper197/pre-review/question", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["ICLR.cc/2017/conference/paper197/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper197/AnonReviewer1"], "content": {"title": "Unsupervised Pretraining ", "question": "Can you compare unsupervised Pretraining for Sequence to Sequence proposed in this paper with its counterpart for DNN (Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition, IEEE Transactions on Audio, Speech, and Language Processing, 2012)? what is the theoretical insight you may have out of this comparison?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959411855, "id": "ICLR.cc/2017/conference/-/paper197/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper197/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper197/AnonReviewer1"], "reply": {"forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "writers": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper197/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959411855}}}, {"tddate": null, "tmdate": 1479352811568, "tcdate": 1479352811563, "number": 2, "id": "HkNeGo5Zg", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "Bkh_wAKZg", "signatures": ["~Prajit_Ramachandran1"], "readers": ["everyone"], "writers": ["~Prajit_Ramachandran1"], "content": {"title": "Re: baseline", "comment": "The model without pretraining (but with the LM objective, as in the ablation study) has a detokenized, truecased BLEU of 21.3 for newstest2014 and 24.3 for newstest2015. As expected, these are above the numbers of Jean et al. (2015), with a +1.9 BLEU point improvement for newstest2015. So we get a +2.7 BLEU point improvement on newstest2015 with pretraining. Note that the model without pretraining uses the LM objective, and we expect its performance to drop without it. The next revision of our paper will include these numbers in Table 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}, {"tddate": null, "tmdate": 1479301016077, "tcdate": 1479300980465, "number": 1, "id": "Bkh_wAKZg", "invitation": "ICLR.cc/2017/conference/-/paper197/public/comment", "forum": "H1Gq5Q9el", "replyto": "H1Gq5Q9el", "signatures": ["~Rico_Sennrich1"], "readers": ["everyone"], "writers": ["~Rico_Sennrich1"], "content": {"title": "baseline", "comment": "what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "pdf": "/pdf/05ed2ee9737f5c6eb33aa3bf3d6e1a6d8cca9301.pdf", "TL;DR": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "paperhash": "ramachandran|unsupervised_pretraining_for_sequence_to_sequence_learning", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"], "conflicts": ["google.com", "illinois.edu"], "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287690172, "id": "ICLR.cc/2017/conference/-/paper197/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1Gq5Q9el", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper197/reviewers", "ICLR.cc/2017/conference/paper197/areachairs"], "cdate": 1485287690172}}}], "count": 14}