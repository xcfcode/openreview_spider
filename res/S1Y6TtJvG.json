{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124432972, "tcdate": 1518472641174, "number": 330, "cdate": 1518472641174, "id": "S1Y6TtJvG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1Y6TtJvG", "signatures": ["~Saining_Xie1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Transferring Task Goals via Hierarchical Reinforcement Learning", "abstract": "In this paper we demonstrate how hierarchical reinforcement learning architectures with guiding rewards can be used to separate high-level task understanding from low-level task execution and to exploit this separation of concerns for transferring high-level task goals e.g. across bodies with very different properties and capabilities.", "paperhash": "xie|transferring_task_goals_via_hierarchical_reinforcement_learning", "_bibtex": "@misc{\n  xie2018transferring,\n  title={Transferring Task Goals via Hierarchical Reinforcement Learning},\n  author={Saining Xie and Alexandre Galashov and Siqi Liu and Shaobo Hou and Razvan Pascanu and Nicolas Heess and Yee Whye  Teh},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Y6TtJvG}\n}", "authorids": ["s9xie@eng.ucsd.edu", "agalashov@google.com", "liusiqi@google.com", "shaobohou@google.com", "razp@google.com", "heess@google.com", "ywteh@google.com"], "authors": ["Saining Xie", "Alexandre Galashov", "Siqi Liu", "Shaobo Hou", "Razvan Pascanu", "Nicolas Heess", "Yee Whye  Teh"], "keywords": [], "pdf": "/pdf/d9bf3c10b9eec0055d5e000db84207a219805e01.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582899542, "tcdate": 1520492750812, "number": 1, "cdate": 1520492750812, "id": "H1vCgvCOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer3"], "content": {"title": "of moderate interest", "rating": "5: Marginally below acceptance threshold", "review": "The paper evaluates the benefit of hierarchical reinforcement learning,\nin the context of being able to reuse a high-level controller, while being able\nto introduce and learn new low-level controllers.  This is evaluated on a 4-legged ant\n(and transferred to its damaged variations) and a steered ball (and transfer to the ball with\nreversed steering).  The LL goals (output by the HL controller) consist of a specification of \none of 8 discrete steering directions.\n\nTransfer learning is important and hierarchical structures are a logical (and common) \napproach to apply in support of transfer learning.\nThe technical aspects of the paper, i.e., the experimental details, are well written.\nThe challenge for this paper is with positioning it with respect to the relative \nabundance of related work in this area, and elucidating the differences and benefits \nof the proposed approach. The specification of a \"direction to steer in\" is in many ways\nan obvious choice of subgoal representation.\n\nMany tasks may not have a clean separation between the high-level and low-level goals,\ni.e., it will the case that a representation that allows for transfer will also be one that\nhas to make significant compromises.  Also, they may not always separate nicely into\n\"what\" and \"how\" concerns. It would have been interesting to see how well steering direction\nwould work given agents that have significantly different speed capabilities, i.e., the\ntarget direction may be the right command for a slow agent, but it may be unwieldy for an\nagent which can build significant momentum, namely travel faster, but therefore also turn more\nslowly. \n\n[Andreas et al 2016]  (and other similar references) could be cited in their final\npublication format, i.e., Proc ICML 2017 for this paper.\n\n[Xue Bin Peng 2017a] and [Xue Bin Peng 2017b] are duplicates. \nIs one of these intended to be DeepLoco, also by Xue Bin Peng?\nThat seems relevant for the work in this submission, given \na similar setup, i.e., specified sub-goal structure, and an ability to reuse HL\ncontrollers across multiple LL controllers.\n\nIt would be interesting to comment on the state distributions encountered by both the LL\nand HL controllers. Ideal transfer would occur if they don't influence each other much.\nAnd where it could begin to fall apart is where they are found to be interdependent in a\nstrong way, implying that changing LL would point to a need to retrain HL, and then \nneeding to retrain/fine-tune the LL, and so forth.\n\nIt would be exciting if the HL could learn how to best reward the LL, i.e., to learn the sub-task representation.\n\nSummary:\n+ transfer learning problem\n+ writing\n- articulation of difference/benefit wrt prior art\n- results are specific to \"steering direction\" (SD) subgoals, and relatively \"easy\" tasks, i.e., likely to be benefit from the SD subgoals and SD rewards.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transferring Task Goals via Hierarchical Reinforcement Learning", "abstract": "In this paper we demonstrate how hierarchical reinforcement learning architectures with guiding rewards can be used to separate high-level task understanding from low-level task execution and to exploit this separation of concerns for transferring high-level task goals e.g. across bodies with very different properties and capabilities.", "paperhash": "xie|transferring_task_goals_via_hierarchical_reinforcement_learning", "_bibtex": "@misc{\n  xie2018transferring,\n  title={Transferring Task Goals via Hierarchical Reinforcement Learning},\n  author={Saining Xie and Alexandre Galashov and Siqi Liu and Shaobo Hou and Razvan Pascanu and Nicolas Heess and Yee Whye  Teh},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Y6TtJvG}\n}", "authorids": ["s9xie@eng.ucsd.edu", "agalashov@google.com", "liusiqi@google.com", "shaobohou@google.com", "razp@google.com", "heess@google.com", "ywteh@google.com"], "authors": ["Saining Xie", "Alexandre Galashov", "Siqi Liu", "Shaobo Hou", "Razvan Pascanu", "Nicolas Heess", "Yee Whye  Teh"], "keywords": [], "pdf": "/pdf/d9bf3c10b9eec0055d5e000db84207a219805e01.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582899314, "id": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper330/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer1"], "reply": {"forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582899314}}}, {"ddate": null, "original": null, "tddate": 1520637626170, "tmdate": 1521582770750, "tcdate": 1520637560037, "number": 2, "cdate": 1520637560037, "id": "SJlY8cgKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer2"], "content": {"title": "Simple approach works on toy problem, but general insights questionable", "rating": "5: Marginally below acceptance threshold", "review": "Pros:\n+ Approach is fairly simple to try\n+ Experiments show significant improvement in rewards over baseline\n\nCons:\n+ The high-level controller seems very simple and learning it jointly with RL may not be that interesting\n+ The idea may be a bit too general or vague\n+ Novelty on the low side\n\nThe submission proposes solving a reinforcement learning task consisting of a high-dimensional robot navigation task using a deep policy gradient method that decomposes the system into high-level and low-level controllers.  These controllers are trained simultaneously, but with different reward functions.  The high-level controller produces subgoals for the low-level controller, and an auxiliary reward function rewards the low-level controller for achieving these subgoals.  Experiments also show that the learned high-level controller is robust to changes in the low-level model.\n\nOn the positive side, this is a relatively simple idea that leads to significant improvements in performance in the problem domain, and the goal of transferring the learned high-level controller to different bodies seems to have been achieved.  \n\nOn the other hand, and from a more philosophical perspective, I question how valuable these kinds of results are in practice.  Showing that an RL agent can learn a policy from low-level observations to low-level controls from scratch is certainly interesting in a way, but introducing more structure such as this high-level / low-level divide seems to erode the novelty of that somewhat.  In particular, is it really necessary to learn the high-level controller simultaneously with the low-level controller?  The high-level controller is basically just learning to navigate a 2D maze, and in practice, isn\u2019t it a bit silly to not use A* for this purpose? Granted, there is also the perception component, but as long as we are doing some mostly-decoupled form of learning, why not use some more direct form of navigation-learning, like inverse reinforcement learning with A* for inference?  Yes, there are caveats to this too, but it just seems like the set of situations in which this approach would be interesting is rather narrow.\n\nI think if the application were something more interesting and realistic, then that would be another matter, since there could be significant novelty in the way the high-and-low-level controllers might be engineered.  As it stands, however, it is unclear how much insight this proposal would provide for such a problem.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transferring Task Goals via Hierarchical Reinforcement Learning", "abstract": "In this paper we demonstrate how hierarchical reinforcement learning architectures with guiding rewards can be used to separate high-level task understanding from low-level task execution and to exploit this separation of concerns for transferring high-level task goals e.g. across bodies with very different properties and capabilities.", "paperhash": "xie|transferring_task_goals_via_hierarchical_reinforcement_learning", "_bibtex": "@misc{\n  xie2018transferring,\n  title={Transferring Task Goals via Hierarchical Reinforcement Learning},\n  author={Saining Xie and Alexandre Galashov and Siqi Liu and Shaobo Hou and Razvan Pascanu and Nicolas Heess and Yee Whye  Teh},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Y6TtJvG}\n}", "authorids": ["s9xie@eng.ucsd.edu", "agalashov@google.com", "liusiqi@google.com", "shaobohou@google.com", "razp@google.com", "heess@google.com", "ywteh@google.com"], "authors": ["Saining Xie", "Alexandre Galashov", "Siqi Liu", "Shaobo Hou", "Razvan Pascanu", "Nicolas Heess", "Yee Whye  Teh"], "keywords": [], "pdf": "/pdf/d9bf3c10b9eec0055d5e000db84207a219805e01.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582899314, "id": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper330/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer1"], "reply": {"forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582899314}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582625835, "tcdate": 1520836415299, "number": 3, "cdate": 1520836415299, "id": "S1DHJjXtf", "invitation": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer1"], "content": {"title": "Not particularly novel, but a useful and thorough experimental analysis", "rating": "7: Good paper, accept", "review": "The paper presents a method for transferring skills between different agent platforms (either morphology or actuation capabilities) by training a body-agnostic high-level controller and a goal-agnostic low-level controller.\n\nThe problem tackled is well-motivated and the evaluation is thorough for a workshop track submission. The idea of high and low level controllers has extensively been explored before (as the paper points out) and it is not clear this paper offers a significantly novel contribution to this area of literature. One of the reasons the approach works is because it employs a human-designed subgoal space and a body-invariant action and observation space. The first assumption to me greatly limits the generality of the method. However, the experimental analysis performed could be useful to the community working in the area of hierarchical reinforcement learning and the paper worthy of publication in the workshop track.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transferring Task Goals via Hierarchical Reinforcement Learning", "abstract": "In this paper we demonstrate how hierarchical reinforcement learning architectures with guiding rewards can be used to separate high-level task understanding from low-level task execution and to exploit this separation of concerns for transferring high-level task goals e.g. across bodies with very different properties and capabilities.", "paperhash": "xie|transferring_task_goals_via_hierarchical_reinforcement_learning", "_bibtex": "@misc{\n  xie2018transferring,\n  title={Transferring Task Goals via Hierarchical Reinforcement Learning},\n  author={Saining Xie and Alexandre Galashov and Siqi Liu and Shaobo Hou and Razvan Pascanu and Nicolas Heess and Yee Whye  Teh},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Y6TtJvG}\n}", "authorids": ["s9xie@eng.ucsd.edu", "agalashov@google.com", "liusiqi@google.com", "shaobohou@google.com", "razp@google.com", "heess@google.com", "ywteh@google.com"], "authors": ["Saining Xie", "Alexandre Galashov", "Siqi Liu", "Shaobo Hou", "Razvan Pascanu", "Nicolas Heess", "Yee Whye  Teh"], "keywords": [], "pdf": "/pdf/d9bf3c10b9eec0055d5e000db84207a219805e01.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582899314, "id": "ICLR.cc/2018/Workshop/-/Paper330/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper330/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper330/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper330/AnonReviewer1"], "reply": {"forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper330/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582899314}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573581312, "tcdate": 1521573581312, "number": 163, "cdate": 1521573580977, "id": "HyS0A0AKf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1Y6TtJvG", "replyto": "S1Y6TtJvG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transferring Task Goals via Hierarchical Reinforcement Learning", "abstract": "In this paper we demonstrate how hierarchical reinforcement learning architectures with guiding rewards can be used to separate high-level task understanding from low-level task execution and to exploit this separation of concerns for transferring high-level task goals e.g. across bodies with very different properties and capabilities.", "paperhash": "xie|transferring_task_goals_via_hierarchical_reinforcement_learning", "_bibtex": "@misc{\n  xie2018transferring,\n  title={Transferring Task Goals via Hierarchical Reinforcement Learning},\n  author={Saining Xie and Alexandre Galashov and Siqi Liu and Shaobo Hou and Razvan Pascanu and Nicolas Heess and Yee Whye  Teh},\n  year={2018},\n  url={https://openreview.net/forum?id=S1Y6TtJvG}\n}", "authorids": ["s9xie@eng.ucsd.edu", "agalashov@google.com", "liusiqi@google.com", "shaobohou@google.com", "razp@google.com", "heess@google.com", "ywteh@google.com"], "authors": ["Saining Xie", "Alexandre Galashov", "Siqi Liu", "Shaobo Hou", "Razvan Pascanu", "Nicolas Heess", "Yee Whye  Teh"], "keywords": [], "pdf": "/pdf/d9bf3c10b9eec0055d5e000db84207a219805e01.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}