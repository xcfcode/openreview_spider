{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396497146, "tcdate": 1486396497146, "number": 1, "id": "BkKUnMU_x", "invitation": "ICLR.cc/2017/conference/-/paper310/acceptance", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper is an interesting contribution, primarily in its generalization of Weston's et al's work on semi-supervised embedding method. You have shown convincingly that it can work with multiple architectures, and with various forms of graph. And the PubMed results are good. To improve the paper in the future, I'd recommend 1) relating better to prior work, and 2) extending your exploration of its application to graphs without features."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396497649, "id": "ICLR.cc/2017/conference/-/paper310/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396497649}}}, {"tddate": null, "tmdate": 1485032439443, "tcdate": 1485032439443, "number": 4, "id": "SJkW2SWvg", "invitation": "ICLR.cc/2017/conference/-/paper310/public/comment", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["~Sujith_Ravi1"], "readers": ["everyone"], "writers": ["~Sujith_Ravi1"], "content": {"title": "AC/Reviewer response", "comment": "\nWe thank the reviewers for all their comments. However we would like to respond, make further clarifications and show new results (see comment #3 below) that should address all reviewer concerns.\n\n1. Response to all reviewers\n\nThis work generalizes the Weston et al.\u2019s work on semi-supervised embedding and extends it to new settings (for example, when only graph inputs & no features are available). Unlike the previous works, we show that the graph augmented training method can work with multiple neural network architectures (Feed Forward NNs, CNNs, RNNs) and on multiple prediction tasks & datasets using \"natural\" as well as \"constructed\" graphs. The experiment results clearly show the effectiveness of this method in all these different settings. Besides the methodology, our study also presents an important contribution towards assessing the effectiveness of graph+neural networks as a generic training mechanism for different architectures & problems, which was not well studied in previous papers. We can add more clarifications in our paper to emphasize this point more clearly. \n\nFurthermore, the reviewers\u2019 concerns should also be addressed with details in comment #3 (please see below), where we show new experiments and direct comparison results against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. \n\n2. Regarding reviewer #2\u2019s comment about accuracy comparison against Zhang et al., 2015.\n\nWe would like to make it clear to the reviewers that the numbers reported on this task (Table 3 in our paper) for the \u201csmall CNN\u201d baseline were obtained from their paper (Zhang et al., 2015 refers to this as \u201cSmall ConvNet\u201d which uses 6 layers). They use far deeper and more complex networks (wrt frame size, hidden units) to achieve the reported accuracies on this task.\nIn comparison, NGM trained with a simple 3-layer CNN achieves much better results (86.90 vs 84.35/85.20) and also produces comparable results to their best 9-layer Large ConvNet. This shows that our graph augmented training method not only achieves better results but also helps train more efficient, compressible networks.\n\n3. Response to all reviewers \u2014 regarding comparison against Weston et al., 2012; Yang et al., 2016 [2] and other graph methods. This, along with experiments already detailed in the paper, should address all the concerns raised about how the new method performs in comparison to previous works.\n\nWe performed new experiments and compare our method on a new task with very limited supervision \u2014 the PubMed document classification task [1]. The task is to classify each document into one of 3 classes, with each document being described by a TF-IDF weighted word vector. The graph is available as a citation network: two documents are connected to each other if one cites the other. The graph has 19,717 nodes and 44,338 edges, with each class having 20 seed nodes and 1000 test nodes. In our experiments we exclude the test nodes from the graph entirely, training only on the labeled and unlabeled nodes.\n\nWe train a feed-forward network with two hidden layers with 250 and 100 neurons, using the l2 distance metric on the last hidden layer. The NGM model is trained with alpha_i = 0.2, while the baseline is trained with alpha_i = 0 (i.e., a supervised-only model). We use self-training to train the model, starting with just the 60 seed nodes as training data. The amount of\u00a0 training data is iteratively increased by assigning labels to the immediate neighbors of the labeled nodes, and retraining the model.\n\nWe compare the final NGM model against the baseline, the Planetoid models (Yang et. al, 2016 [2]), semi-supervised embedding (SemiEmb) (Weston et. al, 2012), manifold regression (ManiReg) (Belkin et. al, 2006), transductive SVM (TSVM), label propagation (LP) and other methods. The results show that the NGM model (without any \u201ctuning\u201d) outperforms the baseline, semi-supervised embedding, manifold regularization and Planetoid-G/Planetoid-T, and compares favorably with the other Planetoid system. We believe that with tuning, NGM accuracy can be improved even further.\n\n\nTable: Results for Pubmed document classification\n\nFeat*              0.698\nSemiEmb*\u00a0\u00a0\u00a0   0.711\u2028\nManiReg*\u00a0\u00a0\u00a0\u00a0   0.707\n\u2028Planetoid-I*\u00a0   0.772\nTSVM*            0.622\u2028\nLP*                 0.630\n\u2028GraphEmb*    0.653\u2028\nPlanetoid-G*\u00a0 0.664\u2028\nPlanetoid-T*   0.757\n\n\u2028------------------------------------\u2028\nFeed forward NN (similar to Feat)\u00a0\u00a0 0.709\u2028\nNGM\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 0.759\n\n*numbers reported in Table 3 from Yang et al., 2016 \n\n\n[1]: Sen, Prithviraj, Namata, Galileo, Bilgic, Mustafa, Getoor, Lise, Galligher, Brian, and Eliassi-Rad, Tina. Collective classification in network data. AI magazine, 29(3):93, 2008.\n[2]: Yang, Zhilin, Cohen, William, and Salakhutdinov, Ruslan. Revisiting Semi-Supervised Learning with Graph Embeddings., 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627343, "id": "ICLR.cc/2017/conference/-/paper310/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJg498clg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper310/reviewers", "ICLR.cc/2017/conference/paper310/areachairs"], "cdate": 1485287627343}}}, {"tddate": null, "tmdate": 1482023810816, "tcdate": 1482023810816, "number": 3, "id": "SkitQvmNl", "invitation": "ICLR.cc/2017/conference/-/paper310/official/review", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer2"], "content": {"title": "Very similar to previous work, rebranded.", "rating": "3: Clear rejection", "review": "The authors introduce a semi-supervised method for neural networks, inspired from label propagation.\n\nThe method appears to be exactly the same than the one proposed in (Weston et al, 2008) (the authors cite the 2012 paper). The optimized objective function in eq (4) is exactly the same than eq (9) in (Weston et al, 2008).\n\nAs possible novelty, the authors propose to use the adjacency matrix as input to the neural network, when there are no other features, and show success on the BlogCatalog dataset.\n\nExperiments on text classification use neighbors according to word2vec average embedding to build the adjacency matrix. Top reported accuracies are not convincing compared to (Zhang et al, 2015) reported performance. Last experiment is on semantic intent classification, which a custom dataset; neighbors are also found according to a word2vec metric.\n\nIn summary, the paper propose few applications to the original (Weston et al, 2008) paper. It rebrands the algorithm under a new name, and does not bring any scientific novelty, and the experimental section lacks existing baselines to be convincing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626732, "id": "ICLR.cc/2017/conference/-/paper310/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626732}}}, {"tddate": null, "tmdate": 1481993491548, "tcdate": 1481993491548, "number": 2, "id": "BJofT1mNg", "invitation": "ICLR.cc/2017/conference/-/paper310/official/review", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer3"], "content": {"title": "Very similar to previous work.", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes the Neural Graph Machine that adds in graph regularization on neural network hidden representations to improve network learning and take the graph structure into account.  The proposed model, however, is almost identical to that of Weston et al. 2012.\n\nAs the authors have clarified in the answers to the questions, there are a few new things that previous work did not do:\n\n1. they showed that graph augmented training for a range of different types of networks, including FF, CNN, RNNs etc. and works on a range of problems.\n2. graphs help to train better networks, e.g. 3 layer CNN with graphs does as well as than 9 layer CNNs\n3. graph augmented training works on a variety of different kinds of graphs.\n\nHowever, all these points mentioned above seems to simply be different applications of the graph augmented training idea, and observations made during the applications.  I think it is therefore not proper to call the proposed model a novel model with a new name Neural Graph Machine, but rather making it clear in the paper that this is an empirical study of the model proposed by Weston et al. 2012 to different problems would be more acceptable.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626732, "id": "ICLR.cc/2017/conference/-/paper310/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626732}}}, {"tddate": null, "tmdate": 1481913010191, "tcdate": 1481913010191, "number": 1, "id": "Hyq3zhbVg", "invitation": "ICLR.cc/2017/conference/-/paper310/official/review", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer1"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The paper proposes a model that aims at learning to label nodes of graph in a semi-supervised setting. The idea of the model is based on the use of the graph structure to regularize the representations learned at the node levels. Experimental results are provided on different tasks\n\nThe underlying idea of this paper (graph regularization) has been already explored in different papers \u2013 e.g 'Learning latent representations of nodes for classifying in heterogeneous social networks' [Jacob et al. 2014],   [Weston et al 2012] where a real graph structure is used instead of a built one. The experiments lack of strong comparisons with other graph models (e.g Iterative Classification, 'Learning from labeled and unlabeled data on a directed graph', ...). So the novelty of the paper and the experimental protocol are not strong enough to accpet the paper.\n\nPros:\n* Learning over graph is an important topic\n\nCons:\n* Many existing approaches have already exploited the same types of ideas, resulting in very close models\n* Lack of comparison w.r.t existing models\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512626732, "id": "ICLR.cc/2017/conference/-/paper310/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512626732}}}, {"tddate": null, "tmdate": 1481853818433, "tcdate": 1481853818433, "number": 3, "id": "BJMto6e4g", "invitation": "ICLR.cc/2017/conference/-/paper310/public/comment", "forum": "SJg498clg", "replyto": "B1UOEngEe", "signatures": ["~Sujith_Ravi1"], "readers": ["everyone"], "writers": ["~Sujith_Ravi1"], "content": {"title": "Zhang et al. baseline", "comment": "Re: reporting other baselines by Zhang et al\n\nThe larger network trained by Zhang et al has convolutional layer size=1024 and fully connected layer size=2048, which is enormously large compared to our layer sizes (32 and 256 respectively). Hence we reported the numbers for the smaller network trained by Zhang et al (referred to as \"Small ConvNet\" in their paper) for a more reasonable comparison (their smaller network is still larger than ours but is closer in size). Even so, the model (TinyCNN with NGM) described in our paper trains much faster and still produces results on par with the performance of a significantly larger network by Zhang et al (\"Large ConvNet\"), which has an accuracy of 87.18 without using a thesaurus, and 86.61 with the thesaurus. We can include this comparison with Table 2 results in the paper.\n\nRe: 20 epochs\n\nWe observed that the model converged within 20 epochs (model loss did not change much) and hence used this as a stopping criterion for this task. Experiments also showed that running the network for longer also did not change the qualitative performance.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627343, "id": "ICLR.cc/2017/conference/-/paper310/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJg498clg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper310/reviewers", "ICLR.cc/2017/conference/paper310/areachairs"], "cdate": 1485287627343}}}, {"tddate": null, "tmdate": 1481847917694, "tcdate": 1481847917688, "number": 3, "id": "B1UOEngEe", "invitation": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer2"], "content": {"title": "Comparison with Zhang et al.", "question": "In section 4.2, why is the number of training epochs limited to 20? Why not reporting other Zhang et al results as a baseline?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481847918352, "id": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481847918352}}}, {"tddate": null, "tmdate": 1481358256164, "tcdate": 1481358256159, "number": 2, "id": "H1_2iEY7e", "invitation": "ICLR.cc/2017/conference/-/paper310/public/comment", "forum": "SJg498clg", "replyto": "Sym8wuB7g", "signatures": ["~Thang_D_Bui1"], "readers": ["everyone"], "writers": ["~Thang_D_Bui1"], "content": {"title": "On prior work", "comment": "Thanks for your comment. We are fully aware of the work of Weston et al. (2012) and would like to clarify our contributions: \n1. We show that graph-augmented training works for a variety of neural network models (FF, CNN and LSTM RNN) and yields improvements over strong baselines.\n2. This can be used for model compression, e.g. CNN with 3 layers using graph loss performs as well as CNN with 9 layers.\n3. We show that the objective works with real-world graphs (with or without input features). Weston et al. (2012) constructs the graphs on the fly using nearest neighbours -- this is rather ad-hoc. In fact, we have not been able to reproduce the results obtained by Weston et al. (2012) on the MNIST dataset.\nPlease also see our response to AnonReview1 [Point #1]."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627343, "id": "ICLR.cc/2017/conference/-/paper310/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJg498clg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper310/reviewers", "ICLR.cc/2017/conference/paper310/areachairs"], "cdate": 1485287627343}}}, {"tddate": null, "tmdate": 1481358191880, "tcdate": 1481358191869, "number": 1, "id": "Skudo4YXe", "invitation": "ICLR.cc/2017/conference/-/paper310/public/comment", "forum": "SJg498clg", "replyto": "H1vONKy7l", "signatures": ["~Thang_D_Bui1"], "readers": ["everyone"], "writers": ["~Thang_D_Bui1"], "content": {"title": "On prior work, baselines and hyperparameters ", "comment": "Thanks for your comments.\n\n\n1. Similarity to aforementioned papers: Thanks for the pointer to both papers.\nThe research thread similar to Revisiting Semi-Supervised Learning with Graph Embeddings is orthogonal to our approach. \nThe aforementioned paper in particular attempts to learn an embedding for graph nodes based on neighbourhood information in a similar spirit to word2vec. In contrast, we provide a graph-augmented objective for learning that works with different types of neural networks (FF, CNN, LSTM) and also show improvements over these strong neural network baselines for several different tasks. \n\nThis paper also re-uses a random walk based context sampling introduced in DeepWalk. Recent work from Grover & Leskovec (2016) shows that node2vec produces better embeddings and significant improvements over DeepWalk, LINE, etc. for multi-label classification on graphs. Hence, we compare against this stronger baseline (node2vec) in our experiments. Please see the paragraph at the end of section 1 for further differences to prior work.\n\n2. Stronger baselines\nWe don\u2019t think we fully understand your question. We compared our methods against relevant state-of-the-art models for tasks 4.2 and 4.3, and a baseline which does use the adjacency matrix as inputs but not the edge information in task 4.1 [note that there are *no* input features for this task]. For task 4.1, the node2vec baseline that we compare against is a \u201csupervised approach\u201d (the approach learns node-level features/embeddings using the graph and subsequently trains a supervised classifier for prediction). If this does not answer your question, could you please clarify?\n\n3. Values of hyperparameters\nWe have included some of these in each section in the experiments. We can include a table for clarity in a revision of the paper in a near future."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627343, "id": "ICLR.cc/2017/conference/-/paper310/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJg498clg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper310/reviewers", "ICLR.cc/2017/conference/paper310/areachairs"], "cdate": 1485287627343}}}, {"tddate": null, "tmdate": 1481111392096, "tcdate": 1481111371191, "number": 2, "id": "Sym8wuB7g", "invitation": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer3"], "content": {"title": "This paper is very similar to the work done by Weston et al. (2012)", "question": "From my understanding this work is very similar to the paper Deep Learning via Semi-Supervised Embedding done by Weston et al. (2012).  This paper claims three extensions on top of Weston et al. (2012):\n\n(a) their approach can handle multiple graphs from multiple domains - but it is also trivial for the network in Weston et al. (2012) to handle such cases;\n(b) they provide more experiments on \"properly constructed\" graphs and different kinds of neural networks - this seems to be a valid point but the novelty of this is questionable;\n(c) they propose to use the graphs as inputs when there is no features to be used - the work by Weston et al. (2012) can also use such features to tackle similar problems.\n\nIt looks to me like the authors just took the Weston et al. (2012) model and rebranded it as \"Neural Graph Machines\", and then did some experiments and not even comparing with Weston et al. (2012).  If the authors do think this paper is significantly different from Weston et al. (2012), then I hope they can clarify what are the fundamental differences that Weston et al. (2012) model cannot do but the model in this paper can do."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481847918352, "id": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481847918352}}}, {"tddate": null, "tmdate": 1480721519059, "tcdate": 1480721519055, "number": 1, "id": "H1vONKy7l", "invitation": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "forum": "SJg498clg", "replyto": "SJg498clg", "signatures": ["ICLR.cc/2017/conference/paper310/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper310/AnonReviewer1"], "content": {"title": "pre-review question", "question": "Dear authors,\n\n The proposed model seems very close to existing works like:\nLearning latent representations of nodes for classifying in heterogeneous social networks. \nRevisiting Semi-Supervised Learning with Graph Embeddings\n...\n(these articles also propose relevant references in the same family of approaches).\n\nCould you please explain the differences with these approaches since the differences seem quite small. Moreover, you only compare with an ''unsupervised'' baseline (node2vec). Could you please provide stronger experimental results by comparing with other existing approaches? Could you also comment on the values of the different hyper-parameters (values of the alpha, size of the latent space, etc...) ? \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481847918352, "id": "ICLR.cc/2017/conference/-/paper310/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper310/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper310/AnonReviewer1", "ICLR.cc/2017/conference/paper310/AnonReviewer3", "ICLR.cc/2017/conference/paper310/AnonReviewer2"], "reply": {"forum": "SJg498clg", "replyto": "SJg498clg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper310/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481847918352}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478285864523, "tcdate": 1478285864510, "number": 310, "id": "SJg498clg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJg498clg", "signatures": ["~Thang_D_Bui1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Neural Graph Machines: Learning Neural Networks Using Graphs", "abstract": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multi- label classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "pdf": "/pdf/cfd5fdae121990fc7cb6ef9bc2163711f71c7fec.pdf", "paperhash": "bui|neural_graph_machines_learning_neural_networks_using_graphs", "keywords": ["Semi-Supervised Learning", "Natural language processing", "Applications"], "conflicts": ["google.com", "cam.ac.uk"], "authors": ["Thang D. Bui", "Sujith Ravi", "Vivek Ramavajjala"], "authorids": ["tdb40@cam.ac.uk", "sravi@google.com", "vramavaj@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 12}