{"notes": [{"id": "rkglvsC9Ym", "original": "rJguiAbcYQ", "number": 230, "cdate": 1538087767545, "ddate": null, "tcdate": 1538087767545, "tmdate": 1545355393661, "tddate": null, "forum": "rkglvsC9Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJlMRkclxE", "original": null, "number": 1, "cdate": 1544753097901, "ddate": null, "tcdate": 1544753097901, "tmdate": 1545354517613, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Meta_Review", "content": {"metareview": "The reviewers agree that the paper is well-written, and they all seem to like the general idea. One of the earlier criticisms was that you did not compare against other robust loss functions, but you have partially rectified that by comparing to L1 in the appendix. As per the request of reviewer 2 I would also compare to the Huber loss.\n\nOne remaining concern is the lack of theoretical justification, which could help address the comment of reviewer 3 regarding blurry images from location uncertainty. The other concern is that you should compare your method using FID scores from a standard implementation so that your numbers are comparable to other papers. Some of the reviewers were impressed, but confused by your relatively low scores.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Well-written and promising method, but some remaining issues around theoretical justification and experimental metrics."}, "signatures": ["ICLR.cc/2019/Conference/Paper230/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper230/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353290991, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper230/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper230/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper230/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353290991}}}, {"id": "HyxhGw8rRX", "original": null, "number": 2, "cdate": 1542969108285, "ddate": null, "tcdate": 1542969108285, "tmdate": 1542969108285, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rylfViud3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Official_Comment", "content": {"title": "Authors do not report standard evaluation metrics", "comment": "Based on the latest authors' reply I need to conclude that I am not changing my score.\nInstead of reporting the FID scores following a standard implementation widely accepted these days,\nit turns out the authors were using their own classifier trained on CelebA to embed the pictures. It is\nmy fault that initially I did not notice this fact, even though it is indeed explicitly stated in the paper.Even\nthough the same evaluation process was applied to all the methods (and in this way the authors were\n \"fair\"), the properties of this reported metric are completely unknown and thus the numbers can not\nbe viewed as reliable for any sort of conclusions. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper230/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper230/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621641, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkglvsC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper230/Authors", "ICLR.cc/2019/Conference/Paper230/Reviewers", "ICLR.cc/2019/Conference/Paper230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper230/Authors|ICLR.cc/2019/Conference/Paper230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper230/Reviewers", "ICLR.cc/2019/Conference/Paper230/Authors", "ICLR.cc/2019/Conference/Paper230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621641}}}, {"id": "H1gOXTikCm", "original": null, "number": 1, "cdate": 1542597920414, "ddate": null, "tcdate": 1542597920414, "tmdate": 1542597920414, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Official_Comment", "content": {"title": "Thanks for your comments", "comment": "Thank you very much for all your insightful comments. We have compared the proposed loss with squared L2 (and L1 in the appendix) on different architectures, demonstrating significantly improvement of the performance of VAE. Still, comparisons with other common losses such as Huber loss, SSIM and perceptual loss are missing. We do need stronger theoretical and experimental results to justify the advantages and practical usages of the proposed log-cosh loss. Thank you again and we should polish this work further.\n \nMoreover, I would like to clarify two additional issues argued by reviewer1.\n(1.1) In the implementation, we do use D_MMD(\\int_x q(z|x) p(x) dx || p(z)), which is the same as WAE [1].\n(1.2) For the calculations of FID scores. We trained a new CNN classification model on CelebA, which achieved a test accuracy of 91.5% on average over human face related features, including gender, smile, make up, etc. The FID was then evaluated on a layer of our own CNN model rather than the default Inception network. Thus, the FID sores might seem different to [1]. We think that a model trained specifically for the CelebA dataset actually distills features most related with face than models trained on other datasets, such as ImageNet. The calculations of FID were actually fair since we compared all methods based on the same CNN. As required by the reviewer, we share the code for L2 VAE and L2 WAE (and also the CNN classification model) trained on CelebA via an anonymous google drive link: https://drive.google.com/drive/folders/1SGE4ghCok-MC6kNPUFW1VH1m8QU6UOum?usp=sharing \n\n[1] Wasserstein Autoencoders. Tolstikhin et al., ICLR, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621641, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkglvsC9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper230/Authors", "ICLR.cc/2019/Conference/Paper230/Reviewers", "ICLR.cc/2019/Conference/Paper230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper230/Authors|ICLR.cc/2019/Conference/Paper230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper230/Reviewers", "ICLR.cc/2019/Conference/Paper230/Authors", "ICLR.cc/2019/Conference/Paper230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621641}}}, {"id": "ryl8ijVj2X", "original": null, "number": 3, "cdate": 1541258141532, "ddate": null, "tcdate": 1541258141532, "tmdate": 1541534174576, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "content": {"title": "Missing theoretical analysis of Log-Cosh Loss  ", "review": "This paper proposes to change the L2 norm of loss function of VAE into hyperbolic cosh function. The idea  and presentation are clear and straightforward. However, the used cosh function does not convince me since when t=a, f(t,a) will still be very large! Also, they will grow fast with exp|at|. The authors are encouraged to provide more detailed proofs for the advantages of cosh function.\n\nApart from the cosh loss, the Huber loss is well-known robust loss function used in statistics and many computer vision applications, and it has the similar properties of cosh function. I feel surprised that the authors do not aware this and do not compare it in experiments. \n\nThe introduction is a bit confusing. GAN is an implicit generative model as it does not have any explicit density form, but the likelihood and prior of vanilla VAE are Gaussian. I am not clear what is the motivation to introduce the cosh loss function.\n\nIf the authors aim to improve the generative quality, there are several works, such as using PixelCNN or other advanced likelihoods, improve the VAE. Besides these, recently MAE uses mutual information as the regularization to improve the quality. \n\nOverall, this work does not convey any theoretical analysis and significant results over state-of-the-art.\n\n\n\n \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper230/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "cdate": 1542234509411, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677633, "tmdate": 1552335677633, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklQIfSq27", "original": null, "number": 2, "cdate": 1541194314535, "ddate": null, "tcdate": 1541194314535, "tmdate": 1541534174372, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "content": {"title": "well written, but contribution is unclear and evaluation insufficient", "review": "+ well written and explained\n+ well motivated\n- Unclear if it helps prevent blurry images\n- No comparison to similar loss functions or different tasks\n\nThe paper is well written and very easy to follow. I really liked the introduction as it reads well and clearly motivates the problem. The authors correctly highlight the two major issues in VAE's and propose to solve one of them (the reconstruction loss).\n\nOne of the major issues is that the proposed solution does not solve the problem of blurry images. There are two reasons why a generative model might produce a blurry output with an L2 (or L1) loss:\n 1. The training data is noisy and the best fitting generation will average this noise. This is the issue the authors propose to solve.\n 2. A much larger issue is that the generative model might be uncertain about the spatial location of objects. Here, again a blurry generation is the most optimal output. However unlike (1.) a different loss, like L1 or log-cosh, does not address this issue. The blurriness primarily comes from the element-wise nature of the loss function. Hence simply making the loss robust to outliers (in terms of color values) is not enough.\n\nThe second major issue in the paper is a lack of comparison to other alternative loss functions. As the authors mention in their intro, there has been a host of proposed solutions to the blurry generation: optimizing L1, SSIM, a perceptual loss (e.g. VGG features) and many more. However, the authors do not compare to any of them, and simply setup their main comparison with a squared L2 loss. I would expect the authors to at least compare to other simple loss functions. At a minimum a comparison should contain:\n * L2 (not squared)\n * L1\n * SSIM\n\nIn my view the weaknesses currently outweigh the strength of the submission.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper230/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "cdate": 1542234509411, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677633, "tmdate": 1552335677633, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylfViud3Q", "original": null, "number": 1, "cdate": 1541077802446, "ddate": null, "tcdate": 1541077802446, "tmdate": 1541534174174, "tddate": null, "forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "content": {"title": "Interesting trick which can be used across various auto-encoder models with a rather convincing experimental evidence.", "review": "The paper proposes a new loss function which can be used in the reconstruction term of various auto-encoder architectures. The pixel-wise cost function \\ell(X, X') = f(X - X'; a) is defined for pairs of two input images X and X' and has one positive real-valued hyperparameter a. For small values of t the function f(t; a) behaves like a quadratic function, while for large t it behaves like |t|. As a consequence, it is smooth, everywhere differentiable (like L2) while not penalizing outliers too hard (like L1). The authors present several experiments conducted on MNIST and Celeba datasets, demonstrating that a simple change of a conventional pixel-wise squared L2 distance with the proposed log-cosh cost function improves the FID scores of generated samples as well as the visual quality of reconstructions (including \"the sharpness\"). \n\nI would say this is clearly an empirical study (even though the authors claim they provide \"theoretical justifications\", they are rather hand wavy), which is not a bad thing in this case. The message of the paper is very clear and I think the authors did a good job in selling their point. The main (and, perhaps, the only) contribution is the proposal to use the log-cosh function as the reconstruction cost. And this proposal is well justified by the set of experiments. \n\nHowever, there are several major issues:\n(1.1) The objective functions reported in appendix A.1 corresponding to WAE have in fact nothing to do with WAE. In WAE the regularizer penalizes the divergence between the prior distribution p(a) and *the aggregated posterior* distribution \\int_x q(z|x) p(x) dx. In other words, D_MMD(q(z|x) || p(z)) in Eq. 8 should be replaced with D_MMD(\\int_x q(z|x) p(x) dx || p(z)) in order to result in the WAE model. In summary, if the authors indeed used objectives reported in Eq. 8 of Appendix A, they were actually not using WAE but rather some other sort of regularized auto-encoders, which in a way are quite similar to VAEs. \n(1.2)  I am surprised to see the reported FID scores for the Celeba data set. Having worked with this data set myself in combination with VAEs and WAEs, I am impressed with the extremely low FID scores: 46 for the vanilla L2 VAE and 30 for the L2 WAE. Note that while in the appendix the authors say they follow the architectural choices provided in [1] while performing the \"L2 WAE Celeba\" experiment, the authors arrive at FID=30 compared to FID=55 reported in the \"Wasserstein Autoencoders\" paper. Also, based on my experience, achieving FID=46 on CelebA with a vanilla VAE is very impressive. Note that the authors use 10^4 of samples to evaluate the FID scores, which is exactly the same as in [1]. This size is known to be large enough to reduce the variance of FID, so the difference (55 - 30) can not be explained by the fluctuations of FID. Therefore, I ask the authors to (anonymously) share the code and/or checkpoints of the 2 particular trained models: L2 VAE and L2 WAE trained on Celeba. \n\nOther comments:\n(2.1) Note that the reconstruction cost function in VAE should be normalized for every value of the code Z, as it corresponds to the logarithm of the likelihood (density) function -log p(X|Z). L2 and L1 costs both correspond to the well known likelihood (decoder) models (Gaussian and Laplace). However, it is hard to say what decoder model (what type of conditional distribution p(X|Z) ) would give rise to the proposed log-cosh function. In particular, the normalizing constant is not known and may depend on Z. In other words, by exchanging the L2 cost with the log-cosh loss in the VAE one looses the theoretical guarantees supporting VAE, including the fact that the objective is the lower bound on the marginal log likelihood. While this is not necessarily a problem (unless one uses the value of the objective as the bound on the marginal log likelihood, which is not the case in this paper), I would suggest mentioning it. Notice that, for instance, in WAE this problem does not appear, as the reconstruction term there does not involve any likelihood functions and thus does not need to be normalized.\n(2.2) In Figure 2 I don't see why the authors did not highlight bad samples in the second row corresponding to their proposed method? I see many badly looking images there. Say, (4, 9) in VAE (MLP) and (8, 9) in VAE (Conv) and (6, 1) in WAE (MLP) and (2, 10) in WAE (Conv), where (i, j) means i-th row, j-th column, indexing starting from 1. \n(2.3) How would the Huber loss perform and how does it compare to the proposed loss?\n\n[1] Wasserstein Autoencoders. Tolstikhin et al., ICLR, 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper230/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder", "abstract": "In Variational Auto-Encoder (VAE), the default choice of reconstruction loss function between the decoded sample and the input is the squared $L_2$. We propose to replace it with the log hyperbolic cosine (log-cosh) loss, which behaves as  $L_2$ at small values and as $L_1$ at large values, and differentiable everywhere. Compared with $L_2$, the log-cosh loss improves the reconstruction without damaging the latent space optimization, thus automatically keeping a balance between the reconstruction and the generation. Extensive experiments on MNIST and CelebA datasets show that the log-cosh reconstruction loss significantly improves the performance of VAE and its variants in output quality, measured by sharpness and FID score. In addition, the gradient of the log-cosh is a simple tanh function, which makes the implementation of gradient descent as simple as adding one sentence in coding. ", "keywords": ["Unsupervised Generative Model", "VAE", "log hyperbolic cosine loss"], "authorids": ["chenpf.cuhk@gmail.com", "gycchen@tencent.com", "shengyuzhang@gmail.com"], "authors": ["Pengfei Chen", "Guangyong Chen", "Shengyu Zhang"], "TL;DR": "We propose to train VAE with a new reconstruction loss, the log hyperbolic cosine (log-cosh) loss, which can significantly improve the performance of VAE and its variants in output quality, measured by sharpness and FID score.", "pdf": "/pdf/e4e3fa07a4125a12498c044ef715710044253eeb.pdf", "paperhash": "chen|log_hyperbolic_cosine_loss_improves_variational_autoencoder", "_bibtex": "@misc{\nchen2019log,\ntitle={Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder},\nauthor={Pengfei Chen and Guangyong Chen and Shengyu Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=rkglvsC9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper230/Official_Review", "cdate": 1542234509411, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkglvsC9Ym", "replyto": "rkglvsC9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677633, "tmdate": 1552335677633, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 7}