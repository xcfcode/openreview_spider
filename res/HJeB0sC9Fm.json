{"notes": [{"id": "HJeB0sC9Fm", "original": "ByxkkZ39F7", "number": 893, "cdate": 1538087885300, "ddate": null, "tcdate": 1538087885300, "tmdate": 1545355443833, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1xoDv41gV", "original": null, "number": 1, "cdate": 1544664931167, "ddate": null, "tcdate": 1544664931167, "tmdate": 1545354473608, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Meta_Review", "content": {"metareview": "This paper proposes a new measure to detect memorization based on how well the activations of the network are approximated by a low-rank decomposition. They compare decompositions and find that non-negative matrix factorization provides the best results. They evaluate of several datasets and show that the measure is well correlated with generalization and can be used for early stopping. All reviewers found the work novel, but there were concerns about the usefulness of the method, the experimental setup and the assumptions made. Some of these concerns were addressed by the revisions but concerns about usefulness and insights remained. These issues need to be properly addressed before acceptance.", "confidence": "2: The area chair is not sure", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper893/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353044964, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353044964}}}, {"id": "rJeP-hYZyN", "original": null, "number": 13, "cdate": 1543769086977, "ddate": null, "tcdate": 1543769086977, "tmdate": 1543769086977, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "rJgi5LI6AQ", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "no title", "comment": "I comment using the numbering we have in the thread above:\n\n1) I find the explanation and the additional experiment convincing.\n\n2) I find the new experiment interesting and *partially* supporting the conjecture about the phases.\n\n3) Now I understand and this explains the behavior. However, the text in the method section calls $A$ a \"layer activation matrix\". That is each row represent a sample and the columns the units in that layer. Next, $A$ is what that gets factorized. This does not imply the interpretation in the previous comment about each spatial location becoming a separate row in the $A$ matrix and, thus, is an incorrect representation of the method (for convolutional layers). So, the method section needs to be rewritten to make this clear.\n\n4) This point is important since the usefulness of the proposed early stopping procedure is the main empirical contribution of the submission.\n\nFinal take: I deeply appreciate the authors effort in addressing these issues. Including all these comments and experiments in the main manuscript will make the work more clear and convincing. However, unfortunately, I still think the work's usefulness from both conceptual and empirical aspects is lacking. From the conceptual point of view, it does not put forward a systematic *and* novel perspective, the phase study of layers is novel/interesting but not systematic, the linearity analysis using NMF is systematic in general but does not put forward novel findings. The empirical usefulness of the proposed method is also at question (see point 4 above). Also, the manuscript had many unclear points, many (or all) of which are cleared through the discussion but requires a reorganization for them to be properly integrated in the text and potentially find other unclear parts. \n\nSo, all in all, given my final view above, I would prefer to see the paper get accepted when all these points are properly addressed (which should be possible for the next ML conference) but would not be too disappointed if it gets accepted to this ICLR conference track."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "rJgi5LI6AQ", "original": null, "number": 12, "cdate": 1543493266534, "ddate": null, "tcdate": 1543493266534, "tmdate": 1543493300768, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "ByxMYy1j0X", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Thank you again for the helpful comments.\n\n1) \"1.a) how sensitive is the new training set accuracy to the choice of the batches 1.b) how many batches do you consider per experiment? 1.c) how sensitive is the study to the number of batches?\"\n\nIn our experiments, we used one batch per class, per randomization probability p, and per network instance.\nIn datasets with 10 outputs (Fashion-MNIST, CIFAR10, SVHN, UrbanSounds) we set the number of batches per network to 10, i.e. 10 multi-class batches or 10 single-class batches (1 batch per class). This was to equate the amount of data seen by our method.\nAdditionally, the batches were allowed to vary across the 10 network instances tested and also per label randomization probability p.\nThis means that to evaluate each accuracy tradeoff in out experiments (Figures 2, 3, 4, 7 and 10) we randomly sampled 100 batches.\nWe note that each batch contained 50 samples throughout our experiments.\n\nThis choice had no significant impact on our results because of the very small variance across batches.\nFor instance, we measured the AuC of the curves in Figure 3(a) over 6 different runs, i.e., each time using a different per-class batch. Below we report, per label randomization probability p, the mean and the standard deviation:\n\n          \t mean\t  std\np=0.0\t0.9756\t0.0008\np=0.2\t0.8779\t0.0013\np=0.4\t0.7704\t0.0039\np=0.6\t0.6847\t0.0066\np=0.8\t0.6381\t0.0038\n\nWe understand the importance of clarity in our experiments, and we will discuss the batch sensitivity and accuracy variance in the paper.\n\n2) \u201cThis is not a satisfactory explanation unless additional experiments are provided. Figure 2.c shows a rapid drop from conv_3_1 to conv_3_2 and then a sharp increase from 4_1 to 4_3. It sounds ad-hoc to me to assume the first drop is a sudden turn of spatial class-information to non-spatial (channel-wise) class-information and the increase is due to just being close to the final classification layer. While this conjecture can be true, proper experiments should be conducted to confirm this.\"\n\nAs suggested by the reviewer, to support our hypothesis, we performed an extensive layer-by-layer evaluation on the fully-connected network we have trained on Fashion-MNIST.\n\nExcept for the network architecture and dataset, the rest of the experiment is identical to the experiment in Figure 2. This is to validate the effect of spatial arrangement in the early layers of our convolutional network.\n\nThe results (seen here: https://ibb.co/K6GQbRP ) show that in this case the early layers are less robust to compression, as per the reviewer's initial intuition, which supports our hypothesis as to the phenomenon observed in Figure 2.\n\n\n3) \"what is the difference between a \u201cwhole feature map\u201d and \u201cactivations\u201d? Do you mean that for the convolutional layers the approximation is done separately per channel and thus there are different scaling factors per channel for each sample?\"\n\nWe apologize for the ambiguity in our terminology. By 'activations' we were referring to individual C-dimensional vectors comprising the NxCxHxW activation tensor.\nFor NMF and PCA we consider every C-dimensional vector as a single datapoint (of which there are N*H*W), while for classification the network views every 1xCxHxW block as a single datapoint. So while every C-dimensional vector is pointing in the same direction, there is variability due to scale and spatial arrangement within each 1xCxHxW block.\n\n\n4) \u201cI appreciate the additional experiments provided in Ap. 6.4 However, for this to be true, it is important to show that the hyperparameters used (smoothing factor for the approximated training set accuracy plot, batchsize, k, and maybe others) are independent of the dataset. Otherwise, it would diminish the benefit of not needing a validation set since one needs to find the best values on a heldout set.\"\n\nWe agree with the reviewer that more investigation and experiments are required for the practical implementation of the early stopping application of our approach. We will clarify this in our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "Hkxm8rUa0Q", "original": null, "number": 11, "cdate": 1543492939442, "ddate": null, "tcdate": 1543492939442, "tmdate": 1543492939442, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "ByxVZCnKRX", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Thank you again for the helpful comments.\n\nWe agree that the task of detecting memorization in general is not conclusively resolved by our paper.\nThe measurements based on parameter-norms, mentioned in the paper, have very limited usefulness in practical applications. We also note that the PCA method was proposed in our work. This was a means to provide a baseline for comparison. However, we can change the title to \"On detecting...\" if there is consensus among the reviewers."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "ByxMYy1j0X", "original": null, "number": 10, "cdate": 1543331705770, "ddate": null, "tcdate": 1543331705770, "tmdate": 1543331705770, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "Byg4UbSXRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Thanks for the additional experiments and comments", "comment": "I appreciate the authors effort in addressing the raised issues by revising the paper, doing additional experiments and replying to comments. I believe the paper has improved, however I see the following main issues are still outstanding:\n\n1)  \u201cBatches are not exhaustive over the dataset.\u201d: 1.a) how sensitive is the new training set accuracy to the choice of the batches 1.b) how many batches do you consider per experiment? 1.c) how sensitive is the study to the number of batches?\n\n2) \u201cThis is because we perform factorization across the channel dimension of the CNN. In early layers most of the information is spread across the spatial dimensions, but as the effective receptive field grows and the spatial resolution of the feature maps decreases with depth, the channel dimension holds more and more information.\u201d This is not a satisfactory explanation unless additional experiments are provided. Figure 2.c shows a rapid drop from conv_3_1 to conv_3_2 and then a sharp increase from 4_1 to 4_3. It sounds ad-hoc to me to assume the first drop is a sudden turn of spatial class-information to non-spatial (channel-wise) class-information and the increase is due to just being close to the final classification layer. While this conjecture can be true, proper experiments should be conducted to confirm this.\n\n3) \u201cWhile with k=1 all activations do point in the same direction in feature space, classification is ultimately performed over a whole feature map, where the spatial arrangement and different scales of activations leads to different predictions.\u201d what is the difference between a \u201cwhole feature map\u201d and \u201cactivations\u201d? Do you mean that for the convolutional layers the approximation is done separately per channel and thus there are different scaling factors per channel for each sample?\n\n4) \u201cour method is useful where a validation set is not available. [...] few-shot learning on MNIST with only 20 images \u201d I appreciate the additional experiments provided in Ap. 6.4 However, for this to be true, it is important to show that the hyperparameters used (smoothing factor for the approximated training set accuracy plot, batchsize, k, and maybe others) are independent of the dataset. Otherwise, it would diminish the benefit of not needing a validation set since one needs to find the best values on a heldout set.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "ByxVZCnKRX", "original": null, "number": 9, "cdate": 1543257596510, "ddate": null, "tcdate": 1543257596510, "tmdate": 1543257596510, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "rkxAEfHXAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Thanks for the rebuttal", "comment": "Thanks the author for the rebuttal. It clarifies most of my concerns. I have updated my score. Though I still think this paper is around borderline as the method can only be used to comparatively study the memorization effects on the same dataset and for the same network architecture, which could already be achieved via simpler previous methods like PCA, ablation studies or even measuring the norms of the layer weights. \n\nIf the paper is accepted, I would like to request the author to modify the paper title to be more specific. The current title sounds like the grand challenge of detecting memorization has been solved in this paper, which is a bit misleading as the general case is still quite open. Maybe the title could be made more specific by mentioning this is an approach based on NMF, or maybe indicating that it only works as a relative comparison on identical tasks and network architectures. "}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "SygfhEWq3m", "original": null, "number": 2, "cdate": 1541178538073, "ddate": null, "tcdate": 1541178538073, "tmdate": 1543257119626, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "content": {"title": "bad clustering == memorization?", "review": "This paper propose a new way of analyzing the robustness of neural network layers by measuring the level of \"non-linearity\" in the activation patterns on samples belonging to the same class, and correlate that to the level of \"memorization\" and generalization.\n\nMore specifically, the paper argues that a good representation cluster all the samples in a class together, therefore, in higher layers, the activation pattern of samples from the same class will be almost identical. In this case, the activation matrix will have a small non-negative rank. An approximation algorithm (via non-negative matrix factorization) is then used to compute the robustness and evaluate the robustness (by replacing the activation matrix with its low rank non-negative activation) is measured in a number of experiments with different amount of random label corruptions. The experiments show that networks trained on random labels are less robust than networks trained on true labels.\n\nWhile the concept is interesting, I find the arguments in the paper a bit vague, and the usefulness of the algorithm might be hampered by its computation complexity, which is not discussed in the paper.\n\nFirst of all, the paper lacks a clear notion of \"memorization\". While it is generally accepted that learns on random labels can be called \"memorization\", the paper seem to be defining it as how well is the network clustering points from the same class. Several questions need to be addressed in order for this notion to be justified:\n\n1. Are (well generalized) networks really clustering samples of the same class to a centroid? It would be great if some empirical verifications are shown. Because the networks are using linear classifier in the last layer to classify the samples, it seems only linearly separability would be suffice for the work, which does not necessarily imply clustering.\n\n2. Given two networks (of the same architecture), assume somehow network-1 decides to use the first 9 layers to compute a well clustered representation, while network-2 decides to use the first 5 layers to do the same thing. Do we say network-1 is (more) memorizing in this case?\n\n3. The notion seems to be more about the underlying task than about the networks. Given the measurement, if a task is more complicated, meaning the input samples in the class have higher variance and requiring more efforts to cluster, then it seems the network will be doing more memorization. In other words, while networks will be doing more memorization when comparing a random label task to a true label task, it might also be \"doing more memorization\" when comparing learning on imagenet to learning on MNIST / CIFAR. One the one hand, this does not seem to fit our \"intuition\" about memorization; on the other hand, the heavy dependency on the underlying data distribution makes it difficult to compare results learned on different data -- especially since the measurements are based on per-class samples, \"random labels\" and \"true labels\" have very different class-conditional distributions.\n\nI also have some questions about Figure 2(c). I will continue numbering the question for easier discussion.\n\n4. Why for all cases, the lower layers all have higher AUC than the higher layers (except the last one)? The argument given in the paper is that the lower layers are the feature extraction phase while the upper layers are memorization phase. I think if clearly verified, this is a very interesting observation. But the paper currently do not have experiments to verify the hypothesis. Also more studies on this with different networks would be good. For example, with deeper networks, does the feature extraction phase include more layers?\n\n5. The p=1 and p<1 curves seem to be very different. If one is to sample more densely between p=0.8 and p=1, would there still be a clear phase transition?\n\nSome other questions:\n\n6. Can you add discussions to the computation requirements for the proposed analysis? This is especially important for the cases where the analysis is used during training as tools to help deciding early stopping.\n\n7. For the early stopping experiment, the main text says \"These include the test error (in blue)\" while in the figure the label axis is \"Test loss\". I'm assuming it is the cross entropy loss given the values are greater than 1. In this case, can you show in parallel the same plots in error rate, as the test error is more important than the test loss and the test loss could sometimes be artificially huge due to high confident mistakes on ambiguous test examples.\n\nSome minor issues:\n\n* Please proof read the paper for typos. E.g. on the 3rd paragraph of the 1st page: \"that networks that networks that\".\n\n* The convention with subplots seem to be putting sub-captions under the figures, not above.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "cdate": 1542234352866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827115, "tmdate": 1552335827115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eP3MSQR7", "original": null, "number": 7, "cdate": 1542832815349, "ddate": null, "tcdate": 1542832815349, "tmdate": 1542832815349, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "SkgNroyY2m", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to reviewer 1", "comment": "We thank the reviewer for the helpful review!\n\n\"The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.\"\nAs suggested, we have performed more experiments for early stopping, and summarize the results in Figure 6c. We have also added early stopping experiments in the setting of few-shot learning in section 6.4 of the appendix (specifically Figure 9 and Table 2).\n\n\"it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA.\"\nIndeed, we added this comment to the text, as well as a note on computation time.\n\n\"The error bars on the plots are often hard to see\"\nThank you for pointing this out. We have improved the visibility of error bars in all figures."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "SkgQvGrQA7", "original": null, "number": 6, "cdate": 1542832730631, "ddate": null, "tcdate": 1542832730631, "tmdate": 1542832730631, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "SygfhEWq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to reviewer 2 (2/2)", "comment": "\"The p=1 and p<1 curves seem to be very different. If one is to sample more densely between p=0.8 and p=1, would there still be a clear phase transition?\"\nThis is indeed interesting. We have added plots regarding the phase shift between the cases p=0.8 and p=1 in section 6.5 of the appendix. In the case of CIFAR-10 and our network architecture, this change happens around p=0.9.\n\n\"Can you add discussions to the computation requirements for the proposed analysis?\"\nAs suggested, we have added a section to the appendix (6.6) discussing the computational complexity of our algorithm. While NMF naturally incurs certain overhead, our implementation runs in reasonable time thanks to GPU acceleration. \n\n\"For the early stopping experiment, the main text says \"These include the test error (in blue)\" while in the figure the label axis is \"Test loss\"\"\nGood catch, we fixed the text.\n\n\"...can you show in parallel the same plots in error rate, as the test error is more important than the test loss\"\nAs suggested, we show the usefulness of our method to early stopping by comparing accuracy curves in newly added section 6.4 of the appendix.\n\n\"The convention with subplots seem to be putting sub-captions under the figures, not above\"\nIt is now fixed.\n\n[1] Bengio et al. \"Representation learning: A review and new perspectives.\" 2013\n[2] Tishby et al. \"Deep learning and the information bottleneck principle.\" 2015\n[3] Amjad, et al. \"How (Not) To Train Your Neural Network Using the Information Bottleneck Principle.\" 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "rkxAEfHXAQ", "original": null, "number": 5, "cdate": 1542832694495, "ddate": null, "tcdate": 1542832694495, "tmdate": 1542832694495, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "SygfhEWq3m", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to reviewer 2 (1/2)", "comment": "We thank the reviewer for the insightful comments!\n\n\"the paper lacks a clear notion of \"memorization\"... The paper seem to be defining it as how well is the network clustering points from the same class\"\nWe have clarified in the text what we (Introduction, second-to-last paragraph). In particular, our definition of memorization is the network implicitly learning a specific mapping from the sample with index i to the class with index j - a \"rule\" which does not benefit the network in terms of improving its generalization.\nWe then suggest that good clustering within approximately linear regions of deep feature space correlates with absence of memorization.\n\n\"...the paper argues that a good representation cluster all the samples in a class together...\"\n\"Are (well generalized) networks really clustering samples of the same class to a centroid?\"\nNMF basis vectors are not centroids in the k-means sense. In particular, a datapoint can be associated with multiple NMF basis vectors and at various scales. Furthermore, we do not claim the existence of a single class-specific cluster, but rather a distribution into a small number of clusters (approximated by k).\n\n\"Because the networks are using linear classifier in the last layer to classify the samples, it seems only linearly separability would be suffice for the work, which does not necessarily imply clustering.\"\nLinear separability of last-layer activations is indeed all that is required for correct classification. While all the networks we studied achieve perfect linear separability on their *training* data, this is clearly not the case for their validation and test data. Training set linear-separability is therefore not a sufficient condition for test set linear-separability. Our aim is to find additional properties of neural network feature space that are indicative of last-layer linear-separability of test data.\nThere are many proposals for what makes a \"good\" feature space in that regard [1,2,3]. In this paper we propose, and give supporting empirical evidence, that feature spaces characterized by a low non-negative rank of single-class activation matrices memorize less and generalize more.\n\n\"Given two networks (of the same architecture), assume somehow network-1 decides to use the first 9 layers to compute a well clustered representation, while network-2 decides to use the first 5 layers to do the same thing. Do we say network-1 is (more) memorizing in this case?\"\nIn such a situation we would prefer the network that more quickly reduces the non-negative rank, because it is a simpler model of the data. This view is based on the general principle of Occam's razor, and is made concrete with our method.\n\n\"the notion seems to be more about the underlying task... if a task is more complicated, meaning the input samples in the class have higher variance and requiring more efforts to cluster, then it seems the network will be doing more memorization... when comparing learning on imagenet to learning on MNIST / CIFAR\"\nIt is absolutely true that some datasets are more complicated than others. We therefore do not propose a global measure for memorization, but rather a comparative measure to evaluate competing networks of the same architecture on the same dataset.\n\n\"Why for all cases, the lower layers all have higher AUC than the higher layers (except the last one)?\"\nIn Figure 2.c, note that for the case p=0, most lower layers actually have lower AuC than higher layers. We interpret this as meaning that without artificially inducing memorization, the network better clusters activations from layer to layer.\n\n\"The argument given in the paper is that the lower layers are the feature extraction phase while the upper layers are memorization phase.\"\nOur hypothesis is based on the statistical similarity of early layers in our compression studies. We altered the text to emphasize it is indeed currently a hypothesis.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "HJlzuZH7C7", "original": null, "number": 4, "cdate": 1542832490056, "ddate": null, "tcdate": 1542832490056, "tmdate": 1542832490056, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "ByePaSxs2X", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to Reviewer 3 (2/2)", "comment": "\"Apart from the last layer, this form of simplicity of the support for the intermediate layers of a good classifier does not seem to be *necessarily*.  ...as long as the activation matrix of each class is linearly separable ... there is no need for it to become simpler\"\nLinear separability of last-layer activations is indeed all that is required for correct classification. While all the networks we studied achieve perfect linear separability on their *training* data, this is clearly not the case for their validation and test data. Training set linear-separability is therefore not a sufficient condition for test set linear-separability. Our aim is to find additional properties of neural network feature space that are indicative of last-layer linear-separability of test data.\nThere are many proposals for what makes a \"good\" feature space in that regard [1,2,3]. In this paper we propose, and give supporting empirical evidence, that feature spaces characterized by a low non-negative rank of single-class activation matrices memorize less and generalize more.\n\n\"different linear regions (polytopes in the input space) should be considered for the linearization of the activation matrix... how can this empirical measurement be translated into a more formal linearity of the global function?\"\nUnder this geometric lens, our observation is that for a particular point cloud, i.e., one associated with a single-class, there is a small number of deep local polytopes where most of the data \"fits\" without being non-linearly projected into the polytope by ReLU. However, associating these deep polytopes with polytopes in input space is a non-trivial problem which is concerned with \"network interpretability\", an active area of research.\n\n\"How can one obtain a measure that is independent of the number of samples in the batch?\"\nThe approach presented in the paper is based on the properties of activation matrices, which inherently depends on a batch and therefore its size. However as mentioned, we have found our measurements to be robust across wide range of batch sizes . \n\n\"there are questions about ... the usefulness of the observation for training better models and/or giving additional insights to what we know\"\nWe show that memorization and generalization are correlated with non-negative rank of activation matrices. A next step would be to regularize this or a dependent quantity. While, the non-negative rank and rectangle cover number are NP-hard to compute, we believe practical regularizers could be derived from this connection. This is a direction of future work that we intend to pursue, and we see value in sharing these results with the wider community to spark further interest in this direction.\n\n[1] Bengio et al. \"Representation learning: A review and new perspectives.\"  2013\n[2] Tishby et al. \"Deep learning and the information bottleneck principle.\"  2015\n[3] Amjad, et al. \"How (Not) To Train Your Neural Network Using the Information Bottleneck Principle.\" 2018\n[4] Zhang et al. \"Understanding deep learning requires rethinking generalization.\" 2016\n[5] Collins et al. \"Deep feature factorization for concept discovery.\" 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "Byg4UbSXRQ", "original": null, "number": 3, "cdate": 1542832460481, "ddate": null, "tcdate": 1542832460481, "tmdate": 1542832460481, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "ByePaSxs2X", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "Response to Reviewer 3 (1/2)", "comment": "We thank the reviewer for the constructive feedback!\n\n\"The experimental setup is unclear\"\nWe have cleared up the ambiguities regarding our experimental setup (section 4.1, paragraph 3), noting that:\n- We use training-set batches\n- Single-class batches are sampled w.r.t. to the training label, i.e., the random/noisy labels for p>0\n- For every value of p we produced a fixed set of random labels.\n- We do not use a fixed batch. We randomly sample batches (up to the class label) for each net.\n- Batches are not exhaustive over the dataset.\n- In our experiments with various batch sizes (20-100) we did not notice significant difference. We used a batch size of 50 through out the paper.\n\n\"How come all networks with different label noise levels end up with the same (100%?) accuracy?\"\nThe y-axis in Figure 2 is *training* set accuracy. All network manage to (over)fit their training data regardless of the level of label randomization [4], and therefore show perfect accuracy under weak/no compression.\n\n\"Why should the performance drop more when linearizing the middle layers (3_2:4_2) than the earlier layers.\"\nThis is because we perform factorization across the channel dimension of the CNN. In early layers most of the information is spread across the spatial dimensions, but as the effective receptive field grows and the spatial resolution of the feature maps decreases with depth, the channel dimension holds more and more information.\n\n\"When k=1 for NMF and PCA, the difference of the activations for different samples becomes a matter of scale. ... shouldn\u2019t all classifications become the same for all samples?\"\nWhile with k=1 all activations do point in the same direction in feature space, classification is ultimately performed over a whole feature map, where the spatial arrangement and different scales of activations leads to different predictions.\n\n\"It would be interesting to study the property of the basis obtained in this border case. The same questions can be studied as one gradually increases k.\"\nQualitative examination of the NMF basis with small values of k is has been undertaken in [5] where in deep layers basis directions are shown to correspond with semantic concepts.\n\n\"it does not seem to provide a better criterion for early stopping\"\nValidation error is the gold standard for stopping criteria, and so when a validation set is available, we do not propose a better alternative. By showing good correlation with validation error, our method is useful where a validation set is not available. To demonstrate this point we added section 6.4 to the appendix, where we perform few-shot learning on MNIST with only 20 images for training. Early stopping with NMF consistently improves the final accuracy of the network.\n\n\"An experiment describing how well are the approximations and how that correlate with memorization is missing\"\nWe show and discuss NMF reconstruction error plots in the appendix (section 6.5). The main difficulty in interpreting the NMF error is scale. The error depends on the magnitude of the activations, which varies across networks, layers and even channels. Network accuracy, on the other hand, is more interpretable and is ultimately the quantity of interest w.r.t. the effect on the neural network."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "ByxNZRV7AQ", "original": null, "number": 2, "cdate": 1542831611803, "ddate": null, "tcdate": 1542831611803, "tmdate": 1542831611803, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "content": {"title": "General comment", "comment": "We would like to thank all reviewers for their thorough and insightful feedback. We are glad that the reviewers found our approach \"novel\" and \"very interesting\", and the paper  \"clear and focused\". \n\nWe have made revisions and additions to the paper guided by the reviews. In particular, we have added more experiments for early stopping (also in the few-shot learning setting), added plots for the NMF approximation error w.r.t. memorization, and discuss the computational cost involved with our method.\n\nBelow we discuss each of the reviewers' comments in detail."}, "signatures": ["ICLR.cc/2019/Conference/Paper893/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616352, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJeB0sC9Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper893/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper893/Authors|ICLR.cc/2019/Conference/Paper893/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers", "ICLR.cc/2019/Conference/Paper893/Authors", "ICLR.cc/2019/Conference/Paper893/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616352}}}, {"id": "ByePaSxs2X", "original": null, "number": 3, "cdate": 1541240255297, "ddate": null, "tcdate": 1541240255297, "tmdate": 1541533602177, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "content": {"title": "Very interesting but not yet a complete work", "review": "The contribution of the paper is in proposing a quantitative measure of memorization based on the assumption that the activations at the deeper layers of a *generalizing* deep network should be invariant to intra-class variations. The measure corresponds to how well can the activation matrix of a batch be approximated by a low-rank decomposition. The paper proposes to use approximate non-negative matrix factorization and compares it to PCA. As for \u201cwellness\u201d it uses the final accuracy of the network after the activation is approximated in some layer(s).\n\nThe composition of the paper and its writing makes it an easy read. The work is novel in the way it proposes to measure memorization to the best of the reviewer\u2019s knowledge. However, the novel insights and/or the practical usefulness of the proposed method seem very limited. Also, there are many questions that comes to my mind that I would appreciate the authors to address:\n\nSpecific questions:\nThe experimental setup is unclear:\nIs the linearization-batch taken from the training set or the test set?\nIf it is taken from the training set, for the case that p>0 (noisy labels), is the batch of a single class obtained from noisy labels or non-noisy labels? \nFor the experiments, is there only one fixed batch used? How is this batch selected? How sensitive the evaluation is to the selection of the batch members and its size?\nDo the batches cover the whole set?\n\n- Figure 2.a and 2.b: How come all networks with different label noise levels end up with the same (100%?) accuracy? Are the fixed samples different for each p? (class labels change for each p).\n\n- Figure 2.c: Why should the performance drop more when linearizing the middle layers (3_2:4_2) than the earlier layers. This seems to be in violation of the assumption about class invariance in deeper layers.\n\n- When k=1 for NMF and PCA, the difference of the activations for different samples becomes a matter of scale. In this case, shouldn\u2019t all classifications become the same for all samples? How does this affect the accuracy? Does it make the evaluation very sensitive to the sampling of the batch? It would be interesting to study the property of the basis obtained in this border case. The same questions can be studied as one gradually increases k.\nSection 4.2: It starts with the sentence \u201cIn this section we show our technique is useful for predicting good generalization in a more realistic setting\u201c. Indeed, the high correlation of the test performance and the proposed memorization measure in this section is very interesting. However, as for usefulness, it does not seem to provide a better criterion for early stopping or other practicalities of ReLU networks. \n\n- An experiment describing how well are the approximations (i.e. activation matrix reconstruction error) and how that correlate with memorization is missing.\n\nSome general questions that come to my mind:\n\n- the paper assumes (e.g. in page 4) that \u201cWhen single-class batches are not approximately linear, even in deep layers, we take this as evidence of memorization\u201d. I have a concern here. Apart from the last layer, this form of simplicity of the support for the intermediate layers of a good classifier does not seem to be *necessarily*. That is, it seems to me that as long as the activation matrix of each class is linearly separable from the activation matrices of the others, there is no need for it to become simpler (by reducing the intra-class variations at the deep layers) for the classification loss to be minimized. Does this mean the paper\u2019s assumption for memorization is not necessarily valid?\n\n- The paper relates the memorization to the extent of local linearity of a deep ReLU network. ReLU networks represent piece-wise linear functions. Thus, in order for this relation to be drawn, probably different linear regions (polytopes in the input space) should be considered for the linearization of the activation matrix. In that regard, how can this empirical measurement be translated into a more formal linearity of the global function?\n\n- The rc number as well as the rank k of a good approximation directly depend on the number of samples in the batch. How can one obtain a measure that is independent of the number of samples in the batch?\n\n\nSummary judgment:\n\nThe paper puts forward an interesting observation using a novel approach. However there are questions about the experiments, discussions around the experiments and the usefulness of the observation for training better models and/or giving additional insights to what we know. Considering that, I think the paper would make a very good workshop paper but needs more work to address the bar of an ICLR conference paper. But I am open to discussion with the authors and other reviewers.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "cdate": 1542234352866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827115, "tmdate": 1552335827115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgNroyY2m", "original": null, "number": 1, "cdate": 1541106491831, "ddate": null, "tcdate": 1541106491831, "tmdate": 1541533601673, "tddate": null, "forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "content": {"title": "Review for \"Detecting Memorization in ReLU Networks\"", "review": "This paper aims to distinguish between networks which memorize and those with generalize by introducing a new detection method based on NMF. They evaluate this method across a number of datasets and provide comparisons to both PCA and random ablations (as in Morcos et al., 2018), finding that NMF outperforms both. Finally, they show that NMF is well-correlated with generalization error and can be used for early stopping. \n\nThis is an overall excellent paper. The writing is clear and and focused, and the experiments are careful and rigorous. The discussion of prior work is thorough. The question of how to detect memorization in DNNs is one of great interest, and this makes nice steps towards this goal. As such, it will likely have significant impact.  \n\nMajor comments:\n\n1) The early stopping section could benefit from more experiments. In particular, it would be helpful to see a scatter plot of the time of peak test loss as a function of NMF/Ablation AuC local maxima and to measure the correlation between these rather than simply showing 3 examples.\n\nMinor comments: \n\n1) While the comparisons to random ablations are mostly fair, it is worth noting that the variance on random ablations appears to be lower than that of NMF and PCA. \n\n2) The error bars on the plots are often hard to see. Increasing the transparency somewhat would be helpful.\n\nTypos: \n\n1) Section 1, third paragraph: \u201cWe show that networks that networks that generalize\u2026\u201d should be \u201cWe show that networks that generalize...\u201d\n\n2) Section 3.1, third paragraph: \u201cBecause threshold is the\u2026\u201d should be \u201cBecause thresholding is the\u2026\u201d\n\n3) Section 3.2, third paragraph: \u201cIn the most non-linear case we would\u2026\u201d should be \u201cIn the most non-linear case, we would\u2026\u201d\n\n4) Figure 2 caption: \u201c...with increasing level of\u2026\u201d should be \u201c...with increasing levels of\u2026\u201d\n\n5) Section 4.1.1, second to last line of last paragraph: missing space before final sentence\n\n6) Figure 4a label: \u201cFahsion-MNIST\u201d should be \u201cFashion-MNIST\u201d\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper893/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Memorization in ReLU Networks", "abstract": "We propose a new notion of 'non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix.\nWe measure this non-linearity by applying non-negative factorization to the activation matrix.\nConsidering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.", "keywords": ["Memorization", "Generalization", "ReLU", "Non-negative matrix factorization"], "authorids": ["edo.collins@epfl.ch", "siavash.bigdeli@epfl.ch", "sabine.susstrunk@epfl.ch"], "authors": ["Edo Collins", "Siavash Arjomand Bigdeli", "Sabine S\u00fcsstrunk"], "TL;DR": "We use the non-negative rank of ReLU activation matrices as a complexity measure and show it (negatively) correlates with good generalization.", "pdf": "/pdf/2dfd3515e65a0499afdf08c19b6d53503a99464f.pdf", "paperhash": "collins|detecting_memorization_in_relu_networks", "_bibtex": "@misc{\ncollins2019detecting,\ntitle={Detecting Memorization in Re{LU} Networks},\nauthor={Edo Collins and Siavash Arjomand Bigdeli and Sabine S\u00fcsstrunk},\nyear={2019},\nurl={https://openreview.net/forum?id=HJeB0sC9Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper893/Official_Review", "cdate": 1542234352866, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJeB0sC9Fm", "replyto": "HJeB0sC9Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper893/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335827115, "tmdate": 1552335827115, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper893/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}