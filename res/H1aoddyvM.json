{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124457517, "tcdate": 1518467236609, "number": 250, "cdate": 1518467236609, "id": "H1aoddyvM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1aoddyvM", "signatures": ["~harsh_satija1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1526043050105, "tcdate": 1526043050105, "number": 1, "cdate": 1526043050105, "id": "HkMhbz7Rz", "invitation": "ICLR.cc/2018/Workshop/-/Paper250/Official_Comment", "forum": "H1aoddyvM", "replyto": "rkK5mRGCz", "signatures": ["ICLR.cc/2018/Workshop/Paper250/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper250/Authors"], "content": {"title": "Opensourcing code", "comment": "Thank you for your interest! We are planning to release the code as soon as possible after the NIPS deadline. It just needs some cleanup. We will post here when the code is publicly available, and update the paper on arxiv with a link.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222446834, "id": "ICLR.cc/2018/Workshop/-/Paper250/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "H1aoddyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper250/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper250/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper250/Reviewers", "ICLR.cc/2018/Workshop/Paper250/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222446834}}, "tauthor": "amyzhang@fb.com"}, {"tddate": null, "ddate": null, "tmdate": 1526027153361, "tcdate": 1526027153361, "number": 1, "cdate": 1526027153361, "id": "rkK5mRGCz", "invitation": "ICLR.cc/2018/Workshop/-/Paper250/Public_Comment", "forum": "H1aoddyvM", "replyto": "H1aoddyvM", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Details about the experiments", "comment": "The paper is really interesting, thank you. \n\nWould it be possible to obtain the training details of the Dynamics and Reward modules ? \n\nThe best would be to open source the code. If it's not possible, could you please give pseudo-code details so that I can try to reproduce your results ? I would need all hyperparameters and training procedure (you might have used some tricks that A3C use to facilitate training)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712624247, "id": "ICLR.cc/2018/Workshop/-/Paper250/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper250/Reviewers"], "reply": {"replyto": null, "forum": "H1aoddyvM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712624247}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521583000518, "tcdate": 1519254696148, "number": 1, "cdate": 1519254696148, "id": "S1lnhuiDz", "invitation": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "forum": "H1aoddyvM", "replyto": "H1aoddyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer1"], "content": {"title": "interesting topic, hard to follow", "rating": "7: Good paper, accept", "review": "The terse workshop submission format makes it really hard to understand what's going on here. Briefly, it appears that one aspect of the work is to learn a model so that model-based decision making can be carried out. (How is decision making done? Is that a matter of using an LQR approach or something?) Another aspect of the work is to learn an encoding (abstraction?) of state so that the model (and planning?) can be made more efficient. Another aspect of the work is transfer---using learning from one environment to improve learning/performance/planning in another.\n\nThe most interesting possible takeaway message for me is the idea that an inverse model (guess the action given the state and next state) plays an essential role in making the learning work. I would love to have seen the paper just focus on this one aspect instead of covering transfer and other topics leaving only room for ablation results that show that turning this aspect off hurts performance. It strikes me that there's a lot more to the story.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521583000291, "id": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper250/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer2"], "reply": {"forum": "H1aoddyvM", "replyto": "H1aoddyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521583000291}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582950739, "tcdate": 1520199544740, "number": 2, "cdate": 1520199544740, "id": "B1Ztw1quf", "invitation": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "forum": "H1aoddyvM", "replyto": "H1aoddyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer4"], "content": {"title": "Interesting approach but presentation is lacking in terms of justification and experimental validation", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a framework for model-based reinforcement learning in which many of the model components are decoupled. In particular, the authors propose to learn the dynamics of the environment using an encoder model, a decoder model, a forward model, and an inverse model, and they propose to learn a policy and value function in a latent representation space instead of the original state space, building upon the work of [Dayan, 1993].  \n\nI found the paper to be generally clear and well-written, and the proposed framework seems sufficiently novel and broad in scope to be an interesting contribution to the model-based reinforcement learning community. The ablation analysis is also helpful for providing better clarity on the incremental value of each component of their proposed approach. \n\nAt the same time, I did have a few questions/concerns about this work:  \n1) It's not clear to me why LSTMs are needed for the encoder-decoder components of the dynamics model. Since the dynamics are given by an MDP, all of the relevant information should be captured in the current state.\n2) The authors do not motivate why learning the reward model in the latent space is better than in the original space.\n3) It still isn't clear to me how much of the performance improvement is due to learning a better dynamics model (using the decoupled framework) and how much is due to finding a latent variable space in which it is easier to learn a good policy.\n\nOther specific comments and questions:\n1) The LSTM architecture used in the experiments is not specified.\n2) The dynamics loss hyperparameters are not specified in the experiments. How sensitive are the results to this choice? Was the tuning of these values performed within the 1M episodes?\n3) There are no error bars for the experiments.\n4) The authors do not specify whether the 1M episodes used in the experiments includes the data used to train the dynamics module. If it did include the data used for training the dynamics model, then the authors do not explain how the data was split. If it did not include this data, then the experimental comparison seems unfair.\n5) I think the authors should compare their approach against at least one other model-based approach. Comparing against a single model-free algorithm seems insufficient.\n6) The authors claim that incorporating the inverse model has \"an important stabilizing effect on the dynamics model\", but I don't see how this was shown.\n \nI think the paper rates reasonably well in terms of clarity and novelty, but that there is room for improvement in terms of significance and quality. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521583000291, "id": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper250/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer2"], "reply": {"forum": "H1aoddyvM", "replyto": "H1aoddyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521583000291}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582784574, "tcdate": 1520631652026, "number": 3, "cdate": 1520631652026, "id": "S12PyFltM", "invitation": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "forum": "H1aoddyvM", "replyto": "H1aoddyvM", "signatures": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer2"], "content": {"title": "Important paper on model-based RL by decoupling dynamics from reward models", "rating": "8: Top 50% of accepted papers, clear accept", "review": "In this paper on deep reinforcement learning, the authors address multi-task learning and transfer learning to a new task in model-free RL by decoupling 1) learning of a dynamical model for the states and actions from 2) learning a task-dependent reward model, effectively transforming the problem to model-based RL where a forward model of the dynamics (state transitions from s_t to s_{t+1} in a representation space Z) and an inverse model of the dynamics (going from s_t and s_{t+1}, through the representation space Z, to the action a_t that cause the change of state) are explicitly learned. This is achieved by using an encoder/decoder architecture with a dynamics LSTM and an inverse decoder in order to learn the state representation, then by freezing the encoder and representation and training policy LSTMs on the frozen representations of the state.\n\nThe idea is very good, well formulated and well executed.\n\nI have some questions though, after reading the paper:\nThis approach also outperforms A3C on several MuJoCo tasks (swimmer, ant, hopper and half-cheetah) on multi-task learning (table 1). Can the authors remind the reader if the input is visual or joint angles? The authors use the words \"generalization\" and \"transfer\", which is confusing; for table 2, is only the encoder frozen and is the policy LSTM retrained on each new task, or is the policy LSTM frozen? Is training the dynamics module on random actions scalable to high-dimensional problems (in terms of actions) and to problems with long-range dependencies?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521583000291, "id": "ICLR.cc/2018/Workshop/-/Paper250/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper250/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper250/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer4", "ICLR.cc/2018/Workshop/Paper250/AnonReviewer2"], "reply": {"forum": "H1aoddyvM", "replyto": "H1aoddyvM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper250/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521583000291}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573557196, "tcdate": 1521573557196, "number": 62, "cdate": 1521573556851, "id": "B1ThC0CtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1aoddyvM", "replyto": "H1aoddyvM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Decoupling Dynamics and Reward for Transfer Learning", "abstract": "Reinforcement Learning (RL) provides a sound decision-theoretic framework to optimize the behavior of learning agents in an interactive setting.  However, one of the limitations to applications of RLto real-world tasks is the amount of data required for learning an optimal policy.  Our goal is to design an RL model that can be efficiently trained on new tasks, and produce solutions that generalize well beyond the training environment. We take inspiration from Successor Features (Dayan, 1993), which decouples the value function representation into dynamics and rewards, and learns them separately.  We take this further by explicitly decoupling learning the state representation, reward function, forward dynamics, and inverse dynamics of the environment. We posit that we can learn a representation space \\mathcal{Z} via this decoupling that makes downstream learning easier as: (1) the modules can be learned separately enabling efficient reuse of common knowledge across tasks to quickly adapt to new tasks; (2) the modules can be optimized jointly leading to a representation space that is adapted to the policy and value function, rather than only the observation space; (3) the dynamics model enables forward search and planning, in the usual model-based RL way. Our approach is the first model-based RL method to explicitly incorporate learning of inverse dynamics, and we show that this plays an important role in stabilizing learning", "paperhash": "zhang|decoupling_dynamics_and_reward_for_transfer_learning", "keywords": ["Reinforcement Learning", "Transfer Learning", "Representation Learning"], "_bibtex": "@misc{\n  zhang2018decoupling,\n  title={Decoupling Dynamics and Reward for Transfer Learning},\n  author={Amy Zhang and Harsh Satija and Joelle Pineau},\n  year={2018},\n  url={https://openreview.net/forum?id=H1aoddyvM}\n}", "authorids": ["harsh.satija@mail.mcgill.ca", "amyzhang@fb.com", "jpineau@cs.mcgill.ca"], "authors": ["Amy Zhang", "Harsh Satija", "Joelle Pineau"], "TL;DR": "Decoupling training dynamics model and reward models to facilitate transfer", "pdf": "/pdf/288b4f4d452564e3ff5cec1a29331a590893e2e0.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 7}