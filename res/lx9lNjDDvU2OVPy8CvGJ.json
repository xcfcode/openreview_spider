{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458142340116, "tcdate": 1458142340116, "id": "4QygOJ4GEhBYD9yOFqjD", "invitation": "ICLR.cc/2016/workshop/-/paper/86/review/12", "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "signatures": ["ICLR.cc/2016/workshop/paper/86/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/86/reviewer/12"], "content": {"title": "A novel algorithm in information geometry based on matching score: nice approach, preliminary experiments, and several possible applications", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents a new algorithm based on information geometric principles according to the theory of information geometry. in particular the authors introduce a new metric over the space of probability distributions, derived from the Hyvarinen divergence measure.\nthe derived gradient algorithm represents an alternative to the natural gradient evaluated with respect to the fisher information matrix.\nThe authors test the proposed approach for the training of a simple toy neural network and show its superiority compared to standard gradient descent.\n\nI think the paper is a good starting point for future works in the training of neural networks and also for other problems where natural gradient has been successfully applied in the past. The paper is well written, and I suggest to accept it for the conference.\n\nSome comments:\n\nPage 1, second paragraph: you write \"When we estimate the parameter \\xi with a diverge D, its parameter space has the Riemannian metric matrix...\" it this this is not well written: the geometry comes from the choice of a specific metric, or from the choice of a diverge function, but it's not directly associated to the estimation procedure in my opinion. Please explain better what you mean.\n\nPage 1, last line: to be more clear I would write \"is composed of the derivatives of the log-likelihood differentiated with respect to..\" similarly i would sat that the fisher information metric is composed of the derivative of the log likelihood.\n\nPage 2: add a space before \"const\", i.e. \"+ const\"\n\nPage 2: third paragraph: \"it requires the average over the unnormalized statistical model such that\", it's not feel written, especially you should change the \"such that\"\nsame paragraph: \"the approximate metric over THE input data\"\n\nPage 2: forth paragraph: \"the inversion of THE matrix\"\n\nPage 2, end of Section 2, can you be more precise when you express the dependence of the complexity on N. is this the number of inputs in the network?\n\nPage 2, Section 3: \"Of the proposed methods\" remove S\n\nPage 2, Section 3: \"N-dimensional probability variable\" this is not properly defined. i would say N-dimensional sample space, or N-dimensional random variable.\n\nPage 3: \"In terms of step number\" I would say \"in terms of number of steps\"\n\nPage 3: Figure: how did you choose the step size for the gradient update?\nFigure: why didn't you add also natural gradient with the fisher information matrix? you say this method is better, however you lack the comparison.\n\nPage 3: Table: what about the time complexity of the algorithm per iteration? install SGD slower?\n\nPage 4 maybe remove A from 4th reference (A Philip David)\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching", "abstract": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method.", "pdf": "/pdf/lx9lNjDDvU2OVPy8CvGJ.pdf", "paperhash": "karakida|adaptive_natural_gradient_learning_based_on_riemannian_metric_of_score_matching", "conflicts": ["u-tokyo.ac.jp", "riken.jp"], "authors": ["Ryo Karakida", "Masato Okada", "Shun-ichi Amari"], "authorids": ["karakida@mns.k.u-tokyo.ac.jp", "Okada@k.u-tokyo.ac.jp", "Amari@brain.riken.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579938430, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579938430, "id": "ICLR.cc/2016/workshop/-/paper/86/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/86/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457676707599, "tcdate": 1457676707599, "id": "BNYVDRL04h7PwR1riX16", "invitation": "ICLR.cc/2016/workshop/-/paper/86/review/11", "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "signatures": ["ICLR.cc/2016/workshop/paper/86/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/86/reviewer/11"], "content": {"title": "Derivation of natural gradient using score matching divergence. Nice theory, toy experiment only.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper shows how natural gradient updates can be computed using a metric derived from the score matching divergence. The same technique could be applied to other divergences. The theory results are very nice. The experimental results are for a toy problem only.\n\nThe paper clearly presents its ideas. There are some minor language issues, but they don't interfere with comprehension.\n\nAs the authors touch on in the conclusion, this technique would need to be combined with other techniques to be useful for large problems (eg block diagonal approximations to the inverse metric). Out of the box, the algorithm requires storing and manipulating the dense inverse of the metric, whose size is quadratic in the number of parameters.\n\nSome specific comments:\n\nthe framework of natural gradient -> the natural gradient framework\nand a two-layer model with the analytically -> and two-layer models with analytically \nas the online learning algorithm such that $$, where -> for an online learning algorithm where $$, and where\n\nThe right side of the inline equation before equation 4 should be $A^{-1} - \\epsilon A^{-1} B A^{-1}$.\n\nmaybe \"N variables\" -> \"N input dimensions\"?\n\nHow was the learning rate eta chosen for SGD and ANG? You shoudl do a grid search? A more compelling experimental section would also include a comparison to quasi-Newton methods. RMSProp would be a natural choice (or ANG+momentum vs. ADAM).\n\nI wonder if the MPF objective might also lead to a good metric. MPF consists of a Taylor series approximation to the KL divergence, so the corresponding metric might be very similar to the Fisher information, but without the intractable normalization constant.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching", "abstract": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method.", "pdf": "/pdf/lx9lNjDDvU2OVPy8CvGJ.pdf", "paperhash": "karakida|adaptive_natural_gradient_learning_based_on_riemannian_metric_of_score_matching", "conflicts": ["u-tokyo.ac.jp", "riken.jp"], "authors": ["Ryo Karakida", "Masato Okada", "Shun-ichi Amari"], "authorids": ["karakida@mns.k.u-tokyo.ac.jp", "Okada@k.u-tokyo.ac.jp", "Amari@brain.riken.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579939055, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579939055, "id": "ICLR.cc/2016/workshop/-/paper/86/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/86/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457647194509, "tcdate": 1457647194509, "id": "2xwyMqP20cpKBZvXtQ9Z", "invitation": "ICLR.cc/2016/workshop/-/paper/86/review/10", "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "signatures": ["ICLR.cc/2016/workshop/paper/86/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/86/reviewer/10"], "content": {"title": "Interesting variant of the natural gradient for the Score Matching metric", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The natural gradient has long been known to offer very interesting convergence properties (notably invariance to parametrization and convergence in fewer steps) compared to traditional gradient descent. However, natural gradient descent has not seen a wide adoption mainly due to its high computational cost. The update is essentially the same as in SGD but the gradient has to be multiplied (at each time step) by the inverse Fisher matrix which is a NxN matrix for a model with N parameters.\n    One important method to lower the computational cost associated with computing the natural gradient is the online adaptive natural gradient update which allows the computation of an on-line estimate of the inverted Fisher matrix (thus avoiding the need to estimate a full metric matrix at each step, and a full NxN matrix inversion at each step).\n\nThe present approach presents an adaptive online natural gradient algorithm for the Score Matching loss instead of the usual KL-divergence.\n\nCompared to the KL-divergence natural gradient, a score matching gradient may be applied in settings where the model has an intractable partition function and where the traditional natural gradient method cannot by applied.\n\nAside a few typos, the paper is quite clear.\n\nThe contribution is new.\n\nOn the down side, the experiments are very limited (one synthetic dataset), but they do demonstrate that the approach can be practical in at least some cases.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching", "abstract": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method.", "pdf": "/pdf/lx9lNjDDvU2OVPy8CvGJ.pdf", "paperhash": "karakida|adaptive_natural_gradient_learning_based_on_riemannian_metric_of_score_matching", "conflicts": ["u-tokyo.ac.jp", "riken.jp"], "authors": ["Ryo Karakida", "Masato Okada", "Shun-ichi Amari"], "authorids": ["karakida@mns.k.u-tokyo.ac.jp", "Okada@k.u-tokyo.ac.jp", "Amari@brain.riken.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579939754, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579939754, "id": "ICLR.cc/2016/workshop/-/paper/86/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "lx9lNjDDvU2OVPy8CvGJ", "replyto": "lx9lNjDDvU2OVPy8CvGJ", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/86/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455801654043, "tcdate": 1455801654043, "id": "lx9lNjDDvU2OVPy8CvGJ", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "lx9lNjDDvU2OVPy8CvGJ", "signatures": ["~Ryo_Karakida1"], "readers": ["everyone"], "writers": ["~Ryo_Karakida1"], "content": {"CMT_id": "", "title": "Adaptive Natural Gradient Learning Based on Riemannian Metric of Score Matching", "abstract": "The natural gradient is a powerful method to improve the transient dynamics of learning by considering the geometric structure of the parameter space. Many natural gradient methods have been developed with regards to Kullback-Leibler (KL) divergence and its Fisher metric, but the framework of natural gradient can be essentially extended to other divergences. In this study, we focus on score matching, which is an alternative to maximum likelihood learning for unnormalized statistical models, and introduce its Riemannian metric. By using the score matching metric, we derive an adaptive natural gradient algorithm that does not require computationally demanding inversion of the metric. Experimental results in a multi-layer neural network model demonstrate that the proposed method avoids the plateau phenomenon and accelerates the convergence of learning compared to the conventional stochastic gradient descent method.", "pdf": "/pdf/lx9lNjDDvU2OVPy8CvGJ.pdf", "paperhash": "karakida|adaptive_natural_gradient_learning_based_on_riemannian_metric_of_score_matching", "conflicts": ["u-tokyo.ac.jp", "riken.jp"], "authors": ["Ryo Karakida", "Masato Okada", "Shun-ichi Amari"], "authorids": ["karakida@mns.k.u-tokyo.ac.jp", "Okada@k.u-tokyo.ac.jp", "Amari@brain.riken.jp"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}