{"notes": [{"id": "kWSeGEeHvF8", "original": "zLdWOa8A_K", "number": 2401, "cdate": 1601308264907, "ddate": null, "tcdate": 1601308264907, "tmdate": 1616045143567, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fG6tpuSUCKD", "original": null, "number": 1, "cdate": 1610040422125, "ddate": null, "tcdate": 1610040422125, "tmdate": 1610474020939, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes benchmark tasks for offline policy evaluation.  The proposed benchmark tasks evaluate the policy with batch data with respect to three metrics, including the standard mean squared error. The paper also evaluate several baseline offline reinforcement learning methods with the benchmark tasks, which will serve as standard baselines.  All of the reviewers are in favor of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422111, "tmdate": 1610474020920, "id": "ICLR.cc/2021/Conference/Paper2401/-/Decision"}}}, {"id": "0A0aneBb8l", "original": null, "number": 2, "cdate": 1603588506762, "ddate": null, "tcdate": 1603588506762, "tmdate": 1606284294054, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review", "content": {"title": "This paper gives a standardized benchmark for off-policy evaluation, which contains fixed logged datasets and various target policies to cover wide range of challenges.", "review": "1. Significance\n- I view this paper has high significance level. As more and more OPE methods arise, papers rely on different datasets and metrics, which makes them hard to compare, and especially see the \"success\" and \"failure\" mode of this method. We need a benchmark dataset on this domain and this paper moves an important step towards this by providing standardized logged dataset, and various policies in different challenging, complex domains.\n\n2. Novelty\n- There is a very similar work by Voloshin et.al (2019), which gives the empirical evaluation of popular OPE methods across various domains. Though not providing the logged data, they give the logging policy to logged the data, I am concerned about the novelty of this work compared with Voloshin et.al (2019). It would be great if the author could discuss the novelty/improvement over that work.\n\n3. Quality & Clarity\n- This paper has a great structure, metrics/choice of datasets/design of policies are well motivated and clearly stated, it is a very easy to read paper.\n\n- This work gives a clear description of the logged datasets/policies/and the performance of various off-policy method. However, I feel it would be great if the authors could also think, and if possible make the following discussions along with this paper:\n\n(1). it would be great if the authors could give some statistics to measure the property of the datasets, such as a measure of the missing support of logging policy, how difference are the logging policy and target policy?\n\n(2). I know the authors discuss some properties of the environment in words, such as sparse/dense rewards, stochasticity of the environment, etc. It would be great to summarize these environment info into a single table.\n\n(3). A very important class  of estimators in OPE is these kind of interpolated estimators between value-based and weighting-based, which contains a trade-off of bias-variance, such as the work by Phillip and Emma (2016). It also shows in Voloshin et.al (2019)'s empirical evaluations and shows very promising performance. However, it is missing in this paper and it would be excited to see how it works in the D4RL tasks.\n\n(4). A further extension from (1) is that the authors did a great job in doing lots of empirical evaluations in various domains. I saw aggregated performance in main paper and individual task performance is listed in Appendix. It would be great to classify them into groups which depends on  the MDP property, the metric measuring difference of logging/target policy, and etc... This will definitely give some further directions like which method perform well in which setting. This is also shows in Voloshin et.al (2019)'s paper but kind of missing in this work.\n\n(5). One difference compared with Voloshin et.al (2019) is that it seems in the logged dataset, they know the propensity. However, I feel the dataset released along the paper does not have propensity. Will is possible to also add some datasets with propensity included?\n\nOverall, I feel this paper addressing an important problem, however, I feel there is still room for improvement to make them have more impact. I am willing to adjust my score if the authors address some of these issues. \n\nRef: \n1.  Philip S. Thomas, Emma Brunskill  Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning.\n2. Cameron Voloshin, Hoang M. Le, Nan Jiang, Yisong Yue Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097227, "tmdate": 1606915778721, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2401/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review"}}}, {"id": "VpL-EJtdRCu", "original": null, "number": 8, "cdate": 1606284277910, "ddate": null, "tcdate": 1606284277910, "tmdate": 1606284277910, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "rW4PRsPCFL", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thanks authors for the response! It clears most of my concerns, though some are left as \"to do task\" in the final version of the paper. I am willing to increase my score and hopefully these changes will be seen in the final version."}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kWSeGEeHvF8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2401/Authors|ICLR.cc/2021/Conference/Paper2401/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment"}}}, {"id": "5owWztjBYVX", "original": null, "number": 7, "cdate": 1605929364977, "ddate": null, "tcdate": 1605929364977, "tmdate": 1605929412397, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "B_sD36S2WyC", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment", "content": {"title": "Author response to reviewer 5", "comment": "Thank you for your valuable feedback. We will address the issues you raised below.\n\n> \"Instead of using the space for 'nice pictures', use it to actually support the claims of the paper, maybe shifting some figures from the appendix to the main text would be more informative.\"\n\nThis is a good idea. Originally, we were concerned readers might not be familiar with some of the environments and tasks. and hoped images may highlight the nature of the tasks. But we agree it is better to move these images to the appendix, and move some results back into the main body. We will incorporate this comment into the final version.\n\n> \"It would be nice to see this dataset contain image data for learning from image input.\"\n\nWe agree that benchmarking OPE methods on datasets with image inputs is valuable. We plan to add this to the benchmark in a future version, but cannot guarantee to have it in time for ICLR\n\n> \"I am highly suspicious about the way the data is generated for the ant-maze environment. Please specify where does the expert data come from.\"\n\nThe expert data for antmaze-large is generated by collecting additional trajectories that reach the goal using a high-level waypoint planner in conjunction with a low-level goal-conditioned policy. This is the same method used to generate the dataset (See Sec. 5 in the D4RL paper), except we collected additional trajectories which reach the goal rather than navigation to random goals. We have updated the paper to clarify this.\n\n> \"I didn't find the code for the algorithms themselves.\"\n\nWe agree that an implementation of OPE algorithms should be open sourced. We obtained the code for the algorithms from the cited papers via personal communication. We are currently actively working with the authors of those cited papers to open source the algorithm code in the coming months.\n\n> \"Can you comment on what kind of normalization was done on the reward function? Ideally it should be normalized across all environments in the benchmark, such that performance is comparable.\"\n\nGood point. For DOPE RL Unplugged, the environments already have normalized rewards. For DOPE D4RL, we normalized by the max discounted return achieved by any policy for that task.\nWe mention this briefly in the caption for Figure 5, but will expand upon this in the appendix.\n\n> \"Maybe also writing down the equations for regret@k and rank correlation would be helpful. It is sometimes simpler to understand the metric once one sees the actual equation.\"\n\nGood suggestion. We will add this in the final version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kWSeGEeHvF8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2401/Authors|ICLR.cc/2021/Conference/Paper2401/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment"}}}, {"id": "rW4PRsPCFL", "original": null, "number": 6, "cdate": 1605929323591, "ddate": null, "tcdate": 1605929323591, "tmdate": 1605929323591, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "0A0aneBb8l", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment", "content": {"title": "Author response to reviewer 2", "comment": "Thank you for your comments. Please let us know if your concerns are addressed.\n\n> \u201cI am concerned about the novelty of this work compared with Voloshin et.al (2019).\u201d\n\nWe updated our paper to make the differences between the proposed benchmark and Voloshin et al. (2019) clearer.\n\nVoloshin et al. (2019) present their primary contribution as an empirical comparison of different OPE methods across a range of situations (e.g., representation mismatch, horizon length, policy mismatch, bad behavior policy estimates), and provide guidance for each. They perform this evaluation on environments with discrete actions including: tabular grid world and graph environments, mountain car, and an Atari environment, namely the Enduro environment. They conclude with important future work problems, and our work directly addresses two of them:\n- OPE for continuous actions, which pose significant fundamental challenges for many OPE methods: All of our tasks are continuous control.\n- Missing data coverage: We don't assume full support of the evaluation policy by the behavior policy data. In fact we design our datasets and policies to ensure a range of missing data coverage (See the description of DOPE D4RL in the Section 4). Crucially, this makes good representation learning a key component for any successful OPE method. For instance, the importance sampling baseline is not competitive on these tasks.\n\nBeyond estimating value, our work provides a large set of evaluation policies trained by offline RL and online RL, making it possible to measure how OPE algorithms perform at ranking these policies. We are not aware of prior standardized benchmarks that enable this. Offline policy selection is a particularly appealing use case for applications. \n\nFinally, we note that because we use existing open-source offline RL datasets as our behavior data, using our benchmarks is more straightforward, reproducible, and standardized. That said,  Voloshin et al and our benchmark are complementary, and we invite future research on OPE to consider both benchmarks for evaluation.\n\n> \u201cI know the authors discuss some properties of the environment in words, such as sparse/dense rewards, stochasticity of the environment, etc. It would be great to summarize these environment info into a single table.\u201d\n\nWe have updated Table 1 with an additional row that details which properties correspond to which tasks.\n\n> \u201c A very important class of estimators in OPE is these kind of interpolated estimators between value-based and weighting-based, which contains a trade-off of bias-variance, such as the work by Phillip and Emma (2016). It also shows in Voloshin et.al (2019)'s empirical evaluations and shows very promising performance. However, it is missing in this paper and it would be excited to see how it works in the D4RL tasks.\u201d\n\nWe agree. The weighted doubly-robust policy evaluation method (labeled DR in our evaluations) is based on Thomas and Brunskill (2016). But it would be good to include MAGIC as well. Time permitting, we will add that baseline to the final version of the paper.\n\n> \u201c One difference compared with Voloshin et.al (2019) is that it seems in the logged dataset, they know the propensity. However, I feel the dataset released along the paper does not have propensity. Will is possible to also add some datasets with propensity included?\u201d\n\nAdding propensity is difficult because the behavior policies used to generate some of the D4RL/RLUnplugged datasets are a mixture of many policies followed by a filtering step to keep the datasets small and challenging. We believe not including the propensity can help encourage future work to focus on *behavior-agnostic* OPE methods, which is typical in many practical applications, where offline datasets of interactions are not amenable to logging propensities (e.g., when the behavior policy is a mix of ML and hard-coded business logic). For the methods that require propensities, we estimate the propensities with behavior cloning. We are happy to share the estimated policy propensities learned by behavioral cloning on our offline datasets if you think that is helpful."}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kWSeGEeHvF8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2401/Authors|ICLR.cc/2021/Conference/Paper2401/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment"}}}, {"id": "L8fBSa4Flbp", "original": null, "number": 5, "cdate": 1605929215276, "ddate": null, "tcdate": 1605929215276, "tmdate": 1605929215276, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "29WgIty6Kny", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment", "content": {"title": "Author response to reviewer 1", "comment": "Thank you for your valuable feedback.\n\n> \"connect with representation learning which is the focus of the ICLR conference [...] name of the work is highly unfortunate\"\n\nThank you. We should make this more clear. Many of the OPE baselines make use of improvements in neural function approximation (e.g., nonlinearities, good initialization, activation normalization, etc) and improvements focused on function approximation for reinforcement learning (e.g., target networks, double DQN, etc). The non-trivial dimensionality of the continuous state and action spaces requires generalization to evaluate novel policies from a fixed offline dataset. We expect other ideas from the deep learning and RL communities to further improve the performance on these benchmarks. Accordingly, we believe the ICLR conference is well suited for this paper and including Deep in the name is appropriate. We will update the paper to reflect this more clearly.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kWSeGEeHvF8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2401/Authors|ICLR.cc/2021/Conference/Paper2401/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment"}}}, {"id": "9EPi9APAoaY", "original": null, "number": 3, "cdate": 1604537119626, "ddate": null, "tcdate": 1604537119626, "tmdate": 1605391649790, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review", "content": {"title": "The review of benchmarks for deep off-policy evaluation", "review": "This article proposes a benchmark of off-policy evaluation, which provides different metrics for policy ranking, evaluation and selection. Offline metrics are provided by evaluating the value function of logged data, and then evaluating absolute error, rank correlation and regret. Verify the effectiveness of different offline evaluation methods. This article provides two evaluation scenarios, one is DOPE RL unplugged, and the other is D4RL. In the experiment, the author verified the benchmark proposed in this article in the MuJoCo environment to evaluate the effectiveness of different offline evaluation methods.\n\nThe paper\u2019s key strengths:\n\n1.\tI think the research direction of this article is very meaningful, because many scenarios in the real world only have offline data, which requires a simulated environment or other offline evaluation methods for strategy evaluation and model selection. This is very crucial for model improvement and evaluation. And the author proposed absolute error and rank correlation in the benchmark to evaluate the strategy. I think this is very innovative.\n2.\tThe author has applied a large number of offline evaluation methods in the experiment to verify in different metrics, and in the appendix has conducted experiments and verifications for many MuJoCo scenarios. I think the experimental results are very rich and very confirmatory.\n\nThe paper\u2019s key weaknesses:\n1.\tThe biggest problem with this article is that the contribution of the article is insufficient and lacks originality. This article proposes a benchmark for off-policy evaluation and verifies different OPE methods, but this article does not compare with other similar benchmarks to verify whether the benchmark proposed in this article is effective.\n2.\tIn the experimental part, this paper verifies different metrics for different OPE methods. However, in Figure 4 and Figure 5, the different methods in the two sets of benchmarks proposed in this article are quite different in different OPE methods. I hope the author can give some comments on the differences between the two sets of evaluation methods.\n3.\tThe author uses the value function in formula 1 to estimate the effect of the strategy. I doubt this method. Because of the attenuation factor here, I think it has an impact on the final value calculated by different methods. Because bellman equation is only an estimate of the value of a certain state, not an absolute strategy benefit, I hope the author will give some explanations here.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097227, "tmdate": 1606915778721, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2401/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review"}}}, {"id": "Vchf-r5w1Mv", "original": null, "number": 3, "cdate": 1605225642833, "ddate": null, "tcdate": 1605225642833, "tmdate": 1605225642833, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "9EPi9APAoaY", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the comments, and we address the issues raised in your review below. Please let us know if our responses address your concerns, or if there are any additional clarifications needed.\n\n> \"... this article does not compare with other similar benchmarks\"\n\nWe updated our paper to make the differences between our benchmark and other recent OPE benchmarks clearer in the paper. Overall, to the best of our knowledge, there are no widely used OPE benchmarks with tasks comparable to the high-dimensional, continuous action problems as presented in this paper, though we would be happy to add any other references that the reviewer might suggest. The most similar work that we are familiar with is Voloshin et al. (2019). Voloshin et al. (2019) present their primary contribution as an empirical comparison of different OPE methods across a range of situations (e.g., representation mismatch, horizon length, policy mismatch, bad behavior policy estimates), and provide guidance for each. They perform this evaluation on environments with discrete actions including: tabular grid world and graph environments, mountain car, and an Atari environment, namely the Enduro environment. They conclude with important future work problems, and our work directly addresses two of them:\n- OPE for continuous actions, which pose significant fundamental challenges for many OPE methods: All of our tasks are continuous control.\n- Missing data coverage: We don't assume full support of the evaluation policy by the behavior policy data. In fact we design our datasets and policies to ensure a range of missing data coverage (See the description of DOPE D4RL in the Section 4). Crucially, this makes good representation learning a key component for any successful OPE method.\n\nBeyond estimating value, our work provides a large set of evaluation policies trained by offline RL and online RL, making it possible to measure how OPE algorithms perform at ranking these policies. We are not aware of prior standardized benchmarks that enable this. Offline policy selection is a particularly appealing use case for applications. \n\nFinally, we note that because we use existing open-source offline RL datasets as our behavior data, using our benchmarks is more straightforward, reproducible, and standardized. That said, these benchmarks are complementary, and we invite future research on OPE to consider both benchmarks for evaluation.\n\n> \u201c In the experimental part, this paper verifies different metrics for different OPE methods. However, ... different methods in the two sets of benchmarks proposed in this article are quite different in different OPE methods.\u201d \n\nWe assume you are referring to the difference in the relative performance of different OPE methods with respect to the different metrics we propose. This is a good question, and please clarify if this is not the case.\n\nWe find that the rank order of OPE methods in terms of Regret@1 and Rank correlation is very similar in Figures 4 and 5, but the order of OPE methods according to Absolute error is sometimes different. This is not too surprising, since in theory an OPE technique that ranks policies correctly does not have to estimate the precise value of each policy correctly. In practice, we find that the importance sampling baseline suffers from this issue, and even though it ranks policies reasonably in D4RL, the value estimates are generally poor. See magenta scatter plots on pages 27-32, where the r-values are acceptable, but the value estimates are poor. We note that for certain applications one may care about the correctness of absolute value estimates of policies while in others one cares about the relative value and ranking of policies, hence both metrics are valuable. We will include this discussion in the paper.\n\n> \u201cThe author uses the value function in formula 1 to estimate the effect of the strategy. [...]\u201d\n\nThank you for pointing out this confusion. We measure expected discounted return in the MDP, which is the expected value under the initial state distribution (i.e., $\\mathbb{E}_{s_0,\\pi}[r_0+\\gamma r_1 + \\gamma^2 r_2 + \\dots ]$ -- note this does not use any learned functions). We updated Eq. 1 to reflect this. \n\nWe would like to emphasize that this ground truth value is independent of the method used to estimate the expected discounted return, and therefore it does not have an \u201cimpact on the final value calculated by different methods\u201d. Please let us know if you have additional concerns.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kWSeGEeHvF8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2401/Authors|ICLR.cc/2021/Conference/Paper2401/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923848829, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Comment"}}}, {"id": "29WgIty6Kny", "original": null, "number": 1, "cdate": 1603386072563, "ddate": null, "tcdate": 1603386072563, "tmdate": 1605024220305, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review", "content": {"title": "Great stuff, I plan on using this myself, but no connection to representation learning as presented", "review": "This work describes a set of benchmarks for offline policy evaluation and selection.  The area is important and well-motivated in the introduction.  \n\nThe work builds upon existing benchmark tasks for offline RL.  However, the increment is meaningful.  By providing the triad of data, trained policies, and online evaluation results, authors have isolated the offline evaluation component, controlled for ancillary aspects, and facilitated experimentation.  In other words, experiments are both easier and more useful.\n\nThe work as presented makes little attempt to connect with representation learning which is the focus of the ICLR conference.  It could be improved in this regard, if authors make connections between estimation and representation (e.g., how are model-based estimators related to or employing representation learning?   what about model-free estimators that have to fit a propensity model to the behaviour policy?  how, if at all, do the baselines described in section 5.1 utilize representation learning, or how could they be improved via representation learning techniques?)\n\nThe name of the work is highly unfortunate.  There is nothing deep about the benchmark in the representation learning sense, and the use of the term deep contributes to the general degradation of the term into a meaningless yet popular adjective.  Unfortunately, les jeux sons faits ... do better next time!\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097227, "tmdate": 1606915778721, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2401/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review"}}}, {"id": "B_sD36S2WyC", "original": null, "number": 4, "cdate": 1604664008452, "ddate": null, "tcdate": 1604664008452, "tmdate": 1605024220121, "tddate": null, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "invitation": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review", "content": {"title": "It's a Necessary Benchmark Paper", "review": "## Summary\n\nThe paper proposes an off-policy evaluation dataset (DOPE) for various control tasks.\nThe authors include various baselines for off-policy evaluation and evaluate them with additional metrics to MSE, namely regret@k and rank correlation.\n\n## Writeup\n\nThe writeup is good. It is clear what the \"features\" of the proposed benchmark are, and what problems the benchmark addresses that the other benchmarks didn't address before. I don't really like that in different images form  dm-control and gym environments pop up in the paper. Since these environments as self-standing were not the contribution of the paper I would remove these images, unless they are used to show a specific point. The space should be used rather to illustrate the point of the paper, that additional metrics are needed for evaluating algorithms / better benchmark is needed.\n\n## Pros\n\nI think that using additional evaluation metrics for off-policy are indeed necessary. The authors show that not one single metric summarizes performance over all of the metrics. This is going to allow researchers to do a more thorough analysis of offline reinforcement learning algorithms.\n\n## Cons\n\nInstead of using the space for \"nice pictures\", use it to actually support the claims of the paper, maybe shifting some figures from the appendix to the main text would be more informative. \nThe paper is not making any improvement over the methods or suggesting any reason why there is such a disparity in performance between different metrics, as such, there are no deeper insights.\nA large chunk of the paper is previous work.\n\n## Comments\n\nIn general, the experimental evaluation is thorough (there are a lot of experiments in the appending with a lot of tables and figures). \n\nTo me it is a bit questionable to which extent this data might count as high-dimensional and complicated, in comparison to real-world of robotics, this data is still quite low dimensional. It would be nice to see this dataset contain image data for learning from image input.\n\nI am highly suspicious about the way the data is generated for the ant-maze environment. Specifically, you use expert data for ant-maze-large because DAPG didn't work. Please specify where does the expert data come from.\n\nI didn't try to run the code, but I looked at it. I didn't find the code for the algorithms themselves. I would expect to have the training code to reproduce the experiments. With the current code I can only evaluate saved policies?\n\nCan you comment on what kind of normalization was done on the reward function? Ideally it should be normalized across all environments in the benchmark, such that performance is comparable. \n\nMaybe also writing down the equations for regret@k and rank correlation would be helpful. It is sometimes simpler to understand the metric once one sees the actual equation. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2401/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2401/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarks for Deep Off-Policy Evaluation", "authorids": ["~Justin_Fu1", "~Mohammad_Norouzi1", "~Ofir_Nachum1", "~George_Tucker1", "~ziyu_wang1", "~Alexander_Novikov1", "~Mengjiao_Yang1", "~Michael_R_Zhang1", "~Yutian_Chen1", "~Aviral_Kumar2", "~Cosmin_Paduraru1", "~Sergey_Levine1", "~Thomas_Paine1"], "authors": ["Justin Fu", "Mohammad Norouzi", "Ofir Nachum", "George Tucker", "ziyu wang", "Alexander Novikov", "Mengjiao Yang", "Michael R Zhang", "Yutian Chen", "Aviral Kumar", "Cosmin Paduraru", "Sergey Levine", "Thomas Paine"], "keywords": ["reinforcement learning", "off-policy evaluation", "benchmarks"], "abstract": "Off-policy evaluation (OPE) holds the promise of being able to leverage large, offline datasets for both evaluating and selecting complex policies for decision making. The ability to learn offline is particularly important in many real-world domains, such as in healthcare, recommender systems, or robotics, where online data collection is an expensive and potentially dangerous process. Being able to accurately evaluate and select high-performing policies without requiring online interaction could yield significant benefits in safety, time, and cost for these applications. While many OPE methods have been proposed in recent years, comparing results between papers is difficult because currently there is a lack of a comprehensive and unified benchmark, and measuring algorithmic progress has been challenging due to the lack of difficult evaluation tasks. In order to address this gap, we present a collection of policies that in conjunction with existing offline datasets can be used for benchmarking off-policy evaluation. Our tasks include a range of challenging high-dimensional continuous control problems, with wide selections of datasets and policies for performing policy selection. The goal of our benchmark is to provide a standardized measure of progress that is motivated from a set of principles designed to challenge and test the limits of existing OPE methods. We perform an evaluation of state-of-the-art algorithms and provide open-source access to our data and code to foster future research in this area.    ", "one-sentence_summary": "A benchmark proposal for off-policy evaluation and policy selection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fu|benchmarks_for_deep_offpolicy_evaluation", "supplementary_material": "/attachment/57358bb4d5338b0b3664c51b8e1b2cf50cee44e8.zip", "pdf": "/pdf/3a90850ebecc25b81a9534180c75842a2b672812.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfu2021benchmarks,\ntitle={Benchmarks for Deep Off-Policy Evaluation},\nauthor={Justin Fu and Mohammad Norouzi and Ofir Nachum and George Tucker and ziyu wang and Alexander Novikov and Mengjiao Yang and Michael R Zhang and Yutian Chen and Aviral Kumar and Cosmin Paduraru and Sergey Levine and Thomas Paine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=kWSeGEeHvF8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kWSeGEeHvF8", "replyto": "kWSeGEeHvF8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2401/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097227, "tmdate": 1606915778721, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2401/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2401/-/Official_Review"}}}], "count": 11}