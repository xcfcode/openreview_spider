{"notes": [{"id": "HJgRCyHFDr", "original": "SkgXS_1Kvr", "number": 2044, "cdate": 1569439701981, "ddate": null, "tcdate": 1569439701981, "tmdate": 1577168291484, "tddate": null, "forum": "HJgRCyHFDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "x795jnCPK", "original": null, "number": 1, "cdate": 1576798739013, "ddate": null, "tcdate": 1576798739013, "tmdate": 1576800897314, "tddate": null, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2044/-/Decision", "content": {"decision": "Reject", "comment": "Since there were only two official reviews submitted, I reviewed the paper to form a third viewpoint.  I agree with reviewer 2 on the following points, which support rejection of the paper:\n1) Only CIFAR is evaluated without Penn Treebank;\n2) The \"faster convergence\" is not empirically justified by better final accuracy with same amount of search cost; and\n3) The advantage of the proposed ACSA over SBMD is not clearly demonstrated in the paper.\n\nThe scores of the two official reviews are insufficient for acceptance, and an additional review did not overturn this view.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725718, "tmdate": 1576800277674, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2044/-/Decision"}}}, {"id": "Syx3ZB43oH", "original": null, "number": 3, "cdate": 1573827843801, "ddate": null, "tcdate": 1573827843801, "tmdate": 1573827843801, "tddate": null, "forum": "HJgRCyHFDr", "replyto": "rJevgbYIFH", "invitation": "ICLR.cc/2020/Conference/Paper2044/-/Official_Comment", "content": {"title": "Response 1", "comment": "Response: Thank you for your comments. We hope to address your issues below:\n\n1) Novelty and relevance of SBMD and ASCA to NAS:\n- Novelty: We respectfully disagree with your comment.  In fact, our work is the first to introduce ASCA and it is *not* an existing generic algorithm.\n- Beta parameter: The beta parameter depends on the activation functions used and on the data. As we acknowledged at submission, this restricts the cases where the theory applies to smooth activation functions (sigmoid, tanh).\n\n2) Contribution of work on top of existing results for mirror descent:\n- While mirror descent is indeed a well-known approach in the optimization literature, its connection to NAS has not been explored. Our theoretical guarantees are largely motivated by this connection and provide significant improvements over existing analysis for NAS (Akimoto et al., 2019; Carlucci et al., 2019; Nayman et al., 2019; Noy et al., 2019; Yao et al., 2019).\n- The guarantees we provide for the ASCA variant of mirror descent are *new* and not previously known in any form outside the Euclidean case.\n\n3) Generalization bounds for NAS\n- We *do* provide a theoretical bound for NAS.  The main generalization result (Theorem 4.1) can be applied to non-convex inner objectives, including for NAS. We discuss what the result means for NAS starting at the bottom of page 7, with reference to existing theoretical work on complexity of the set of local minima of deep nets and a discussion of what further understanding can be gained."}, "signatures": ["ICLR.cc/2020/Conference/Paper2044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgRCyHFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference/Paper2044/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2044/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2044/Reviewers", "ICLR.cc/2020/Conference/Paper2044/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2044/Authors|ICLR.cc/2020/Conference/Paper2044/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147139, "tmdate": 1576860530679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference/Paper2044/Reviewers", "ICLR.cc/2020/Conference/Paper2044/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2044/-/Official_Comment"}}}, {"id": "BygN0VE2jH", "original": null, "number": 2, "cdate": 1573827787911, "ddate": null, "tcdate": 1573827787911, "tmdate": 1573827787911, "tddate": null, "forum": "HJgRCyHFDr", "replyto": "B1lBMGw2KB", "invitation": "ICLR.cc/2020/Conference/Paper2044/-/Official_Comment", "content": {"title": "Response 2", "comment": "Response: Thank you for your comments. We hope to address your issues below:\n\nTheoretical analysis:\nWe would like to emphasize that the convergence guarantees improve significantly upon several previous NAS analyses (Akimoto et al., 2019; Carlucci et al., 2019; Nayman et al., 2019; Noy et al., 2019; Yao et al., 2019). To our knowledge they are the first results that are both non-asymptotic (finite-time convergence) and optimize a quantity of direct interest (empirical risk objective).\n\nValidity of exponentiated-gradient update:\n(1) NAS experiments: While we agree that the experiments would benefit from an additional dataset, we decided to focus on CIFAR-10 due to the high computational cost associated with running these experiments.  Similar to Li & Talwalkar 2019, we have also observed that the variance associated with stage 3 evaluation of architectures is much higher on the Penn Treebank dataset and chose to instead focus our resources in thoroughly evaluating EDARTS on the lower variance CIFAR-10 benchmark.  As stated in the last paragraph of the paper, we follow a higher bar for reproducibility than many other NAS publications (e.g., DARTS, SNAS, XNAS, ASAP, ProxylessNAS, etc) and report results for EDARTS for 3 different sets of seeds on CIFAR-10; EDARTS reaches ~2.70% test error on 2 out of the 3 runs.  We have not seen similar broad reproducibility results for other NAS methods.\n(2) Same search cost as first-order DARTS: the search cost is the same since we train for the same number of epochs.  The faster convergence rate is reflected in the better resulting architecture.  \n(3) Kernel experiments: Please note that the kernel experiments were motivated by understanding weight-sharing and its generalization guarantees on a simpler problem (kernel ridge regression), not as a test of the performance of our optimizer. As a result, the successive halving method that exceeds exponentiated-gradient on those experiments is also an algorithm proposed in this paper, and it may be viewed as a hard-cutoff version of exponentiated-gradient. Furthermore, successive halving would be difficult to apply directly in the larger NAS search space.\n\nASCA vs. SBMD:\n(1) Need for such an alternative: The motivation behind our paper is to theoretically understand NAS methods. Several NAS methods have found it useful to run many iterations on both the shared-weights and architecture-weights before switching (e.g. ENAS by Pham et al., 2018 and MdeNAS by Zheng et al., 2019). This approach is reflected in the ASCA algorithm and not in SBMD.\n(2) Respective advantages: While most (but not all, as discussed above) NAS methods prefer an SBMD-style approach, ASCA may be preferable when fast solvers are available for strongly-convex relaxations of the problem at hand.\n\nWording:\nThank you for pointing these out - they will be corrected."}, "signatures": ["ICLR.cc/2020/Conference/Paper2044/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgRCyHFDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference/Paper2044/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2044/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2044/Reviewers", "ICLR.cc/2020/Conference/Paper2044/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2044/Authors|ICLR.cc/2020/Conference/Paper2044/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147139, "tmdate": 1576860530679, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2044/Authors", "ICLR.cc/2020/Conference/Paper2044/Reviewers", "ICLR.cc/2020/Conference/Paper2044/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2044/-/Official_Comment"}}}, {"id": "rJevgbYIFH", "original": null, "number": 1, "cdate": 1571356910576, "ddate": null, "tcdate": 1571356910576, "tmdate": 1572972390392, "tddate": null, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2044/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This work proposes an algorithm for handling the weight-sharing neural architecture search problem. It also derives generalization bound for this problem.\n\nThe reviewer has several concerns:\n\n1) the SBMD and ASCA algorithms are existing generic algorithms. The analysis in this work also looks very generic. There is a sense of disconnection with the considered training problems. The reviewer would like to see more discussions on how to connect the algorithms with specific NAS problems. For example, what is the beta parameter when training a NAS problem?\n\n2) The convergence rate improvement brought by using mirror descent has been long known. It is not easy to see what is the contribution of this work.\n\n3) The generalization part seems to be meaningful. But it may be much stronger if the NAS problem can also have a theoretical bound. It is less appealing to only discuss cases with strongly convex objectives."}, "signatures": ["ICLR.cc/2020/Conference/Paper2044/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2044/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575848680599, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2044/Reviewers"], "noninvitees": [], "tcdate": 1570237728577, "tmdate": 1575848680610, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2044/-/Official_Review"}}}, {"id": "B1lBMGw2KB", "original": null, "number": 2, "cdate": 1571742221046, "ddate": null, "tcdate": 1571742221046, "tmdate": 1572972390359, "tddate": null, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "invitation": "ICLR.cc/2020/Conference/Paper2044/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "I have not worked in the optimization filed and I am only gently followed the NAS field. I might under-valued the theoretical contribution.\n\nThis work provides  theoretical analysis for the NAS using weight sharing in two aspects: \n1) The authors give non-asymptotic stationary-point convergence guarantees (based on stochastic block mirror descent (SBMD) from Dang and Lan (2015)) for the empirical risk minimization (ERM) objective associated with weight-sharing. Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015). The author also provided an alternative to SBMD that uses alternating successive convex approximation (ASCA) which has similar convergence rate. \n2) The author provide generalization guarantees for this objective over structured hypothesis spaces associated with a finite set of architectures.\n\nMy biggest concern is the validity of the proposed exponentiated gradient update, at least empirically. We indeed observed slightly improvement in test error over DARTS on the CIFAR10 benchmark but how reproducible the results are? Can you compare at least on the other benchmark (PENN TREEBANK) used in Liu et al 2019? Also, comparing to first order DARTS, search cost is the same and this is hard to justify the better convergence rate for EDARTS. In addition, the results on feature map selection is not very encouraging as the gap to the successive halving is significant.\n\nThe author proposed ASCA, as an alternative method to SBMD. Why we need such alternative? What is the advantage of ASCA comparing to SBMD? When should I use ASCA and when SBMD? How do they empirically different? \n\nThen I feel some wording can be improved. For example, \"while requiring computation training \u2026\u201d,  \u201c\u2026which may be of independent interest\u201d.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2044/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2044/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["khodak@cmu.edu", "me@liamcli.com", "ninamf@cs.cmu.edu", "talwalkar@cmu.edu"], "title": "On Weight-Sharing and Bilevel Optimization in Architecture Search", "authors": ["Mikhail Khodak", "Liam Li", "Maria-Florina Balcan", "Ameet Talwalkar"], "pdf": "/pdf/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "TL;DR": "An analysis of the learning and optimization structures of architecture search in neural networks and beyond.", "abstract": "Weight-sharing\u2014the simultaneous optimization of multiple neural networks using the same parameters\u2014has emerged as a key component of state-of-the-art neural architecture search. However, its success is poorly understood and often found to be surprising. We argue that, rather than just being an optimization trick, the weight-sharing approach is induced by the relaxation of a structured hypothesis space, and introduces new algorithmic and theoretical challenges as well as applications beyond neural architecture search. Algorithmically, we show how the geometry of ERM for weight-sharing requires greater care when designing gradient- based minimization methods and apply tools from non-convex non-Euclidean optimization to give general-purpose algorithms that adapt to the underlying structure. We further analyze the learning-theoretic behavior of the bilevel optimization solved by practical weight-sharing methods. Next, using kernel configuration and NLP feature selection as case studies, we demonstrate how weight-sharing applies to the architecture search generalization of NAS and effectively optimizes the resulting bilevel objective. Finally, we use our optimization analysis to develop a simple exponentiated gradient method for NAS that aligns with the underlying optimization geometry and matches state-of-the-art approaches on CIFAR-10.", "keywords": ["neural architecture search", "weight-sharing", "bilevel optimization", "non-convex optimization", "hyperparameter optimization", "model selection"], "paperhash": "khodak|on_weightsharing_and_bilevel_optimization_in_architecture_search", "original_pdf": "/attachment/edea8b2b5b1ba3dadca4ca28d087178a5bfbf57f.pdf", "_bibtex": "@misc{\nkhodak2020on,\ntitle={On Weight-Sharing and Bilevel Optimization in Architecture Search},\nauthor={Mikhail Khodak and Liam Li and Maria-Florina Balcan and Ameet Talwalkar},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgRCyHFDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgRCyHFDr", "replyto": "HJgRCyHFDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2044/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575848680599, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2044/Reviewers"], "noninvitees": [], "tcdate": 1570237728577, "tmdate": 1575848680610, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2044/-/Official_Review"}}}], "count": 6}