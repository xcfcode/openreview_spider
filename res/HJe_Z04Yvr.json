{"notes": [{"id": "HJe_Z04Yvr", "original": "ByxSeIEdwr", "number": 970, "cdate": 1569439231653, "ddate": null, "tcdate": 1569439231653, "tmdate": 1583912042548, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "oT72q94FQy", "original": null, "number": 1, "cdate": 1576798711095, "ddate": null, "tcdate": 1576798711095, "tmdate": 1576800925250, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper offers an innovative approach to adjusting style transfer parameters.\nThe reviewers were consistent, and all recommend acceptance.  I concur. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712525, "tmdate": 1576800261924, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper970/-/Decision"}}}, {"id": "rJgU0j9sjB", "original": null, "number": 4, "cdate": 1573788622067, "ddate": null, "tcdate": 1573788622067, "tmdate": 1573788935398, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "B1xU8is7qS", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment", "content": {"title": "Addressing Blind Review #1", "comment": "\nWe thank the reviewer for their helpful and comprehensive review. We updated the paper the address the main raised issues and clarify the questions. Please find the detailed answers below.\n\n==============================\n\n[Q] I'd highly encourage the authors to consolidate some of the early parts of the paper and greatly expand the technical section. For example, the first two paragraphs in section 4 can easily go into the introduction to avoid having to motivate the method twice.\n\n[A]  Thank you for the great suggestion. We reformatted the paper by merging the suggested paragraph with the introduction as well as merging the background with proposed method. We also added more details regarding the technical aspects of our proposed method. This improves the structure of the paper and provides a more coherent and consistent story. The technical details of the paper are easier to read and understand.\n\n=====================================\n\n[Q] Figure reference bottom page 2 broken (??)\n[A] Fixed in the updated version. Thanks for pointing out."}, "signatures": ["ICLR.cc/2020/Conference/Paper970/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_Z04Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper970/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper970/Authors|ICLR.cc/2020/Conference/Paper970/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163293, "tmdate": 1576860550695, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment"}}}, {"id": "HygJNiqsoH", "original": null, "number": 3, "cdate": 1573788455420, "ddate": null, "tcdate": 1573788455420, "tmdate": 1573788920522, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "ByeD9kIN5S", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment", "content": {"title": "Addressing Blind Review #2", "comment": "\nWe thank the reviewer for their helpful and comprehensive review. We updated the paper the address the main raised issues and clarify the questions. Please find the detailed answers below.\n\n==============================\n\n[Q] I am not sure to understand why playing on instance normalization weights turns to be effective for learning an equivalent of a reduction of per-layer loss. I would have liked more motivation and intuition behind it.\n\n[A] We thank the reviewer for asking clarification questions. We updated Section 3.2 (proposed method) of the paper to provide more clarification. In conditional instance normalization rather than learning the Gamma and Beta parameters of batch normalization layer, we generate them as a function of some input. Although these parameters are a tiny fraction of model\u2019s parameters, they are quite effective in changing the outputs (check Perez 2018 and Dumoulin 2016 for more details). In other words, these parameters combined with non-linearities can manipulate feature maps in numerous ways including scaling, negating, filtering and selective thresholding. We have independent Gamma and Beta parameters for each feature map which gives the model an exponential number of ways (with respect to the number of feature maps) to affect the feature representation. Previous works (e.g. Dumoulin 2016) have shown that these parameters are very effective in generating models such as style transfer; where we can generate images with different styles by changing only these parameters. That was our main intuition for choosing conditional instance normalization for generating adjustable stylized images. \n\n========================\n\n[Q] it is unclear how the specific approach of learning instance normalization weights extends beyond style transfer.\n\n[A] Exploring this idea beyond style transfer is for sure one of our research projects. Instance normalization in particular has been used mostly for style transfer in previous works. However, in theory, it should be possible to condition any predictive or generative model on a few of its own hyperparameter (essentially any parameter that changes the generated output and is available at the training time), the same way that we conditioned our model on loss weights. This should allow for adjustment of these hyperparameters after the training (which comes at the cost of a more complex model and therefore more computation). There is already some qualitative evidence that the same method can be applied to other areas but unfortunately they are not comprehensive enough to be published as a scientific paper, yet :(, and requires more exploration. \n\n========================\n\n[Q] I found Figure 7 important because it guarantees learning has the desired effect. However, there is a bit a lack of baseline/topline: how do the true losses decrease when their weights increase?\n\n[A] We would like to thank the reviewer for raising this point which we %100 agree with. Unfortunately, adding a proper baseline requires training the base model for hundreds of times which was out of the time budget of the rebuttal, but this is surely an addition which we will consider for the final version of the paper. Nevertheless, it is worth mentioning that our main goal from this visualization was to demonstrate that the value of loss decreases, as expected, by adjusting the input parameters. That is why the y-axis is normalized. However, it sounds completely reasonable to draw the same graph for the base model by retraining it multiple times. \n\n========================\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper970/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_Z04Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper970/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper970/Authors|ICLR.cc/2020/Conference/Paper970/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163293, "tmdate": 1576860550695, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment"}}}, {"id": "HyxqK55sjB", "original": null, "number": 2, "cdate": 1573788289726, "ddate": null, "tcdate": 1573788289726, "tmdate": 1573788904193, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "HJgiTy_GsB", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment", "content": {"title": "Addressing Blind Review #3 ", "comment": "\nWe thank the reviewer for their helpful and comprehensive review. We updated the paper the address the main raised issues and clarify the questions. Please find the detailed answers below.\n\n==============================\n\n[Q] The \"adjust loss weights\" are able to control the synthesized output, but does each of them have a particular meaning? For example, if I want to generate other outputs more realistic, colorful, or in other styles, is it possible for the users to (roughly) know which parameter should be adjust, and how? \n[Q] Following the above question, I suggest the authors to provide more qualitative results with different \"loss weights\", and give some detailed explanations.\n\n[A]\nWe would like to thank the reviewer for asking clarification questions. We updated the caption of Figure 3 to make this more clear. In short, there is a one to one correspondence between each parameter and the weight of each loss term which means, adjusting each parameter is similar to adjusting the weight of corresponding loss term. Since we are using VGG layers for computing our loss function, lower level attributes corresponds to lower level VGG features (such as lines and curves) while higher level attributes correspond to higher level VGG features (such as shapes).This can also be explained by the increasing receptive field sizes as well as feature complexity along the processing hierarchy of VGG (check Gatys 2016 for more details).  Adjusting color and other more high level features requires defining a separate loss for each one of these features at the loss layer. \n\nWe updated the caption of Figure 3 to make this more clear. More examples can be found in Figure 11. For more examples, we highly recommend visiting the project\u2019s website at https://goo.gl/PVWQ9K which contains numerous results both interactive and non-interactive. \n\n=======================\n[Q] Some typos and missing links could be corrected.\n[A] Thanks for pointing out the problems. We fixed the missing links and some other typos in the updated version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper970/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe_Z04Yvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper970/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper970/Authors|ICLR.cc/2020/Conference/Paper970/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504163293, "tmdate": 1576860550695, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper970/Authors", "ICLR.cc/2020/Conference/Paper970/Reviewers", "ICLR.cc/2020/Conference/Paper970/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Comment"}}}, {"id": "HJgiTy_GsB", "original": null, "number": 3, "cdate": 1573187523199, "ddate": null, "tcdate": 1573187523199, "tmdate": 1573187523199, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper proposed a generative model for image style transfer in real time. In particular, comparing to the existing work, the proposed method is able to generate a series of transferred images instead of one, and more importantly, users can adjust different parameters without re-training the network to control over the synthesized output. The proposed method was evaluated on publicly-available datasets, and achieved convincing experimental results.\n\nPros:\n- The problems that the paper tried to solve is interesting and important. I think the proposed model could make some contributions to related research communities.\n- The idea of learning and incorporating \"adjustable loss weights\" is interesting and reasonable.\n- The paper is well written and easy to understand.\n- Experimental results appear to be promising.\n\nCons:\n- The \"adjust loss weights\" are able to control the synthesized output, but does each of them have a particular meaning? For example, if I want to generate other outputs more realistic, colorful, or in other styles, is it possible for the users to (roughly) know which parameter should be adjust, and how?\n- Following the above question, I suggest the authors to provide more qualitative results with different \"loss weights\", and give some detailed explanations.\n- Some typos and missing links could be corrected.\n\nI think the proposed approach is reasonable and and experimental results are convincing. I'm not an expert in this area, and I'm not sure about the method's novelty. If other reviewers find other existing work that proposed very similar ideas, then this would have an impact on my recommendation.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916667387, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper970/Reviewers"], "noninvitees": [], "tcdate": 1570237744294, "tmdate": 1575916667400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Review"}}}, {"id": "B1xU8is7qS", "original": null, "number": 1, "cdate": 1572219726018, "ddate": null, "tcdate": 1572219726018, "tmdate": 1572972528993, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Strengths:\n + Interesting problem\n + Good results\nAreas of improvement:\n - Structure of paper\n - Main technical section (4.1) very hard to understand.\n\nThe paper works on a very interesting and important problem. To the best of my knowledge there is no real time style transfer method that can adjust certain style parameters after it has been trained. The paper addresses this by parametrizing the normalization layers in the style transfer model, and training with a parametrized loss function. The results look compelling both in the paper, and the supplemental webpage.\n\nThe main area of improvement is the structure of the paper itself. The paper spends 4 pages on introduction, motivation and background. This unfortunately only leaves less than half a page for the main technical section. This main technical section is very dense, technical choices poorly motivated, and the section is impossible to understand in a single read (I had to read it 3-4 times). I'd highly encourage the authors to consolidate some of the early parts of the paper and greatly expand the technical section. For example, the first two paragraphs in section 4 can easily go into the introduction to avoid having to motivate the method twice. The main technical section should be motivated better. I had to constantly jump forth and back between background and technical section to understand where the technical changes are applied. It would help to have a more consistent story.\n\n\nMinor:\n * Figure reference bottom page 2 broken (??)"}, "signatures": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916667387, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper970/Reviewers"], "noninvitees": [], "tcdate": 1570237744294, "tmdate": 1575916667400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Review"}}}, {"id": "ByeD9kIN5S", "original": null, "number": 2, "cdate": 1572261774620, "ddate": null, "tcdate": 1572261774620, "tmdate": 1572972528949, "tddate": null, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "invitation": "ICLR.cc/2020/Conference/Paper970/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an approach for style transfer with controlable parameters. The controllable parameters correspond to the weights associated to \"style losses\" or ordinary style transfer models (distance between gram matrices of generated vs style image at specific layers of a network). The authors propose to learn a single architecture that takes these parameters as input to generate an image that resembles what would be generated by optimizing directly on these parameters. Examples of transfer and of the effect of these parameters are given. A quantitative evaluation shows that the effect of changing the parameters of the new network has the effect of reducing the loss at the desired layers.\n\nOverall, I found the idea of learning controllable parameters interesting. The technical contribution is to show that learning the weights of instance normalization as a function of the hyperparameters actually is satisfactory. To be honest, I am not sure to understand why playing on instance normalization weights turns to be effective for learning an equivalent of a reduction of per-layer loss. I would have liked more motivation and intuition behind it.\n\nThe paper seems overall a bit incremental with respect to prior work on style transfer. While adjustable parameters could have application in more general tasks of image generation (for instance, in the line of work on disentangled representations), it is unclear how the specific approach of learning instance normalization weights extends beyond style transfer. As such, my feeling is that the paper is borderline.\n\nother comments:\n- I found Figure 7 important because it guarantees learning has the desired effect. However, there is a bit a lack of baseline/topline: how do the true losses decrease when their weights increase?"}, "signatures": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper970/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@uiuc.edu", "golnazg@google.com"], "keywords": ["Image Style Transfer", "Deep Learning"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/a6a7059cf21d7328b44c35109b916904bc2828b5.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer", "code": "https://goo.gl/PVWQ9K", "_bibtex": "@inproceedings{\nBabaeizadeh2020Adjustable,\ntitle={Adjustable Real-time Style Transfer},\nauthor={Mohammad Babaeizadeh and Golnaz Ghiasi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe_Z04Yvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/808c180f6ae5cbdd2ef138effd65cda3c524b546.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe_Z04Yvr", "replyto": "HJe_Z04Yvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper970/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575916667387, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper970/Reviewers"], "noninvitees": [], "tcdate": 1570237744294, "tmdate": 1575916667400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper970/-/Official_Review"}}}], "count": 8}