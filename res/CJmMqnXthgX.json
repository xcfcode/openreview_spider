{"notes": [{"id": "CJmMqnXthgX", "original": "dwZVkI-R69", "number": 3111, "cdate": 1601308345069, "ddate": null, "tcdate": 1601308345069, "tmdate": 1614985645488, "tddate": null, "forum": "CJmMqnXthgX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "L8MLnzfABY", "original": null, "number": 1, "cdate": 1610040516662, "ddate": null, "tcdate": 1610040516662, "tmdate": 1610474124904, "tddate": null, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "invitation": "ICLR.cc/2021/Conference/Paper3111/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers liked the direction of the paper but unanimously agree that, in its current version, it is not strong enough to justify publication at ICLR. There was no rebuttal from the authors to consider. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040516649, "tmdate": 1610474124888, "id": "ICLR.cc/2021/Conference/Paper3111/-/Decision"}}}, {"id": "TDOBXcFjlsu", "original": null, "number": 1, "cdate": 1603136409193, "ddate": null, "tcdate": 1603136409193, "tmdate": 1605024065866, "tddate": null, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "invitation": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review", "content": {"title": "Empirical study on the expressiveness of graph embeddings", "review": "The paper deals with supervised graph classification comparing GNN and graph kernel approaches. The authors define an intractable graph similarity functions, which boils down to a normalized graph edit distance. The authors then empirically study how well well-known graph kernel and GNN architectures align within their predictions with this distance. To that, the authors generate small-scale, random graph datasets consisting of various types of graphs.\n\nThe authors find out that graph kernels still seem to be competitive compared to GNNs, and in some settings, GNNs do not perform better than trivial baselines.\n\nPro:\n- Easy to read \n- Cover of related work is good\n\nCons:\n- Most results are already known, and have been found in previous studies\n- Not much insight is given, and no methodological contribution (How can we overcome limitations of current models?)\n- Generation of synthetic dataset seems arbitrary, no motivation given. Extremely small. \n\nRemarks:\n- Please give more motivation for dataset generation and details on generation\n- You might consider citing the works of Andreas Loukas on the limits of GNNs\n- p. 2. \"simialrity\" -> similarity\n- p. 3. please give a proof/citation that shows that solving (1) is indeed not tractable. Just stating number of permutations as a reason is not enough.\n- p. 4. The GR kernel can be trivially extended to work with labeled graphs, moreover RW kernel can also be extended to work with attributed graphs\n- p. 4. GraphHopper kernel is not SOTA\n\nQuestions:\n- Why should the similarity functions of (1) be suitable for all data distributions? E.g., on some data distributions local properties might be more important.\n- What is the motivation behind first training on IMDB-BINARY? This seems arbitrary.\n- Why did you fix GNN hyperparameters? How were the hyperparameters for the kernels chosen?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3111/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3111/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082046, "tmdate": 1606915804791, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review"}}}, {"id": "58pQniYfI-9", "original": null, "number": 2, "cdate": 1603853891156, "ddate": null, "tcdate": 1603853891156, "tmdate": 1605024065789, "tddate": null, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "invitation": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review", "content": {"title": "A very interesting empirical study, but very hard to interpret.", "review": "Summary: This paper provides an intensive empirical study on the expressiveness of graph kernels and graph neural networks. It defines an exact but computationally demanding similarity of graphs defined over all possible permutations for adjacency matrices, eq (1), and generate a synthetic dataset of graphs to the scale (up to 9 nodes) where we can compute this quantity. The nomalized kernel value of k_ij of graph kernels and the cosine similarities of GNN-embedded vectors are compared to this exact similarity eq(1) in terms of its correlation coefficients and mean squared errors over carefully chosen graph kernels and graph neural networks in the context of research about the expressiveness. The empirical results are presented when node features are all the same single values (1 or 1.0), and when node features are two values of the degree and the number of triangles in which it participates to. Also, the paper discussed several concrete examples that have a large disagreement on the similarity against the exact similarity eq(1).\n\nComments: This is a very interesting empirical study including not only GNNs but also graph kernels in the scope, but at the same time, it would be still very hard to interpret those presented results. This study is motivated by many existing papers on the GNN expressivenss, and there should be some explicit efforts (or discussions, at least) to connect and relate these empirical results to what we already know on this theme, for example, isomorphism, WL-test relationship, hard-to-discriminate examples, etc.\n\n- The most confusing points would be the disagreement between the correlation measures and the MSE measures. What can we conclude or learn from these results where graph kernels have lower MSE but also relatively lower correlation than GNNs? The paper even lacks any concluding remarks or conclusion section. (by the way, the caption of Fig.6 should be corrected)\n\n- All of the results are based on the criteria of eq (1), but whether this purely adjacency-matrix based similarity would be worth characterizing the expressiveness aspects of GKs and GNNs would be a bit unconvincing. In particular, we know WL-kernel or several GNNs would have a same level of expressiveness as WL tests. But then, why graph pairs in Figure 6 are hard to get its similarities...? A simple WL test seems enough to discriminate these pairs. Are there any rational explanations for this?\n\n- The pre-training (?) over IMDB-BINARY is also not well motivated, and would lack any context. It's from movie reviews in Internet Movie Database (IMDB). How can we expect any information useful for characterizing the synthetic dataset to be learned?\n\n- Also, the worst case analysis over sum, mean, max aggregators would be also misleading. As seen in the GIN paper (Xu et al, ICLR2019), the expressiveness study suggested that we need sum aggregators (rather than mean or max) for the purpose like the paper's synthetic dataset, didn't it?\n\n- In the context of the expressiveness study, it would be quite of great interest to scrutinize how the exact graph topology relates to the graph kernels or graph neural networks even for very small graphs. But in the practical context, the graph topology is only part of the information to generate a good vector representations of nodes or graphs for downstream tasks. Consider examples like the MNIST graph (28x28 2D grid graph) in the ChebNet paper, and how GNNs can use pixel values representing hand-written characters over the grid.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3111/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3111/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082046, "tmdate": 1606915804791, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review"}}}, {"id": "qW37RKVPV-Y", "original": null, "number": 3, "cdate": 1603889389790, "ddate": null, "tcdate": 1603889389790, "tmdate": 1605024065718, "tddate": null, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "invitation": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review", "content": {"title": "Interesting analysis but with strong limitations", "review": "This paper proposes to study the expressiveness of some graph kernels and graph neural networks. To this aim, the authors build a synthetic dataset of graphs and compute the pairwise similarities, which are compared to an oracle function supposed to quantify how much two graphs are isomorphic.\n\nThe objective pursued in this paper seems to me to be interesting and useful for practitioners but the manuscript has important limitations.\n\n\n1- The oracle similarity introduced in the paper has good properties, that one can expect from a similarity function. However, taking this function as a reference oracle in the whole sequel is not properly justified. How is this similarity function canonical? It would be interesting to cite some references on this question.\n\nFor example, does the reference oracle give better accuracy scores on some classification problems than the kernels and neural networks used in the paper? This would be an important indication to justify the choice of this reference.\n\nSection 3: Even if I agree that the number of permutation matrices is of order n!, it does not prove that the optimization problem is intractable. There exist other algorithms than exhaustive search.\n\n\n2- The synthetic dataset is both small (only 191 graphs) and with a small variety of graphs (between 2 and 9 vertices, between 1 and 36 edges). Under these conditions, isn't it difficult to draw solid conclusions?\n\n\n3- I am not sure that we can expect from a GNN to evaluate accurately the similarity between two graphs from the synthetic dataset at stake, if it was trained on a very different classification dataset?\n\nI have another concern with the fact that some kernels and neural networks capture how the attributes are connected to each other, and do not really catch the topology of an unannotated graph. In this context, giving the same attribute value to all nodes as proposed on page 6 just to be able to use kernels designed for annotated graphs seems strange.\n\n\nFor all these reasons, I believe that the empirical study presented in this paper is not strong enough to justify its publication in this conference.\n\n\nTypos\npage 2: simiarlity\npage 2: expessiveness", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3111/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3111/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082046, "tmdate": 1606915804791, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review"}}}, {"id": "NNEc3gfl7Ak", "original": null, "number": 4, "cdate": 1603961199988, "ddate": null, "tcdate": 1603961199988, "tmdate": 1605024065655, "tddate": null, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "invitation": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review", "content": {"title": "Empirical expressivity of graph kernels and graph neural networks", "review": "The paper is an empirical study on the expressive power of graph kernels (GKs) and graph neural networks (GNNs). The paper considers the question whether GKs and GNNs can approximate a fixed graph similarity. The chosen graph similarity is maximal when graphs are isomorphic. Its computation is intractable for most graphs (complexity in $n!$). Experiments are done on very small graphs (at most 9 nodes). Experimental results are provided without any clear conclusion.\n\nOne motivation of the paper is to complement many existing studies on the expressive power of GNNs. As most studies consider graph isomorphism testing, the present paper considers graph similarities. But the paper considers an intractable graph similarity related to the graph isomorphism property because the similarity is maximal when the graphs are isomorphic. Therefore it should be made clear what could be the benefit of the proposed approach. Moreover I am quite concerned with the following questions: why should graph kernels correspond to the chosen graph similarity ? Why should the dot product between graph representations at the penultimate layer correspond to the chosen graph similarity ? Experiments are done on very small graphs and the conclusions are not convincing enough. Therefore, in my opinion, the paper is not ready for publication.\n\nDetailed comments.\n\nIntroduction. The notion of graph similarity should be made more precise. Many graph similarities could be defined for graphs, some of them only based on the graph structure and others using node or edge features. Here, the paper only considers a structural graph similarity related to graph isomorphism. It is not clear to me whether GKs or GNNs should be able to compute this so-called \"true similarity between graphs\". It is not clear to me how the ability to compute such a graph similarity is related to the graph isomorphism property.\n\nRelated work. Please make precise the relation between this section and Appendix A.1\n\nSection 3. The choice of the similarity measure should be justified. Other graph similarities have been defined in the scientific literature. Some graph similarities use the graph structure and node features. Please make precise the objective of Appendix A.2\n\nSection 4. In Section 4.5.1, a second experiment is considered where \"the GNNs are first trained on the IMDB-BINARY dataset\". Please explain how are trained the GNNs in the first experiment and in the second experiment. As such I do not see why the dot product between graph representations at the penultimate layer should correspond to the chosen graph similarity. The GNNs could be trained to compute the chosen similarity. I do not understand why this has not been considered.\n\nSection 4. It is not easy to draw conclusions of the experimental results. \n\nThe paper does not have a conclusion.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3111/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3111/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks", "authorids": ["~Giannis_Nikolentzos1", "george.panagopoulos@polytechnique.edu", "~Michalis_Vazirgiannis1"], "authors": ["Giannis Nikolentzos", "George Panagopoulos", "Michalis Vazirgiannis"], "keywords": [], "abstract": "Graph neural networks and graph kernels have achieved great success in solving machine learning problems on graphs. Recently, there has been considerable interest in determining the expressive power mainly of graph neural networks and of graph kernels, to a lesser extent.  Most studies have focused on the ability of these approaches to distinguish non-isomorphic graphs or to identify specific graph properties. However, there is often a need for algorithms whose produced graph representations can accurately capture similarity/distance of graphs. This paper studies the expressive power of graph neural networks and graph kernels from an empirical perspective. Specifically, we compare the graph representations and similarities produced by these algorithms against those generated by a well-accepted, but intractable graph similarity function. We also investigate the impact of node attributes on the performance of the different models and kernels. Our results reveal interesting findings. For instance, we find that theoretically more powerful models do not necessarily yield higher-quality representations, while graph kernels are shown to be very competitive with graph neural networks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nikolentzos|an_empirical_study_of_the_expressiveness_of_graph_kernels_and_graph_neural_networks", "pdf": "/pdf/53bfc7e9bfcd7a05fe3b6e52cfb07f417243c568.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jcdOhyZed2", "_bibtex": "@misc{\nnikolentzos2021an,\ntitle={An Empirical Study of the Expressiveness of Graph Kernels and Graph Neural Networks},\nauthor={Giannis Nikolentzos and George Panagopoulos and Michalis Vazirgiannis},\nyear={2021},\nurl={https://openreview.net/forum?id=CJmMqnXthgX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "CJmMqnXthgX", "replyto": "CJmMqnXthgX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3111/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082046, "tmdate": 1606915804791, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3111/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3111/-/Official_Review"}}}], "count": 6}