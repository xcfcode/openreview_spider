{"notes": [{"id": "rk4Qso0cKm", "original": "HJgGbfk5tQ", "number": 610, "cdate": 1538087835260, "ddate": null, "tcdate": 1538087835260, "tmdate": 1556947787283, "tddate": null, "forum": "rk4Qso0cKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xsyihZe4", "original": null, "number": 1, "cdate": 1544829666664, "ddate": null, "tcdate": 1544829666664, "tmdate": 1545354530597, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper610/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353154306, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353154306}}}, {"id": "SJeDntQ3k4", "original": null, "number": 11, "cdate": 1544464815198, "ddate": null, "tcdate": 1544464815198, "tmdate": 1544464884965, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "HkgXvWptJN", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Thanks for introducing your work!", "comment": "Thank you very much for introducing your recent paper on this topic! Since the paper is available after the ICLR submission deadline, we were not aware of this work. We will include some discussions and comparisons in our paper:  \n- Based on our understanding, although both papers use Bayesian method to defense, the algorithms are quite different: your algorithm contains two separate SGLD sampling procedures to sample both adversarial samples and model weights, while we do not sample the adversarial samples. Instead, we integrate the adversarial training process into a single min-max optimization problem. \n- Our method (in the current form) is using variational Bayes and it makes adversarial training process much more efficient. In fact, our algorithm has time complexity similar to the original adversarial training. This can also be observed from experimental results: we are able to scale to complex datasets like CIFAR or even ImageNet-143. We are curious about how your algorithm perform under such situation and will conduct some comparisons. \n- Lastly, it seems that your paper/code are publicly available after October 26 and our submission is on September 27, so we couldn\u2019t include the comparison/discussion in our submission. But we will definitely add discussions/comparisons into our final version. "}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "HkgXvWptJN", "original": null, "number": 3, "cdate": 1544307034979, "ddate": null, "tcdate": 1544307034979, "tmdate": 1544307034979, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "content": {"comment": "Our NeurIPS2018 work \"Bayesian Adversarial Learning\" also approaches the adversarial training from a Bayesian perspective, where MCMC is employed for sampling both adversarial examples and the parameters of the classifier network. Bayesian Adversarial Learning is a general framework for improving robustness of neural network to the adversarial examples. One special case of our framework is Bayesian Neural Network combined with adversarial training, when the \"point\" estimate of adversarial examples is used. Therefore, I think it might be interesting to have a discussion about our work. \n\nN. Ye and Z. Zhu. \"Bayesian Adversarial Learning\" NeurIPS2018. ", "title": "It might be interesting to have a discussion about our NeurIPS2018 work \"Bayesian Adversarial Learning\""}, "signatures": ["~Zhanxing_Zhu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zhanxing_Zhu1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311795109, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rk4Qso0cKm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311795109}}}, {"id": "SyeHI_C72X", "original": null, "number": 3, "cdate": 1540773965240, "ddate": null, "tcdate": 1540773965240, "tmdate": 1543319922990, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "content": {"title": "Interesting contribution", "review": "After feedback: I would like to thank the authors for careful revision of the paper and answering and addressing most of my concerns. From the initial submission my main concern was clarity and now the paper looks much more clearer. \n\nI believe this is a strong paper and it represents an interesting contribution for the community.\n\nStill things to fix:\na) a dataset used in 4.2 is not stated\nb) missing articles, for example, p.5 \".In practice, however, we need a weaker regularization for A small dataset or A large model\"\nc) upper case at the beginning of a sentence after question: p.8 \"Is our Adv-BNN model susceptible to transfer attack? we answer\" - \"we\" -> \"We\"\n====================================================================================\n\nThe paper proposes a Bayesian neural network with adversarial training as an approach for defence against adversarial attacks. \n\nMain pro:\nIt is an interesting and reasonable idea for defence against adversarial attacks to combine adversarial training and randomness in a NN (bringing randomness into a new level in the form of a BNN), which is shown to outperform both adversarial training and random NN alone.\n\nMain con:\nClarity. The paper does not crucially lack clarity but some claims, general organisation of the paper and style of quite a few sentences can be largely improved.\n\nIn general, the paper is sound,  the main idea appears to be novel and the paper addresses the very important and relevant problem in deep learning such as defence against adversarial attacks. Writing and general presentation can be improved especially regarding Bayesian neural networks, where some clarity issues almost become quality issues. Style of some sentences can be tuned to more formal.\n\nIn details:\n1. The organisation of Section 1.1 can be improved: a general concept \"Attack\" and specific example \"PGD Attack\" are on the same level of representation, while it seems more logical that \"PGD Attack\" should be a subsection of \"Attack\". And while there is a paragraph \"Attack\" there is no paragraph \"Defence\" but rather only specific examples\n2. The claim \u201cwe can either sample w \u223c p(w|x, y) efficiently without knowing the closed-form formula through the method known as Stochastic Gradient Langevin Dynamics (SGLD) (Welling & Teh, 2011)\u201d sounds like SGLD is the only sampling method for BNN, which is not true, see, e.g., Hamiltonian Monte Carlo (Neal\u2019s PhD thesis 1994). It is better to be formulated as \"through, for example, the method ...\"\n3. Issues regarding eq. (7):\n   a) Why there is an expectation over (x, y)? There should be the joint probability of all (x, y) in the evidence.\n   b) Could the authors add more details about why it is the ELBO given that it is unconventional with adversarial examples added?\n   c)  It seems that it should be log p(y | x^{adv}, \\omega) rather than p(x^{adv}, y | \\omega). \n   d) If the authors assume noise component, i.e., y = f(x; \\omega) + \\epsilon, then they do not need to have a compulsory Softmax layer in their network, which is important, for example, for regression models. Then the claim \u201cour Adv-BNN method trains an arbitrary Bayesian neural network\u201d would be more justified\n4. It would make the paper more self-contained if the Bayes by Backprop algorithm would be described in more details (space can be taken from the BNN introduction). And it seems to be a typo that it is Bayes by Backprop rather than Bayes by Prop\n5. There are missing citations in the text:\n    a) no models from NIPS 2017 Adversarial Attack and Defence competition (Kurakin et al. 2018) are mentioned\n    b) citation to justify the claim \u201cC&W attack and PGD attack (mentioned below) have been recognized as\ntwo state-of-the-art white-box attacks for image classification task\u201d\n    c) \u201cwe can approximate the true posterior p(w|x, y) by a parametric distribution q_\u03b8(w), where the unknown parameter \u03b8 is estimated by minimizing KL(q_\u03b8(w) || p(w|x, y)) over \u03b8\u201d - there are a variety of works in approximate inference in BNN, it would be better to cite some of them here\n    d) citation to justify the claim \"although in these cases the KL-divergence of prior and posterior is hard to compute and practically we replace it with the Monte-Carlo estimator, which has higher variance, resulting in slower convergence rate.\u201d\n6. The goal and result interpretation of the correlation experiment is not very clear\n7. From the presentation of Figure 4 it is unclear that this is a distribution of standard deviations of approximated posterior.\n8. \u201cTo sum up, our Adv-BNN method trains an arbitrary Bayesian neural network with the adversarial examples of the same model\u201d \u2013 unclear which same model is meant\n9. \"Among them, there are two lines of work showing effective results on medium-sized convolutional networks (e.g., CIFAR-10)\" - from this sentence it looks like CIFAR-10 is a network rather than a dataset\n10. In \"Notations\" y introduction is missing\n11. It is better to use other symbol for perturbation rather than \\boldsymbol\\delta since \\delta is already used for the Dirac delta function\n12. \u201cvia tuning the coefficient c in the composite loss function\u201d \u2013 the coefficient c is never introduced\n\nMinor:\n1. There are a few missing articles, for example, in Notations, \u201cIn this paper, we focus on the attack under THE norm constraint\u2026\u201d\n2. Kurakin et al. (2017) is described in the past tense whereas Carlini & Wagner (2017a) is described in the present tense\n3. Inner brackets in eq. (2) are bigger than outer brackets\n4. In eq. (11) $\\delta$ is not bold\n5. In eq. (12) it seems that the second and third terms should have \u201c-\u201d rather than \u201c+\u201d\n6. Footnote in page 6 seems to be incorrectly labelled as 1 instead of 2\n\n\n\n\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "cdate": 1542234420333, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335762989, "tmdate": 1552335762989, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hye80M2JnX", "original": null, "number": 2, "cdate": 1540502221998, "ddate": null, "tcdate": 1540502221998, "tmdate": 1542895827670, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "content": {"title": "An approach that can work well in practice, but not principled", "review": "I have read the feedback and discussed with the authors on my concerns for a few rounds. \n\nThe revision makes much more sense now, especially by removing section 3.3 and replacing it with more related experiments.\n\nI have a doubt on whether the proposed method is principled (see below discussions). The authors responded honestly and came up with some other solution. A principled approach of adversarially training BNNs is still unknown, but I'm glad that the authors are happy to think about this problem. \n\nI have raised the score to 6. I wouldn't mind seeing this paper accepted, and I believe this method as a practical solution will work well for VI-based BNNs. But again, this score \"6\" reflects my opinion that the approach is not principled.\n\n=========================================================\n\nThank you for an interesting read.\n\nThe paper proposes training a Bayesian neural network (BNN) with adversarial training. To the best of my knowledge the idea is new (although from my perspective is quite straight-forward, but see some discussions below). The paper is well written and easy to understand. Experimental results are promising, but I don't understand how the last experiment relates to the main idea, see comments below.\n\nThere are a few issues to be addressed in revision:\n\n1. The paper seems to have ignored many papers in BNN literature on defending adversarial attacks. See e.g. [1][2][3][4] and papers citing them. In fact robustness to adversarial attacks is becoming a standard test case for developing approximate inference on Bayesian neural networks. This means Figure 2 is misleading as in the paper \"BNN\" actually refers to BNN with mean-field variational Gaussian approximations.\n\n2. Carlini and Wagner (2017a) has discussed a CW-based attack that can increase the success rate of attack on (dropout) BNNs, which can be easily transferred to a corresponding PGD version. Essentially the PGD attack tested in the paper does not assume the knowledge of BNN, let alone the adversarial training. This seems to contradict to the pledge in Athalye et al. that the defence method should be tested against an attack that is aware of the defence.\n\n3. I am not exactly sure if equation 7 is the most appropriate way to do adversarial training for BNNs. From a modelling perspective, if we can do Bayesian inference exactly, then after marginalisation of w, the model does NOT assume independence between datapoints. This means if we want to attack the model, then we need to do \n\\min_{||\\delta_x|| < \\gamma} log p(D_adv), \nD_adv = {(x + \\delta_x, y) | (x, y) \\sim \\sim D_tr},\nlog p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw.\nNow the model evidence log p(D_adv) is intractable and you resort to variational lower-bound. But from the above equation we can see the lower bound writes as\n\\min_{||\\delta_x|| < \\gamma} \\max_{q} E_{q} [\\sum_{(x, y) \\sim D_tr} \\log p(y|x + \\delta_x, w) ] - KL[q||p],\nwhich is different from your equation 7. In fact equation 7 is a lower-bound of the above, which means the adversaries are somehow \"weakened\".\n\n4. I am not exactly sure the purpose of section 3.3. True, that variational inference has been used for compressing neural networks, and the experiment in section 3.3 also support this. However, how does network pruning relate to adversarial robustness? I didn't see any discussion on this point. Therefore section 3.3 seems to be irrelevant to the paper.\n\nSome papers on BNN's adversarial robustness:\n[1] Li and Gal. Dropout Inference in Bayesian Neural Networks with Alpha-divergences. ICML 2017\n[2] Feinman et al. Detecting Adversarial Samples from Artifacts. arXiv:1703.00410\n[3] Louizos and Welling. Multiplicative Normalizing Flows for Variational Bayesian Neural Networks. ICML 2017 \n[4] Smith and Gal. Understanding Measures of Uncertainty for Adversarial Example Detection. UAI 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "cdate": 1542234420333, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335762989, "tmdate": 1552335762989, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xh1O74AQ", "original": null, "number": 9, "cdate": 1542891491581, "ddate": null, "tcdate": 1542891491581, "tmdate": 1542891491581, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "r1l_tTGfCm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Yes this method sounds more aligned to Bayesian decision theory :)", "comment": "...although you might need some careful derivation to figure out which data to be conditioned on, how many datapoint counts in as observations (so that the uncertainty is well calibrated), etc.\n\nI would encourage you to work on this direction in the future, in order to have a principled method to adversarially train BNNs. The following references might be helpful for reading:\n\nhttp://proceedings.mlr.press/v15/lacoste_julien11a/lacoste_julien11a.pdf\nhttps://arxiv.org/pdf/1805.03901.pdf"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "r1l_tTGfCm", "original": null, "number": 8, "cdate": 1542757760364, "ddate": null, "tcdate": 1542757760364, "tmdate": 1542759080518, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "Bye2N8dbAX", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for introducing this question!  We haven't tried any other inference methods during the implementation stage of this paper, but we think it is possible to extend our method, Please see our responses below: \n\n1. The adversarial dataset D_adv not only depends on the training data D, but also the posterior p(w | D). So our method should be iterative in its nature (find adversarial examples --> inference on D_adv --> find new adversarial examples ...).\n\n2. For general inference methods that posterior is not trained (e.g. by optimization method), we may still find an iterative algorithm, as shown below:\n\nSuppose we have a \"black-box\" algorithm that can do inference on data D, and the posterior is p(w|D), we may return the sample distribution p*(w) by the following iterative algorithm:\n\nInput: original training dataset D\n1. Initialize posterior p0(w) := p(w | D), set loop variable i = 0\n2. Perturb dataset D to get D_adv := {x+eps^* | eps^* = argmin_{ ||eps||<delta } \\int_w p(y | x+eps, w) * p_i(w) dw, forall x\\in D}. We can simulate the integration by sampling from p_i(w)\n3. Run the \"black-box\" inference algorithm on D_adv to get new posterior p_{i+1}(w) := p(w | D_adv)\n4. Set i = i + 1\n5. GoTo step 2 until p_i(w) converges ( to p*(w) ).\n6. return p*(w).\n\nThe above algorithm is a natural extension of Algorithm 1 in the paper, except that here we allow the use of a more general inference method and assume the attack is on the whole dataset instead of a subset. \n\nQ: Does this method still encourage nice properties of BNNs? \n A: We think p*(w) should do the job. After all, both algorithms involve the adversarial game between inference method and the attacker.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "Bye2N8dbAX", "original": null, "number": 7, "cdate": 1542714932360, "ddate": null, "tcdate": 1542714932360, "tmdate": 1542714932360, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "SJeXtbTEpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Is your method a principled way to train BNNs with adversaries?", "comment": "I appreciate your efforts on responding my review and updating your paper. Now the extra experiments look much more relevant to the paper which is good. Still, I would like to discuss with you, on whether your method is a principled way to train BNNs with adversaries.\n\nLet us set aside hyper-parameter optimizations for now and assume we have selected a good prior for the weights w. In your method you only use adversarial inputs as the observation, therefore, the exact posterior is p(w |D_adv), with D_adv containing adversarial inputs crafted on all x \\in D.\n\nNote that if we can draw samples from the exact posterior p(w | D_adv), then in principle BNN requires **no training**, and in prediction time the BNN should be robust to adversarial examples that are crafted in a similar way as D_adv. So in this idealized setting, the adversarial game cannot be played between the adversaries and the exact posterior, because the exact posterior is not obtained by optimization. \n\nApparently in practice we cannot sample from the exact posterior, and VI does introduce optimization methods to approximate p(w | D_adv). I have no problem for optimizing a lower-bound, however, I doubt whether the underlying idea of your approach is principled. In other words, does your idea generalize to other BNN inference methods, e.g. message passing and SG-MCMC? Does your method still encourage nice properties of BNNs, e.g. calibrated uncertainty?\n\nI would like to see a discussion on this topic. Either you need to be more specific and say your method applies to VI-BNN only, or you need to justify why your approach is principled."}, "signatures": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "Hylsz71KpQ", "original": null, "number": 6, "cdate": 1542152978626, "ddate": null, "tcdate": 1542152978626, "tmdate": 1542154202521, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "SklNhuR_67", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Reply", "comment": "Yes, strictly speaking the inequality may not hold for PGD attack, because it is not guaranteed to find the optimal adversarial perturbation. Thanks for your reminding! \n\nAlthough in our experiments on five models (no-defense, BNN, Adv. Training, RSE, Adv-BNN), we did not observe any violation of this inequality (as you can see in Fig. 3, all correlations are within range [0, 1]).\n\nIn the latest revision, we fixed this problem by changing \"correlation\" measure to \"affinity\" measure, and corrected the imprecise sentences.\n\nAgain, thanks for catching this mistake!"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "HyxExoXr6X", "original": null, "number": 4, "cdate": 1541909228186, "ddate": null, "tcdate": 1541909228186, "tmdate": 1542154043354, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Change list", "comment": "Below are the major differences in the revised paper:\n\n1. We removed section 3.3, because we agree with Reviewer 3 that this part is less relevant to the topic. Meanwhile, two new experiments are added to show the effectiveness of our method\n\n2. Shorten introduction part, remove unnecessary background information.\n\n3. Added more details when deriving the ELBO, as well as our main objective function (Eq. 7,8,9)\n\n4. We cited some relevant papers to support some claims, according to the useful suggestions of Reviewer 2. We also improved the organization of Section 1.1\n\n5. We added more details why our adversarial attack algorithm is sound, given the randomness of BNN. This is discussed in Appendix A.\n\n6. We replaced the python code in Algorithm 1 with pseudo code.\n\n7. Motivations for the transfer attack experiment\n\n8. Fixed the imprecise description in Section 4.2, this was noticed by an anonymous reader\n\n9. Fixed many typos.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "SklNhuR_67", "original": null, "number": 2, "cdate": 1542150315580, "ddate": null, "tcdate": 1542150315580, "tmdate": 1542150315580, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "SyggL18wpm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "content": {"comment": "Re: 2, my point was not that you should not assume you know the weights. Rather, PGD is only an approximation to the best attack. If you actually had the worst-case attack then I agree that Acc[B|A] >= Acc[B | B] but given that you are making an approximation this need not hold.\n\nBest,\nSame commenter as above", "title": "response"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311795109, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rk4Qso0cKm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311795109}}}, {"id": "SJeXtbTEpQ", "original": null, "number": 2, "cdate": 1541882235380, "ddate": null, "tcdate": 1541882235380, "tmdate": 1542087508743, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "Hye80M2JnX", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Thanks for your helpful suggestions!", "comment": "Please see the revised paper as well as the change list for details, we believe that the revised paper has already addressed the issues. Below we give more details on that,\n\n1. The references you mentioned are indeed very relevant to our topic, we discussed some of them in the introduction section. However, we still think it does not diminish the main contributions of our paper due to the following reasons:\n\ni) [1] includes one small scale experiment on MNIST dataset, the goal is to show that although the Bayesian NN is still easily \u201ccheated\u201d by adversarial images, the uncertainty of predictions also increases. Meaning the Bayesian NN is aware of the epistemic uncertainty. And the authors explored this nice property in adversarial detection. \n\nSimilar to [1], the experiments in [3] are still small scale (MNIST/CIFAR10), although the paper shows that the Bayesian NN has stronger adversarial robustness than a plain NN, the authors also admit that \u201cadversarial examples are harder to escape and be uncertain about in CIFAR10, due to higher dimension\u201d. In contrast, our proposed AdvBNN has made a huge progress in adversarial robustness: the accuracy under strong adversarial attack algorithm on even more complex, high dimensional datasets is much higher than baselines (including the Bayesian NN).\n\nii) [2] and [4] are both on adversarial detection, while our focus is the adversarial defense, these are similar topics but different scenarios.\n\nYes, perhaps it is not very suitable to call \"BNN with factorized gaussian as approximated posterior\" simply as BNN, because it does not include the previous works on BNN + adversarial attacks. But it is very straightforward to extend our work to include other inference methods.\n\nI think the major contribution of our work is that we show Bayesian neural networks empower the robustness of adversarially trained neural networks. Moreover, we demonstrate that even the most simple approximate inference method can benefit a lot to model robustness, and our method scales easily to large datasets (not just MNIST).\n\n\n2. In fact we already assumed the attacker knows the structure of BNN in our setting (using the same approach in Carlini and Wagner (2017a) and Athalye et al). We briefly mentioned this in Section 3.1 in the initial version, and we have added more details in the revised version (see Appendix). Therefore, as you can see in our Figure 2 that BNN has a very low accuracy under attack in all datasets, which does not contradict to Athalye et al. We also use the same attack (assume the adversary knows every details of model) to test the robustness for the proposed AdvBNN model. Our conclusion is that BNN itself does not help much, but using the proposed framework, one can combine the idea of BNN with adversarial training to achieve much better robustness. \n\nAthalye et al. does not negate the effectiveness of adversarial training, for detailed information, please refer to their Github page: https://github.com/anishathalye/obfuscated-gradients, there is a table comparing the performance of different methods, among them, the adversarial training (Madry et al) has a pretty good accuracy.\n\n\n3. We are not quite sure if we understand your point, do you mean the actual objective function should be\n                                            \\min_{||\\delta_x||} \u2026. \\max_{q}.... \nwhile our objective function is \n                                            \\max_{q} \u2026\u2026 \\min_{||\\delta_x||} \u2026\u2026\nand so you think Eq 7 is an lower bound of your equation?\n\nOur objective function Eq 7 is indeed a lower bound of your proposed equation, this is because we are maximizing the \u201cworst case\u201d evidence lower bound. So the \\max_{q} should be moved to the leftmost position. \n\nIn summary, in training the model, we need to do\n                                          \\max_q \\min_{ ||\\delta_x|| < \\gamma } log p(D_adv),\n                              where log p(D_adv) = \\log \\int \\prod_{(x, y) \\sim D_tr} p(y|x + \\delta_x, w) p(w) dw\nThis can be further simplified to our objective function. \n\nWe have added more details in the revised paper to make it clearer.\n\n\n4. Sorry about the confusion, we also think section 3.3 is diverged from the main topic, in the revised paper, we replaced this experiment with other controlled experiments. We hope these experiments can strengthen our findings."}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "SyggL18wpm", "original": null, "number": 5, "cdate": 1542049608114, "ddate": null, "tcdate": 1542049608114, "tmdate": 1542051738129, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "HJxoBdEI6X", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Thanks for your comments!", "comment": "I am very glad to see your comments, both positive and negative ones.\n\nWe have uploaded the model checkpoints to the GitHub page, sorry we cannot disclose the github link but you may find it easily.\n\nAs to your other comments, here are my thoughts on that:\n\n1. Our deduction is an extremely simplified version to the robust optimization objective. As you can see, we disagree on whether regularizing just at the training points is a good approximation to Lipschitz regularization at the neighborhoods.\n\nTo me, the differences between the two regularizations are just up to higher order terms. After all, recall the PGD adversarial training sets L_inf maximum distortion to ~0.03, which is very small compared to the distance between two different images and the Taylor expansion to low order terms precisely track the original objective. So yes this simplified regularizer leads to \"gradient masking\", but in (Liu & Hsieh, 2018) we see even the neighborhood regularizer makes small curvature very locally, it cannot guarantee a lot to the test set.\n\nSo the question is ---\"when we only have limited number of training samples, how to guarantee a small curvature on the whole data generating distribution? \"\n\nOur claim (not this paper's claim) is that in order to guarantee the robustness on the whole distribution, both point-wise regularization and regularize across neighborhoods may not be sufficient, but of course we can argue that the latter regularization method is a better choice.\n\n\n2. Sorry about the confusion, in fact, the Acc[B | B] denotes the accuracy of attacking model B with model B, both models have the same architecture and *weights*. So technically it belongs to the white-box attack. We see even if the model B performs \"gradient masking\", if we know everything inside the model, we can still easily attack it. That's why Acc[B|A] >= Acc[B | B] is always true, because white-box attack is the strongest.\n\nWe are aware that traditionally, Acc[B | B] should assume we only know the architecture but not weights, the reason we made such adaption is that we want to guarantee the relation above, and therefore a valid correlation measure (0<= \\rho <= 1).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "HJxoBdEI6X", "original": null, "number": 1, "cdate": 1541978179418, "ddate": null, "tcdate": 1541978179418, "tmdate": 1541978179418, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "content": {"comment": "I had a couple comments, some positive and some negative.\n\nOn the positive side, I appreciate that the authors carefully test for convergence of PGD (as in Figure 4) and also perform investigations on the number of models needed in the ensemble. I found both of these results helpful to me and it raised my overall impression of the paper substantially. I also found it admirable that hyperparameter settings and detailed explanations of the attack, defense, etc. were included, which aids in reproducibility. I would be even happier if the authors made their model weights publicly available so that others can test the robustness claims.\n\nIn the other direction, I wanted to raise a few concerns with claims made in the paper, although I don't see these as serious issues (rather a case of uncareful writing).\n\nAt the end of Section 3 the paper claims:\n\"In other words, the adversarial training can be simplified to Lipschitz regularization, and if the\nmodel generalizes, the local Lipschitz value will also be small on the test set. Yet, as (Liu & Hsieh,\n2018) indicates, for complex dataset like CIFAR-10, the local Lipschitz is still very large on test set,\neven though it is controlled on training set.\"\n\nI'm not sure this is a correct take-away. The issue with Lipschitz regularization is not necessarily that it does not generalize to the test set, but that regularizing the Lipschitz constant *only at individual points* is not sufficient; rather, we want the Lipschitz constant to be small across an entire neighborhood of each of the train/test points. Regularizing the pointwise Lipschitz constant tends not to do this and instead tends to lead to \"gradient masking\" where the gradient at a given data point is uninformative due to high curvature near that point. For a stylized illustration of this, see Figure 1 of the following paper from last year's ICLR: https://arxiv.org/abs/1801.09344.\n\n\"Obviously, it is always easier to find adversarial examples through the target model itself, so we have Acc[B|A] \u2265 Acc[B|B] \". This is not true, in fact if a model performs gradient masking often the best way to attack it is via transferring from a similar model that does not have such masking. It certainly does not hold mathematically that Acc[B | A] >= Acc[B | B].", "title": "some comments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311795109, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rk4Qso0cKm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311795109}}}, {"id": "r1xcGYhNTX", "original": null, "number": 1, "cdate": 1541880082058, "ddate": null, "tcdate": 1541880082058, "tmdate": 1541966437046, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "SyeHI_C72X", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Thanks for your helpful suggestions!", "comment": "Please see the revised paper as well as the change list for details, below we address your comments. We find your comments very informative and we absorbed most of them in the new version.\n\n\n1. We revised Section 1.1 following your suggestions. Specifically, we merged the PGD attack into the Attack part, and we also modified the defense part in the same way.\n\n2. Thanks for pointing out this mistake, we agree that we left HMC behind when writing the initial draft, we modified this sentence as suggested.\n\n3. (a) We are indeed meant to it, we changed a lot to eq. (7) in response to the suggestions of all reviewers, I hope the revised version is clearer.\n    (b) We added more details why the new objective function is still an ELBO in the updated version, briefly speaking, we made a lower bound of the original ELBO, and the lower bound of ELBO is still an evidence lower bound. \n    (c) Good point, we modified the expression in the revision, thanks for pointing out.\n    (d) It is a very good suggestion, adding an error term makes our model more general to both regression and classification problems, thanks!\n\n4. We added a brief introduction to Bayes by Backprop. The space is really limited so forgive us if you find this part hands-waving.\n\n5. We added more citations to support our claims\n\n6. We gave the motivation of this experiment in section 4.2. \n    The goal of this experiment is to test the robustness under black-box attack, specifically we answer the question: \u201cHow does the Adv-BNN perform under transfer attack from other models?\u201d and the key finding is our AdvBNN model is also very robust to blackbox attack, no matter which the source model is. Blackbox defense is also a very important task because in reality, attackers may not have access to the target model.\n\n7. We agree with Reviewer 2 that Section 3.3 is not necessary and not quite relevant to the main point of this paper, so we removed this subsection. Instead, we added two other experiments aiming at showing the sample efficiency as well as the robustness of our model.\n\n8 ~ 12. Thanks for pointing out our mistakes, we fixed all the typos and unclear parts as suggested.\n\n\nAbout your minor points\n---------------------------------\n1, 2, 3, 4, 6: Thanks for pointing out our typos! We fixed all of them in the revised paper.\n5: I think Eq. (12) should be the plus sign, because we are doing the Taylor expansion: f(x+\\delta, w) ~ f(x)+\\delta^T \\nabla f(x) + ...\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "S1x4D9aETX", "original": null, "number": 3, "cdate": 1541884508044, "ddate": null, "tcdate": 1541884508044, "tmdate": 1541963893491, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "Byxndvbkh7", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "content": {"title": "Thanks for your helpful suggestions! ", "comment": "We thank the reviewer for valuing our paper and giving informative suggestions, below we address your comments in detail.\n\nMajor Weaknesses:\n1. Thanks for pointing out this mistake, this issue is also noticed by AnonReviewer 2 and we have already fixed it in the revised paper.  And perhaps it will be clearer to think our objective function as an expectation on the original data, rather than on x_adv. Because the new objective function is a lower bound of the original ELBO.\n\n2. The evidence is not calculated on x_adv, but on the original data $x$. So it does not interfere with Jensen's ineq. when deriving the ELBO. I think it will be clearer to see the revised paper, where we give more details regarding the objective function.\n\n3. We have renamed to \"Bayes by Backprop\" in the revised version.\n\nWe agree with you that the local reparamterization trick has much smaller variance during the training time, replacing Bayes-by-Backprop by local reparameterization trick will definitely have a faster convergence. The reason is that we didn\u2019t think very carefully at the implementation stage and somehow \"forgot\" it. Nevertheless, both algorithms should yield similar results and we will definitely try this idea and replace the code base.\n\nWe want to address that our main goal is to combine Bayesian NN with adversarial training, and there are many ways we could do the approximate inference efficiently. Here we only choose a naive approach considering its simplicity and effectiveness.\n\nIn the revised paper, we give readers a reminder that local reparametrization trick should perform better.\n\n\nMinor issues:\n1. The original introduction includes intro, background and related work. We have split it into 2 sections and shortened each of them. We have also added more details in the proposed method section. \n\n2. Thanks for your suggestion, we rewrite the algorithm box to pseudo code in order to make it looks more formal.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620657, "tddate": null, "super": null, "final": null, "reply": {"forum": "rk4Qso0cKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper610/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper610/Authors|ICLR.cc/2019/Conference/Paper610/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers", "ICLR.cc/2019/Conference/Paper610/Authors", "ICLR.cc/2019/Conference/Paper610/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620657}}}, {"id": "Byxndvbkh7", "original": null, "number": 1, "cdate": 1540458356139, "ddate": null, "tcdate": 1540458356139, "tmdate": 1541533845158, "tddate": null, "forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "invitation": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "content": {"title": "A nice paper that bridges adversarial training and Bayesian neural nets", "review": "The paper extends the PGD adversarial training method (Madry et al., 2017) to Bayesian Neural Nets (BNNs). \nThe proposed method defines a generative process that ties the prediction output and the adversarial input \npattern via a set of shared neural net weights. These weights are then assinged a prior and \nthe resultant posterior is approximated by variational inference.\n\nStrength:\n  * The proposed approach is incremental, but anyway novel.\n  * The results are groundbreaking.\n  * There are some technical flaws in the way the method has been presented, \nbut the rest of the paper is very well-written.\n\nMajor Weaknesses:\n\n  * Equation 7 does not seem to be precise. First, the notation p(x_adv, y | w) is severely misleading. If x_adv is also an input, no matter if stochastic or deterministic, the likelihood should read p(y | w, x_adv). Furthermore, if the resultant method is a BNN with an additional expectation on x_adv, the distribution employed on x_adv resulting from the attack generation process should also be written in the form of the related probability distribution (e.g. N(x_adv|x,\\sigma)).\n\n  * Second, the constraint that x_adv should lie within the \\gamma-ball of x has some implications on the validity of\nthe Jensen's inequality, which relates Equation 7 to proper posterior inference.\n\n  * Blundell et al.'s algorithm should be renamed to \"Bayes-by-BACKprop\". This is also an outdated inference technique for quite many scenarios including the one presented in this paper. Why did not the authors benefit from the local reparametrization trick that enjoy much lower estimator variance? There even emerge sampling-free techniques that nullify this variance altogether and provide much more stable training experience.\n\nAnd Some Minor Issues:\n\n  * The introduction part of paper is unnecessarily long and the method part is in turn too thin. As a reader, I would prefer getting deeper into the proposed method instead of reading side material which I can also find in the cited articles.\n\n  * I do symphathize and agree that Python is a dominant language in the ML community. Yet, it is better scientific writing practice to provide language-independent algorithmic findings as pseudo-code instead of native Python.\n\nOverall, this is a solid work with a novel method and very strong experimental findings. Having my grade discounted due to the technical issues I listed above and the limitedness of the algorithmic novelty, I still view it as an accept case.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper610/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network", "abstract": "We present a new algorithm to train a robust neural network against adversarial attacks. \nOur algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. \nInstead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14% accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu, 2017) under PGD attack with 0.035 distortion, and the gap becomes even larger on a subset of ImageNet.", "keywords": [], "authorids": ["xqliu@cs.ucla.edu", "yaoli@ucdavis.edu", "crwu@ucdavis.edu", "chohsieh@cs.ucla.edu"], "authors": ["Xuanqing Liu", "Yao Li", "Chongruo Wu", "Cho-Jui Hsieh"], "TL;DR": "We design an adversarial training method to Bayesian neural networks, showing a much stronger defense to white-box adversarial attacks", "pdf": "/pdf/2ef7dc041cc1292919d9be7aa9b8b70f3c0e6bb0.pdf", "paperhash": "liu|advbnn_improved_adversarial_defense_through_robust_bayesian_neural_network", "_bibtex": "@inproceedings{\nliu2018advbnn,\ntitle={Adv-{BNN}: Improved Adversarial Defense through Robust Bayesian Neural Network},\nauthor={Xuanqing Liu and Yao Li and Chongruo Wu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rk4Qso0cKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper610/Official_Review", "cdate": 1542234420333, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rk4Qso0cKm", "replyto": "rk4Qso0cKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper610/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335762989, "tmdate": 1552335762989, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper610/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 18}