{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124443802, "tcdate": 1518470861671, "number": 287, "cdate": 1518470861671, "id": "HyL0IKJwM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyL0IKJwM", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "tmdate": 1524671067525, "tcdate": 1524671067525, "number": 3, "cdate": 1524671067525, "id": "HkmPMXRhG", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Revised version", "comment": "Thank you for the helpful feedback. We have substantially revised the paper for clarity and tried to more clearly explain when we would expect a benefit from an action-dependent baseline.\n\nA revised and expanded version can be found here: https://arxiv.org/abs/1802.10031"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623560, "id": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "reply": {"replyto": null, "forum": "HyL0IKJwM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623560}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582799250, "tcdate": 1520624214778, "number": 1, "cdate": 1520624214778, "id": "SkkDGDeYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer3"], "content": {"title": "State-action baselines do not help reduce of policy gradients when used for continuous control problems.", "rating": "7: Good paper, accept", "review": "The paper presents an interesting critique of state-action baselines and how such baselines affect the variance of policy gradient estimators. The authors make two main contributions towards establishing efficacy of such baselines.\n\n (i) The first contribution is to decompose the variance of policy gradients into three components, $\\sigma_t, \\sigma_a, \\sigma_s$. On continuous control tasks it is observed that $\\sigama_t$ >> $\\sigma_a$. While using an optimal state-action baseline would improve the \\sigma_a term, the dominant $\\sigma_t$ term still contributes to large variance.\n\n(ii) When using GAE, the $\\sigma_t$ term is reduced. However, the state-action and state based baselines produce similar variance estimates and these variance estimates are larger compared to when using an oracle. \n\n(iii) The second contribution is to point out errors in the open source implementation. The first source of error is that in the implementation of Q-Prop/IPG adaptive normalization is applied to only a few terms which can introduce a bias. Same sample reuse (as done in open source implementations) can lead to biased estimation of policy gradients. After removing the bias in the policy gradients, there is no advantage observed in state-action baselines.\n\nI am not an expert in RL. But with my limited understanding,  I can see that the paper clarifies some important misconceptions. It would have been great, if the authors explained in detail why action/state baselines provide improvement in discrete problems.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582799031, "id": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer2"], "reply": {"forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582799031}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582758420, "tcdate": 1520646017538, "number": 2, "cdate": 1520646017538, "id": "S1ttvhetG", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer1"], "content": {"title": "Interesting paper that decomposes the sources of variance for RL policy gradients, identifies bugs in prior work, and proposes a simple modification that works", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper is well written, and the key contributions are supported by illustrative experiments. I reviewed the arXiv version of the paper (Feb 27 version); a few minor typos still remain (e.g. eqn in 9.2 needs a superscript \\Sigma^a). Interesting paper that decomposes the sources of variance for RL policy gradients, identifies bugs in prior work, and proposes a simple modification that works.\n\nSignificance: Good. Isolates bugs in prior art, cleanly motivates fixes. The specific parametrization of horizon-aware value functions (Section 5) can be better motivated -- from a stylized variance formula for LQG to the \\hat{V} eqn in Section 5 is a big leap! If the parametrization was argued for in a more principled way, I'm sure the paper becomes much more significant (can become the default way for parametrizing value functions).\n\nClarity: Good. There were a few expt details that were not clearly listed (e.g. for figure 2/Appendix 10, what was the policy pi? Is the x-axis measuring the same quantity as Fig 1 (task horizon)? Was pi being updated online as we move along the x-axis?)\n\nQuality: Excellent.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582799031, "id": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer2"], "reply": {"forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582799031}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582734235, "tcdate": 1520667473451, "number": 3, "cdate": 1520667473451, "id": "SJFIsbWtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer2"], "content": {"title": "An interesting observation but with a limited scope", "rating": "6: Marginally above acceptance threshold", "review": "In the context of policy gradient methods, the state-dependent baseline has been used to reduce the variance of the gradient estimate. More recently several papers have suggested the use of state-action-dependent baseline, in order to further reduce the variance. This paper empirically studies the benefit of such a state-action-dependent baseline for various control tasks.\nIts main theoretical contribution is the decomposition of the variance to several components, corresponding to the variance coming from (roughly speaking) the randomness of trajectory, a baseline-related term, and the randomness of the advantage-weighted gradient of the log-probability of the policy over states.\n\nThe paper empirically shows that for certain benchmarks and a certain architectural choice for NN, the variance term corresponding to the baseline contributes less than the term related to the randomness of the trajectory. Therefore, it does not matter much to use a state-action-dependent vs. state-dependent baselines.\n\nThe paper tries to justify the success observed in previous work that used state-action-dependent baselines. It identifies some bugs in each implementation, which explains the apparent superior performance. In other words, the benefit was not due to the use of state-action-dependent baseline.\n\nI believe the paper has an interesting observation, albeit with a quite limited scope. The paper does not show an impossibility result that state-action-dependent baseline cannot help (most likely it actually does for some problems, as mentioned in the Discussion section). It does not characterize when we can expect benefit from the action-dependent baseline and when we cannot. What it shows is that for some specific problems, using state-action-dependent baseline, implemented with a particular choice of architecture, does not help. I would consider this as something that might be good to be known by practitioners.\n\nAlso the fact that the authors have identified the bugs in other people\u2019s implementations is commendable, and probably helps improving the available codebase.\n\nSo overall, I think this is an alright workshop paper, but its contributions are not major.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582799031, "id": "ICLR.cc/2018/Workshop/-/Paper287/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper287/AnonReviewer2"], "reply": {"forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper287/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582799031}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573553215, "tcdate": 1521573553215, "number": 44, "cdate": 1521573552874, "id": "SkY3CA0FG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520010173363, "tcdate": 1520010173363, "number": 2, "cdate": 1520010173363, "id": "ryBa7ZP_M", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "forum": "HyL0IKJwM", "replyto": "SkVub5sPf", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Revised and extended version posted on arXiv", "comment": "We have posted a revised and extended version of the paper on arXiv (https://arxiv.org/abs/1802.10031)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623560, "id": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "reply": {"replyto": null, "forum": "HyL0IKJwM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623560}}}, {"tddate": null, "ddate": null, "tmdate": 1519260011867, "tcdate": 1519260011867, "number": 1, "cdate": 1519260011867, "id": "SkVub5sPf", "invitation": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "forum": "HyL0IKJwM", "replyto": "HyL0IKJwM", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Erratum", "comment": "When preparing the code for release, I found that I had mistakenly claimed that the implementation of Backpropagation Through the Void (Grathwohl et al. 2018) updated the value function before forming the policy gradient estimate. On closer inspection, this is *not* an issue with their implementation, however, they still suffer from two additional issues (see Appendix 7.3 for details). This incorrect claim does not affect the experiments we ran with their code and does not change the conclusions. We have changed the text to reflect this and will upload a new version when revisions are allowed.\n\nFinally, we emphasize that these observations are restricted to the continuous control experiments, and the rest of the experiments in that paper (Grathwohl et al. 2018) use a separate codebase that is unaffected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Mirage of Action-Dependent Baselines in Reinforcement Learning", "abstract": "Model-free reinforcement learning with flexible function approximators has shown success in goal-directed sequential decision-making problems. Policy gradient methods are a widely used class of stable model-free algorithms and typically, a state-dependent baseline or control variate is necessary to reduce the gradient estimator variance. Several recent papers extend the baseline to depend on both the state and action, and suggest that this enables significant variance reduction and improved sample efficiency without introducing bias into the gradient estimates. To better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains. We confirm this unexpected result by reviewing the open-source code accompanying these prior papers, and show that subtle implementation decisions cause deviations from the methods presented in the papers and explain the sources of the previously observed empirical gains.", "paperhash": "tucker|the_mirage_of_actiondependent_baselines_in_reinforcement_learning", "keywords": ["reinforcement learning", "action-dependent baseline", "variance reduction", "policy gradient"], "_bibtex": "@misc{\n  tucker2018the,\n  title={The Mirage of Action-Dependent Baselines in Reinforcement Learning},\n  author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},\n  year={2018},\n  url={https://openreview.net/forum?id=HyL0IKJwM}\n}", "authorids": ["gjt@google.com", "sbhupatiraju@google.com", "sg717@cam.ac.uk", "ret26@cam.ac.uk", "zoubin@eng.cam.ac.uk", "svlevine@eecs.berkeley.edu"], "authors": ["George Tucker", "Surya Bhupatiraju", "Shixiang Gu", "Richard E. Turner", "Zoubin Ghahramani", "Sergey Levine"], "TL;DR": "We decompose the variance of the policy gradient estimator and numerically show that learned state-action-dependent baselines do not in fact reduce variance over a state-dependent baseline in the commonly tested benchmark domains.", "pdf": "/pdf/660439b8e82783a9b34d1dfa1c683cc419ba8f73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712623560, "id": "ICLR.cc/2018/Workshop/-/Paper287/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper287/Reviewers"], "reply": {"replyto": null, "forum": "HyL0IKJwM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712623560}}}], "count": 8}