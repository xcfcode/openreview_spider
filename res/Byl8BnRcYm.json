{"notes": [{"id": "HJxw7Rz3FE", "original": null, "number": 29, "cdate": 1554947615381, "ddate": null, "tcdate": 1554947615381, "tmdate": 1554947635375, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "content": {"title": "Implementation", "comment": "The Tensorflow implementation is available at https://github.com/XinyiZ001/CapsGNN"}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605461, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1544/Authors|ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605461}}}, {"id": "BkgugyPUuV", "original": null, "number": 28, "cdate": 1553522415548, "ddate": null, "tcdate": 1553522415548, "tmdate": 1553522496253, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "S1lMPzS8u4", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "content": {"title": "Number of epoch", "comment": "Hi, just as what is mentioned in the paper. The exact number of epochs depends on the validation accuracy. \n\nWhen I did the experiments, I conducted 10-fold cross validation and for each fold I will run enough and a same number of epochs so that the models are overfitting on almost all the validation folds.  Then I chose the testing accuracy of the model which achieves the highest accuracy on corresponding validation fold as the final reported accuracy. \n\nSo here, I can provide you the largest number of epochs I set for each dataset and you can find the exact number of epochs based on your validation data:\n\n           MUTAG: 2000\n        ENZYMES: 3000\n       PROTEINS: 2000\n                 D&D: 300\n                 NCI1: 1500\n           COLLAB: 300\n           IMDB-B: 2000\n          IMDB-M: 2000 (but usually reach highest within 500 epochs)\n  REDDIT-M5K: 150 (can try more epochs)\nREDDIT-M12K: 150 (can try more epochs)\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605461, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1544/Authors|ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605461}}}, {"id": "S1lMPzS8u4", "original": null, "number": 3, "cdate": 1553515098127, "ddate": null, "tcdate": 1553515098127, "tmdate": 1553515098127, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Public_Comment", "content": {"comment": "I tried to implement the paper in PyTorch: https://github.com/benedekrozemberczki/CapsGNN.\n\nWhat was the actual number of epoch usually used in the paper?", "title": "Implementation"}, "signatures": ["~Benedek_Rozemberczki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Benedek_Rozemberczki1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311572391, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Byl8BnRcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311572391}}}, {"id": "Byl8BnRcYm", "original": "SJlsyl39KX", "number": 1544, "cdate": 1538087997944, "ddate": null, "tcdate": 1538087997944, "tmdate": 1552120557793, "tddate": null, "forum": "Byl8BnRcYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1eVSzvZe4", "original": null, "number": 1, "cdate": 1544806972451, "ddate": null, "tcdate": 1544806972451, "tmdate": 1545354494895, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Meta_Review", "content": {"metareview": "AR1 asks for a clear experimental evaluation showing that capsules and dynamic routing help in the GCN setting. After rebuttal, AR1 seems satisfied that routing in CapsGNN might help generate 'more representative graph embeddings from different aspects'. AC strongly encourages the authors to improve the discussion on these 'different aspects' as currently it feels vague. AR2 is initially concerned about experimental evaluations and whether the attention mechanism works as expected, though, he/she is happy with the revised experiments. AR3 would like to see all biological datasets included in experiments. He/she is also concerned about the lack of ability to preserve fine structures by CapsGNN. The authors leave this aspect of their approach for the future work.\n\nOn balance, all reviewers felt this paper is a borderline paper. After going through all questions and responses, AC sees that many requests about aspects of the proposed method have not been clarified by the authors. However, reviewers note that the authors provided more evaluations/visualisations etc. The reviewers expressed hope (numerous times) that this initial attempt to introduce capsules into GCN will result in future developments and  improvements. While AC thinks this is an overoptimistic view, AC will give the authors the benefit of doubt and will advocate a weak accept.\n\nThe authors are asked to incorporate all modifications requested by the reviewers. Moreover, 'Graph capsule convolutional neural networks' is not a mere ArXiV work. It is an ICML workshop paper. Kindly check all ArXiV references and update with the actual conference venues.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "The reviewers hope to see further improvements."}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1544/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352799705, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352799705}}}, {"id": "ByxrZRED3m", "original": null, "number": 1, "cdate": 1540996604958, "ddate": null, "tcdate": 1540996604958, "tmdate": 1543598493010, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "content": {"title": "A long paper with incomplete experiments", "review": "The paper fuses Capsule Networks with Graph Neural Networks. The idea seems technically correct and is well-written. With 13 pages the paper seems really long. Moreover, the experimental part seems to be too short. So, the theoretical and experimental part is not well-balanced.\n\nMinor concerns/ notes to the authors:\n1.\tPage 1: The abbreviation GNN is used before it is defined.\n2.\tPage 2: I guess there is a mistake in your indices. Capital N == n or?\n3.\tPage 4: What is \\mathbf{I}? I guess you mean the identity matrix.\n4.\tPage 4: Could you define/describe C_all?\n5.\tPage 5: Can you describe how you perform the coordinate addition or add a reference?\n6.\tPage 6: The idea to use reconstruction as regularization method is not new. May you can add a respective reference?\n7.\tPage 8: The abbreviations in your result tables are confusing. They are not aligned with the text. For example, what is Caps-CNN for a model?\n\nMy major concern is about your experimental evaluation. Under a first look the result tables looking great. But that\u2019s due to fact, that you marked the two best values in bold type. To be more precise, the method WL is in the most cases better than your proposed method. This makes me wondering if there is a real improvement by your method. It would be easier to decide if you would present the training/inference times and the number of parameters. By having that, I could relate your results regarding an accuracy-complexity tradeoff.  Moreover, your t-SNE and attention visualizations are not convincing. As you may know, the output of a t-SNE strongly dependents on the chosen hyper-parameters like the perplexity, etc. You not mentioned the setting of these values. Additionally, it is hard to decide if your embeddings are good or not because you are not presenting a baseline or referencing a respective work. You are complaining that this is due to the space restrictions. But you have unlimited capacity in the appendix. So please provide some clarifying plots. Finally, I\u2019m also not convinced that your attention mechanism works as expected. It\u2019s again due to missing baseline results and/or a reference. If it\u2019s not possible to add one of them, you could perform an easy experiment where you freeze your fully-connected layers of the attention module to fixed values (maybe such that it performs just an averaging) and repeat your experiments. In case your attention module works as expected you should observe a real change in terms of accuracy and in your visualizations too.\nYou could also think about to publish your code or present further results/plots in a separate blog. \n\nUpdate:\nAccording to the revised version which addresses a lot of my concerns, I vote for marginally above acceptance threshold.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "cdate": 1542234207125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969047, "tmdate": 1552335969047, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyxlLbgy14", "original": null, "number": 9, "cdate": 1543598407925, "ddate": null, "tcdate": 1543598407925, "tmdate": 1543598407925, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "rylqC9ptp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "content": {"title": "Review update", "comment": "Thanks for the revised version and the comments. I will update my review accordingly. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605461, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1544/Authors|ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605461}}}, {"id": "HJgaY4upCX", "original": null, "number": 7, "cdate": 1543500933309, "ddate": null, "tcdate": 1543500933309, "tmdate": 1543500933309, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "HJgwoDTYT7", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "content": {"title": "Review updated", "comment": "Thanks for providing a revised version with additional experimental results. I have updated my review accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605461, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1544/Authors|ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605461}}}, {"id": "S1gMPXrcn7", "original": null, "number": 2, "cdate": 1541194586349, "ddate": null, "tcdate": 1541194586349, "tmdate": 1543500863585, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "content": {"title": "Capsule networks for graphs without convincing motivation and experimental evaluation", "review": "The authors provide an architecture that applies recent advances in the field of capsule networks in the graph neural network domain. First, hierarchical node level capsules are extracted using GCN layers. Second, after weighting each capsule by the output of a proposed attention module, graph level capsules are computed by performing a global dynamic routing. These graph level capsules are used for training a capsule classifier using a margin loss and a reconstruction loss.\n\nThe general architecture seems to be a reasonable application of the capsule principle in the graph domain, following the proof of concept MNIST architecture proposed by Sabour et al.\n\nMy main concern is that I have problems grasping the motivation behind using capsules in the given scenario. Besides an unprecise motivation in the introduction, there is no clear reason why the routing mechanism helps with solving the given tasks. Capsule networks capture pose covariances by applying a linear, trainable transformation to pose vectors and computing the agreement of the resulting votes. It is not clear to me how discrete information like graph connectivity can be encoded in a pose vector so that linear transformations are able to match different \"connectivity poses\".\n\nIs there a more formal argument that explains why capsules should be able to capture more information about the input graph than other GCNNs?\n\nAlso, some design choices seem to be quite arbitrary. One example is using the last feature maps of the GCN as positions for coordinate addition. Is there a theoretical/intuitive motivation for this?\n\nResults for the given experiments show improvement on some graphs. However, the authors proposed several concepts: a global pooling method using dynamic routing, an attention mechanism, a novel reconstruction loss, interpreting deep node embeddings as spatial positions. It is not clear to what extent the individual aspects of the method contribute to the gains. The qualitative capsule embedding analysis is interesting. However, this part needs a comparison to standard global graph embeddings to see if there is a significant difference.\n\nIn my opinion, the paper needs:\n1) a clear experimental evaluation showing that capsules and the dynamic routing lead to improved results (i.e. by providing an ablation study to show which gains result from the attention-based global pooling mechanism, the reconstruction loss, the dynamic routing and from the coordinate addition), or\n2) a more precise motivation for the use of dynamic routing to capture correlation between pose vectors in graphs in general (i.e. formal arguments why the method is stronger in capturing statistics or for what types of graphs it provides more discriminative power).\n\nOverall, the paper does not convince me that capsules and dynamic routing provide advantages if used like the authors propose. Therefore, I tend to voting for rejecting the paper as long as points 1) and 2) are not addressed properly.\n\n\nMinor remarks:\n\n- There are quite a lot of grammatical errors (especially missing articles).\n\n--------------------------\nUpdate:\nThe authors addressed some of the weak points mentioned above adequately. The experimental evaluation was significantly improved and the results are a nice contribution. However, the theoretical contribution and the poor motivation of capsules in the graph context remain weak points. I have updated my rating accordingly.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "cdate": 1542234207125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969047, "tmdate": 1552335969047, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BylgrNNTCX", "original": null, "number": 6, "cdate": 1543484472507, "ddate": null, "tcdate": 1543484472507, "tmdate": 1543484472507, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "rylqC9ptp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "content": {"title": "WL is computational efficient ", "comment": ">> So kernel-based algorithms require computationally intensive preprocessing effort especially when processing large datasets. \n\nI would like to note that WL is computational efficient and scales to large data sets when using a linear SVM. Moreover, the WL optimal assignment kernel [1] provides better accuracy results on many data sets. But I agree that WL in many cases still is a fair baseline.\n\n[1] On Valid Optimal Assignment Kernels and Applications to Graph Classification\nNils M. Kriege, Pierre-Louis Giscard, Richard C. Wilson, NIPS 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605461, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byl8BnRcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1544/Authors|ICLR.cc/2019/Conference/Paper1544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers", "ICLR.cc/2019/Conference/Paper1544/Authors", "ICLR.cc/2019/Conference/Paper1544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605461}}}, {"id": "rJlwCtK5nm", "original": null, "number": 3, "cdate": 1541212622997, "ddate": null, "tcdate": 1541212622997, "tmdate": 1541533047516, "tddate": null, "forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "content": {"title": "The proposed CapsGNN is original and achieves good results on some datasets; Some more discussions may further help.", "review": "This paper was written with good quality and clarity. Their idea was original and experiment results show the proposed CapsGNN is effective in large graph data analysis, particularly on graphs with macroscopic properties.\n\nPros:\n\n1) The paper makes a clear and detailed comparison between the proposed CapsGNN and the related models in section 3.2.\n\n2) Use of capsules nets and routing in CapsGNN are close to that in the original CapsNet, with the core characteristics (and potential advantages) of capsules and dynamic routing being perserved in the proposed CapsGNN to handle the targeted problem. \n\n3) The comparison and model analysis are thorough and comprehensive.\n\nCons or unclear points:\n\n1) Why the paper does not include all biological datasets (6 datasets in total, only 4 used in this papaer) presented in (Verma & Zhang, 2018) in the experiment section. The experiments in Verma & Zhang, (2018) show that the GCAPS-CNN achieved SOTA results on nearly all biological datasets. Does GCAPS-CNN outperformed CapsGNN on biological datasets? It will be nice if there is comparison on more datasets and more analysis is provided between CapsGNN and GCAPS-CNN.\n\n2) Why CapsGNN is not suitable for preserving information of fine structures? Can the authors give more explanation and discussions?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1544/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Capsule Graph Neural Network", "abstract": "The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.", "keywords": ["CapsNet", "Graph embedding", "GNN"], "authorids": ["xinyi001@e.ntu.edu.sg", "elhchen@ntu.edu.sg"], "authors": ["Zhang Xinyi", "Lihui Chen"], "TL;DR": "Inspired by CapsNet, we propose a novel architecture for graph embeddings on the basis of node features extracted from GNN.", "pdf": "/pdf/cfae26427301ca2a6bf1790f51e810fe41040f3a.pdf", "paperhash": "xinyi|capsule_graph_neural_network", "_bibtex": "@inproceedings{\nxinyi2018capsule,\ntitle={Capsule Graph Neural Network},\nauthor={Zhang Xinyi and Lihui Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=Byl8BnRcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1544/Official_Review", "cdate": 1542234207125, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byl8BnRcYm", "replyto": "Byl8BnRcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335969047, "tmdate": 1552335969047, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}