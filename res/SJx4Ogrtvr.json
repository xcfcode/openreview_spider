{"notes": [{"id": "SJx4Ogrtvr", "original": "HJeps3etwS", "number": 2393, "cdate": 1569439851571, "ddate": null, "tcdate": 1569439851571, "tmdate": 1577168265274, "tddate": null, "forum": "SJx4Ogrtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xinlin.li1@huawei.com", "vahid.partovinia@huawei.com"], "title": "Random Bias Initialization Improving Binary Neural Network Training", "authors": ["Xinlin Li", "Vahid Partovi Nia"], "pdf": "/pdf/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "TL;DR": "Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization", "abstract": "Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.  However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \nWe analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.", "keywords": ["Binarized Neural Network", "Activation function", "Initialization", "Neural Network Acceleration"], "paperhash": "li|random_bias_initialization_improving_binary_neural_network_training", "original_pdf": "/attachment/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "_bibtex": "@misc{\nli2020random,\ntitle={Random Bias Initialization Improving Binary Neural Network Training},\nauthor={Xinlin Li and Vahid Partovi Nia},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx4Ogrtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0UJ0v0KhtO", "original": null, "number": 1, "cdate": 1576798747992, "ddate": null, "tcdate": 1576798747992, "tmdate": 1576800888046, "tddate": null, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "invitation": "ICLR.cc/2020/Conference/Paper2393/-/Decision", "content": {"decision": "Reject", "comment": "The article studies the behaviour of binary and full precision ReLU networks towards explaining differences in performance and suggests a random bias initialisation strategy. The reviewers agree that, while closing the gap between binary networks and full precision networks is an interesting problem, the article cannot be accepted in its current form. They point out that more extensive theoretical analysis and experiments would be important, as well as improving the writing. The authors did not provide a rebuttal nor a revision. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xinlin.li1@huawei.com", "vahid.partovinia@huawei.com"], "title": "Random Bias Initialization Improving Binary Neural Network Training", "authors": ["Xinlin Li", "Vahid Partovi Nia"], "pdf": "/pdf/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "TL;DR": "Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization", "abstract": "Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.  However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \nWe analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.", "keywords": ["Binarized Neural Network", "Activation function", "Initialization", "Neural Network Acceleration"], "paperhash": "li|random_bias_initialization_improving_binary_neural_network_training", "original_pdf": "/attachment/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "_bibtex": "@misc{\nli2020random,\ntitle={Random Bias Initialization Improving Binary Neural Network Training},\nauthor={Xinlin Li and Vahid Partovi Nia},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx4Ogrtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724870, "tmdate": 1576800276585, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2393/-/Decision"}}}, {"id": "H1lAIDb-Kr", "original": null, "number": 1, "cdate": 1570998102224, "ddate": null, "tcdate": 1570998102224, "tmdate": 1572972344120, "tddate": null, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "invitation": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "The paper proposes a method for bias initialization and shows that it improves training for BNN.\n\nI vote to reject the paper. Main points against are: (1) is no theory and very limited experiments (2) Bad writing.\n\nDetailed remarks:\n - The level of english is not good enough all over the paper, example: \"It is more common to use low-bit quantized networks such as Binary Neural Networks (BNNs)\u05f4 more common then what? (I also disagree on the scientific claim)\n- The authors claim that XNOR nets and such have \"memory occupation is significantly larger than the pure 1-bit solution like the vanilla BNN.\". While this is true for training, it is not true for inference which is in many cases where one needs to use limited hardware.\n- The paper main claim is the data equality and hyperplane equality are the main strengths of ReLU, but doesn't give any justification or even intuition into why this is the case. I am not convinced that these points are important, and the paper did nothing to try to persuade me.\n- Data point equality shouldn't hold for ReLU networks with non-zero bias initialization as well.\n- The experiments show promising results but only on cifar10 and only with the outdated BNN, also as a necessary baseline it would be important to show the effect of the bias initialization on ReLU networks. \n\n\nI believe the paper shows promising initial results but needs to strengthen them considerably. It also needs to improve the writing. A better justification for the method, even if it only at an intuitive level would help considerably."}, "signatures": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xinlin.li1@huawei.com", "vahid.partovinia@huawei.com"], "title": "Random Bias Initialization Improving Binary Neural Network Training", "authors": ["Xinlin Li", "Vahid Partovi Nia"], "pdf": "/pdf/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "TL;DR": "Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization", "abstract": "Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.  However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \nWe analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.", "keywords": ["Binarized Neural Network", "Activation function", "Initialization", "Neural Network Acceleration"], "paperhash": "li|random_bias_initialization_improving_binary_neural_network_training", "original_pdf": "/attachment/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "_bibtex": "@misc{\nli2020random,\ntitle={Random Bias Initialization Improving Binary Neural Network Training},\nauthor={Xinlin Li and Vahid Partovi Nia},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx4Ogrtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654380192, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2393/Reviewers"], "noninvitees": [], "tcdate": 1570237723458, "tmdate": 1575654380205, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review"}}}, {"id": "Byg1CKm9KH", "original": null, "number": 2, "cdate": 1571596742899, "ddate": null, "tcdate": 1571596742899, "tmdate": 1572972344085, "tddate": null, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "invitation": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: This paper tries to improve the training for the binary neural network.\n\nWeaknesses:\n[-] A lack of related works. There have been many related works about BNN in these years (after 2017), but the authors do not have a quick summary of them.\n[-] More reference. e.g, when authors mention 'many related works require to store the full-precision activation map during the inference stage',  some reference is necessary.\n[-] Weak Motivation: The authors argue 'We analyze the behaviour of the full-precision neural network with ReLU activation' in the abstract. However, in Section 3, I cannot find any analysis. Only writing down the backward and forward cannot be called analysis. Initialization is different from the training dynamics. Assumptions and theorems should be highlighted. \n[-] Poor writing: A lot of typos. Only in the last paragraph in Section 2, I find many typos,  e.g. 'replaced replacing ReLU activation', 'any relaated works'.\n\nQuestions:\n[.] In experiments, what structure is used for ResNet? ResNet-18-like or ResNet-110-like? (The results for these two kinds of structure are totally different for binary neural network, as the difference in the number of channels) \n[.] In experiments, the performance of the baselines seems lower than related papers? Do the authors increase the number of channels in each layer as the other people do? It can improve the result a lot, and I wonder whether the improvement still exists in this setting.\n[.] In experiments, only CIFAR10 results have been reported, but I wonder what is the error bar looks like? (Do the authors run the experiments several times and calculate the variance?)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xinlin.li1@huawei.com", "vahid.partovinia@huawei.com"], "title": "Random Bias Initialization Improving Binary Neural Network Training", "authors": ["Xinlin Li", "Vahid Partovi Nia"], "pdf": "/pdf/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "TL;DR": "Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization", "abstract": "Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.  However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \nWe analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.", "keywords": ["Binarized Neural Network", "Activation function", "Initialization", "Neural Network Acceleration"], "paperhash": "li|random_bias_initialization_improving_binary_neural_network_training", "original_pdf": "/attachment/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "_bibtex": "@misc{\nli2020random,\ntitle={Random Bias Initialization Improving Binary Neural Network Training},\nauthor={Xinlin Li and Vahid Partovi Nia},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx4Ogrtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654380192, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2393/Reviewers"], "noninvitees": [], "tcdate": 1570237723458, "tmdate": 1575654380205, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review"}}}, {"id": "BJeE7ZOaKr", "original": null, "number": 3, "cdate": 1571811611700, "ddate": null, "tcdate": 1571811611700, "tmdate": 1572972344051, "tddate": null, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "invitation": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a method to initialize the bias terms in neural network layers, and argues that the proposed method improve the performance of binary neural networks (BNNs). The paper justifies the proposed method by analyzing the geometric properties of the ReLU and the hard tanh (htanh) activation functions, as well as by empirical results on the CIFAR-10 dataset using the (binary variants) of VGG-7 and ResNet. \n\nWhile closing the performance gap between BNNs and their full-precision counterparts is an interesting problem of practical importance, this paper has several limitations: \n\n(1) the analysis of geometric properties of ReLU/htanh is not sufficiently precise and clear;\n(2) the paper does not clearly present the connections between the htanh activation function and the straight-through estimator employed in back-propagating the gradients in training a BNN;\n(3) the experimental results are too limited on just one dateset, and only error rate on validation set is reported, however, lower error rate on validation set won't guarantee better performance on test set;\n(4) the presentation is imprecise and unpolished.\n\n\nMinor comments:\n\nSection 2: \n\"Tang et al. replaced replacing ReLU\" -> \"Tang et al. replaced ReLU\"\n\"many relaated works\" -> \"many related works\"\n\nSection 3: \nplease define the symbols used in Equation (1)\ntitle of Figure 2: \"behavior of ReLu\" -> \"behavior of ReLU\""}, "signatures": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2393/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xinlin.li1@huawei.com", "vahid.partovinia@huawei.com"], "title": "Random Bias Initialization Improving Binary Neural Network Training", "authors": ["Xinlin Li", "Vahid Partovi Nia"], "pdf": "/pdf/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "TL;DR": "Improve saturating activations (sigmoid, tanh, htanh etc.) and Binarized Neural Network with Bias Initialization", "abstract": "Edge intelligence especially binary neural network (BNN) has attracted considerable attention of the artificial intelligence community recently. BNNs significantly reduce the computational cost, model size, and memory footprint.  However, there is still a performance gap between the successful full-precision neural network with ReLU activation and BNNs. We argue that the accuracy drop of BNNs is due to their geometry. \nWe analyze the behaviour of the full-precision neural network with ReLU activation and compare it with its binarized counterpart. This comparison suggests random bias initialization as a remedy to activation saturation in full-precision networks and  leads us towards an improved BNN training. Our numerical experiments confirm our geometric intuition.", "keywords": ["Binarized Neural Network", "Activation function", "Initialization", "Neural Network Acceleration"], "paperhash": "li|random_bias_initialization_improving_binary_neural_network_training", "original_pdf": "/attachment/a5406188b7c10c122b77bcd0a49b4de43e033c4a.pdf", "_bibtex": "@misc{\nli2020random,\ntitle={Random Bias Initialization Improving Binary Neural Network Training},\nauthor={Xinlin Li and Vahid Partovi Nia},\nyear={2020},\nurl={https://openreview.net/forum?id=SJx4Ogrtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJx4Ogrtvr", "replyto": "SJx4Ogrtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2393/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654380192, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2393/Reviewers"], "noninvitees": [], "tcdate": 1570237723458, "tmdate": 1575654380205, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2393/-/Official_Review"}}}], "count": 5}