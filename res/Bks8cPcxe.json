{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1486615506152, "tcdate": 1478290003158, "number": 414, "id": "Bks8cPcxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bks8cPcxe", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "content": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396568689, "tcdate": 1486396568689, "number": 1, "id": "BkWsnzLOg", "invitation": "ICLR.cc/2017/conference/-/paper414/acceptance", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers find value in the contributions. ", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396569221, "id": "ICLR.cc/2017/conference/-/paper414/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396569221}}}, {"tddate": null, "tmdate": 1484838026999, "tcdate": 1481908361673, "number": 2, "id": "HkMceoZVl", "invitation": "ICLR.cc/2017/conference/-/paper414/official/review", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "content": {"title": "ICLR is not the right venue for this paper.", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents DeepDSL, a \"domain specific language (DSL) embedded in\nScala, that compiles deep networks written in DeepDSL to Java source code\". It\nintroduces its syntax and the key concepts which differentiate it\nfrom other existing frameworks, such as Torch7, Theano, Caffe, TensorFlow,\nCNTK, Chainer and MXNet. It also benchmarks speed and memory usage against\nTensorFlow and Caffe on a variety of convolutional neural network architectures.\n\nThe paper is clear and well written and it does a good job of presenting DeepDSL\nin the context of existing deep learning frameworks.\n\nHowever, I don't think ICLR is the right venue for this type of work. Some of\nthe ideas it presents are interesting, but overall the paper lacks novelty and\npotential impact and stays firmly within the realm of deep learning framework\nwhitepapers such as [1,2,3,4], which to my knowledge don't have a precedent of\nbeing accepted at venues like ICLR.\n\n[1]: Bergstra, James, et al. \"Theano: A CPU and GPU math compiler in Python.\"\nProc. 9th Python in Science Conf. 2010.\n\n[2]: Bastien, Fr\u00e9d\u00e9ric, et al. \"Theano: new features and speed improvements.\"\narXiv preprint arXiv:1211.5590 (2012).\n\n[3]: Abadi, Mart\u0131n, et al. \"Tensorflow: Large-scale machine learning on\nheterogeneous distributed systems.\" arXiv preprint arXiv:1603.04467 (2016).\n\n[4]: The Theano Development Team et al. \"Theano: A Python framework for fast\ncomputation of mathematical expressions.\" arXiv preprint arXiv:1605.02688\n(2016).\n\nUPDATE: The rating has been revised to a 6 following the authors' reply.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595123, "id": "ICLR.cc/2017/conference/-/paper414/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595123}}}, {"tddate": null, "tmdate": 1484837963840, "tcdate": 1484837963840, "number": 2, "id": "BJ4I4IRUl", "invitation": "ICLR.cc/2017/conference/-/paper414/official/comment", "forum": "Bks8cPcxe", "replyto": "rJzc0BM4x", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "content": {"title": "Reviewer Response", "comment": "Thank you for pointing out this specific section of the conference instructions to me. Given this and the fact that the other reviewers are ready to accept the paper, I'm updating my rating to 6 to reflect my opinion of the paper's quality."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587823, "id": "ICLR.cc/2017/conference/-/paper414/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587823}}}, {"tddate": null, "tmdate": 1482211085547, "tcdate": 1482179766555, "number": 3, "id": "rJ02NpS4x", "invitation": "ICLR.cc/2017/conference/-/paper414/official/review", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper presents and evaluates a Scala-based deep learning framework called \u201cDeepDSL,\u201d describing the language\u2019s syntactic and performance benefits with respect to existing frameworks.\n\n\nPros:\n\nThe use of Scala is unique among deep learning frameworks, to my knowledge, making this framework interesting for Scala users.  The fact that Scala compiles to Java and therefore cross-platform support comes for free is also nice.\n\nThe ability to inspect memory information as shown in Figure 3 is interesting and potentially useful for large networks or situations where memory is limited.\n\nDeepDSL compares favorably with existing frameworks in terms of memory use and speed for many common convolutional network architectures.\n\n\nCons:\n\nThere appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow).  This makes certain applications, such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without further extension of the underlying API.  (Note: I might be wrong about this -- and please correct me if I am -- but all the examples in the paper are nets trained by gradient descent on a single objective, and do not share parameters or access gradients directly.)\n\nThe paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the compactness of its own syntax.  This is misleading as Caffe has officially supported a compact network definition style called \u201cNetSpec\u201d for years -- see a ~15 line definition of AlexNet [1].  Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which happens to have a human-readable text format.\n\nDeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or incorrectly oversell the novelty of the framework.  Some examples:\n\n\u201cThis separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.\u201d  This separation is not unique -- it\u2019s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it\u2019s not the default workflow there).\n\n\u201cThe difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as \u2018networks\u2019 but as abstract \u2018functions\u2019.\u201d There is no notion of a \u201cnetwork\u201d in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in DeepDSL.  I asked about this statement, and the response didn\u2019t convince me otherwise.  The counterexample given was that in TensorFlow the number of input channels needs to be specified separately for each convolution.  This is only true using the low-level API and can easily be worked around with higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2].  It may be true that DeepDSL is more \u201cbatteries included\u201d for writing compact network definitions than these other frameworks, but the paper\u2019s claims seem to go beyond this.\n\n\nOverall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will consider upgrading my rating if these statements are removed or amended to be more technically precise.\n\n\n[1] https://github.com/BVLC/caffe/blob/master/examples/pycaffe/caffenet.py#L24\n[2] https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/alexnet.py#L92\n\n=====================\n\nUpdate: the authors have revised their paper to address the concerns that I considered grounds for rejection in my review.  I've upgraded my rating from 5 (below threshold) to 7 (good paper, accept).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595123, "id": "ICLR.cc/2017/conference/-/paper414/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595123}}}, {"tddate": null, "tmdate": 1482211023050, "tcdate": 1482211023050, "number": 1, "id": "rywRR48Vx", "invitation": "ICLR.cc/2017/conference/-/paper414/official/comment", "forum": "Bks8cPcxe", "replyto": "rkMFnNIVg", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "content": {"title": "updated rating", "comment": "Thanks for the quick response!  With your latest revisions, the paper looks good to me.  I've upgraded my rating to accept."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587823, "id": "ICLR.cc/2017/conference/-/paper414/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587823}}}, {"tddate": null, "tmdate": 1482210425937, "tcdate": 1482210425937, "number": 6, "id": "rkMFnNIVg", "invitation": "ICLR.cc/2017/conference/-/paper414/public/comment", "forum": "Bks8cPcxe", "replyto": "rJ02NpS4x", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "writers": ["~Xiao_Bing_Huang1"], "content": {"title": "we uploaded a revision to address the concerns.", "comment": "Thank you for the comments. Below is our reply.\n\n> There appears to be special privileged handling of parameters, gradients, and updates in the compilation process itself (as in Caffe), rather than  \n> having gradients/updates as a normal part of the full user-defined computation graph (as in Theano + TensorFlow).  This makes certain applications, \n> such as RNNs (which require parameter sharing) and GANs (which require gradients wrt multiple objectives), impossible to implement in DeepDSL without \n> further extension of the underlying API.   \n\nWe do provide API for gradient derivation and updates. For the examples we encoded gradient updates inside the \"Loop\" class since it is the same for the networks we tested.\n\nTo clarify this, we have added some text within Section 5 (compilation). \n\nThe gradient of a loss expression w.r.t. a parameter p can be implemented as \" loss.grad(p) \", which is a tensor expression. \nGradient update is written using expression of the form \" Update(p, gradient, alpha, beta) \" which represents the computation: \n      p = p * beta + gradient * alpha.\n\nThe gradient updates for all parameters are put into a list and then passed to optimization functions to obtain a list of optimized IR expressions ready for code generation.\n\nWe have not implemented RNN or GAN, which remains our future work though we do not think gradient update is a problem. \n \n> The paper repeatedly refers to line counts from the verbose Protobuf-based low-level representation of networks in Caffe to demonstrate the \n> compactness of its own syntax.  This is misleading as Caffe has officially supported a compact network definition style called \u201cNetSpec\u201d for years -- \n> see a ~15 line definition of AlexNet [1].  Given that, Protobuf is essentially an intermediate representation for Caffe (as with TensorFlow), which \n> happens to have a human-readable text format.\n\nYou are right. The compact encoding is not unique to DeepDSL and we removed all mentioning of this.\n \n> DeepDSL is not especially novel when compared with existing frameworks, which is not a problem in and of itself, but some statements misleadingly or > incorrectly oversell the novelty of the framework.  Some examples:\n\n> \u201cThis separation between network definition and training is an unique advantage of DeepDSL comparing to other tools.\u201d  This separation is not unique \n> -- it\u2019s certainly a feature of Caffe where the network definition is its own file, and can be attained in TensorFlow as well (though it\u2019s not the \n> default workflow there).\n\nOur sentence is perhaps misleading and we replaced it. \nWe meant to say that the compiled Java source code (used for training) is independent of the network definition and DeepDSL compiler. \nThe benefit is that the compiled Java code only depends on a small Java library (that calls cuDNN) -- not the rest of the DeepDSL or Scala. \n \n> \u201cThe difference [between our framework and Theano, TensorFlow, etc.] is that we do not model deep networks as \u2018networks\u2019 but as abstract \n> \u2018functions\u2019.\u201d There is no notion of a \u201cnetwork\u201d in Theano or TensorFlow (not sure about the others) either -- there are only functions, like in \n> DeepDSL.  I asked about this statement, and the response didn\u2019t convince me otherwise.  The counterexample given was that in TensorFlow the number of \n> input channels needs to be specified separately for each convolution.  This is only true using the low-level API and can easily be worked around with \n> higher-level wrappers like TensorFlow Slim -- e.g., see the definition of AlexNet [2].  It may be true that DeepDSL is more \u201cbatteries included\u201d for \n> writing compact network definitions than these other frameworks, but the paper\u2019s claims seem to go beyond this.\n\nYou are right that the difference syntax-wise is really minor. We removed the text regarding this. \n \n> Overall, the DeepDSL framework seems to have real value in its use of Scala and its memory/speed efficiency as demonstrated by the experiments, but \n> the current version of the paper contains statements that overclaim novelty in ways that are misleading and unfair to existing frameworks. I will \n> consider upgrading my rating if these statements are removed or amended to be more technically precise. \n\nWe have updated our paper with the above changes and we wish you would view it in a positive light.\n\nthank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587949, "id": "ICLR.cc/2017/conference/-/paper414/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587949}}}, {"tddate": null, "tmdate": 1481952906549, "tcdate": 1481952906549, "number": 5, "id": "rJzc0BM4x", "invitation": "ICLR.cc/2017/conference/-/paper414/public/comment", "forum": "Bks8cPcxe", "replyto": "HkMceoZVl", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "writers": ["~Xiao_Bing_Huang1"], "content": {"title": "software tool is a relevant topic", "comment": "In the list of suggested topics for this conference, there is\n\n- Implementation issues, parallelization, software platforms, hardware\n\nwhich we understood to include software platforms such as DeepDSL.  \n\n\nWhile the main theme of the conference is on learning representation, software tools for learning deserve some attention from users such as the audience of this conference. Many researchers use existing libraries for their deep learning research but picking the right tool, using them correctly, and knowing how to extend them can save a lot time and energy. \n\nAs to novelty and potential impact, DeepDSL is the first DL tool that compiles to Java source code with very few dependencies, which greatly improves the portability of the tool. Running it on Windows is as easy as running it on Linux or Unix. DeepDSL is also significantly more efficient than Tensorflow and Caffe in complex networks such as Googlenet and Resnet. \n\nWhile researchers focus on algorithms to improve accuracy, speed, and memory usage, tools like DeepDSL are able to improve speed and memory usage at implementation level. Through venue like ICLR, we hope to reach the target audience. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587949, "id": "ICLR.cc/2017/conference/-/paper414/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587949}}}, {"tddate": null, "tmdate": 1481906167189, "tcdate": 1481906167189, "number": 1, "id": "r1kZu5WEl", "invitation": "ICLR.cc/2017/conference/-/paper414/official/review", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer1"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents a domain specific language for the specification of deep learning models. The intermediate representation offers many possibilities for optimization and to focus on speed or runtime.\n\nThe paper is well-written and makes conclusive statements and comparisons. The experiments cover five fundamentally differenct CNN architectures, each evaluated for two batch sizes. They include the two competing frameworks Tensorflow and Caffe and show convincing performance. Overall, the paper is well-written and structured.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595123, "id": "ICLR.cc/2017/conference/-/paper414/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595123}}}, {"tddate": null, "tmdate": 1480739125384, "tcdate": 1480739125379, "number": 3, "id": "BkTNtp17g", "invitation": "ICLR.cc/2017/conference/-/paper414/public/comment", "forum": "Bks8cPcxe", "replyto": "ByaMPo1Xg", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "writers": ["~Xiao_Bing_Huang1"], "content": {"title": "The difference between network and function is that part of network may not be reused like a function", "comment": "Thanks for your comments. Below is our clarification of the difference between network and DeepDSL function.\n\nIn DeepDSL, tensor functions are DSL entities. There is no network concept in DeepDSL and a network is just a tensor function (composed one).\n\nThe example you gave is similar to the \"o\" operation in DeepDSL though it is a Python function. \n\nThe difference may seems superficial but considering that a Python function does not expose its argument before it is applied to the argument.\nThis poses a problem. For example, in Tensorflow, you may have trouble extracting the output channel size of a convolution layer before calling\nthe tf.nn.conv2d function to form the convolution layer. As a result, when you compose two convolution layers, you may have to specify the input and output channel sizes for the second convolution layer. \n \nThat is, before calling \"compose(conv1, relu, conv2, relu)\", you may have to fix the in_channel size for conv2 using the out_channel size of conv1.\n\nOf course, there is always a way to get around this but having DSL abstractions for tensor functions makes it explicit.\n\nFor example, given\n\nval cv1 = CudaLayer.convolv(\"cv1\", 5, 20) // 5 x 5 kernel, output channel 20, stride 1, padding 0\nval cv2 = CudaLayer.convolv(\"cv2\", 5, 50) // 5 x 5 kernel, output channel 50, stride 1, padding 0\n\nThe tensor type of cv1 is List(N, K, M1, M2) -> List(N, 20, (M1-5)+1, (M2-5)+1)).\n\nThe tensor type of cv2 is List(N1, K1, M3, M4) -> List(N1, 50, (M3-5)+1, (M4-5)+1). \n\nThe tensor type of cv1 o cv2 is \n\n         List(N, K, M1, M2) -> List(N, 50, (((M1-5)+1)-5)+1, ((M2-5)+1)-5)+1)\n\nwhere N, K, M1, M2 are dimension variables and (((M1-5)+1)-5)+1, ((M2-5)+1)-5)+1 are dimension expressions that won't be evaluated until code generation.\n\n\nDuring composition of cv1 and cv2, the dimension variable K1 is substituted by 20, N1 is replaced by N, M3 and M4 are replaced by (M1-5)+1 and (M2-5)+1 respectively. \n\nNote that \"K\" remains a dimension variable and may be substituted by other dimension expressions when \"cv1 o cv2\" is composed with other tensor functions. \n \n\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587949, "id": "ICLR.cc/2017/conference/-/paper414/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587949}}}, {"tddate": null, "tmdate": 1480732809584, "tcdate": 1480732575324, "number": 2, "id": "Hywo1hJQx", "invitation": "ICLR.cc/2017/conference/-/paper414/public/comment", "forum": "Bks8cPcxe", "replyto": "B1W1YbJ7e", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "writers": ["~Xiao_Bing_Huang1"], "content": {"title": "We revised experiments to include memory consumption for Tensorflow and runtime/memory performance for DeepDSL in memory efficient mode, and also added comparison to Theano's optimization", "comment": "Thank you for your comments and suggestions. Below is our reply:\n\n1. Using your excellent suggestion (greatly appreciated!), we are able to measure the memory use of Tensorflow for our tests. The detailed information is included in the revised PDF file. A quick summary is that DeepDSL uses less memory than Tensorflow in all but Vgg. \n\n2. We also included runtime and memory performance for DeepDSL in memory efficient mode. In this mode, DeepDSL uses 10~30%  less memory with similar runtime overhead except Googlenet and Vgg where higher percentage of memory saving is achieved with relatively less runtime penalty.\n\n3. About similarity to Theano's graph optimization, we had added a few paragraphs towards the end of the review section. \n\nDeepDSL does share some similarity in the optimization steps such as redundancy removal and simplification strategy.\n\nHowever, DeepDSL does not have a computation graph specifically since DeepDSL expressions are actually just expressions. There is no explicit or implicit graph of any kind. DeepDSL expressions are optimized locally for each expression first. Then the expressions are broken down into simple assignment expressions (SSA transformation) where global optimization such as common subexpression elimination, code scheduling, in-place computation, and deallocation are implemented.\n\nAnother difference is in code generation, where Theano generates C code from its low-level functions while DeepDSL code generation is through a simple single-pass code generator that goes through the optimized DSL expressions (in single-assignment form) and print Java code as strings.    "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587949, "id": "ICLR.cc/2017/conference/-/paper414/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587949}}}, {"tddate": null, "tmdate": 1480730473688, "tcdate": 1480730388839, "number": 3, "id": "ByaMPo1Xg", "invitation": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer2"], "content": {"title": "Distinction between \"networks\" and \"functions\"", "question": "I don't understand the distinction being drawn between \"networks\" and \"functions\" in the \"network reuse\" subsection (Section 3).   To me it seems like this is exactly how Theano and TensorFlow (and possibly others) already work.  e.g., if I have a \"relu\" function and a \"conv\" function, I could compose these into a two-layer \"network\" function as follows (in untested Python, with '.'s used for indentation):\n\ndef compose(*funcs):\n..def f(x):\n....for func in funcs:\n......x = func(x)\n....return x\n..return f\n\nnetwork = compose(conv, relu, conv, relu)\n\nCould you expand more on the distinction you're making?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959296236, "id": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959296236}}}, {"tddate": null, "tmdate": 1480689881544, "tcdate": 1480689881539, "number": 2, "id": "B1W1YbJ7e", "invitation": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer1"], "content": {"title": "Evaluation comment and question", "question": "As far as I understand, computation graph optimization in Theano does not differ much (at least on first sight) to your optimization approach. Could you detail more the differences?\n\nIn Figure 5, you detail the GPU memory usage. A first comment concerns the statement \"Since Tensorflow allocates all GPU memory regardless of actual usage, we only compare DeepDSL and Caffe\" (caption Fig. 5). I want to point out this behaviour is the default, but can be changed. You can get the session configuration by using, e.g., `sess_config = tf.ConfigProto()` before creating the session and then specify `sess_config.gpu_options.allow_growth = True`. In that case, Tensorflow will not preallocate the full memory for its pool.\n\nIn any case, it would be important to also have the values for the memory efficient, not only the runtime efficient mode of DeepDSL in Figure 5!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959296236, "id": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959296236}}}, {"tddate": null, "tmdate": 1480623099298, "tcdate": 1480623099292, "number": 1, "id": "H1mb4W0Gx", "invitation": "ICLR.cc/2017/conference/-/paper414/public/comment", "forum": "Bks8cPcxe", "replyto": "r1zV4xCfx", "signatures": ["~Xiao_Bing_Huang1"], "readers": ["everyone"], "writers": ["~Xiao_Bing_Huang1"], "content": {"title": "We handle symbolic gradients transparently and here are the details for the answer", "comment": "Hi, glad to answer your question! \n\nFirst, regarding the second-order gradient support:\n\n1. We already support second-order gradients for scalar values;\n2. We do not support second-order gradients for tensor values currently but supporting it should not require extensive changes since everything is abstract.\n\nNow we describe a bit details with the below examples. \n\nSuppose e is a scalar expression, DeepDSL supports all the below already:\n\n   A. if x is a scalar variable, d e / d x is still a scalar\n   B. if v is a tensor, d e / d v is a tensor\n   C. (d e / d v) ( d v' ) will be a tensor gradient\n   D d v' / d v is a tensor gradient\n\nBut we don't have d (d v1 / d v2) / d v3 since we don't see a use for it.\n\nEverything is currently handled internally and transparent to the user. However, we certainly can expose the gradient handling to allow users to manipulate symbolic gradients. It is just that we don't see a use case yet for this functionality. Maybe you can explain a bit on why this is needed and under what circumstance so we can add the support in the next version. \n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287587949, "id": "ICLR.cc/2017/conference/-/paper414/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bks8cPcxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper414/reviewers", "ICLR.cc/2017/conference/paper414/areachairs"], "cdate": 1485287587949}}}, {"tddate": null, "tmdate": 1480619050133, "tcdate": 1480619050127, "number": 1, "id": "r1zV4xCfx", "invitation": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "signatures": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper414/AnonReviewer3"], "content": {"title": "Gradients question", "question": "Does DeepDSL allow to manipulate symbolic gradients (e.g. to allow the symbolic representation of second-order gradients)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "abstract": "In recent years, Deep Learning (DL) has found great success in domains such as multimedia understanding. However, the complex nature of multimedia data makes it difficult to develop DL-based software. The state-of-the-art tools, such as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their applicable domains, are programming libraries with fixed user interface, internal representation, and execution environment. This makes it difficult to implement portable and customized DL applications.\n\nIn this paper, we present DeepDSL, a domain specific language (DSL) embedded in Scala, that compiles deep networks written in DeepDSL to Java source code. Deep DSL provides \n\n(1) intuitive constructs to support compact encoding of deep networks; \n(2) symbolic gradient derivation of the networks; \n(3) static analysis for memory consumption and error detection; and \n(4) DSL-level optimization to improve memory and runtime efficiency. \n\nDeepDSL programs are compiled into compact, efficient, customizable, and portable Java source code, which operates the CUDA and CUDNN interfaces running on NVIDIA GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL with a number of popular DL networks. Our experiments show that the compiled programs have very competitive runtime performance and memory efficiency compared to the existing libraries.", "pdf": "/pdf/3a5b86bbf579c07b187d2c46b9b143e3def72c19.pdf", "TL;DR": "DeepDSL(a DSL embedded in Scala) that compiles deep learning networks written in DeepDSL to Java source code, which runs on any GPU equipped machines with competitive efficiency as existing state-of-the-art tools (e.g. Caffe and Tensorflow)", "paperhash": "zhao|deepdsl_a_compilationbased_domainspecific_language_for_deep_learning", "authors": ["Tian Zhao", "Xiao Bing Huang", "Yu Cao"], "keywords": ["Deep learning", "Applications", "Optimization"], "conflicts": ["uwm.edu", "cs.uml.edu", "fresnostate.edu", "utc.edu"], "authorids": ["tzhao@uwm.edu", "xiaobing@uwm.edu", "ycao@cs.uml.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959296236, "id": "ICLR.cc/2017/conference/-/paper414/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper414/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper414/AnonReviewer3", "ICLR.cc/2017/conference/paper414/AnonReviewer1", "ICLR.cc/2017/conference/paper414/AnonReviewer2"], "reply": {"forum": "Bks8cPcxe", "replyto": "Bks8cPcxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper414/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959296236}}}], "count": 15}