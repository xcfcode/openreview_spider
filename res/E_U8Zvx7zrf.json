{"notes": [{"id": "E_U8Zvx7zrf", "original": "OY3o2K6dPFn", "number": 1285, "cdate": 1601308143652, "ddate": null, "tcdate": 1601308143652, "tmdate": 1614985631096, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4eeIEIerBc", "original": null, "number": 1, "cdate": 1610040529502, "ddate": null, "tcdate": 1610040529502, "tmdate": 1610474138882, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "invitation": "ICLR.cc/2021/Conference/Paper1285/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "No discussion or answers to concerns are offered by the authors.\nGiven this, the current consensus remains the same as the initial review status, and AC's meta-review cannot provide any additional information.\n\nThis leads to rejection"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040529489, "tmdate": 1610474138866, "id": "ICLR.cc/2021/Conference/Paper1285/-/Decision"}}}, {"id": "5u9L-cafLA8", "original": null, "number": 3, "cdate": 1603991065709, "ddate": null, "tcdate": 1603991065709, "tmdate": 1606812041919, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "invitation": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review", "content": {"title": "The novelty is not too high. Comparison with more advanced baselines is needed.", "review": "This paper proposes a method, called OLCO3, to reduce the communication cost in distributed learning. Experiments on real datasets are used for evaluation. The paper is well written. \n\nThe main idea of OLCO3 is to combine many existing communication reduction methods, including pipelining, gradient compression and periodic averaging. There does exist some novelty in OLCO3, but the novelty is not high. Furthermore, there has appeared one similar paper[A] which combines sparsification, quantization and local SGD into the same framework for communication reduction. But this paper does not cite [A], and empirical comparison with [A] is also not provided. \n\nAnother shortcoming of OLCO3 is that the computation-communication overlapping technique will introduce an extra memory cost of O(sd), which might be unacceptable for large deep models with a huge d when s is relatively large. \n\nFor experiments, the convincingness can be improved if test accuracy/training error vs. wall-clock time is also provided. \n  \n[A]. Basu, Debraj, et al. \"Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations.\" Advances in Neural Information Processing Systems. 2019.\n\n\n------------------------\nAfter discussion:\n\nThe authors do not provide rebuttal. Hence, I keep the original opinion to give this paper a weak reject.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1285/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1285/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122185, "tmdate": 1606915808326, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1285/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review"}}}, {"id": "4agwuSfFuW", "original": null, "number": 1, "cdate": 1603312671927, "ddate": null, "tcdate": 1603312671927, "tmdate": 1605024482879, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "invitation": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review", "content": {"title": "Some major issues with the novelty and experimental setups", "review": "--- Summary\n\nThis paper proposes OLCO3, a new delay-tolerant SGD communication scheme and training framework for distributed deep neural network training. OLCO3 combines the existing ideas of Stale synchronous Parallel, batching the communication of doing multiple iterations, and gradient compression to achieve more communication efficiency. OLCO3 also uses staleness compensation and compression compensation techniques to improve model convergence under high stateness. Theoretical analysis shows that OLCO3 converges under SGD and momentum SGD. The evaluation was done with ResNet models on Cifar-10 and ImageNet datasets. Under a high staleness delay tolerance 56, OLCO3 achieves better convergence and has lower communication traffic than the baseline methods.\n\n\n--- Strengths\n\n- The paper is clearly written and has theoretical grounds.\n\n\n--- Major issues \n\nMy major issue with this paper is its novelty and experimental setups.\n\n- OLCO3 simply applies three existing delay-tolerant SGD techniques together. I agree that sometimes it needs special designs to combine multiple techniques and that successfully combining them could be an important contribution, but I am not sure it is the case for this work. For example, [Cui et al., 2014], used Stale synchronous Parallel and batched the communication for multiple iterations. [Lin et al., 2018] used gradient compression, and they also accumulated all the compression residuals and added them together with the gradients of the next iteration. So OLCO3's communication scheme and compression compensation techniques are not really novel.\n\n* [Cui et al., 2014] Exploiting bounded staleness to speed up big data analytics. USENIX ATC 2014.\n* [Lin et al., 2018] Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training. ICLR 2018.\n\n- The experiments in this paper used a delay tolerance of 56, which I think is too high for ResNet training. A delay tolerance of 56 means the workers will only communicate every 56 iterations when s=1, or they will have to tolerate 56 iterations of stale data when p=1. The ResNet-50 model has less than 25 million model parameters, and when represented in float32, they are only 100 MB in size. The sizes of the gradients are the same as the model parameters, so they are also 100 MB. I don't think it makes a lot of sense to delay the communication of just 100 MB of data for 56 iterations, especially when there are only two or four 4-GPU machines connected with 40 Gbps Ethernet. Showing that OLCO3 outperforms the other methods under this unrealistic delay tolerance is not very meaningful. The paper mentions that the high delay tolerance could be useful to federated learning, but federated learning is not very common right now, and the paper's experiments are not performed on a federated learning setup either.\n\n- The evaluation also did not measure the wall-clock training time. So it is not clear to me 1) how using the more stale gradient updates speeds up the training and 2) how much computational overhead the gradient compression incurs. I suggest the authors compare the wall-clock training time of their system with the wall-clock training time of synchronous SGD using the state-of-the-art distributed training frameworks.\n\n\n--- Other comments\n\n- Section 2, second paragraph, \"Note that Pipe-SGD is different from asynchronous SGD (Ho et al., 2013; Lian et al., 2015) which computes stochastic gradient using stale model and does not parallelize the computation and communication of a worker.\"\nThis statement is not correct. The State Synchronous Parallel parameter server proposed by [Ho et al., 2013] and [Cui et al., 2014] actually pipelines the communication with computation. Also, whether the communication is pipelined with the computation or not is orthogonal to the communication scheme. For example, the communication can be pipelined with the computation even for synchronous SGD because the model parameter updates from the training backward pass come out layer by layer, and the parameter server systems usually send out the computed parameter updates of the upper layers before the updates of the lower layers are computed.\n\n- Section 5, experimental setup.\nWhy was ResNet-50 model used for the ImageNet dataset but a larger ResNet-110 model was used for the much smaller Cifar-10 dataset?\n\n- Figure 1.\nPlease explain whether the communication budget is the total budget over all workers or the per-worker budget. The graph shows that Cifar-10 only has less than 1 GB of communication traffic even for the baseline synchronous SGD model. If that's the case, it is arguable whether it is indeed useful to do gradient compression when the total traffic for the whole training is only 1 GB.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1285/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1285/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122185, "tmdate": 1606915808326, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1285/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review"}}}, {"id": "ubgXw8cWvf", "original": null, "number": 2, "cdate": 1603811894704, "ddate": null, "tcdate": 1603811894704, "tmdate": 1605024482810, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "invitation": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review", "content": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training ", "review": "Review: This paper studies distributed training of neural networks. The major obstacles in distributed training are communication costs and communication delays. In the literature there exists different methods which attempt to overcome these two issues but, as far as the authors claim, none of the existing algorithms succeeds in dealing with both these aspects at the same time. The authors propose a novel distributed method OLCO_3 which is designed to address both communication costs and communication delays. In particular, the authors propose two variants of the method: OLCO_3 TC which comprises pipelining and compensation, and OLCO_3 VQ which comprises pipelining with communication-dependent compressor. Both the versions of OLCO_3 are analyzed from a theoretical perspective: under some assumptions the authors conduct a theoretical analysis of the convergence of the proposed schemes. Finally, the method in its two variants is benchmarked and compared with state-of-the-art distributed algorithms. \n\n\n+ The authors makes a comprehensive review of the literature and the major techniques used in distributed training of NNs. \n\n+ The authors are focusing on two really critical aspects of distributed training: communication costs and delays. These two aspects represent the bottleneck of distributed training and contributes in these directions would be of great impact.  \n\n \nConcerns: \n\n- The paper is not well-written. In addition to some typos, the style is confused and therefore the paper results hard to read. The content is not clearly explained and presented. Overall the paper requires some re-writing and polishing.\n\n \n- The theoretical results are not commented enough and the derived bounds do require some extra explanation and contextualization.\n\n\n- Since the OLCO_3 method is motivated by the authors in terms of communication and delays efficiency, the theoretical analysis should also maybe account more for these two aspects and underline the major advantages with respect to the state-of-the-art methods in terms of communication costs and delays handling.\n \n\n- Regarding the benchmark section, only test metrics are shown but test metrics are not directly related to the convergence but rather with the generalization properties, while the theoretical results focus on the convergence. \n\n---------------------------------------------------------------------------------------------------------------------------------------------------", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1285/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1285/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122185, "tmdate": 1606915808326, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1285/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review"}}}, {"id": "XqkxGFgVSeF", "original": null, "number": 4, "cdate": 1604680445897, "ddate": null, "tcdate": 1604680445897, "tmdate": 1605024482685, "tddate": null, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "invitation": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review", "content": {"title": "initial review", "review": "The paper considers delay-tolerant and communication-efficient in distributed training and proposes a training framework OLCO3 with local update steps, staleness compensation, and compression compensation. The proposed OLCO3 can be generalized to OLCO3-TC & OLCO3-OC (master-slave case), and OLCO3-VQ (both master-salve and all-reduce case).\n\n### pros.\n* the paper is well-written; the arguments are supported by both empirical and theoretical results.\n* the convergence analyses are provided for OLCO3-VQ and OLCO3-TC, for both SGD and momentum SGD.\n* in numerical results, both iid and non-iid cases are considered.\n\n### cons.\n* missing compression compensation baseline. the paper considers local update, staleness compensation, and compression compensation, but in the empirical results, only local SGD and some delay tolerance methods are considered. it is suggested to also include the results from the powerSGD and the signSGD, thus the readers can identify the source of quality loss.\n* different compression operators are used for different OLCO3 variants. I noticed that in the evaluation part, the paper considers  OLCO3-OC with signSGD, OLCO3-VQ with powerSGD, and OLCO3-TC with signSGD. it is encouraged to justify such a design choice.\n* unclear practical impact. even though it is intuitive to design a training system that has local update steps, staleness compensation, and compression compensation, its practical impact is still unclear to me (due to the trade-off between test accuracy and system performance). it is encouraged to include (e.g. a simulated) results to illustrate the potential trade-off (e.g. time-to-accuracy) on different distributed training scenarios (e.g. differ in latency, bandwidth, local computation capability).\n* the ideas like local update steps, staleness compensation, and compression compensation, have been well developed in the distributed machine learning community. though I do acknowledge the efforts of formulating/combining these ideas into a unified framework, the significance (novelty) of the paper might still have some limitations (as the proof seems quite standard to me). I would like to encourage authors to provide comprehensive empirical results to justify the pros and cons of the proposed scheme, as well as some practical guidelines.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1285/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1285/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Delay-Tolerant Local SGD for Efficient Distributed Training", "authorids": ["~An_Xu1", "~Xiao_Yan2", "~Hongchang_Gao1", "~Heng_Huang1"], "authors": ["An Xu", "Xiao Yan", "Hongchang Gao", "Heng Huang"], "keywords": ["Delay-tolerant", "communication-efficient", "distributed learning"], "abstract": "The heavy communication for model synchronization is a major bottleneck for scaling up the distributed deep neural network training to many workers. Moreover, model synchronization can suffer from long delays in scenarios such as federated learning and geo-distributed training. Thus, it is crucial that the distributed training methods are both \\textit{delay-tolerant} AND \\textit{communication-efficient}. However, existing works cannot simultaneously address the communication delay and bandwidth constraint. To address this important and challenging problem, we propose a novel training framework OLCO\\textsubscript{3} to achieve delay tolerance with a low communication budget by using stale information. OLCO\\textsubscript{3} introduces novel staleness compensation and compression compensation to combat the influence of staleness and compression error. Theoretical analysis shows that OLCO\\textsubscript{3} achieves the same sub-linear convergence rate as the vanilla synchronous stochastic gradient descent (SGD) method. Extensive experiments on deep learning tasks verify the effectiveness of OLCO\\textsubscript{3} and its advantages over existing works.", "one-sentence_summary": "We propose a delay-tolerant AND communication-efficient training method for distributed learning.", "pdf": "/pdf/669f7d42004ef8a1c1400d67c30c5d7f3cf9efb6.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xu|delaytolerant_local_sgd_for_efficient_distributed_training", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1wYPEyr9YQ", "_bibtex": "@misc{\nxu2021delaytolerant,\ntitle={Delay-Tolerant Local {\\{}SGD{\\}} for Efficient Distributed Training},\nauthor={An Xu and Xiao Yan and Hongchang Gao and Heng Huang},\nyear={2021},\nurl={https://openreview.net/forum?id=E_U8Zvx7zrf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E_U8Zvx7zrf", "replyto": "E_U8Zvx7zrf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1285/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122185, "tmdate": 1606915808326, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1285/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1285/-/Official_Review"}}}], "count": 6}