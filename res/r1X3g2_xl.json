{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1494099989326, "tcdate": 1478176939094, "number": 62, "id": "r1X3g2_xl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1X3g2_xl", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396334288, "tcdate": 1486396334288, "number": 1, "id": "H1LnjGIux", "invitation": "ICLR.cc/2017/conference/-/paper62/acceptance", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks. \n \n The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons). \n \n The AC thus recommends accepting this work as a poster.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396334806, "id": "ICLR.cc/2017/conference/-/paper62/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396334806}}}, {"tddate": null, "tmdate": 1483814911117, "tcdate": 1483664207539, "number": 4, "id": "HkwLjwnHx", "invitation": "ICLR.cc/2017/conference/-/paper62/public/comment", "forum": "r1X3g2_xl", "replyto": "Skzr55l4e", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for the review! \n\n\u201cIn Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\u201d\nYes, we added the IMDB performance with Naive Bayes SVM by Wang and Manning (2012) to the Table 2.\n\n\u201cAs another baseline, did you consider dropping words, i.e. masking noise?\u201d\nThe performance using dropping words is demonstrated by Dai and Le (2015) (SA-LSTM in Table 2 in our paper.), and it is little bit better than our baseline ( 7.24% with SA-LSTM, and 7.33% with our baseline). \nWe also tested dropping words ( randomly masking each embedding of the word ) on our baseline models, however, we could not find any improvement from the baseline.\n\n\u201cI am not sure I understand why virtual adversarial is worse than the baseline in Table 5.\u201d\nIt is because we optimized epsilon between [1.0, 10.0] on each dataset. If epsilon is set to 0, the performance should be same as the baseline. The reason why we optimized epsilon between such a narrow range is that training the LSTM model is computationally costly, and on IMDB, Elec and RCV1, the optimal epsilons seem to be within [1.0, 10.0] on each validation set.\n\n\u201cI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training.\u201d\nWe added the performance with transductive SVMs on IMDB, Elec and RCV1 by Johnson and Zhang (2015) to Table 2 and 4, and our proposed method outperforms the SVM-based methods.\nNote that transductive learning is somewhat different from semi-supervised learning because transductive learning allows the model to look at the test set.\nWe added an explanation of the similarities between adversarial training methods and SVM-based methods in the related work section. Please see the revised version of our paper. \n\n\u201cAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. \u201c\nThe similarities and differences between adversarial training and other methods like CAEs, double backprop, tangent propagation, etc. are described in the Deep Learning textbook ( http://www.deeplearningbook.org/contents/regularization.html , http://www.deeplearningbook.org/contents/representation.html ) so we don\u2019t feel it\u2019s necessary to repeat this amount of background explanation in the conference paper. There is also a separate ICLR 2015 workshop paper describing the connections between adversarial training and contrastive divergence: Goodfellow, On distinguishability criteria for estimating generative models. https://arxiv.org/abs/1412.6515\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744428, "id": "ICLR.cc/2017/conference/-/paper62/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1X3g2_xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper62/reviewers", "ICLR.cc/2017/conference/paper62/areachairs"], "cdate": 1485287744428}}}, {"tddate": null, "tmdate": 1483664241175, "tcdate": 1483664241175, "number": 5, "id": "rkFOsvhBg", "invitation": "ICLR.cc/2017/conference/-/paper62/public/comment", "forum": "r1X3g2_xl", "replyto": "r1TrCM7Eg", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the review! \n\n\u201cit is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze.\u201c\nAn LSTM is a kind of RNN. Simple RNNs, such as those based on linear transformation and tanh at each time step, are very difficult to interpret, just as LSTMs are. LSTMs may even be somewhat easier to interpret because information is propagated through time using addition.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744428, "id": "ICLR.cc/2017/conference/-/paper62/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1X3g2_xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper62/reviewers", "ICLR.cc/2017/conference/paper62/areachairs"], "cdate": 1485287744428}}}, {"tddate": null, "tmdate": 1483664106710, "tcdate": 1483663835324, "number": 3, "id": "BJXyqv2rx", "invitation": "ICLR.cc/2017/conference/-/paper62/public/comment", "forum": "r1X3g2_xl", "replyto": "ByxiEPX4l", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "We added results with SVM-based methods and explain the similarities between SVM-based methods and adversarial training methods.", "comment": "Thank you for the review! \nAs you suggested, we added results on IMDB performance with Naive Bayes SVM by Wang and Manning (2012) to Table 2 in the paper. \nWe also added results with transductive SVMs on IMDB, Elec and RCV1 by Johnson and Zhang (2015) to Table 2 and 4.\nWe confirmed that adversarial and virtual adversarial training outperform the SVM-based approaches.\nAdditionally, we explain the similarities between SVM-based methods and adversarial training methods in the related work section. Please see the revised version of our paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744428, "id": "ICLR.cc/2017/conference/-/paper62/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1X3g2_xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper62/reviewers", "ICLR.cc/2017/conference/paper62/areachairs"], "cdate": 1485287744428}}}, {"tddate": null, "tmdate": 1482024088151, "tcdate": 1482024088151, "number": 3, "id": "ByxiEPX4l", "invitation": "ICLR.cc/2017/conference/-/paper62/official/review", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/conference/paper62/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper62/AnonReviewer2"], "content": {"title": "Reads well, missing theoretical differences with past techniques.", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose to apply virtual adversarial training to semi-supervised classification.\n\nIt is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.\n\nIn terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).\n\nConcerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).\n\nOverall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711572, "id": "ICLR.cc/2017/conference/-/paper62/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper62/AnonReviewer4", "ICLR.cc/2017/conference/paper62/AnonReviewer3", "ICLR.cc/2017/conference/paper62/AnonReviewer2"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711572}}}, {"tddate": null, "tmdate": 1482006084908, "tcdate": 1482006084908, "number": 2, "id": "r1TrCM7Eg", "invitation": "ICLR.cc/2017/conference/-/paper62/official/review", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/conference/paper62/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper62/AnonReviewer3"], "content": {"title": "Good paper. The idea is simple but its result contributes new knowledge to the litterature", "rating": "7: Good paper, accept", "review": "This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. \n\nI only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711572, "id": "ICLR.cc/2017/conference/-/paper62/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper62/AnonReviewer4", "ICLR.cc/2017/conference/paper62/AnonReviewer3", "ICLR.cc/2017/conference/paper62/AnonReviewer2"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711572}}}, {"tddate": null, "tmdate": 1481876776762, "tcdate": 1481862741433, "number": 2, "id": "ryTIC1-Ne", "invitation": "ICLR.cc/2017/conference/-/paper62/public/comment", "forum": "r1X3g2_xl", "replyto": "HyKnE6l4e", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "Thanks for the comment!", "comment": "We did not try that constraint on the datasets we used in our paper,\nbut I think the balancing constraint would improve the performance especially on RCV1 dataset (The classes in RCV1 dataset are unbalanced).\nI have tried similar constraint with virtual adversarial training on semi-supervised learning task on MNIST (I constrained the marginal distribution p(y) := \\sum_x p(y|x)q(x) to be uniform) and it stabilized the training process and improved performance over the training without the constraint. \n\nThank you for your comment!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744428, "id": "ICLR.cc/2017/conference/-/paper62/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1X3g2_xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper62/reviewers", "ICLR.cc/2017/conference/paper62/areachairs"], "cdate": 1485287744428}}}, {"tddate": null, "tmdate": 1481852081085, "tcdate": 1481852081085, "number": 2, "id": "HyKnE6l4e", "invitation": "ICLR.cc/2017/conference/-/paper62/pre-review/question", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/conference/paper62/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper62/AnonReviewer2"], "content": {"title": "balancing constraint", "question": "Did the author try setups were classes were unbalanced? (In past semi-supervised literature, balancing constraints were important).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481852081546, "id": "ICLR.cc/2017/conference/-/paper62/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper62/AnonReviewer3", "ICLR.cc/2017/conference/paper62/AnonReviewer2"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481852081546}}}, {"tddate": null, "tmdate": 1481841209773, "tcdate": 1481841209767, "number": 1, "id": "Skzr55l4e", "invitation": "ICLR.cc/2017/conference/-/paper62/official/review", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/conference/paper62/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper62/AnonReviewer4"], "content": {"title": "The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.", "rating": "7: Good paper, accept", "review": "*** Paper Summary ***\n\nThis paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.\n\n*** Review Summary ***\n\nThe paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.\n\n*** Detailed Review ***\n\nThe paper reads well. I have only a few comments regarding experiments and link to prior resarch:\n\nExperiments:\n\n- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?\n- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?\n- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?\n\nRelated Work:\n\nI think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.\n\nAlso it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.\n\n*** References ***\n\nMarginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.\nStacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.\nLearning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.\nLearning Deep Architectures for AI, Yoshua Bengio 2009\nLarge Scale Transductive SVMs. Ronan Collobert et al 2006\nOptimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512711572, "id": "ICLR.cc/2017/conference/-/paper62/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper62/AnonReviewer4", "ICLR.cc/2017/conference/paper62/AnonReviewer3", "ICLR.cc/2017/conference/paper62/AnonReviewer2"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512711572}}}, {"tddate": null, "tmdate": 1480987766329, "tcdate": 1480987729852, "number": 1, "id": "Sk9L4qmXl", "invitation": "ICLR.cc/2017/conference/-/paper62/public/comment", "forum": "r1X3g2_xl", "replyto": "ryaPN5k7g", "signatures": ["~Takeru_Miyato1"], "readers": ["everyone"], "writers": ["~Takeru_Miyato1"], "content": {"title": "Re. Experimental settings", "comment": "Thanks for your comments!\n>Is the virtual adversarial training used in the pre-training phase?\nNo, we used virtual adversarial training only in the fine-tune phase.\n> Also In the results of Table 2, for the line of \"Virtual Adv\" and \"Adv+Virtual Adv\", the cost term of formula (3) are applied to all the samples, or only applied to unlabeled samples?\nThe former is correct. We calculate the virtual adversarial training loss (3) on both labeled and unlabeled samples.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287744428, "id": "ICLR.cc/2017/conference/-/paper62/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1X3g2_xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper62/reviewers", "ICLR.cc/2017/conference/paper62/areachairs"], "cdate": 1485287744428}}}, {"tddate": null, "tmdate": 1480725605545, "tcdate": 1480725605541, "number": 1, "id": "ryaPN5k7g", "invitation": "ICLR.cc/2017/conference/-/paper62/pre-review/question", "forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "signatures": ["ICLR.cc/2017/conference/paper62/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper62/AnonReviewer3"], "content": {"title": "Experiment settings", "question": "Hi,\n\nIs the virtual adversarial training used in the pre-training phase? Also In the results of Table 2, for the line of \"Virtual Adv\" and \"Adv+Virtual Adv\", the cost term of formula (3) are applied to all the samples, or only applied to unlabeled samples?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "abstract": "Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting.\nHowever, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations.\nWe extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself.\nThe proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks.\nWe provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.\n", "pdf": "/pdf/42ed913e7e3a7178dc41d1f9881d8d38420f9f07.pdf", "paperhash": "miyato|adversarial_training_methods_for_semisupervised_text_classification", "conflicts": ["kyoto-u.ac.jp", "atr.jp", "preferred.jp", "google.com", "openai.com"], "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning"], "authors": ["Takeru Miyato", "Andrew M. Dai", "Ian Goodfellow"], "authorids": ["takeru.miyato@gmail.com", "adai@google.com", "ian@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481852081546, "id": "ICLR.cc/2017/conference/-/paper62/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper62/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper62/AnonReviewer3", "ICLR.cc/2017/conference/paper62/AnonReviewer2"], "reply": {"forum": "r1X3g2_xl", "replyto": "r1X3g2_xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper62/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481852081546}}}], "count": 12}