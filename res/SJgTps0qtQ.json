{"notes": [{"id": "SJgTps0qtQ", "original": "Bklk-sTct7", "number": 847, "cdate": 1538087877275, "ddate": null, "tcdate": 1538087877275, "tmdate": 1545355389934, "tddate": null, "forum": "SJgTps0qtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ByxZ_GAxlV", "original": null, "number": 1, "cdate": 1544770152616, "ddate": null, "tcdate": 1544770152616, "tmdate": 1545354520972, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Meta_Review", "content": {"metareview": "The paper presents a strategy for randomizing the underlying physical hyper-parameters of RL environments to improve policy's robustness. The paper has a simple and effective idea, however, the machine learning content is minimal. I agree with the reviewers that in order for the paper to pass the bar at ICLR, either the proposed ideas need to be extended theoretically or it should be backed with much more convincing results. Please take the reviewers' feedback into account and improve the paper.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "The paper needs improvement"}, "signatures": ["ICLR.cc/2019/Conference/Paper847/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper847/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353063474, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper847/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper847/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper847/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353063474}}}, {"id": "H1ezADy9CQ", "original": null, "number": 4, "cdate": 1543268297985, "ddate": null, "tcdate": 1543268297985, "tmdate": 1543268297985, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "BJgPWcE0hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your review."}, "signatures": ["ICLR.cc/2019/Conference/Paper847/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609217, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJgTps0qtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper847/Authors|ICLR.cc/2019/Conference/Paper847/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609217}}}, {"id": "rygMev1q0m", "original": null, "number": 3, "cdate": 1543268074201, "ddate": null, "tcdate": 1543268074201, "tmdate": 1543268074201, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "SklnoXrk6m", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your review.\n\n--------------------------------\nI) Response to your questions:\n\n1. K and M are unrelated. 'M' is the total number of environment settings can be changed. K is the number of 'tasks' initialized under a specific environment configuration - i.e. after sampling some settings, we train on those settings for K episodes without changing them\n\n2. Tasks are initialized uniformly over the state space of the task. Environment settings are initialized per the sampling policy defined by the bandit.\n\n--------------------------------\nII) Please clarify:\n\nIn your review, you have a sentence, which seems to end abruptly: \"For instance, the authors claim that\". Could you please clarify what you meant to say?\n\nYou mention \u201cmore sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2]\u201d, however neither references [1] nor [2] in your review seem to address such suites. Could you please clarify which suites you are referring to?\n\n--------------------------------\nIII) Primary contributions of this work and novelty:\n\nPrimarily, we sought to introduce a learning scheme that would combat the problem of policy brittleness in RL when policies are exposed to environmental behavior not seen in training. We recognize that work has been done in model-based RL to combat these problems, however, we specifically focus on the model-free setting.\n\nWe focus specifically on a model-free formulation because measuring the properties of a test environment may sometimes be impossible or infeasible - for example, in a real-world task, adding sensors and processing to perceive the properties of the environment may prove too expensive to be feasible. If the action policy could instead be robust to changes, the problem is somewhat alleviated. \n\nTo your point about model parameters needing to be known a priori during training, we would like to note that we also test our trained policies against environmental settings that were not explicitly trained for. For example, as shown in Table 3, we train the pendulum with only masses 2, 4, 6, 8 and 10, but masses 1, 3, 5, 7 and 9 are also tested and appear to be implicitly handled. We observed similar trends with ball-pushing. The apparent efficacy demonstrated by this result also seemed to justify the choice of discrete bandit scheme.\n\nWhen discussing inadvertent generalization, we were specifically referring to the observation that the brittleness of RL policies seems to change quite significantly, and is even seemingly mitigated, in response to small changes in the environment during training, in ways that allow it to remain robust despite lacking a model of the environment in test time. To the best of our knowledge, this has not been addressed in prior work.\n\n--------------------------------\nIV) Addressing related works:\n\n1. We note that model-free randomization (reference [4] in your review) is effectively uniform random sampling employed during training - which is equivalent to the \u2018joint\u2019 training, which we use a baseline.\n\n2. Adaptive randomization (reference [3]) appears to adapt the training distribution by testing policies in a target environment . However, in our work, we do not assume access to tests on target domains during training, disallowing us from adopting a similar adaptation approach.\n\n3. We sought to demonstrate our algorithm on a few controls tasks, as is the current norm in deep RL, where it was tentatively more intuitive to understand the control schemes and strategies. However, we view our approach as more generally applicable to deep RL and thus did not focus on comparing it against controls techniques like MRAC, MPC or LQR. Unlike traditional controls, our method is directly extensible to tasks where variations in the task environment are not limited to changes in control system dynamics - examples include training object-detection/recognition to be more robust to scene lighting, or game AI to be more robust to level design.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper847/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609217, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJgTps0qtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper847/Authors|ICLR.cc/2019/Conference/Paper847/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609217}}}, {"id": "r1lZIB19CX", "original": null, "number": 2, "cdate": 1543267656744, "ddate": null, "tcdate": 1543267656744, "tmdate": 1543267656744, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "HyeiUJ1laQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your review.\n\nCitation issues have been resolved in the latest revision of the paper.\n\nWhile a PID controller is likely to solve the problems we addressed, our focus was not on being able to implement a control policy for individual tasks, but rather to demonstrate a technique to combat policy brittleness in generic deep RL - where learned policies fail when presented with data from domains different to those they were trained on, which is often a problem in machine learning."}, "signatures": ["ICLR.cc/2019/Conference/Paper847/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609217, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJgTps0qtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper847/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper847/Authors|ICLR.cc/2019/Conference/Paper847/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers", "ICLR.cc/2019/Conference/Paper847/Authors", "ICLR.cc/2019/Conference/Paper847/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609217}}}, {"id": "HyeiUJ1laQ", "original": null, "number": 3, "cdate": 1541562195087, "ddate": null, "tcdate": 1541562195087, "tmdate": 1541562195087, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "content": {"title": "An interesting view point on robustness.", "review": "This paper investigated the robustness of RL policies learning under different environmental conditions. \n\nBased on the observations that policies learnt in different experimental settings lead to different generalizability, the authors proposed an EXP3 based reward-guided curriculum for improving policy robustness. The algorithm was tested on inverse pendulum, cart-pole balancing, and ball-pushing in OpenAI gym.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding. Papers in the references were not properly cited.\n\nIt is an interesting discovery that different environment brewed different policies with different robustness/generalizability in daily life. However, these are also easily derivable in physics, especially in the three experiments tested in the paper. It would be more complete to compare with PID controllers.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper847/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "cdate": 1542234363515, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815805, "tmdate": 1552335815805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklnoXrk6m", "original": null, "number": 2, "cdate": 1541522339958, "ddate": null, "tcdate": 1541522339958, "tmdate": 1541533640789, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "content": {"title": "Curriculum design for dynamics randomization with a Bandits style method during training. ", "review": "The paper looks at the problem of generalization across physical parameter varaition in learning for continuous control. The paper presents a method to develop a sampling based curriculum over env. settings for training robust agents. \n\n\n* The paper makes an interesting observation on inadvertent generalization in robust policy learning. \nHowever, the examples in both the cartpole and the pendulum cases seem not to be watertight. \nFor instance, the authors claim that \nBut from a dynamical system perspective in both cases, the controller is operating near limits. \nThe solution and subsequent generalization depend more on the topology of the solution space. \nA heavy Pendulum is an overdamped system and required the policy to operate at the limits of action to generate momentum for swing up. Hence a solution for a lighter pendulum in implicitly included. Similarly, the rolling ball is an underdamped system, and where the policy operates near zero limits in light ball case to prevent the system from going unstable. Adding mass results in damping which makes it easier. In this case, as well the solution space is implicitly contained.\n\n\nBut this is not a novel observation. Similar observations have been made for Robust control and Model-Reference Adaptive Control. \nThe paper also overlooks a number of related works in model-free randomization [4], adaptive randomization [3], adversarial randomization [5,6]. The method also does not compare with model-based methods for adaptive policy learning and iLQR based methods to handle this problem [2, 7].\n\n\nThe argument that the method is model-free is perhaps not as acceptable since the model parameters need to be known apriori for adaptation. The policy itself may be model-free but that is a design choice. \nA good experimental evaluation for this is generalization across known unknowns and unknown unknowns. \n\n\n* The algorithm itself is reasonable but the problem setup and choice of a discrete dynamics parameter choices are questionable. The bandit style method operates over a discrete decision set. \nIt also assumes in the multi-parameter setting that they are independent, which may not be true very often. \n\nThe algorithm proposed itself isnt novel, but would have been justified if the results supported the use of such a method. \n\n* Experiments are quite weak. \nBoth the experimental domains are rather simplistic with smooth nonlinear dynamics. There are more sophisticated and interesting continuous control environments such as control suite [1] or manipulation suite [2].  \n\nIt would be useful to see how tis method works in more complicated domains and how the performance compares with simpler methods such as joint brute-force randomization both in performance and in computation.  \n\nQuestions: \n1. Please provide details of Algorithm 1. How are the quantities K and M related? \n2. What is the process of task initialization? What information is required and what priors are used. Uniform prior over what range?\n\n\nIn summary, the authors explore an interesting adaptive curriculum design method. However, in its current form, the work needs more thought and empirical evaluation for the sake of completeness. \n\n\nReferences:\n1. Model Reference Adaptive Control [https://doi.org/10.1007/978-1-4471-5102-9_116-1\n]\n2. ADAPT: Zero-Shot Adaptive Policy Transfer for Stochastic Dynamical Systems [https://arxiv.org/abs/1707.04674]\n3. EPOpt: Learning Robust Neural Network Policies Using Model Ensembles [https://arxiv.org/abs/1610.01283]\n4. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World\n[https://arxiv.org/abs/1610.01283]\n5. Certifying Some Distributional Robustness with Principled Adversarial Training [https://arxiv.org/pdf/1710.10571.pdf]\n6. Adversarially Robust Policy Learning: Active Construction of Physically-Plausible Perturbations [http://vision.stanford.edu/pdf/mandlekar2017iros.pdf]\n7. Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization [https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf]\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper847/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "cdate": 1542234363515, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815805, "tmdate": 1552335815805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgPWcE0hQ", "original": null, "number": 1, "cdate": 1541454335382, "ddate": null, "tcdate": 1541454335382, "tmdate": 1541533640537, "tddate": null, "forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "content": {"title": "The papers proposes methods to robustify reinforcement learning algorithms against environment uncertainty which arises due to parametric variability. This is a interesting paper with promising results. What would make this paper a clear accept is the addition of experiments with high dimensional systems with more unknown parameters.  ", "review": "- Does the paper present substantively new ideas or explore an under-explored or highly novel question? \n\nThe paper claimed that there is limited work on the investigating the sensitivity of RL caused by the physics variations of the environment, such as object weight, surface friction, arm dynamics, etc. So the paper proposed learning a stochastic curriculum, guided by episodic reward signals (which is their contribution compared with previous related work) to develop policies robust to environmental perturbation.  Overall the combination of ideas is novel but the experimental results are limited in scope. \n\n- Does the results substantively advance the state of the art?\n\nThe results advance the state of the art, since they are compared against : 1) the best results observed via a grid search (oracle) on policies trained exclusively on specific individual environment settings; 2) Policies trained under a mixed training structure, where the environment settings are varied every episode during training, with the episode settings drawn uniformly at random from a list of values of interest. Their 3 experiment results are competitive with 1) and much better than 2).\n\n- Will a substantial fraction of the ICLR attendees be interested in reading this paper? \n\nYes, because the robustness of RL policies to changes in the physic parameters of the environment has not been well explored. Although previous investigations exist, and this paper\u2019s algorithm is the combination of EXP3 and DDPG, it is still interesting to see them combined together to solve model uncertainty problem of RL with very good simulation results.\n\n- Would I send this paper to one of my colleagues to read? \n\n  I would definitely send the paper to my colleagues to read. \n\n\n- In terms of quality:  \n\nClear motivation; substantiated literature review; but the algorithms proposed are not novel and the question of whether the method will scale to more unknown parameters is not answered. \n\n- I terms of clarity:  \n\nEasy to read.\u2013Experimental evaluation is clearly presented.\n\n- Originality:  The problem of developing an automated curriculum for learning generalization over environment settings for a given RL task is formulated as a multi-armed bandit problem, and EXP3 algorithm is used to minimize regret and maximize the actor\u2019s rewards. Itis a very interesting application of EXP3, although such inspiration is drawn from a former multi-task NLP paper Graves et al. (2017).\n\n- In terms of significance:  \n\n The paper is definitely interesting and presents an  promising  direction. The significance is  limited because of the simplicity of the examples considered in the experimental session. It would be interesting to see how this method performs in problems with more states and more unknown parameters.   \n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper847/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning", "abstract": "Conventional reinforcement learning rarely considers how the physical variations in the environment (eg. mass, drag, etc.) affect the policy learned by the agent. In this paper, we explore how changes in the environment affect  policy generalization. We observe experimentally that, for each task we considered, there exists an optimal environment setting that results in the most robust policy that generalizes well to future environments. We propose a novel method to exploit this observation to develop robust actor policies, by automatically developing a sampling curriculum over environment settings to use in training. Ours is a model-free approach and experiments demonstrate that the performance of our method is on par with the best policies found by an exhaustive grid search, while bearing a significantly lower computational cost.", "keywords": ["Reinforcement Learning", "Policy Robustness", "Policy generalization", "Automated Curriculum"], "authorids": ["sidmys@bu.edu", "rplatt@ccs.neu.edu", "saenko@bu.edu"], "authors": ["Siddharth Mysore", "Robert Platt", "Kate Saenko"], "TL;DR": "By formulating the learning curriculum as a bandit problem, we present a principled approach to motivating policy robustness in continuous controls tasks.", "pdf": "/pdf/d4156afbd41056513c5b4467592f74eeb9b5fe81.pdf", "paperhash": "mysore|exploiting_environmental_variation_to_improve_policy_robustness_in_reinforcement_learning", "_bibtex": "@misc{\nmysore2019exploiting,\ntitle={Exploiting Environmental Variation to Improve Policy Robustness in  Reinforcement Learning},\nauthor={Siddharth Mysore and Robert Platt and Kate Saenko},\nyear={2019},\nurl={https://openreview.net/forum?id=SJgTps0qtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper847/Official_Review", "cdate": 1542234363515, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJgTps0qtQ", "replyto": "SJgTps0qtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper847/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815805, "tmdate": 1552335815805, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper847/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}