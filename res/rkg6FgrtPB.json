{"notes": [{"id": "rkg6FgrtPB", "original": "SkeKLAetwr", "number": 2451, "cdate": 1569439876732, "ddate": null, "tcdate": 1569439876732, "tmdate": 1577168226049, "tddate": null, "forum": "rkg6FgrtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "z0WprZhpD", "original": null, "number": 1, "cdate": 1576798749451, "ddate": null, "tcdate": 1576798749451, "tmdate": 1576800886461, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Decision", "content": {"decision": "Reject", "comment": "Unfortunately the paper is confusingly written, and there is only agreement by all reviewers on the rejection of the paper.  Indeed, if all reviewers and the area chair do not interpret the paper well, the authors' best response would be to rewrite the papers rather than disagree with all reviewers.\n\nIn the area chair's opinion, the current form the paper does not merit publication.  The authors are advised to address the reviewers' concerns, rework the paper, and submit to a conference again.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724780, "tmdate": 1576800276483, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Decision"}}}, {"id": "Sygu74JooS", "original": null, "number": 3, "cdate": 1573741599550, "ddate": null, "tcdate": 1573741599550, "tmdate": 1573741599550, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "rygnNYcCFB", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank you for your valuable comments. We provide the answers below for the concerns raised in the review. \n \n(1)\nThe point of this paper is not to produce a new machine learning framework for classification tasks, but to understand how the animal brain could work, by studying biologically plausible neural networks under the lens of machine learning.  \n\n(2)\nOur approach doesn\u2019t always have to be used for a linear classifier NNE_x(y) = x^T W y, the NNE could be any function (see Lemma 1 and Theorem 1). We analyze the linear NNE in Theorem 2 to show that for any linear target function, NNE converges to an allele distribution arbitrarily close to the true labelling function, with high probability. The parameters that are optimized during the training process of NNE are the allele probabilities p. x is sampled from p as mentioned in section 2, paragraph 1. After sampling x, we use the weight generator matrix W of each layer to generate weights Wx of that layer of NNE. Updates to the allele probabilities p indirectly update the weights of all the layers of NNE simultaneously. Hence, although in linear case, NNE formulation looks similar to linear regression, in multilayer NNE, it\u2019s not. \n\n(3)\nThe way the probabilities are updated is given in equation 3, this is consistent with the weak selection regime as mentioned in section 1, paragraph 3 of our paper. While there is not explicit back-propagation, our point is that weak selection is implicitly performing something similar in spirit (this is not vanilla back-propagation since the weights of the links in the networks cannot be updated directly).  \n\n(4)\nIn lemma 1 we show that the performance of the allele, f, is in fact a function of the gradient of the loss function. In gradient descent-based optimization, it is well known that the decrease in the value of loss function after each iteration is proportional to the squared norm of the gradient (for e.g., see section 9.3 in [1]). Theorem 1 says that, given the update rule of the allele probabilities in equation (3), something similar holds for NNE! By choosing a small enough learning rate, the expected decrease in the loss at generation t+1 is proportional to the squared norm of the gradient at generation t taken at the coordinates of the allele distribution which are far from 0 or 1.  \n\n(5)\nBeta defines the sparsity of each weight in the weight generation matrix. We want each weight w_ij of the network to be a sparse random combination of the genes x so that update to an allele probability p(i) affects only a small number of synapses in the network that depend on x(i).  \n\nReferences: \n\n[1]  Stephen Boyd and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press, New York, NY, USA. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6FgrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2451/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2451/Authors|ICLR.cc/2020/Conference/Paper2451/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141155, "tmdate": 1576860556833, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment"}}}, {"id": "rygLcMyojS", "original": null, "number": 2, "cdate": 1573741197819, "ddate": null, "tcdate": 1573741197819, "tmdate": 1573741197819, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "S1eZ0ExCuH", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank you for your valuable comments. We provide the answers below for the concerns raised in the review: \n\nIn this paper, we propose two approaches to understand the process of learning in animal brain, that opens up the exciting possibility of improved learning using insights from evolution and the brain; 1) NNE (Neural Net Evolution), inspired by the standard update rule in population genetics, that succeeds in creating neural networks that perform simple labelling tasks with modest but promising preliminary results, both theoretical and experimental, suggesting that the neural networks in animal brain could have evolved, 2) DNN (Dopaminergic Neural Network) that is inspired from the recently established results on dopaminergic plasticity, which also give promising results on the classification tasks, supporting the plausibility of plasticity based updates in animal brains. \n \nThe approaches used in this paper are consistent with evolution in genetics, and we provide explanations and corrections to the minor issues raised in the review. Nevertheless, these issues don\u2019t seem significant enough to hinder the understanding of the methods proposed. \n\n(1)\nSTDP is an acronym for Spike-Time Dependent Plasticity. This appears in all the relevant works cited in the introduction section of the paper.  \n\n(2)\nWe consider a simple case when the alleles are 0 and 1. In case of more than 2 possible alleles, our method can be easily extended by appropriately encoding the alleles. \n\n(3)\nWe define gene as a single bit of information with two alleles 0 and 1 (section 1, paragraph 3), and genotype as a string of alleles (section 2, paragraph 1); both these definitions are consistent with those of genomics. We don\u2019t use the term phenotype explicitly in our method. \n\n(4)\nAs mentioned in section 1, paragraph 3, each gene has two alleles 1 or 0. At each generation, we fix the allele probabilities. Hence, the probability of a gene i having allele 1 for the genotypes of that generation is fixed. This sentence shall be updated in the revised version of the paper to remove the ambiguity. \n\n(5)\nAs mentioned in section 2, paragraph 1, n is the number of genes for each genotype. Hence, each genotype is a binary vector of size n, i.e, x \\in {0,1}^n. \n\n(6)\nThis is a minor typo. We correct it to y ~ D everywhere for consistency. \n\n(7)\nThis is a minor typo. We correct it to L(NNE_x(t), \\ell(y)). \n\n(8)\np^t is the allele probability distribution at generation t. We will add some explanation to make this clear.   \n\n(9)\nAs mentioned in the sentence after equation (3), \u201cThe multiplier \\epsilon captures the small degree to which the performance of this task by the animal confers an evolutionary advantage leading to larger progeny.\u201d \\epsilon can be viewed as the \u201clearning rate\u201d. \n\n(10)\nWe have provided exact reference to this sentence multiple times in the introduction - [Burger (2000); Chastain et al. (2014)] in section 1 paragraph 1 and 3. We also briefly explain in section 1 paragraph 3 what \u201cweak selection\u201d means. \n\n(11)\nThis is a minor typo.  \n\n(12)\n\\gamma is implicitly defined and can be calculated from the last equality in the first equation array on page 4. \n\n(13)\nd is defined in the proof of theorem 2 to be the size of the set J, which is again defined in the proof of theorem 2.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6FgrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2451/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2451/Authors|ICLR.cc/2020/Conference/Paper2451/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141155, "tmdate": 1576860556833, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment"}}}, {"id": "S1l46g1ojr", "original": null, "number": 1, "cdate": 1573740731739, "ddate": null, "tcdate": 1573740731739, "tmdate": 1573740731739, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "ByedtDasKS", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank you for your valuable comments. We provide the answers below for the concerns raised in the review. \n\n(1)\nIn section 2 we prove that, if the classifier to be learned is linear, then evolution indeed succeeds in creating neural network that classifies well, i.e., we show that the necessary updates to synapses, to perform the classification task, could happen in light of evolution. Our experimental results in table 1 on MNIST support this. We are not suggesting that the evolutionary mechanism is employed during a lifetime, only part of it is, to elicit feedback. Our paper takes the approach that evolutionary dynamics inspires a new type of ANN.  \n\n(2)\nAs mentioned in page 2 paragraph 4 in our submission, one of the recent investigations [Yagishita et al., 2014)] reveals that the release of dopamine affects the structural plasticity of certain synapses, within a narrow period of time after the synapse\u2019s firing. Since the proposed DNN implements a very similar mechanism \u2013 the release of dopamine is captured by the favorable outcome (via error) and the synaptic update is a function of this favorable outcome \u2013 it could be considered biologically plausible.  \n "}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6FgrtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2451/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2451/Authors|ICLR.cc/2020/Conference/Paper2451/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141155, "tmdate": 1576860556833, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Authors", "ICLR.cc/2020/Conference/Paper2451/Reviewers", "ICLR.cc/2020/Conference/Paper2451/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Comment"}}}, {"id": "S1eZ0ExCuH", "original": null, "number": 1, "cdate": 1570796745343, "ddate": null, "tcdate": 1570796745343, "tmdate": 1572972336473, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "First of all, I must confess that my knowledge is quite limited to read this paper. Perhap the authors present something that I can not catch up at the present.\n\nI conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. \nThe paper is somewhat cumbersome in the introduction that makes the reader (here is myself) can not understand the main idea. At first the authors introduce about evolution in genetics and genomics that is a bit different from what I known. Then the authors claim that they can show that the brain circuits can evolved in their model.\nThere are so many mistake and/or typos in sentences and in mathematical formulation. These make me can not finish reading the paper. I have to stop reading at the end of Section 2.\nHere are my concerns and questions:\n1). What is \"STDP\" in the 2nd papragraph in Introduction?\n2). in the 3rd papragraph in Introduction: \"Suppose that the brain circuitry for a particular classi\ufb01cation task, such as \u201cfood/not food\u201d,is encoded in the animal\u2019s genes, assuming each gene to have two alleles 0 and 1\". This is realistics, the allels in animal is 0 1 or 2 if encoded.\n3). The authors use the words \"gene, genotype, phenotype\" in a special way that is different to what I known in genomics (in GWAS).\n4).  in the 3rd papragraph in Introduction: \"At each generation, a gene is an independent binary variable with \ufb01xed probability of 1\". What do you mean by fixed probability of 1? I can not understand in anysense that I know.\n5). In Section 2, What is n? the authors start the mathematical formulation but I can not find out what is n? Is it the sample size?\n6). In Section 2, paragraph 2, you define y~ \\mathcal{D}, BUT then in all formulas later you denote y ~ D. What is D??? I can not understand.\n7).  In Section 2, paragraph 2, you define a label of y as \\ell(y), BUT then in the 1st sentence of the 3rd paragraph in Section 2 you wrote    L(NNE_x(t), l(y)). What is l(.) here ???\n8). In the equations (1) and (2), what is p^t  ???  You have NOT defined it.\n9). Right after equations (1) and (2), What is \\epsilon ???? Can NOt understand.\n10). The sentence right after the equation (3): \"This is the standard update rule in population genetics under the weak selection assumption.\" This is NOT trivial to me, and even the machine learning comunity, we do not know this rule, it is not obvious. PLEASE provide exact reference.\n11). the first equation in the PROOF of LEMMA 1 wrong  \\mathcal{L} (p^t)  should be equal to E_{x~p^t}  NOT p.\n12). in the PROOF of Theorem 1, I can NOT find out where \\gamma has been defined?\n13). What is d  in Theorem 2? is it the dimension? I make too many guesses !!!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665371933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Reviewers"], "noninvitees": [], "tcdate": 1570237722625, "tmdate": 1575665371945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review"}}}, {"id": "ByedtDasKS", "original": null, "number": 2, "cdate": 1571702656119, "ddate": null, "tcdate": 1571702656119, "tmdate": 1572972336436, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper the authors propose a method for training neural networks using evolutionary methods. The aim of developing this method is to provide a biological alternative to back-propagation. The authors prove that their method converges and with high probability succeeds in learning linear classification problems. Another method is also proposed which is linked to dopaminergic neurons.\n\nIn terms of presentation, the paper is generally clear and well-written. I was not able to assess the importance of the theoretical contributions of the work as my research is not in this area, so my comments are limited to the other aspects.\n\nWith regard to the biological plausibility of the method, it is unclear to me how the evolutionary method proposed here can enable learning in typical scenarios such as conditioning experiments in animals. The learning processes in animals typically occurs in short time spans (for example a few training sessions for conditioning to stimuli predicting food/no food) and therefore I don\u2019t find it plausible to suggest evolutionary methods across generations are behind such forms of learning. Perhaps what the authors have in mind applies more to other forms of behaviour such as innate and involuntary responses in animals formed across generations rather than ongoing updates in synaptic plasticity as an animal adjusts its behaviour using environmental feedback. But then in this case the biological plausibility of the method seems fairly limited and not really an alternative to methods such as back-propagation.\n\nThe other biological aspect of the proposed work is the connection to dopamine and using the sign of gradients for updating the weights. I think connecting the current learning rule to the activity of dopamine neurons requires quantitative comparisons with experimental data, otherwise although I agree that the method is biologically inspired, but whether it is biologically plausible is not clear.\n\nBased on the above comments, I think the work will benefit from further developments before being ready for publication."}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665371933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Reviewers"], "noninvitees": [], "tcdate": 1570237722625, "tmdate": 1575665371945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review"}}}, {"id": "rygnNYcCFB", "original": null, "number": 3, "cdate": 1571887412353, "ddate": null, "tcdate": 1571887412353, "tmdate": 1572972336399, "tddate": null, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "invitation": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper argues that Artificial Neural Network (ANN) lack in biological plausibility because of the back-propagation process. Therefore, the authors provide an alternative approach, named neural net evolution (NNE) that follows evolutionary theory. This approach uses a large number of genotypes (in the form of vector with binary logits) that will evolve overtime during training. It does not require to calculate the gradient explicitly. The authors have conducted some experiments on MNIST using ANN with only one hidden layer. The experimental results show that the NNE can learn the classification task reasonably well considering that no explicit back propagation is used. \n\nI think overall the motivation to combine ANN with evolutionary theory is very interesting. The reviewer is not very familiar with evolutionary theory. So I judge this paper in the perspective of machine learning, from which I think the current approach is a week variant of back-propagation that still relies on gradient (see detailed comments below). Based on this, I give my rating. \n\nThe approach is formulated as NNE_x(y) = (x^T)*(W^T)*y. In traditional linear regression, W is the weight to be learnt. In this paper's formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d. with certain probabilities. The parameters to be optimized is x, which is named as a genotype that is viewed as a vector x \\in {0, 1}^n. So first of all, as W is fixed so the formulation is very similar to a traditional linear regression with an additional linear transform. The difference is that x is a binary vector with probabilities. These probabilities are optimized over time. \n\nFrom the Equations 1), 2) and 3), the probabilities are updated in a way to minimize the loss. This is kind of similar to back-propagation. Then the probabilities are updated and thus x is changed as well. In my understanding, this is still gradient-based optimization. I do not see it fundamental different to back-propagation. This is my main concern about this work. \n\nI did not check the details of Theorem 1. Could the authors please comment what is the purpose of Theorem 1 before proving it? This part is unclear to me in this paper. \n\nOne more question, for the W matrix,  the authors choice beta = 0.0025 in the experiment. Is there any particular reason for this choice? Or does it matter what value to choice as it is fixed anyway? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2451/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sruthi@comp.nus.edu.sg", "anandl@iisc.ac.in", "christos@columbia.edu", "vempala@gatech.edu", "y.naganand@gmail.com"], "title": "Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity", "authors": ["Sruthi Gorantla", "Anand Louis", "Christos H. Papadimitriou", "Santosh Vempala", "Naganand Yadati"], "pdf": "/pdf/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "abstract": "Artificial neural networks (ANNs) lack in biological plausibility, chiefly because backpropagation requires a variant of plasticity (precise changes of the synaptic weights informed by neural events that occur downstream in the neural circuit) that is profoundly incompatible with the current understanding of the animal brain. Here we propose that backpropagation can happen in evolutionary time, instead of lifetime, in what we call neural net evolution (NNE). In NNE the weights of the links of the neural net are sparse linear functions of the animal's genes, where each gene has two alleles, 0 and 1. In each generation, a population is generated at random based on current allele frequencies, and it is tested in the learning task. The relative performance of the two alleles of each gene over the whole population is determined, and the allele frequencies are updated via the standard population genetics equations for the weak selection regime. We prove that, under assumptions, NNE succeeds in learning simple labeling functions with high probability, and with polynomially many generations and individuals per generation. We test the NNE concept, with only one hidden layer, on MNIST with encouraging results. Finally, we explore a further version of biologically plausible ANNs inspired by the recent discovery in animals of dopaminergic plasticity: the increase of the strength of a synapse that fired if dopamine was released soon after the firing.", "keywords": ["Biological plausibility", "dopaminergic plasticity", "allele frequency", "neural net evolution"], "paperhash": "gorantla|biologically_plausible_neural_networks_via_evolutionary_dynamics_and_dopaminergic_plasticity", "original_pdf": "/attachment/7587b1d8e81138b84d713d63e3663a0107201238.pdf", "_bibtex": "@misc{\ngorantla2020biologically,\ntitle={Biologically Plausible Neural Networks via Evolutionary Dynamics and Dopaminergic Plasticity},\nauthor={Sruthi Gorantla and Anand Louis and Christos H. Papadimitriou and Santosh Vempala and Naganand Yadati},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6FgrtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6FgrtPB", "replyto": "rkg6FgrtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2451/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575665371933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2451/Reviewers"], "noninvitees": [], "tcdate": 1570237722625, "tmdate": 1575665371945, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2451/-/Official_Review"}}}], "count": 8}