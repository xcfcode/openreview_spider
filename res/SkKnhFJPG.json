{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124440816, "tcdate": 1518472368941, "number": 321, "cdate": 1518472368941, "id": "SkKnhFJPG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SkKnhFJPG", "signatures": ["~Brian_Cheung1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Convolutional vs. Recurrent Neural Networks for Audio Source Separation", "abstract": "We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.", "paperhash": "mobin|convolutional_vs_recurrent_neural_networks_for_audio_source_separation", "keywords": ["convolutional neural networks", "speech", "source seperation"], "_bibtex": "@misc{\n  mobin*2018convolutional,\n  title={Convolutional vs. Recurrent Neural Networks for Audio Source Separation},\n  author={Shariq Mobin* and Brian Cheung* and Bruno Olshausen},\n  year={2018},\n  url={https://openreview.net/forum?id=SkKnhFJPG}\n}", "authorids": ["shariqmobin@berkeley.edu", "bcheung@berkeley.edu", "baolshausen@berkeley.edu"], "authors": ["Shariq Mobin*", "Brian Cheung*", "Bruno Olshausen"], "TL;DR": "Compared to the traditionally used recurrent neural networks, convolutional neural networks show robust performance in audio source separation. We create a new dataset which evaluates how well source separation models generalize to real world mixtures.", "pdf": "/pdf/ffd2f13506a5cec995ea9988ca8db03241624d0a.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582876547, "tcdate": 1520541663299, "number": 1, "cdate": 1520541663299, "id": "SkvJx71FM", "invitation": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer1"], "content": {"title": "comments", "rating": "6: Marginally above acceptance threshold", "review": "The paper investigates using dilated CNNs for speech separation and compares with BLSTM on various tasks. The technique itself is not novel but the application on speaker modeling in the separation task is sort of interesting.   The experimental results are supportive.  One concern however is about the computational complexity of dilated CNNs.  The authors should say a few words about it.  Furthermore, there are a few acronyms used without definitions. (e.g. SDR, DPCL, etc. ). Last but not least, I wonder if the authors can talk a bit more on their RealTalkLibri dataset such as how the data is transcribed, the environments (SNR, noise types) and speakers (gender, age, dialect, etc.)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional vs. Recurrent Neural Networks for Audio Source Separation", "abstract": "We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.", "paperhash": "mobin|convolutional_vs_recurrent_neural_networks_for_audio_source_separation", "keywords": ["convolutional neural networks", "speech", "source seperation"], "_bibtex": "@misc{\n  mobin*2018convolutional,\n  title={Convolutional vs. Recurrent Neural Networks for Audio Source Separation},\n  author={Shariq Mobin* and Brian Cheung* and Bruno Olshausen},\n  year={2018},\n  url={https://openreview.net/forum?id=SkKnhFJPG}\n}", "authorids": ["shariqmobin@berkeley.edu", "bcheung@berkeley.edu", "baolshausen@berkeley.edu"], "authors": ["Shariq Mobin*", "Brian Cheung*", "Bruno Olshausen"], "TL;DR": "Compared to the traditionally used recurrent neural networks, convolutional neural networks show robust performance in audio source separation. We create a new dataset which evaluates how well source separation models generalize to real world mixtures.", "pdf": "/pdf/ffd2f13506a5cec995ea9988ca8db03241624d0a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582876365, "id": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper321/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer3"], "reply": {"forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582876365}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582857010, "tcdate": 1520566613665, "number": 2, "cdate": 1520566613665, "id": "r1ALbYktM", "invitation": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer2"], "content": {"title": "Reasonable modification to the DANet, new dataset with a few issues", "rating": "6: Marginally above acceptance threshold", "review": "This paper describes a modification of the deep attractor network (DANet) source separation framework to use a deep CNN instead of a deep BLSTM.  It also includes the recording of a 4.5 hour dataset of speech played through computer speakers in a real room. The new network appears to achieve similar or slightly better performance to the baseline DANet and is reported to use 1.6M parameters vs DANet's 16M.  \n\nThere are a few issues with the paper.  The experiments show a \"DPCL\" system in certain conditions, but this is never defined.  It is presumably the original deep clustering system.  In the experiments shown in figure 4, it is not clear what the x-axis refers to.  For the noise experiment, is that the position of the noise in a single utterance?  Is it the duration of the noise?  Is it averaged over several utterances?  How many utterances were involved in the duration experiment?  The duration of the signals does not affect the CNN, so it is only for the purposes of the clustering, right?  This should be stated.\n\nThe abstract claims that \"Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models\" this is a well-known finding, c.f., the REVERB challenge results, but the current paper's results do not show that this is the case for ALL neural networks, just these two.  It also states \"the convolutional model showing superior ability to generalize to new environments\" but from the results on the new RealTalkLibri dataset this is 3.5dB SDR vs 3.3dB SDR, which is rather small.\n\nIn addition, the recording conditions of the new data should be described in more detail.  What kind of room was this?  How big was it?  What was in it?  What was its reverberation time (RT60)?  Where were the speakers placed?  Were they moved between utterances or at all?  Was there ambient noise in the room?  If so, was it stationary or non-stationary?\n\nThe playback of sounds over speakers is not really \"real\" per se, but it is perhaps slightly more real than recording impulse responses and simulating reverberation, and it is more real than just adding two dry recordings together.  It is less real than recording actual people talking because actual people move when they are talking, but this brings its own problems, as noted in the paper, of not having ground truth.  \n\nThe use of a separate playback of the same sound as \"ground truth\" for the mixture seems a bit questionable.  Why not just record each sound separately through the channel and then add them after the fact?  Sounds do add in air and in microphones linearly, unless there is clipping.  You can make multiple recordings of each sound if you want, or just add them in many combinations.\n\nOverall, the model is a reasonable modification to the DANet, although if its main feature is having fewer parameters, then the experiments could explore that further by measuring the effect of different numbers of parameters in each model.  And the dataset is promising, but could be strengthened slightly in its construction and more so in its description.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional vs. Recurrent Neural Networks for Audio Source Separation", "abstract": "We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.", "paperhash": "mobin|convolutional_vs_recurrent_neural_networks_for_audio_source_separation", "keywords": ["convolutional neural networks", "speech", "source seperation"], "_bibtex": "@misc{\n  mobin*2018convolutional,\n  title={Convolutional vs. Recurrent Neural Networks for Audio Source Separation},\n  author={Shariq Mobin* and Brian Cheung* and Bruno Olshausen},\n  year={2018},\n  url={https://openreview.net/forum?id=SkKnhFJPG}\n}", "authorids": ["shariqmobin@berkeley.edu", "bcheung@berkeley.edu", "baolshausen@berkeley.edu"], "authors": ["Shariq Mobin*", "Brian Cheung*", "Bruno Olshausen"], "TL;DR": "Compared to the traditionally used recurrent neural networks, convolutional neural networks show robust performance in audio source separation. We create a new dataset which evaluates how well source separation models generalize to real world mixtures.", "pdf": "/pdf/ffd2f13506a5cec995ea9988ca8db03241624d0a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582876365, "id": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper321/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer3"], "reply": {"forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582876365}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582756666, "tcdate": 1520647119018, "number": 3, "cdate": 1520647119018, "id": "ByvRj3lKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer3"], "content": {"title": "Existing CNN architecture (dilated conv. nets) paired with existing source separation framework (attractor nets) and performs on-par to slightly better than RNN version. Lower novelty. Marginal Reject.", "rating": "5: Marginally below acceptance threshold", "review": "The authors utilize convolutional rather than recurrent networks in the recently introduced\nattractor network framework for source separation, and show that it performs slightly better, despite\nusing 10x less parameters. Furthermore, their cnn architecture generalizes somewhat better to mismatched test data, and source separation dataset based on librispeech is defined and released for use. \n\nWhile the results in this paper will be of great interest to audio source separation researchers, wrt research on learning feature representations the paper is somewhat lacking in novelty, since they are simply pairing an existing convolutional architecture (Fisher et al., 2015) with a recently introduced but existing source separation framework (Chen et al, 2017).\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional vs. Recurrent Neural Networks for Audio Source Separation", "abstract": "We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.", "paperhash": "mobin|convolutional_vs_recurrent_neural_networks_for_audio_source_separation", "keywords": ["convolutional neural networks", "speech", "source seperation"], "_bibtex": "@misc{\n  mobin*2018convolutional,\n  title={Convolutional vs. Recurrent Neural Networks for Audio Source Separation},\n  author={Shariq Mobin* and Brian Cheung* and Bruno Olshausen},\n  year={2018},\n  url={https://openreview.net/forum?id=SkKnhFJPG}\n}", "authorids": ["shariqmobin@berkeley.edu", "bcheung@berkeley.edu", "baolshausen@berkeley.edu"], "authors": ["Shariq Mobin*", "Brian Cheung*", "Bruno Olshausen"], "TL;DR": "Compared to the traditionally used recurrent neural networks, convolutional neural networks show robust performance in audio source separation. We create a new dataset which evaluates how well source separation models generalize to real world mixtures.", "pdf": "/pdf/ffd2f13506a5cec995ea9988ca8db03241624d0a.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582876365, "id": "ICLR.cc/2018/Workshop/-/Paper321/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper321/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper321/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper321/AnonReviewer3"], "reply": {"forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper321/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582876365}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573576545, "tcdate": 1521573576545, "number": 142, "cdate": 1521573576206, "id": "HJZRC0CFG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SkKnhFJPG", "replyto": "SkKnhFJPG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional vs. Recurrent Neural Networks for Audio Source Separation", "abstract": "We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have significant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.", "paperhash": "mobin|convolutional_vs_recurrent_neural_networks_for_audio_source_separation", "keywords": ["convolutional neural networks", "speech", "source seperation"], "_bibtex": "@misc{\n  mobin*2018convolutional,\n  title={Convolutional vs. Recurrent Neural Networks for Audio Source Separation},\n  author={Shariq Mobin* and Brian Cheung* and Bruno Olshausen},\n  year={2018},\n  url={https://openreview.net/forum?id=SkKnhFJPG}\n}", "authorids": ["shariqmobin@berkeley.edu", "bcheung@berkeley.edu", "baolshausen@berkeley.edu"], "authors": ["Shariq Mobin*", "Brian Cheung*", "Bruno Olshausen"], "TL;DR": "Compared to the traditionally used recurrent neural networks, convolutional neural networks show robust performance in audio source separation. We create a new dataset which evaluates how well source separation models generalize to real world mixtures.", "pdf": "/pdf/ffd2f13506a5cec995ea9988ca8db03241624d0a.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}