{"notes": [{"id": "ryxnY3NYPS", "original": "SygenxDpIB", "number": 93, "cdate": 1569438851886, "ddate": null, "tcdate": 1569438851886, "tmdate": 1583912023339, "tddate": null, "forum": "ryxnY3NYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "joFl7S6as3", "original": null, "number": 1, "cdate": 1576798687269, "ddate": null, "tcdate": 1576798687269, "tmdate": 1576800947847, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes an approach for forecasting diverse object trajectories using determinantal point processes (DPP). Past trajectory is mapped to a latent code and a conditional VAE is used to generate the future trajectories. Instead of using log-likelihood of DPP, the propose method optimizes expected cardinality as a measure for diversity. While there are some concerns about the core method being incremental in novelty over some existing DPP based methods, the context of the paper is different from these papers (ie, diverse trajectories in continuous space) and reviewers have appreciated the empirical improvements over the baselines, in particular over DPP-NLL and DPP-MAP in latent space. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710709, "tmdate": 1576800259775, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper93/-/Decision"}}}, {"id": "r1eEQVNMiH", "original": null, "number": 4, "cdate": 1573172252372, "ddate": null, "tcdate": 1573172252372, "tmdate": 1573226071662, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "rkgSq-KstS", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 1)", "comment": "Thank you for your review and feedback. In the following, we will address your comments and concerns, especially the ones w.r.t. experiments.\n\n** Variable size of the output set **\n\nFor now, we only consider generating sets of variable size instead of a fixed or minimal size, because we want to be able to filter out redundant samples for down-stream tasks. For example, if there is indeed only one possible trajectory and all N samples are the same, we want to just output that one trajectory and remove all duplicates.\n\n\n** Comments on (Elfeki et al., ICML 2019) **\n\n(Elfeki et al., ICML 2019) is an interesting work with great results and we will cite it in a revised version. \nI would expect (Elfeki et al., ICML 2019) to be able to capture all modes of the data distribution as the cVAE does. But since the sampling procedure in the paper is still the same as traditional generative models, i.e., independently sampling latent codes according to the prior and no repulsion between samples is modeled, it should perform similarly to the cVAE which will generate most samples from major modes for imbalanced data. It would be interesting to combine our approach with theirs to further improve the sample diversity of GANs.\n\n\n** NLL Baseline **\n\nWe indeed compare with NLL in our new experiments below. In short, NLL is stable for a small number of samples (10), but for a larger number of samples (50) it still leads to numerical instability.\n\n\n** Large dataset experiments and more baselines (Requested by R2 & R3) **\n\nOur method can generalize to larger datasets as shown in the new experiments below.\n\nDataset: We use a large public human motion dataset *Human3.6M* to perform a new experiment on human motion forecasting. We use past 0.5s to forecast future motions in 2s.\n\nBaselines:\n(1) NLL (requested by R2 & R3):  using the negative log-likelihood of the ground set under DPP as the loss. We also add eps * I to improve its stability.\n(2) DSF-COS (different kernels as requested by R3):  using cosine similarity as the similarity measure for the DPP kernel.\n(3) cVAE: random sampling from the Gaussian prior.\n(4) cVAE-LDPP (requested by R2):  We sample 100 latent codes and use DPP MAP inference on the latent codes to obtain N diverse latent codes, which are decoded into a set of trajectories.\n\nResults: \nFor the metrics, lower (ADE, FDE) and larger (ASD, FSD) are better.\n\n Number of samples N = 10\n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                                ADE     FDE     ASD     FSD    \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  DSF (Ours)        0.340   0.521   0.381   0.621  \n  NLL                     0.335   0.514   0.343   0.496  \n  DSF-COS            2.588   1.584   5.093   5.718  \n  cVAE                   0.363   0.549   0.235   0.360  \n  cVAE-LDPP        0.373   0.554   0.280   0.426  \n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n Number of samples N = 50\n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                              ADE     FDE     ASD     FSD    \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  DSF (Ours)       0.236   0.306   0.313   0.415  \n  NLL                          X          X          X           X  \n  DSF-COS           0.978   0.891   2.007   1.968  \n  cVAE                  0.276   0.369   0.160   0.220  \n  cVAE-LDPP       0.277   0.365   0.176   0.240  \n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n*X means that NLL is unable to learn a model due to numerical instability\n\n(1) Comparison with a different kernel:\nThe baseline DSF-COS uses cosine similarity for the DPP kernel, we can see that it is able to achieve high diversity but its samples are overly diverse and have poor quality which is indicated by large ADE and FDE. \n\n(2) DSF vs NLL:\nWe can see that for a small number of samples (N=10), our method achieves better diversity (larger ASD, FSD) and similar ADE, FSD to NLL. For a larger number of samples (N=50), NLL becomes unstable even with a large eps=1e-3 added to the diagonal. This behavior of NLL, i.e., stable for small N but unstable for large N, matches our intuition that NLL becomes unstable when samples become similar because when there are more samples, it is easier to have similar trajectories. And the reason why NLL cannot handle similar trajectories is that the DPP kernel L becomes singular as discussed in the paper (Section 4.2).\n\n(3) DPP MAP inference in the latent space:\nFor the cVAE-LDPP, it is indeed able to generate more diverse samples than cVAE but the diversity is worse than DSF. We think the main reason is that the mapping from the latent space to the data space is highly non-linear, and a diverse set of latent codes won\u2019t necessarily result in a diverse set of samples.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper93/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxnY3NYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper93/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper93/Authors|ICLR.cc/2020/Conference/Paper93/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176499, "tmdate": 1576860538398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment"}}}, {"id": "BJghs-NzoB", "original": null, "number": 2, "cdate": 1573171619999, "ddate": null, "tcdate": 1573171619999, "tmdate": 1573189087612, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "rklqqtBhYB", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (Part 1)", "comment": "Thank you for your review and feedback. In the following, we will address your comments and questions.\n\n\n** Why similarity and quality defined in different space **\n\nThe similarity is defined in the data space because such a similarity metric can reflect the true diversity of the data samples. And since the mapping from the latent space to the data space is represented by a deep neural network, which is highly non-linear, the data diversity cannot be well modeled in the latent space.\nThe quality is defined in the latent space because it is harder to compute the analytic likelihood in the data space (this is a common problem for both VAE and GAN). For cVAE, we can only evaluate the variational lower bound approximately with Monte-Carlo sampling. For likelihood-free models like GAN, it becomes even harder. In contrast, it is easy to compute the likelihood of the latent code since it is sampled from a multivariate Gaussian, and given the fact that we treat most major and minor modes equally, we think it is both reasonable and practical to use the latent code likelihood as a quality metric.\n\n\n** Will the network find a trivial solution **\n\nThe diagonal elements of $\\mathbf{L}(\\gamma)$ are bounded by the base quality $\\omega$ so the trace (sum of eigenvalues) of $\\mathbf{L}(\\gamma)$ is also bounded. We believe the trivial solution you are referring to is when the cardinality reaches maximum and $\\mathbf{L}(\\gamma)$ becomes a diagonal matrix with elements $\\omega$. This means all samples are inside the probable region (the R sphere) since their quality is $\\omega$, and the samples are far away from each other because off-diagonal values $S_{ij}$ are zero. This is a desired behavior because it makes the samples both diverse and probable. Therefore, the trivial solution you referred to is actually a desired solution.\n\n\n** Comparison with MIC **\n\nWe will definitely cite [1] in a revised version as it is a great paper and is closely related. We agree that [1] and we both use the cardinality of a DPP as a metric, but we want to point out two important differences:\n(1) The context is different. [1] is focused on the subset selection problem, so it is called maximum *induced* cardinality (MIC). If we understand correctly, the theories and approximation methods developed in [1] are geared toward subset selection. In contrast, our method aims to use cardinality as an objective to optimize a set of trajectories in continuous space.\n(2) The motivation is different. The main motivation behind [1] for using the induced cardinality is that the user engaged set E is different from the recommended set S, causing a train-test mismatch, and MIC aligns better with the user engagement semantics. In our case, we use the cardinality because it is more suitable than NLL loss for continuous optimization with large deep neural networks due to its tolerance of similar samples. As the samples produced by the network can easily become similar during SGD updates, NLL-based approaches often face numerical instability. A concurrent work [2] also noticed this problem with NLL, and has to develop special optimization techniques to address it. We will also show in our new experiments that using cardinality is able to overcome this problem.\n\n[1] Gillenwater, Jennifer A., et al. \"Maximizing induced cardinality under a determinantal point process.\" Advances in Neural Information Processing Systems. 2018.\n[2] \"Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient.\" Submitted to ICLR 2020.\n\n\n** Using MIC instead of MAP inference **\n\nMIC may not be suitable for our application as it requires the set size k to be pre-determined (if any size is ok, the ground set will have the maximum cardinality), which is the case for the recommendation system setting but not for our trajectory forecasting setting, because we do not know in advance how many trajectories are diverse inside the ground set, e.g., sometimes all trajectories are the same but sometimes all trajectories are very different.\n\n\n** MAP inference on the latent codes **\n\nThis is an interesting baseline, and we will show results for it in our new experiments. As we mentioned before, due to the non-linear mapping from latent space to data space, data diversity may not be well reflected in the latent space.\n\n\n** Speed of DSF **\n\nThe speed is similar to NLL, as NLL also needs to do matrix inverse when computing gradients, and we didn\u2019t observe a noticeable difference between speed of these two methods. Additionally, DSF converges quickly and all our experiments take less than one hour to train the DSF, even for the large dataset experiments below.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper93/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxnY3NYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper93/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper93/Authors|ICLR.cc/2020/Conference/Paper93/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176499, "tmdate": 1576860538398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment"}}}, {"id": "ByluB7NfiH", "original": null, "number": 3, "cdate": 1573172032126, "ddate": null, "tcdate": 1573172032126, "tmdate": 1573184946039, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "BJghs-NzoB", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (Part 2) ", "comment": "\n** Add eps * identity to the kernel matrix for NLL **\n\nThanks for your suggestion. We have used it in our baseline and will show the results in the new experiments below. In short, it is able to stabilize for a small number of samples (10) but still leads to instability for a larger number of samples (50). \n\n\n** Make k and $\\omega$ learnable **\n\nIt is possible to make k and $\\omega$ learnable given that we impose some constraints or regularization on them. Because if k and $\\omega$ are unconstrained, the network can make k arbitrarily small and $\\omega$ arbitrarily large to achieve high cardinality.\nIn our opinion, it is better to let users control k and $\\omega$ as a way to trade-off between diversity and quality. It is also possible to condition the network on k and $\\omega$ so users can achieve such trade-off dynamically without retraining the network.\n\n\n** Large dataset experiments and more baselines (Requested by R2 & R3) **\n\nWe have further performed more extensive experiments to test the generalization of our method.\n\nDataset: We use a large public human motion dataset *Human3.6M* to perform a new experiment on human motion forecasting. We use past 0.5s to forecast future motions in 2s.\n\nBaselines:\n(1) NLL (requested by R2 & R3):  using the negative log-likelihood of the ground set under DPP as the loss. We also add eps * I to improve its stability.\n(2) DSF-COS (different kernels as requested by R3):  using cosine similarity as the similarity measure for the DPP kernel.\n(3) cVAE: random sampling from the Gaussian prior.\n(4) cVAE-LDPP (requested by R2):  We sample 100 latent codes and use DPP MAP inference on the latent codes to obtain N diverse latent codes, which are decoded into a set of trajectories.\n\nResults: \nFor the metrics, lower (ADE, FDE) and larger (ASD, FSD) are better.\n\n Number of samples N = 10\n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                                ADE     FDE     ASD     FSD    \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  DSF (Ours)        0.340   0.521   0.381   0.621  \n  NLL                     0.335   0.514   0.343   0.496  \n  DSF-COS            2.588   1.584   5.093   5.718  \n  cVAE                   0.363   0.549   0.235   0.360  \n  cVAE-LDPP        0.373   0.554   0.280   0.426  \n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n Number of samples N = 50\n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                              ADE     FDE     ASD     FSD    \n \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  DSF (Ours)       0.236   0.306   0.313   0.415  \n  NLL                          X          X          X           X  \n  DSF-COS           0.978   0.891   2.007   1.968  \n  cVAE                  0.276   0.369   0.160   0.220  \n  cVAE-LDPP       0.277   0.365   0.176   0.240  \n \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n*X means that NLL is unable to learn a model due to numerical instability\n\n(1) Comparison with a different kernel:\nThe baseline DSF-COS uses cosine similarity for the DPP kernel, we can see that it is able to achieve high diversity but its samples are overly diverse and have poor quality which is indicated by large ADE and FDE. \n\n(2) DSF vs NLL:\nWe can see that for a small number of samples (N=10), our method achieves better diversity (larger ASD, FSD) and similar ADE, FSD to NLL. For a larger number of samples (N=50), NLL becomes unstable even with a large eps=1e-3 added to the diagonal. This behavior of NLL, i.e., stable for small N but unstable for large N, matches our intuition that NLL becomes unstable when samples become similar because when there are more samples, it is easier to have similar trajectories. And the reason why NLL cannot handle similar trajectories is that the DPP kernel L becomes singular as discussed in the paper (Section 4.2).\n\n(3) DPP MAP inference in the latent space:\nFor the cVAE-LDPP, it is indeed able to generate more diverse samples than cVAE but the diversity is worse than DSF. We think the main reason is that the mapping from the latent space to the data space is highly non-linear, and a diverse set of latent codes won\u2019t necessarily result in a diverse set of samples.\n\n** Network architecture of DSF **\n\nWe show the architecture of the DSF in Fig. 6 (Appendix B).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper93/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxnY3NYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper93/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper93/Authors|ICLR.cc/2020/Conference/Paper93/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176499, "tmdate": 1576860538398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment"}}}, {"id": "B1evjEEMsH", "original": null, "number": 5, "cdate": 1573172383383, "ddate": null, "tcdate": 1573172383383, "tmdate": 1573178243108, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "r1eEQVNMiH", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (Part 2) ", "comment": "\n** Selecting radius R and $\\rho$ **\n\nThe trade-off between accuracy (quality) and robustness (diversity) should be application dependent. For example, safety-critical applications should use a larger R to consider a more diverse set of trajectory samples. We can also assign a score to each sample based on the latent code likelihood so that minor modes will have lower scores, this will allow users to rank the trajectories and prioritize more probable trajectories.\n\nMoreover, it would be interesting to condition the DSF on these user-specific parameters (R, k), so that users can achieve the trade-off between robustness and quality dynamically without the need for retraining the DSF.\n\n\n** Comparison with MIC **\n\nWe will definitely cite [1] in a revised version as it is a great paper and is closely related. We agree that [1] and we both use the cardinality of a DPP as a metric, but we want to point out two important differences:\n(1) The context is different. [1] is focused on the subset selection problem, so it is called maximum *induced* cardinality (MIC). If we understand correctly, the theories and approximation methods developed in [1] are geared toward subset selection. In contrast, our method aims to use cardinality as an objective to optimize a set of trajectories in continuous space.\n(2) The motivation is different. The main motivation behind [1] for using the induced cardinality is that the user engaged set E is different from the recommended set S, causing a train-test mismatch, and MIC aligns better with the user engagement semantics. In our case, we use the cardinality because it is more suitable than NLL loss for continuous optimization with large deep neural networks due to its tolerance of similar samples. As the samples produced by the network can easily become similar during SGD updates, NLL-based approaches often face numerical instability. A concurrent work [2] also noticed this problem with NLL, and has to develop special optimization techniques to address it. We also have shown in our new experiments that using cardinality is able to overcome this problem.\n\n[1] Gillenwater, Jennifer A., et al. \"Maximizing induced cardinality under a determinantal point process.\" Advances in Neural Information Processing Systems. 2018.\n[2] \"Deep Learning of Determinantal Point Processes via Proper Spectral Sub-gradient.\" Submitted to ICLR 2020.\n\n\n** Distance should be squared in Eq. 8 **\n\nThat\u2019s indeed a typo and thank you for noticing it. We will fix it in a revised version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper93/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxnY3NYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper93/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper93/Authors|ICLR.cc/2020/Conference/Paper93/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176499, "tmdate": 1576860538398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment"}}}, {"id": "r1lM4l4MjH", "original": null, "number": 1, "cdate": 1573171241646, "ddate": null, "tcdate": 1573171241646, "tmdate": 1573171241646, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "HJlvetnpFH", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for your review and suggestions. In the following, we will address your comments.\n\n** How to avoid overly diverse and very improbable trajectories **\n\nOur method provides two ways to trade-off between diversity and quality (trajectories that are more probable). (1) We can reduce the $\\rho$ or equivalently $R$ in the quality term to force samples to move closer to the origin of the latent space, which results in more probable trajectories. (2) We can reduce the $k$  in the similarity metric to change the scale of the RBF kernel so that the samples have less incentive to move away from each other because the gain from it will be exponentially smaller. To optimize the cardinality, the DSF network will instead focus on moving samples toward more probable regions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper93/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxnY3NYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper93/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper93/Authors|ICLR.cc/2020/Conference/Paper93/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176499, "tmdate": 1576860538398, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper93/Authors", "ICLR.cc/2020/Conference/Paper93/Reviewers", "ICLR.cc/2020/Conference/Paper93/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Comment"}}}, {"id": "rkgSq-KstS", "original": null, "number": 1, "cdate": 1571684748694, "ddate": null, "tcdate": 1571684748694, "tmdate": 1572972639563, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Caveat: I am very familiar with DPPs, but unfamiliar with the literature of trajectory forecasting, autonomous driving, etc.\n\nSummary: This work introduces a generative model for diverse sequences based on a DPP, with the goal of providing likely yet non-overlapping possible future trajectories to models that require such information for safety concerns (eg, autonomous driving). Rather than using the DPPs negative log-likelihood as a measure of the trajectory set's diversity, the authors use the DPP's expected sample size as a proxy for a diversity metric. Experimentally, the authors show on two tasks that this approach generates more diverse, relevant trajectories than several competing baselines.\n\nRecommendation: I recommend this paper be accepted, with the caveat that I am not well equipped to evaluate the experimental contribution of this paper, which in my opinion is the most important part of this work. I would have liked to see more extensive experiments, in particular (a) other baselines (DPP NLL as a diversity loss, different DPP kernels) and (b) experiments on larger datasets.\n\nHigh level comments and questions:\n\n- When generating the trajectory set (Alg. 3), your algorithm may generate sets of variable size (as the DPP NLL is log-submodular but not necessarily increasing). Did you also consider generating sets of fixed size, or of a minimal size? \n\n- Previous work has also looked at using DPPs to alleviate mode collapse in GANs (Elfeki et al., ICML 2019). Could you comment on how you expect such a method would perform on your chosen tasks? \n\n- The intuition that the DPP negative log-likelihood will overwhelmingly penalize subsets that contain a few similar trajectories seems very reasonable. However, I would have liked to see that intuition verified through an experiment that used the NLL as a diversity loss.\n\n- The training sets for both experiments seem fairly small (1100 and 9400 training examples). Could you comment on this choice, and on whether you expect your experimental results to generalize to much larger datasets?\n\n- Your motivating example is autonomous vehicles and safety concerns; with that example in mind, could you comment on how to select the radius R (or, equivalently, \\rho) in Eq. 9? It seems like this choice would have significant downstream implications on the trade-off between accuracy and robustness to unpredictability.\n\n- The idea of using the expected subset size as a metric for diversity is compelling. The authors may want to take a look at (Gillenwater et al., NeurIPS'18), which uses a similar metric as a proxy for user engagement. \n\nMinor comments: \n- after Eq. 2: please consider defining the acronym ELBO before using it.\n- Eq. 8: if you are using an exponentiated quadratic, I believe the distance should be squared.\n- Eq. 10: consider citing the relevant proposition from (Kulesza & Taskar).\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575781158333, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper93/Reviewers"], "noninvitees": [], "tcdate": 1570237757179, "tmdate": 1575781158346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Review"}}}, {"id": "rklqqtBhYB", "original": null, "number": 2, "cdate": 1571735954173, "ddate": null, "tcdate": 1571735954173, "tmdate": 1572972639526, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper presents a novel approach for forecasting object trajectories (e.g., predicted paths of vehicles) forcing diversity of outputs. The authors adopt determinantal point processes (DPPs) to capture the diversity and propose a diversity sampling function (DSF) which consists of a neural network. Its trainable parameter (i.e., \\gamma) maps the past trajectory into a set of latent codes and they are decoded to feasible trajectories by a pre-trained conditional variational autoencoder (cVAE). The DSF is trained by maximizing the diversity of the output trajectory. But, since the standard log-likelihood function can be singular, they additionally present an objective function for diversity by maximizing the expected cardinality, which admits replicated outputs. In experiments, the proposed method finds more diverse paths than other competitors under both for synthetic 2D objects and human motions.\n\nThis paper is well-written and easy to understand. The contribution can be important as it performs better than other generative networks that are not forcing diversity. Unlike them, the proposed method can capture the diversity trajectories, which requires for safety-critical applications. \n\nMain concerns:\n\n1. The DPP kernel consists of a similarity (equation (8)) and a quality score (equation (9)). However, it is unnatural that the similarity is defined in the data space (x) and the quality is defined in latent space (z). A more naive approach is to define the DPP kernel as a function of x or z. Is there any specific reason to define those scores are defined in different space?\n\n2. To maximize the expected cardinality, it is enough that the eigenvalues of L(\\gamma) become large. Does the network find the trivial solution? E.g., all eigenvalues are the same as a very large value. In addition, the proposed diverse loss is not a new approach. The maximum induced cardinality of a DPP was proposed by [1] and its various properties were also studied therein. \n\n3. In a work of [1], a method to optimize the induced cardinality (a similar to diverse loss in this paper) is proposed. And it would be great to compare the maximum induced cardinality (also can be approximated by the greedy algorithm) to the MAP for inference of diverse trajectory.\n\n4. It is also possible to apply DPP MAP inference to a set of latent codes generated from the encoder of cVAE. Then, the decoder can map the diverse latent variables to feasible data trajectories. Are these outputs also diverse? or does the diversity of latent space reflect the diversity in the data trajectory space? \n\n5. Computing the gradient of the proposed diverse loss is expensive since it is the trace of an inverse of a parameterized matrix. How long does it take to learn the proposed DSF compared to other methods? \n\n6. To avoid that the determinants become zero, a popular choice is to shift all eigenvalues with a small amount (this can be done by adding eps * identity matrix to the kernel matrix). Did the authors investigate other practical diverse losses?\n\n7. It seems to be possible to train the parameters of the kernel matrix, i.e., k in equation (8) and \\omega in equation (9). Did the author try to learn those parameters?\n\nOverall, this work proposes an approach combined cVAE with a DPP for forecasting diverse trajectories and the empirical results are promising as it outperforms other methods. But, its novelty is incremental and competitors are not actually the models capturing diversity. I vote for a weak acceptance but depending on clarifications on the above concerns in an author response, I would be willing to increase the score.\n\nMinor comments:\n\n1 . Please specify the network architecture of DSF and details on the parameter \\gamma.\n\n\n[1] Gillenwater, Jennifer A., et al. \"Maximizing induced cardinality under a determinantal point process.\" Advances in Neural Information Processing Systems. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575781158333, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper93/Reviewers"], "noninvitees": [], "tcdate": 1570237757179, "tmdate": 1575781158346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Review"}}}, {"id": "HJlvetnpFH", "original": null, "number": 3, "cdate": 1571829999188, "ddate": null, "tcdate": 1571829999188, "tmdate": 1572972639490, "tddate": null, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "invitation": "ICLR.cc/2020/Conference/Paper93/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method to diversify samples generated from a VAE. The method is based on determinantal point processes in the latent space and relies on specifying a kernel in the sample space and a quality metric in the latent space. \n\nIncreasing diversity of samples from VAE-like models is an important and common problem and the authors present a reasonably generic method for solving it. The experiments are sensible and show clear improvement over a reasonable selection of baselines. One improvement I would suggest is adding a more explicit discussion of how the proposed method avoids generating overly diverse, very improbable trajectories. It is also somewhat suspicious that the trajectories shown in Figure 3 are terminated so early."}, "signatures": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper93/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yyuan2@cs.cmu.edu", "kkitani@cs.cmu.edu"], "title": "Diverse Trajectory Forecasting with Determinantal Point Processes", "authors": ["Ye Yuan", "Kris M. Kitani"], "pdf": "/pdf/dbe4a1eab1eabba68d5e92b6ae29819c3f7f7999.pdf", "TL;DR": "We learn a diversity sampling function with DPPs to obtain a diverse set of samples from a generative model.", "abstract": "The ability to forecast a set of likely yet diverse possible future behaviors of an agent (e.g., future trajectories of a pedestrian) is essential for safety-critical perception systems (e.g., autonomous vehicles). In particular, a set of possible future behaviors generated by the system must be diverse to account for all possible outcomes in order to take necessary safety precautions. It is not sufficient to maintain a set of the most likely future outcomes because the set may only contain perturbations of a dominating single outcome (major mode). While generative models such as variational autoencoders (VAEs) have been shown to be a powerful tool for learning a distribution over future trajectories, randomly drawn samples from the learned implicit likelihood model may not be diverse -- the likelihood model is derived from the training data distribution and the samples will concentrate around the major mode of the data. In this work, we propose to learn a diversity sampling function (DSF) that generates a diverse yet likely set of future trajectories. The DSF maps forecasting context features to a set of latent codes which can be decoded by a generative model (e.g., VAE) into a set of diverse trajectory samples. Concretely, the process of identifying the diverse set of samples is posed as DSF parameter estimation. To learn the parameters of the DSF, the diversity of the trajectory samples is evaluated by a diversity loss based on a determinantal point process (DPP). Gradient descent is performed over the DSF parameters, which in turn moves the latent codes of the sample set to find an optimal set of diverse yet likely trajectories. Our method is a novel application of DPPs to optimize a set of items (forecasted trajectories) in continuous space. We demonstrate the diversity of the trajectories produced by our approach on both low-dimensional 2D trajectory data and high-dimensional human motion data.", "keywords": ["Diverse Inference", "Generative Models", "Trajectory Forecasting"], "paperhash": "yuan|diverse_trajectory_forecasting_with_determinantal_point_processes", "_bibtex": "@inproceedings{\nYuan2020Diverse,\ntitle={Diverse Trajectory Forecasting with Determinantal Point Processes},\nauthor={Ye Yuan and Kris M. Kitani},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxnY3NYPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/8812e4ec163adee9779653cde18f8c00e98aad27.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxnY3NYPS", "replyto": "ryxnY3NYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper93/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575781158333, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper93/Reviewers"], "noninvitees": [], "tcdate": 1570237757179, "tmdate": 1575781158346, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper93/-/Official_Review"}}}], "count": 10}