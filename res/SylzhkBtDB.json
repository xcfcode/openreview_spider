{"notes": [{"id": "NvlGf_jfLY6", "original": null, "number": 1, "cdate": 1601178928110, "ddate": null, "tcdate": 1601178928110, "tmdate": 1601178928110, "tddate": null, "forum": "SylzhkBtDB", "replyto": "rJea5zaPir", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Public_Comment", "content": {"title": "Open Source Code", "comment": "Hi Authors,\n\nThank you for the very interesting work. Could I please follow-up on link to the open source code regarding, \"Our code will be open-sourced after the reviewing process.\"\n\nBest Regards,\nJoshua"}, "signatures": ["~Joshua_Yee_Kim1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Joshua_Yee_Kim1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504187352, "tmdate": 1576860571217, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Public_Comment"}}}, {"id": "SylzhkBtDB", "original": "Hkl7ngyFDr", "number": 1943, "cdate": 1569439657789, "ddate": null, "tcdate": 1569439657789, "tmdate": 1588463128461, "tddate": null, "forum": "SylzhkBtDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "nKybGpU3_5", "original": null, "number": 1, "cdate": 1576798736550, "ddate": null, "tcdate": 1576798736550, "tmdate": 1576800899815, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Many existing approaches in multi-task learning rely on intuitions about how to transfer information. This paper, instead, tries to answer what does \"information transfer\" even mean in this context. Such ideas have already been presented in the past, but the approach taken here is novel, rigorous and well-explained.\n\nThe reviewers agreed that this is a good paper, although they wished to see the analysis conducted using more practical models. \n\nFor the camera ready version it would help to make the paper look less dense.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712561, "tmdate": 1576800261971, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Decision"}}}, {"id": "SJeBOI5jjB", "original": null, "number": 6, "cdate": 1573787245398, "ddate": null, "tcdate": 1573787245398, "tmdate": 1573787245398, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylK5Ywoir", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Thanks for raising this question", "comment": "We use alternative optimization in our implementation of Alg 1. For each epoch, we iterate over all the task batches. If the current batch is from task $i$, then the SGD is applied on $A_i$ and $R_i$. The other parameters are fixed. We have revised Alg 1 to clarify this step and also included the description of our SGD implementation in Appendix C.3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "SylK5Ywoir", "original": null, "number": 5, "cdate": 1573775761189, "ddate": null, "tcdate": 1573775761189, "tmdate": 1573775761189, "tddate": null, "forum": "SylzhkBtDB", "replyto": "rJea5zaPir", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Response ", "comment": "Thanks for your detailed rebuttal and efforts for making the paper better.\n\nMost of my confusions have been solved and the only unclear point might be in question (2):\n\n\u201cStep 3, how to jointly minimize R_1,\\dots, R_k, A_1, \\dots, A_k ?\u201d\nI am wondering the high level solution. For example If we are only considering a simple linear model, jointly optimization means that we simply set a global parameter $\\phi  = (R_1,\\dots, R_k, A_1, \\dots, A_k) $ and apply gradient descent over $\\phi$ of the loss (seems more difficult).  Or we use alternative optimization (seems more common),  for each optimization step we fix all parameters except one parameter and optimize only that one parameter.\n\nOverall I keep my current decision and think it is indeed a good paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "rJea5zaPir", "original": null, "number": 2, "cdate": 1573536404705, "ddate": null, "tcdate": 1573536404705, "tmdate": 1573538584161, "tddate": null, "forum": "SylzhkBtDB", "replyto": "rkl9a2T3KS", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for appreciating our theoretical insights and the strength of our work. We appreciate the reviewer\u2019s effort to provide detailed comments which we have incorporated in the revision. Here are our changes with respect to each comment.\n\n  > \u201c1. I suggest the author to merge the Figure 3 and Data generation (Page 4) part for a better presentation. e,g which \u201cdiff.covariance\u201d is task 3 or 4?  And why we use different rotation matrix Q_i ?\u201d\n\n    - We have revised Figure 3 and the Data generation paragraph in Sec. 2.3 to clarify the two issues.\n    - Figure 3 is simplified to 2 curves without affecting the message. The caption connects the figure to the generation process. And the generation process is revised accordingly in reference to the figure.\n    - The different rotation matrices Q_i are used to create a \u201ccovariate shift\u201d between the two tasks. As Figure 3 shows, this shift leads to a negative transfer of MTL in the regime where the number of source data points is small.\n\n  > \u201c2. In algorithm 1 (Page 5), I suggest the author use a formal equation (like algorithm 2) instead of descriptive words.\n     -- Step 2, I have trouble in understanding this step.\n     -- Step 3, how to jointly minimize R_1,\\dots, R_k, A_1, \\dots, A_k ? we use loss (3)  or other losses?\n     -- I suggest that the author release the code for a better understanding.\u201d\n\n    - We have revised the description of algorithm 1 to define a formal loss. To minimize the modified loss over the alignment matrices and the output layers, we use standard training procedures (mini-batch SGD, cf. Appendix C.3). Our code will be open-sourced after the reviewing process.\n\n  > \u201c3. For theorem 2, can we find some \u201coptimal\u201d c to optimize the right part ? Since 6c + \\frac{1}{1-3c}\\frac{\\epsilon}{\\X_2\\theta_2} might be further optimized.\u201d\n\n    - The error bound $6c + \\frac{1}{1-3c}\\frac{||\\epsilon||}{||X_2\\theta_2||}$ decreases with c so the smaller c is the better. We have revised Theorem 2 and added a discussion regarding the error bound (Appendix B.2.2).\n\n  > \u201c4. In section 3.3. (Figure 6)  of the real neural network, the model capacity is the dimension of Z or simply the dimension before last fc-layer?\u201d\n\n    - The model capacity is the dimension before the last fc-layer. We have revised this sentence to make it clear.\n\n  > \u201c5. Some parts in the appendix can be better illustrated:\n    (a) I am not clear how proposition 4 can derive proposition 1.\n    (b) Page 15, proving fact 8: last line \\frac{1}{k^4}sin(a^{prime},b^{prime}) should be \\frac{1}{k^4}sin^{2}(a^{prime},b^{prime}).\u201d\n\n    - (a) We have revised proposition 4 so that it becomes more clear that it can derive proposition 1. In particular, proposition 4 states that the subspace of the shared module is all that matters, hence having the $\\{\\theta_i\\}$\u2019s in its column span suffices. Proposition 1 instantiates this intuition.\n    - (b) Thanks for catching the typo. We have fixed it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "SkgiOcavjB", "original": null, "number": 4, "cdate": 1573538419096, "ddate": null, "tcdate": 1573538419096, "tmdate": 1573538419096, "tddate": null, "forum": "SylzhkBtDB", "replyto": "rJlRuaviKB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the positive feedback and for the appreciation of our theoretical contribution and our effort on writing. We respond to the two comments under \u201ccons\u201d here.\n\n  > \u201cThere is not much of novelty in the algorithm and architecture. Their method is very similar to domain adaptation but for multi-learning setting.\u201d\n\n    - We do agree that in domain adaptation, it is well-understood that the divergence between the source and target distributions can cause negative transfer. Hence, the general recipe is to correct this divergence by matching the source distribution to the target. In the multi-task setting, however, the interaction/interference between the tasks is much more complicated, e.g. positive and negative effects can happen at the same time (e.g. figure 6). To determine the type of interference, we provide a theoretical framework to study this question in linear and ReLU models and we develop theory to identify the components which cause positive and negative transfers.\n    - We would like to emphasize that our covariance alignment algorithm and SVD-based reweighing scheme are both consequences derived from our theory. The additional experiments we added into the revision verify that the alignment algorithm can correct misaligned task data for linear models (Appendix C.5), and we have shown that it works well for highly non-linear networks (Sec. 3.2). Our insight for these empirical results is that there exists an alignment matrix that corrects the differences between the task covariances, which can cause negative effect in MTL. We believe that this insight is applicable to more sophisticated architectures.\n    - In addition to the algorithms, our theoretical framework provides some general rules of thumb and tools to help MTL in practice, including i) We show that the capacity of the shared MTL module should not exceed the total capacities of all the STL modules; ii) We propose the cosine similarity score to measure the similarities of task data and track the progress of the alignment procedure.\n\n  > \u201cIn the Theorem 2, they have assumed parameter $c <= 1/3$. They have not provided any insight of how much restrictive this assumption is.\u201d\n\n    - In Theorem 2, the assumption that $c <= 1/3$ arises when we deal with the label noise of task 2. If there is no noise for task 2, then this assumption is not needed. If there is noise for task 2, this assumption is satisfied when $\\sin(\\theta_1, \\theta_2)$ is less than $1/(3\\kappa(X_2))$. This is satisfied when the two single-task models are close enough, which is intuitively necessary to guarantee positive transfer. Indeed our experiments also show that the value of $\\sin(\\theta_1, \\theta_2)$ affects performance (Figure 8 and 9 in Appendix C.4).\n    - Theorem 2 guarantees positive transfers in MTL, when the source and target models are close enough and the number of source samples is large. While the intuition is folklore in MTL, we provide a formal justification in the linear and ReLU models to quantify the phenomenon.\n    - We have added these discussions to provide more insight on Theorem 2 into the revision in Appendix B.2.2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "r1x_6VTwsS", "original": null, "number": 3, "cdate": 1573536960119, "ddate": null, "tcdate": 1573536960119, "tmdate": 1573536960119, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SJeNT1T3YH", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for appreciating the contribution of our work, and we are grateful to the suggestions which help improve our work. We added experiments to evaluate our method on linear and ReLU models regarding the suggested gap between our theory and the experiments. We added the related work [1] and provided comparative experiments with [1]. We addressed the parts that are confusing as suggested. Here are our responses regarding each comment.\n\nResponse to major comments:\n\n  > \u201cOne issue with the submission is that there is a significant gap between the theory and experimental sections as theory only covers linear models and the experiments don\u2019t include linear models and purely focus on deep networks.\u201d\n\n    - We have added experiments to evaluate our alignment method (Alg 1) on linear and ReLU-activated models for synthetic data (Appendix C.4). Our method can indeed help these models in addition to deep networks.\n\n  > \u201cAdditional assumptions (1D labels, same input dimensionality across all tasks) should be emphasised to clarify limitations of all derivations.\u201d\n\n    - We have revised Sec. 2.1 to emphasize that the labels are 1D in our model (the same input dimensionality assumption is also stated in Sec. 2.1). A multi-label problem with k types of labels can be modeled by k tasks with the same covariates but different labels.\n\n  > \u201cWhere previous work addressed model similarity it often looks at models in the context of existing datasets (i.e. taking the data into account to describe boundaries etc) such that the emphasised novelty at looking at data similarity is to be taken with a grain of salt.\u201d\n\n    - We agree that if we compare two task models trained from the datasets, their similarity already depends on the data. In our experience in looking at LSTM/CNN/MLP models on sentiment analysis, however, we have observed that just measuring the similarity of the model weights or feature outputs is too crude to tell whether or not MTL shows positive or negative benefit, even for a single layer. This is likely due to the differences between the task data and the specific model used. And there is currently no theoretical framework to answer this question.\n    - To provide a more precise answer, we formulate this question in a simple setup. Our theory disentangles the model part and the data part. By doing so, figure 2 shows that task data similarity plays a second-order effect after controlling model similarity to be the same. Our intuition is that this arises from the shift of the covariance matrices between the task data. Our theory formalizes the covariances and our experiments show the benefit of aligning the covariance matrices on deep networks.\n    - We have revised the third paragraph in the intro to make it more clear.\n \n  > \u201cWhile the model with non-linear activation is mentioned at places, nearly all theorems rely on the linear model instead such that it might make sense to either work towards generalising the theorems or emphasising that most only apply to linear models.\u201d\n\n    - We thank Reviewer 2 for pointing these out. We have extended the theoretical result of Sec. 2.2 so that it applies to ReLU settings. The theoretical result of section 2.3 also applies to ReLU settings. So the only result which does not apply to the ReLU setting is proposition 3 in Sec. 2.4. The question of characterizing the optimization landscape in non-linear ReLU models is not well-understood based on the current theoretical understanding of neural networks. We think this is an open research direction and we have stated this question in the revised version.\n\nResponse to minor comments:\n\n  > \u201cy is used as label and as data terminology at different parts of the text\u201d\n\n    - Thanks for pointing out this issue. We have corrected the use of y in the revision.\n\n  > \u201cthe model in the first set of experiments has lower capacity than most models individually, suggesting that the capacity should be smaller even for individual tasks to prevent overfitting. An ablation over model capacities is mentioned but missing for 3.3\u201d\n\n    - We have added an ablation study over model capacities to show the performance of MTL and STL as we vary the capacities (Appendix C.5). This indicates the best performing capacities we choose in Figure 6. We also added plots on CNN/MLP to show the same results.\n\n  > \u201ccomparison against existing multitask loss weighting techniques should be performed [1]\u201d\n\n    - Thanks for pointing out this work. We have compared our method (Alg 1) to the techniques in [1]. This is added as a benchmark in Sec. 3.1. Our SVD-based scheme performs favorably on the ChestX-ray14 dataset. The results are in Sec. 3.2 and ablation results are in Sec. 3.3 and Appendix C.5. Across all 14 tasks, our scheme outperforms [1] by 1.3% AUC score.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "rkee1Z6PiS", "original": null, "number": 1, "cdate": 1573535959724, "ddate": null, "tcdate": 1573535959724, "tmdate": 1573536035397, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment", "content": {"title": "Summary of the revision and the response", "comment": "We thank all the reviewers for the positive feedback and the detailed comments. In response to the reviewers\u2019 suggestions, we have revised our paper, including three sets of additional experimental results to consolidate our results as follows.\n\n  1- Clarify our model assumption on 1D label and how to model multi-label problems, the example of figure 3 and the data generation description, and the theory part (formal description of Alg 1, extendable to ReLU or not, discussion on Theorem 2 in Appendix B.2.2).\n\n  2- Additional experiments on linear and ReLU models to validate our alignment method (Appendix C.5). This confirms that our method (Alg 1) can help linear models in addition to deep networks, as Reviewer #2 asked about.\n\n  3- We conduct an additional experiment to compare our SVD-based reweighting scheme to the loss weighting techniques of Kendall et al.\u201918, as Reviewer #2 requested. On the ChestX-ray14 dataset, we found that our method improves performance by 1.3% AUC score compared to the suggested work (Sec. 3.2).\n\n  4- Additional ablation studies on model capacities to further validate our results (Appendix C.5), as Reviewer #2 asked about.\n\nFinally, we respond to all the comments raised by the reviewers in detail. The comments have all been incorporated into the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylzhkBtDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1943/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1943/Authors|ICLR.cc/2020/Conference/Paper1943/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148614, "tmdate": 1576860537467, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Authors", "ICLR.cc/2020/Conference/Paper1943/Reviewers", "ICLR.cc/2020/Conference/Paper1943/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Comment"}}}, {"id": "rJlRuaviKB", "original": null, "number": 1, "cdate": 1571679606106, "ddate": null, "tcdate": 1571679606106, "tmdate": 1572972403556, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper studies how to improve the multi-task learning from both theoretical and experimental viewpoints. More specifically, they study an architecture where there is a shared model for all of the tasks and a separate module specific to each task. They show that data similarity of the tasks, measured by task covariance is an important element for the tasks to be constructive or destructive. They theoretically find a sufficient condition that guarantee one task can transfer positively to the other; i.e. a lower bound of the number of data points that one task has to have. Consequently, they propose an algorithm which is basically applying a covariance alignment method to the input. \nThe paper is well-written, and easy to follow. \nPros:\nA new theoretical analysis for multi-task learning, which can give insight of how to improve it through data selection.\nThey empirically show that their algorithm improves the multi-task learning on average by 2.35%. \n\nCons:\nThere is not much of novelty in the algorithm and architecture. Their method is very similar to domain adaptation but for multi-learning setting.\nIn the Theorem 2, they have assumed parameter c <= 1/3. They have not provided any insight of how much restrictive this assumption is.  \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737889735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Reviewers"], "noninvitees": [], "tcdate": 1570237730043, "tmdate": 1575737889751, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review"}}}, {"id": "SJeNT1T3YH", "original": null, "number": 2, "cdate": 1571766204485, "ddate": null, "tcdate": 1571766204485, "tmdate": 1572972403522, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The submission investigates multitask learning (MTL) and develops new theories around MTL with linear models and linear+ReLU. In the experimental section, the authors improve performance in sentiment analysis on subtasks of the GLUE benchmark (building on BERT - highly non-linear neural network) and show a SVD-based task loss reweighting scheme on an multi-label image classification dataset. \n\nThe submission is overall well written though some paragraphs (2.1-2.3, in particular the example section) would benefit from additional effort towards clearer sentences. One issue with the submission is that there is a significant gap between the theory and experimental sections as theory only covers linear models and the experiments don\u2019t include linear models and purely focus on deep networks. The benefits of a bottleneck in multitask learning are well known (based empirical results). However, it is helpful that the additional theoretical results (given strong assumptions) provide some grounding. \nWhile the model with non-linear activation is mentioned at places, nearly all theorems rely on the linear model instead such that it might make sense to either work towards generalising the theorems or emphasising that most only apply to linear models.\n\nAdditional assumptions (1D labels, same input dimensionality across all tasks) should be emphasised to clarify limitations of all derivations. Where previous work addressed model similarity it often looks at models in the context of existing datasets (i.e. taking the data into account to describe boundaries etc) such that the emphasised novelty at looking at data similarity is to be taken with a grain of salt.\n\nOverall, the paper contributes to the conversation around multitask learning but would benefit from comparing again external work on multitask learning (e.g. see under minor) and from bridging between theory and experiments (e.g. experiments with the models described in the theory section - linear/ReLU).\n\nMinor:\n- y is used as label and as data terminology at different parts of the text. \n- the model in the first set of experiments has lower capacity than most models individually, suggesting that the capacity should be smaller even for individual tasks to prevent overfitting.\n- An ablation over model capacities is mentioned but missing for 3.3\n- comparison against existing multitask loss weighting techniques should be performed [1]\n\n\n[1] Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics\nAlex Kendall, Yarin Gal, Roberto Cipolla 2017\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737889735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Reviewers"], "noninvitees": [], "tcdate": 1570237730043, "tmdate": 1575737889751, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review"}}}, {"id": "rkl9a2T3KS", "original": null, "number": 3, "cdate": 1571769537613, "ddate": null, "tcdate": 1571769537613, "tmdate": 1572972403478, "tddate": null, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "invitation": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper analyzed the principles for a successful transfer in the hard-parameter sharing multitask learning model. They analyzed three key factors of multi-task learning on linear model and relu linear model: model capacity (output dimension after common transformation), task covariance (similarity between tasks) and optimization strategy (influence of re-weighting algorithm), with theoretical guarantees. Finally they evaluated their assumptions on the state-of-the-art multi-task framework (e.g GLUE,CheXNet), showing the benefits of the proposed algorithm.\n\nMain comments:\n\nThis paper is highly interesting and strong. The author systematically analyzed the factors to ensure a good multi-task learning. The discovering is coherent with with previous works, and it also brings new theoretical insights (e.g. sufficient conditions to induce a positive transfer in Theorem 2). The proof is non-trivial and seems technically sound.\n\nMoreover, they validated their theoretical assumptions on the large scale and diverse datasets (e.g NLP tasks, medical tasks) with state-of-the-art baselines, which verified the correctness of the theory and indicated strong practical implications.\n\nMinor comments:\nThe main message of the paper is clear but some parts still confuse me:\n1. I suggest the author to merge the Figure 3 and Data generation (Page 4) part for a better presentation. e,g which \u201cdiff.covariance\u201d is task 3 or 4 ?  And why we use different rotation matrix Q_i ? \n\n2. In algorithm 1 (Page 5) , I suggest the author use a formal equation (like algorithm 2) instead of descriptive words.\n     -- Step 2, I have trouble in understading this step.\n     -- Step 3, how to jointly minimize R_1,\\dots, R_k, A_1, \\dots, A_k ? we use loss (3)  or other losses ?\n     -- I suggest that the author release the code for a better understanding.\n\n3. For theorem 2, can we find some \u201coptimal\u201d c to optimize the right part ? Since 6c + \\frac{1}{1-3c}\\frac{\\epsilon}{\\X_2\\theta_2} might be further optimized \n\n4. In section 3.3. (Figure 6)  of the real neural network, the model capacity is the dimension of Z or simply the dimension before last fc-layer ?\n\n5. Some parts in the appendix can be better illustrated:\n    (a) I am not clear how proposition 4 can derive proposition 1.\n    (b) Page 15, proving fact 8: last line \\frac{1}{k^4}sin(a^{prime},b^{prime}) should be \\frac{1}{k^4}sin^{2}(a^{prime},b^{prime}). \n\n\nOverall I think it is a good work with interesting discoverings for the multi-task learning. I think it will potentially inspire the community to have more thoughts about the transfer learning. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1943/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["senwu@cs.stanford.edu", "hongyang@cs.stanford.edu", "chrismre@stanford.edu"], "title": "Understanding and Improving Information Transfer in Multi-Task Learning", "authors": ["Sen Wu", "Hongyang R. Zhang", "Christopher R\u00e9"], "pdf": "/pdf/d5f06dad9eae7e9312f393d6bd3dc29336c6b0b5.pdf", "TL;DR": "A Theoretical Study of Multi-Task Learning with Practical Implications for Improving Multi-Task Training and Transfer Learning", "abstract": "We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtained a 2.35% GLUE score average improvement on 5 GLUE tasks over BERT LARGE using our alignment method. We also design an SVD-based task re-weighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.", "keywords": ["Multi-Task Learning"], "paperhash": "wu|understanding_and_improving_information_transfer_in_multitask_learning", "_bibtex": "@inproceedings{\nWu2020Understanding,\ntitle={Understanding and Improving Information Transfer in Multi-Task Learning},\nauthor={Sen Wu and Hongyang R. Zhang and Christopher R\u00e9},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SylzhkBtDB}\n}", "original_pdf": "/attachment/3f3aa09455a2751180827cdca5fb7ab3998b6393.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylzhkBtDB", "replyto": "SylzhkBtDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1943/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737889735, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1943/Reviewers"], "noninvitees": [], "tcdate": 1570237730043, "tmdate": 1575737889751, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1943/-/Official_Review"}}}], "count": 12}