{"notes": [{"tddate": null, "replyto": null, "ddate": null, "writable": true, "tmdate": 1490988489341, "tcdate": 1478379313190, "number": 606, "replyCount": 0, "id": "r1YNw6sxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1YNw6sxg", "signatures": ["~Alex_X._Lee1"], "readers": ["everyone"], "content": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396703457, "tcdate": 1486396703457, "number": 1, "id": "B1wXTGIdl", "invitation": "ICLR.cc/2017/conference/-/paper606/acceptance", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers viewed the paper favourably, although there were some common criticisms. In particular, the demonstration would be more convincing on a more difficult task, and this seems like an intermediate step on the way to an end-to-end solution. There were also questions of being able to reproduce the results. I would strongly recommend that the authors take this suggestions into account.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396704010, "id": "ICLR.cc/2017/conference/-/paper606/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396704010}}}, {"tddate": null, "tmdate": 1481943621357, "tcdate": 1481926232910, "number": 2, "id": "rJWwLyGVx", "invitation": "ICLR.cc/2017/conference/-/paper606/official/review", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer4"], "content": {"title": "Excellent technical paper, but emphasis on control rather than representation", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper investigates the benefits of visual servoing using a learned\nvisual representation. The authors  propose to first learn an action-conditional\nbilinear model of the visual features (obtained from a pre-trained VGG net) from\nwhich a policy can be derived using a linearization of the dynamics. A multi-scale,\nmulti-channel and locally-connected variant of the bilinear model is presented.\nSince the bilinear model only predicts the dynamics one step ahead, the paper\nproposes a weighted objective which incorporates the long-term values of the\ncurrent policy. The evaluation problem is addressed using a fitted-value approach.\n\nThe paper is well written, mathematically solid, and conceptually exhaustive.\nThe experiments also demonstrate the benefits of using a value-weighted objective\nand is an important contribution of this paper. This paper also seems to be the\nfirst to outline a trust-region fitted-q iteration algorithm. The use of\npre-trained visual features is also shown to help, empirically, for generalization.\n\nOverall, I recommend this paper as it would benefit many researchers in robotics.\nHowever, in the context of this conference, I find the contribution specifically on\nthe \"representation\" problem to be limited. It shows that a pre-trained VGG\nrepresentation is useful, but does not consider learning it end-to-end. This is not\nto say that it should be end-to-end, but proportionally speaking, the paper\nspends more time on the control problem than the representation learning one.\nAlso, the policy representation is fixed and the values are approximated\nin linear form using problem-specific features. This doesn't make the paper\nless valuable, but perhaps less aligned with what I think ICLR should be about.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526586, "id": "ICLR.cc/2017/conference/-/paper606/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526586}}}, {"tddate": null, "tmdate": 1481934822141, "tcdate": 1481934822141, "number": 3, "id": "SkC1OWMNl", "invitation": "ICLR.cc/2017/conference/-/paper606/official/review", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer3"], "content": {"title": "Good approach for visual servoing, but could use better experimentation", "rating": "7: Good paper, accept", "review": "The paper proposes a novel approach for learning visual servoing based on Q-iteration. The main contributions of the paper are:\n\n1. Bilinear dynamics model for predicting next frame (features) based on action and current frame\n2. Formulation of servoing with a Q-function that learns weights for different feature channels\n3. An elegant method for optimizing the Bellman error to learn the Q-function\n\nPros:\n+ The paper does a good job of exploring different ways to connect the action (u_t) and frame representation (y_t) to predict next frame features (y_{t+1}). They argue in favour of a locally connected bilinear model which strikes the balance between computation and expressive ability. \n\nCons:\n- While, sec. 4 makes good arguments for different choices, I would have liked to see more experimental results comparing the 3 approaches: fully connected, convolutional and locally connected dynamics.\n\nPros: \n+ The idea of weighting different channels to capture the importance of obejcts in different channels seems more effective than treating errors across all channels equally. This is also validated experimentally, where unweighted performance suffers consistently.\n+ Solving the Bellman error is a difficult problem in Q-learning approaches. The current paper presents a solid optimization scheme based on the key-observation that scaling Q-function parameters does not affect the best policy chosen. This enables a more elegant FQI approach as opposed to typical optimization schemes which (c_t + \\gamma min_u Q_{t+1}) fixed. \n\nCons:\n- However, I would have liked to see the difference between FQI and such an iterative approach which holds the second term in Eq. 5 fixed.\n\nExperimental results:\n- Overall, I find the experimental results unsatisfying given the small scale and toy simulations. However, the lack of benchmarks in this domain needs to be recognized.\n- Also, as pointed out in pre-review section, the idea of modifying the VGG needs to be experimentally validated. In its current form, it is not clear whether the modified VGG would perform better than the original version.\n\nOverall, the contribution of the paper is solid in terms of technical novelty and problem formulations. However, the paper could use stronger experiments as suggested to earlier to bolster its claims.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526586, "id": "ICLR.cc/2017/conference/-/paper606/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526586}}}, {"tddate": null, "tmdate": 1481853067848, "tcdate": 1481853067848, "number": 1, "id": "rkE5O6gVx", "invitation": "ICLR.cc/2017/conference/-/paper606/official/review", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer1"], "content": {"title": "Interesting reinforcement learning paper needing maybe a little bit more work on improving the benchmark and exposition", "rating": "7: Good paper, accept", "review": "1) Summary\n\nThis paper proposes to tackle visual servoing (specifically target following) using spatial feature maps from convolutional networks pre-trained on general image classification tasks. The authors combine bilinear models of one-step dynamics of visual feature maps at multiple scales with a reinforcement learning algorithm to learn a servoing policy. This policy is learned by minimizing a regularized weighted average of distances to features predicted by the aforementioned model of visual dynamics.\n\n2) Contributions\n\n+ Controlled experiments in simulation quantifying the usefulness of pre-trained deep features for visual servoing.\n+ Clear performance benefits with respect to many sensible baselines, including ones using ground truth bounding boxes.\n+ Principled learning of multi-scale visual feature weights with an efficient trust-region fitted Q-iteration algorithm to handle the problem of distractors.\n+ Good sample efficiency thanks to the choice of Q-function approximator and the model-based one-step visual feature dynamics.\n+ Open source virtual city environment to benchmark visual servoing.\n\n3) Suggestions for improvement\n\n- More complex benchmark:\nAlthough the environment is not just a toy synthetic one, the experiments would benefit greatly from more complex visual conditions (clutter, distractors, appearance and motion variety, environment richness and diversity, etc). At least, the realism and diversity of object appearances could be vastly improved by using a larger number of 3D car models, including more realistic and diverse ones that can be obtained from Google SketchUp for instance, and populating the environment with more distractor cars (in traffic or parked). This is important as the main desired quality of the approach is robustness to visual variations.\n\n- End-to-end and representation learning:\nAlthough the improvements are already significant in the current synthetic experiments, it would be interesting to measure the impact of end-to-end training (i.e. also fine-tuning the convnet), as it is possibly needed for better generalization in more challenging visual conditions. It would also allow to measure the benefit of deep representation learning for visual servoing, which would be relevant to ICLR (there is no representation learning so far, although the method can be straightforwardly adapted as the authors mention briefly).\n\n- Reproducibility:\nThe formalism and algorithms are clearly explained, but there is a slightly overwhelming mass of practical tricks and implementation details described with varying levels of details throughout the paper and appendix. Grouping, simplifying, or reorganizing the exposition of these implementation details would help, but a better way would probably consist in only summarizing the most important ones in section and link to an open source implementation of the method for completeness.\n\n- Typos:\np.2: \"learning is a relative[ly] recent addition\"\np.2: \"be applied [to] directly learn\"\n\n4) Conclusion\n\nIn spite of the aforementioned limits of the experiments, this paper is interesting and solid, in part thanks to the excellent reply to the pre-review questions and the subsequent improved revision. This leads me to believe the authors are more than capable of following to a significant extent the aforementioned suggestions for improvement, thus leading to an even better paper.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526586, "id": "ICLR.cc/2017/conference/-/paper606/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526586}}}, {"tddate": null, "tmdate": 1481723801507, "tcdate": 1481723801501, "number": 3, "id": "S1-s100Ql", "invitation": "ICLR.cc/2017/conference/-/paper606/public/comment", "forum": "r1YNw6sxg", "replyto": "ryt7Hfwme", "signatures": ["~Alex_X._Lee1"], "readers": ["everyone"], "writers": ["~Alex_X._Lee1"], "content": {"title": "We have responded to your questions below and updated the paper accordingly.", "comment": "1. There isn\u2019t any guarantee that the features will transfer well for the new setting since they were trained for a different architecture and even for a completely different task. However, as long as the features have enough information for the servoing task (e.g. spatial feature maps where the relevant and distractor entities can be represented with different subsets of the feature maps), the reinforcement learning procedure will be able to learn how to use the given features so as to minimize the discounted costs.\n\nWe haven\u2019t tried servoing with respect to the features of the original network (yet).\n\n2. Although we didn\u2019t find it necessary to train the Q-network end-to-end, doing so might indeed lead to better servoing performance. In that case, the batch optimization of FQI (Equation 7) can be optimized with gradient descent or an stochastic variant of it. We updated the end of Section 5.2 to briefly mention this possibility.\n\n3. Equation 6 is not similar to line search nor temporal difference updates. Unlike line search, Equation 6 finds the global minimum of the Bellman error for a reparametrization of the weights as opposed to taking a step in a descent direction. Unlike temporal difference, Equation 6 uses the most up-to-date Q-values as opposed to the previous estimates of them (i.e. the same Q function is used for both $Q(s_t, u_t)$ and $Q(s_{t+1}, u_{t+1})$.\n\n4. There is no existing benchmark (because it is an active task), but our open-source framework provides a new simulated benchmark: https://github.com/alexlee-gk/citysim3d/tree/baseline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287502857, "id": "ICLR.cc/2017/conference/-/paper606/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1YNw6sxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper606/reviewers", "ICLR.cc/2017/conference/paper606/areachairs"], "cdate": 1485287502857}}}, {"tddate": null, "tmdate": 1481723721837, "tcdate": 1481723721830, "number": 2, "id": "rJM8y00mg", "invitation": "ICLR.cc/2017/conference/-/paper606/public/comment", "forum": "r1YNw6sxg", "replyto": "SkY53qkml", "signatures": ["~Alex_X._Lee1"], "readers": ["everyone"], "writers": ["~Alex_X._Lee1"], "content": {"title": "We have updated the paper to include numbers for sample complexity, clarify the distance metrics and norms, and explicitly define the MDP used in RL. We have also responded to your questions below.", "comment": "1. Our method actually combines model-based dynamics and model-free Q-learning. By leveraging prior knowledge on the dynamics and using a sample-efficient algorithm for learning a Q-function approximator, we are able to learn a servoing policy with significantly less trajectories: the policy learned with CEM used 12800 trajectories whereas the policy learned with our approach used only 200 trajectories (100 for FQI and 100 for validation). We have updated Table 1 and Appendix C.3 to include the total number of trajectories used for learning.\n\n2. We'll add a discussion of attentional mechanisms, and likely some of the techniques we propose could improve existing attentional methods. However, we would emphasize that the components of our approach, such as the dynamics model and trust-region fitted Q iteration, have substantial novelty beyond current attention methods.\n\n3.1 Thank you for your feedback. The norm in Equation (1) is actually not L2, but rather a generic distance metric, which is later defined in Equation (4). To improve clarity, we have labeled all the L2 norms with the subscript 2 in the updated paper. In addition, we now explicitly define the MDP in Section 3 (Problem Statement).\n\nThe action space is a vector of the robot\u2019s linear velocity and the angular velocity constrained around the vertical axis (i.e. tilt and roll angles are fixed), and the cost function is the one defined in Equation 10, Appendix B.\n\n3.2 The optimization of Equation (6) is useful because it is guaranteed to decrease the Bellman error and because both Q-values in the objective are the most up-to-date values. In contrast, the optimization of Equation (7) is only guaranteed to decrease the Bellman error for averager Q-functions and it uses the previous estimates of the Q-values for the target Q-values.\n\nThe update $\\alpha^{(k - \\frac{1}{2})} \\theta^{(k-1)}$ is not a minimization update nor some kind of line search. It simply updates the value of $\\theta$ after the positive scaling $\\alpha$ has been optimized in this reparametrization: $\\theta = \\alpha \\theta^{(k-1)}$."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287502857, "id": "ICLR.cc/2017/conference/-/paper606/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1YNw6sxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper606/reviewers", "ICLR.cc/2017/conference/paper606/areachairs"], "cdate": 1485287502857}}}, {"tddate": null, "tmdate": 1481723620717, "tcdate": 1481723620499, "number": 1, "id": "rknJJACXg", "invitation": "ICLR.cc/2017/conference/-/paper606/public/comment", "forum": "r1YNw6sxg", "replyto": "Hy-fhmnMe", "signatures": ["~Alex_X._Lee1"], "readers": ["everyone"], "writers": ["~Alex_X._Lee1"], "content": {"title": "We ran additional comparisons against a best-case bounding box detector, as well as baselines with a variety of hand-engineered features, and found in all cases that our method performed substantially better.", "comment": "1) Thank you for pointing this out. Visual tracking is indeed related to this work and we will update the related work section in the paper to discuss the relationship between visual tracking and servoing. We have now added experimental comparisons with servoing with respect to bounding boxes as a baseline. To obtain the best performance, we use ground truth bounding boxes (both in 2d and in 3d), which provides strictly more information than what a real tracker returns. We also use ground truth depth and the actual dynamics of the car. Details of the baselines are given in Appendix C.4.1 and the results are given in Table 3 of the updated paper. In brief, our approach outperforms all others.\n\n2) This work seeks to find whether deep features trained for image classification can be used for visual servoing using a simple control law that uses one-step dynamics. In addition, we seek to find (1) how can these features be effectively used for visual servoing and (2) whether using them leads to better generalization.\n\nWe haven\u2019t looked into trajectory prediction methods. Even though such methods could be used for object following (under assumptions that are different from visual servoing), they wouldn\u2019t answer the questions that we seek to find out in this work.  We will update the related work section in the paper to discuss such methods.\n\n3) Thank you for the suggestion. We have now tried servoing with respect to SIFT, SURF and ORB feature keypoints as a baseline. We provided some ground truth information to aid this baseline: we filtered out the feature keypoints that lie outside the target object (i.e. the car), both in the goal image and the current image. Details of the baselines are given in Appendix C.4.2 and the results are given in Table 3 of the updated paper.\n\nServoing with respect to these features is far worse than the other baseline methods. This is, in part, because the feature extraction and matching process introduces compounding errors. Similar results were found by Collewet & Marchand (2011), who proposed photometric visual servoing (i.e. servoing with respect to pixel intensities) and showed that their approach outperforms, by an order of magnitude, classical visual servoing that uses SURF features.\n\n4) Real-world experiments require very substantial systems engineering efforts to set up quadcopters and vehicles to be tracked, which we haven\u2019t undertaken (yet). However, we do believe our approach is well aligned with what one would hope for when running in the real world (relatively low sample complexity; and at training time the RL algorithm could, for example, access a groundtruth reward signal from a GPS tracking device). There is no existing benchmark (because it is an active task), but our open-source framework provides a new simulated benchmark: https://github.com/alexlee-gk/citysim3d/tree/baseline.\n\nIn regards to testing on artificially restricted field of views (rather than full visual servoing): We believe that\u2019s a simpler task that\u2019d be solved well with bounding box extractors.  However, as evidenced in the experiments we added (see answer to question 1), even ground-truth bounding boxes are a poor fit for visual servoing control, hence studying this setup isn\u2019t a great fit for studying visual servoing in 3D environments.\n\n5) The contribution of this paper is to study whether deep spatial feature maps are useful for visual servoing. In this work, we use VGG features because they are commonly used as off-the-shelf features, but in principle any spatial feature maps could be used in the proposed framework. We show (1) how to effectively use these visual features such that the simple control law for servoing can be used to minimize the cost for a particular task, and (2) that using visual features derived from a deep convolutional neural network pre-trained on image classification leads to some degree of generalization. For the first point, we show (1.1) how to construct a Q-function approximator that is greedily optimal with respect to the servoing policy and (1.2) how to learn it with a small number of trajectories, which is possible due to the structure imposed on the Q-function approximator and the fact that we are leveraging the learned feature dynamics."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287502857, "id": "ICLR.cc/2017/conference/-/paper606/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1YNw6sxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper606/reviewers", "ICLR.cc/2017/conference/paper606/areachairs"], "cdate": 1485287502857}}}, {"tddate": null, "tmdate": 1481217312623, "tcdate": 1481217312615, "number": 3, "id": "ryt7Hfwme", "invitation": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer3"], "content": {"title": "Clarifications on modified VGG used for feature dynamics", "question": "1. The VGG used in the paper retains the pre-trained weights of VGG learned from imagenet but modifies the architecture to remove max-pooling and replaces conv with dilated conv. Since the conv layers in original VGG were trained with pooled input from preceding layers, I am not sure that they would transfer well to the new setting. Have you tried retaining the original VGG with 224 x 224 input, max-pooling and 1-dilated conv?\n\n2. Given that the original VGG was intended for object classification, wouldn't it be better to fine-tune the entire network with the new multi-scale loss for predicting features in the next time-step. This will no longer be a ridge-regression problem as stated in Sec.4.3. However, it will ensure that the network is adapted to the new setting.\n\n3. Is Eq. 6 in FQI similar to line-search for temporal difference updates? Is it significantly better than simple temporal difference updates with fixed step size?\n\n4. The experimental setup seems fairly limited: the synthetic car examples in the test set are not visually different from those in the training set. The authors claim that one of the contributions of the paper is to develop a method which can handle \"extensive visual variation\", \"moving objects, occlusions and other complex phenomena\". Are there better benchmarks for visual servoing which reflect these challenges?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481217313203, "id": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481217313203}}}, {"tddate": null, "tmdate": 1480727697124, "tcdate": 1480727697119, "number": 2, "id": "SkY53qkml", "invitation": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer4"], "content": {"title": "Sample complexity, relation to attention, formulation", "question": "1. In order to avoid the need for \"large amounts of experience\", you propose to learn a predictive model. To my knowledge, there is still no consensus in RL on whether model-based  is faster than model-free in general. I would like if you could substantiate your point. Is it that robotics domains admit models which are \"nicer\" to learn (smooth dynamics due to structured physics and so on) and thus lead to sample efficient model-based algorithms ?\n\nAlong with the final scores in table 1, I think that it would be great if you could report on the sample efficiency. \n\n2. The way that you phrase your problem on page 3 is rather interesting \"[...] since it weights the errors of distractors objects [...]\". Have you considered posing your problem as an \"attention mechanism\" in the same vein as the recent wave of attention-based DL methods ? To which extent could the existing attention methods be used for your problem ? And from the opposite: could you use your approach to devise better attention models in classical supervised learning ? \n\n3.1 [Problem statement] I find this aspect confusing: \"distance metric that encodes the goal for particular task\". What you might be trying to say is that by minimizing (1) (which I assume is the L2 in feature space) your policy is implicitly learning a mapping from its action space to a representation in which the goal can easily be expressed. In general, I would find useful if you could be more explicit in your problem formulation: what is the action space precisely, what would the cost function be in general, do you assume an MDP ? \n\n3.2 I'm not sure why the optimization step described in (6) is useful. Also why is the update $\\alpha^(k - \\frac{1}{2}) \\theta^{k-1}$ a valid minimization step ? Is it some kind of line search ? \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481217313203, "id": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481217313203}}}, {"tddate": null, "tmdate": 1480502281104, "tcdate": 1480502281099, "number": 1, "id": "Hy-fhmnMe", "invitation": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "signatures": ["ICLR.cc/2017/conference/paper606/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper606/AnonReviewer1"], "content": {"title": "Pre-review questions", "question": "Following the ICLR reviewing guidelines, here are a few short pre-review questions:\n\n1) Why is visual tracking not considered as related work or baseline? This is one of the core problems in computer vision, straightforward to apply to the task of servoing (which is much simpler if you know where the object is located), and has a vast literature tackling the challenges mentioned by the authors (and more). It seems to me hard to defend completely ignoring this large part of the literature, and not comparing to, say, the top trackers from the VOT challenge (http://votchallenge.net). Note that although some of them use handcrafted features, many use deep learning and have very fast open source implementations.\n\n2) Have you compared to trajectory prediction methods? Instead of modeling visual feature dynamics, directly modeling the actual dynamics is a standard approach. Furthermore, it does not have the problems that the authors have to work around (too strong invariance of pre-trained deep features, high dimensionality, etc). Beyond the motivation in the introduction, this is needed, in my humble opinion, to quantitatively justify the need for all these different/additional difficulties and their associated necessary approximations.\n\n3) Have you tried using hand-crafted features (e.g., SIFT, SURF, HOG) as a baseline? It is much better than pixels, and often better than pre-trained (or even fine-tuned) deep features for instance-level modeling tasks like tracking or matching, because image-level classification deep networks are (by design) too invariant (not just because of max pooling). Several papers in retrieval or matching have made this point already, and this is confirmed by your experiments that require selecting the appropriate feature layers depending on the test scenarios. Adding a SIFT or HOG baseline is very simple to do (cf. official examples part of OpenCV), and in my humble opinion, mandatory to substantiate the claim made by the authors that they outperform hand-crafted features (raw pixels are not hand-crafted features).\n\n4) Have you run any real-world experiments? The toy synthetic experiments seem overly simplistic to me in terms of visual challenges (appearance variation, occlusions, clutter, lighting conditions, variety, etc). In addition, I think they do not allow to explore the main challenge of visual servoing that the authors mention: the fact that different objects might be very close in appearance. This is very often the case in practice (e.g., for pedestrians or cars), and one of the main focus of research on tracking (especially multi-object tracking, cf. the KITTI or MOT benchmarks). Is there no existing real-world benchmark for visual servoing? One can also imagine using real-world videos (e.g., from an aerial drone, several such public datasets exist) and simulated realistic dynamics coupled (for instance) with an artificially restricted field-of-view.\n\n5) Can you clarify a bit what is the main novelty and contribution? Using pre-trained VGG features is not a significant one in my opinion (especially in light of the aforementioned points), and the most interesting part of the paper (learning the weighting of the features) is only very succinctly described in 5.3."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Visual Servoing with Deep Features and Fitted Q-Iteration", "abstract": "Visual servoing involves choosing actions that move a robot in response to observations from a camera, in order to reach a goal configuration in the world. Standard visual servoing approaches typically rely on manually designed features and analytical dynamics models, which limits their generalization capability and often requires extensive application-specific feature and model engineering. In this work, we study how learned visual features, learned predictive dynamics models, and reinforcement learning can be combined to learn visual servoing mechanisms. We focus on target following, with the goal of designing algorithms that can learn a visual servo using low amounts of data of the target in question, to enable quick adaptation to new targets. Our approach is based on servoing the camera in the space of learned visual features, rather than image pixels or manually-designed keypoints. We demonstrate that standard deep features, in our case taken from a model trained for object classification, can be used together with a bilinear predictive model to learn an effective visual servo that is robust to visual variation, changes in viewing angle and appearance, and occlusions. A key component of our approach is to use a sample-efficient fitted Q-iteration algorithm to learn which features are best suited for the task at hand. We show that we can learn an effective visual servo on a complex synthetic car following benchmark using just 20 training trajectory samples for reinforcement learning. We demonstrate substantial improvement over a conventional approach based on image pixels or hand-designed keypoints, and we show an improvement in sample-efficiency of more than two orders of magnitude over standard model-free deep reinforcement learning algorithms. Videos are available at http://rll.berkeley.edu/visual_servoing.", "pdf": "/pdf/2dcbe533f9ca6ddcd46697671e4b591c5d8933a1.pdf", "TL;DR": "We use deep semantic features, learned predictive dynamics, and reinforcement learning to efficiently learn a visual servoing policy that is robust to visual variations.", "paperhash": "lee|learning_visual_servoing_with_deep_features_and_fitted_qiteration", "keywords": ["Computer vision", "Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "google.com", "openai.com"], "authors": ["Alex X. Lee", "Sergey Levine", "Pieter Abbeel"], "authorids": ["alexlee_gk@cs.berkeley.edu", "svlevine@cs.berkeley.edu", "pabbeel@cs.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481217313203, "id": "ICLR.cc/2017/conference/-/paper606/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper606/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper606/AnonReviewer1", "ICLR.cc/2017/conference/paper606/AnonReviewer4", "ICLR.cc/2017/conference/paper606/AnonReviewer3"], "reply": {"forum": "r1YNw6sxg", "replyto": "r1YNw6sxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481217313203}}}], "count": 11}