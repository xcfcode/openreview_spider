{"notes": [{"id": "9OZp8gLjwso", "original": null, "number": 1, "cdate": 1615835181302, "ddate": null, "tcdate": 1615835181302, "tmdate": 1615835181302, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Comment", "content": {"title": "Final Version", "comment": "Thank you to all the reviewers and the ACs for their time and helpful comments! We have uploaded our final camera-ready version. For posterity, we applied f-AnoGAN to our brain MRI out-of-distribution task and report results in the appendix. Since this experiment was not performed during the review period, we leave our original analysis unchanged and simply include these results as a reference for future readers."}, "signatures": ["~Ahsan_Mahmood1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ahsan_Mahmood1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xoHdgbQJohv", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649448175, "tmdate": 1610649448175, "id": "ICLR.cc/2021/Conference/Paper3465/-/Comment"}}}, {"id": "xoHdgbQJohv", "original": "3WxtbGMYOD7", "number": 3465, "cdate": 1601308384561, "ddate": null, "tcdate": 1601308384561, "tmdate": 1615834269798, "tddate": null, "forum": "xoHdgbQJohv", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "yGybtQK7c8k", "original": null, "number": 1, "cdate": 1610040390661, "ddate": null, "tcdate": 1610040390661, "tmdate": 1610473984929, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "There seems to be some disagreement between Reviewers, with some borderline scores and some very good scores. After careful consideration of both reviews and answers, and after reading the updated version of the paper with some detail, I believe the approach is valuable. The use of scores for detecting out-of-distribution data is very novel and presents a number of opportunities for further research, both theoretically and empirically. Overall, my recommendation is to ACCEPT the paper. As a brief summary, I highlight below some pros and cons that arose during the review and meta-review processes.\n\nPros:\n- Straightforward method. \"Trivial application\".\n- Novel application to medical images.\n- Robustness of default hyper-parameters.\n- Future open sourcing of the code and model checkpoints.\n- Topic highly relevant to the ICLR community.\n- Well-written paper + relatively good visualizations.\n\nCons:\n- Lack of comparison with other existing approaches.\n- Intuition/explanation/motivation on why the method works could be improved.\n- Effect of hyper-parameters could be further discussed/analyzed.\n- Concerns about applicability of the approach.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040390647, "tmdate": 1610473984913, "id": "ICLR.cc/2021/Conference/Paper3465/-/Decision"}}}, {"id": "lDMeG7QJ2P5", "original": null, "number": 4, "cdate": 1604015018492, "ddate": null, "tcdate": 1604015018492, "tmdate": 1606767974689, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review", "content": {"title": "Limited novelty, insufficient experimental or theoretical analysis", "review": "This paper apply multi scale score estimates to out-of-distribution detection. They demonstrate the usefulness of multi scale estimates and adopt auxiliary model to identify outlier data. The proposed method is evaluated on two different settings and is effective for out-of-distribution detection.\n\nStrength:\n+ The motivation of the proposed method is clear. The proposed method makes sense.\n+ The proposed method is quite simple. Seems easy to implement.\n\nWeakness:\n1. The writing of the paper needs further improvement. This paper is based on denoting auto encoder and Noise Conditioned Score Network. But the introduction of these important works is not very clear. \n\n2. The novelty of the method is marginal. They apply previous multi scale score estimate method on out-of-distribution detection settings. Such application is trivial.\n\n3. Experiment settings in the paper is quite simple. The proposed method is not rigorously studied in complex datasets. The improvement of previous works for separating SVNH and MNIST is not signifiant. The method doesn't compare with previous works when applying on brain scan images.\n\n4. Important theoretically analysis is missing. The proposed method has several important hyper parameters: number of scales, sigma value for each scale, etc. Real data distribution could be very complex, in this case, how to select these parameters? Discussions about how to the effect of these parameters are missing. \n\n-------\n\nUpdate after rebuttal:\n\nI appreciate the efforts of providing a hyper parameter study. Thanks for the clarification about dataset used in the paper.  \nI would like to increase my rating from 4 to 5.  Since the proposed method is somewhat ad-hoc (shared concern among other reviewers), either experimental or theoretic analysis is important to understand when and why it works. However, I don't think these analysis are sufficient in current form. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075359, "tmdate": 1606915769797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3465/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review"}}}, {"id": "gZEraxKlusv", "original": null, "number": 1, "cdate": 1603284151548, "ddate": null, "tcdate": 1603284151548, "tmdate": 1606747720900, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review", "content": {"title": "Interesting work on OOD detection, but could be improved by a more intuitive explanation, and more analysis.", "review": "Thank you for the clarifications.\nI did not change my rating, since I am unclear how the proposed method compares to SOTA beyond CIFAR-10/SVHN.\nTable 14 suggests that Likelihood Ratios is considerably better than the proposed method.\nFurthermore, neither in Table 1, nor in Section 5.4, I can find any results of the Likelihood Ratio method.\n----\nThe paper addresses the problem of detecting out-of-distribution (OOD) samples at test time, i.e. samples which belong to a class for which there was no training data.\nFor that purpose the authors propose to represent each sample x using $||s(x,\\sigma_1)||, ..., ||s(x,\\sigma_L)||$, where $s(x,\\sigma) =  \\nabla_x \\log q_{\\sigma}(x)$, and $q_{\\sigma}$ is the the original model probability $p(x)$ + gaussian noise with variance $\\sigma^2$. They call this L-dimensional space the score norm space.\nThe authors experimentally show that OOD samples tend to be rather distinct from in-distribution samples in the score norm space.\nThey exploit this, and propose to train either a Gaussian Mixture Model, Autoregressive Flow, or k-nearest neighbor model with the training data's score norm space representation.\n\nStrong points:\n- On CIFAR-10/SVHN they show that their method performs better than the Likelihood Ratios methods from (Ren et al 2019).\n- On several other baseline datasets they show that their method performs better than Confidence Thresholding (DeVries & Taylor, 2018) and ODIN (Liang et al 2017).\n\n\nUnclear/Weak points:\n\n- The proposed method is quite ad-hoc. Therefore, it would be helpful to include some experimental/theoretic analysis of why the method works, and when it does not work.\nThe authors try to provide some intuition in Section 2.1, though the explanation seems confusing to me:\non page 2, the authors argue that a small value of $p(x)$ is not a good method to detect outlier samples (referring to Nalisnick et al 2018), \nbut the Toy example in Section 2.1, page 3, discusses how their method can detect samples for which $p(x)$ is low. \n\n- The experimental results would be more convincing if their method were compared to a recent method like Likelihood Ratios (Ren et al 2019) also on other datasets than CIFAR-10/SVHN.\nFor example, (Ren et al 2019) also showed results for FashionMNIST/MNIST.\n\n- How sensitive is the method to the choice of L and other hyperparameters?\n\n\nMinor:\n- In the reference list the authors should at least add the conference name to each publication. \n- \"has been observe\" -> - \"has been observed\"\n- \"other unseen datasets It is important\" -> \"other unseen datasets. It is important\"\n- \"loglikelihoods\" -> \"log-likelihoods\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075359, "tmdate": 1606915769797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3465/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review"}}}, {"id": "Qd-RLNLg-gS", "original": null, "number": 7, "cdate": 1606253234214, "ddate": null, "tcdate": 1606253234214, "tmdate": 1606253234214, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Uploaded Code", "comment": "We have uploaded a zip file containing the code we used to train NCSN and our auxiliary models. Additionally, we include notebooks for our main experiments in Section 5 and the 1D GMM analysis in Section 2."}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "kVM1IURW2ZS", "original": null, "number": 6, "cdate": 1606243238755, "ddate": null, "tcdate": 1606243238755, "tmdate": 1606243238755, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "3DHf1uEYOIm", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Thanks for the response. There are remaining concerns.", "comment": "Thanks for the response. \nI wish there are more results rather than just text response.\n\nResponse #1 regarding robustness of hyperparameters is good..\n\nI see two problems with response #2.  A) the baseline added is the simplest possible baseline for general image data that is known to be weak.  In my original review, I suggested baselines like AnoGAN which had been shown effective on medical images.  I'm not sure how the choice of baseline was made.  B) You back up the argument that NCSN can scale to 3D data using unreported results.  Without the additional results, this claim remains unjustified.   \n\nOther minor edits and responses are fine.  I would consider increasing the score if my remaining concerns can be further addressed.  At the moment, the section about MRI data lacks empirical support.   \n\nBest,"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "e7FkdKrmGMb", "original": null, "number": 5, "cdate": 1606240940496, "ddate": null, "tcdate": 1606240940496, "tmdate": 1606240940496, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "lDMeG7QJ2P5", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Addressing the Concerns of AnonReviewer2", "comment": "We appreciate the comments you\u2019ve made and would like to address some of your concerns.\n\n#1. Even though these datasets are trivial to separate by humans, we want to emphasize that these experimental scenarios (e.g.  CIFAR vs SVHN) are the defacto standard in quantifying the performance for any out-of-distribution detector. Consequently, this out-of-distribution testbed has been used by [1], [2], [3], [4] and others. Moreover, Nalisnick et al. [5] showed that deep generative models such as Glow can in fact be fooled by outlying datasets, even for obvious cases such as CIFAR vs SVHN. Even methods specifically built for the purposes of out-of-distribution detection (such as ODIN) have struggled to accurately separate these easy-for-humans datasets. Therefore, we believe that achieving state-of-the-art performance in this landscape is still a worthwhile endeavor before moving on to more difficult scenarios.\n\n#2. We have added a hyperparameter analysis section (as highlighted in the manuscript). Our results show that the model is stable near our defaults, which perform near-optimal already. Furthermore, all our main experiments were run with the same defaults, showing that they do not need to be tuned on a per-dataset basis and can generalize well to different image data domains. Note the significant differences between CIFAR-10, SVHN, and brain MRI domains. Due to these reasons, we can recommend our defaults for various scenarios especially when anomalies are not known beforehand.\n\nReferences\n\n[1] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\nexamples in neural networks. ICLR, 2017.\n\n[2] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing The Reliability of Out-of-distribution Image\nDetection in Neural Networks. 6th International Conference on Learning Representations, ICLR\n2018 - Conference Track Proceedings, jun 2017.\n\n[3] J. Ren, Peter J. Liu, E. Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon, and\nBalaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In NeurIPS, 2019.\n\n[4] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.\n\n[5] Nalisnick, Eric, et al. \"Do Deep Generative Models Know What They Don't Know?.\" International Conference on Learning Representations. 2018.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "iwppQY4VF_i", "original": null, "number": 4, "cdate": 1606240629025, "ddate": null, "tcdate": 1606240629025, "tmdate": 1606240656428, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "ZPivueMgy-S", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Addressing the Comments of AnonReviewer3", "comment": "Thanks for all your comments! We have updated the paper to better reflect our potential next steps. One shortcoming of the method is the disjoint phases of learning i.e training the NCSN first and then the auxiliary models. We would like to explore the possibilities of joint training to improve the representations learnt by the NCSN. Additionally, we are also looking into the possibility of producing per pixel scores, which would allow us to generate heatmaps of anomalous regions in an image. \n\nPlease find our updated manuscript with all changed sections and some of your suggested edits highlighted with a colored sidebar.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "QxrCkixDdXq", "original": null, "number": 3, "cdate": 1606240342765, "ddate": null, "tcdate": 1606240342765, "tmdate": 1606240342765, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "gZEraxKlusv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Addressing the Comments of AnonReviewer4", "comment": "Thanks for all the comments! We have addressed some of your concerns below.\n\n#1. We apologize for the unclear description, we updated our paper to reflect that $p(x)$ (likelihood) is indeed a good method for detecting outliers. In fact, having a very low likelihood with respect to the data distribution is a strong indicator of outlierness. \nHowever, Nalisnick et al. showed that current deep likelihood methods struggle to produce low likelihoods for out-of-distribution samples. We present our method as an alternative to using $p(x)$ by considering scores (gradients of $\\log p(x)$ ) instead, with the added insight that we need to consider these scores at multiple scales of perturbation.\n\n#2. In Section 5.4, we compare our method against Likelihood Ratios. We have further added an additional experiment comparing our performance on FashionMNIST vs MNIST to Likelihood Ratios and ODIN in the Appendix.\n\n#3. We included a section looking at hyperparameter sensitivity. Our results show that the default hyperparameters seem to perform near-optimal, with minimal improvements after tuning. Furthermore, we would like to emphasize that all our main experiments in Section 5 utilized the same hyperparameters, regardless of the training dataset (including the brain MRI scans). This shows (at least empirically) that our defaults generalize well to different image data domains. \n\nThank you for the helpful suggestions, we have updated the paper accordingly. Please note that we highlighted all changed sections in the revised manuscript with a colored sidebar.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "3DHf1uEYOIm", "original": null, "number": 2, "cdate": 1606190931173, "ddate": null, "tcdate": 1606190931173, "tmdate": 1606231242477, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "tX5fX0b2N4", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment", "content": {"title": "Addressing the Concerns of AnonReviewer1", "comment": "Thank you for your in-depth comments! All your feedback was appreciated. Below we have tried to answer your concerns to the best of our abilities.\n\n#1. We acknowledge the need to determine whether our scheme is sensitive to its hyperparameters. To that effect, we have included a hyperparameter analysis in the paper. Our experiments show that the defaults already perform near optimal. Additionally, we would like to emphasize that all our main experiments were performed with the same hyperparameters despite the model being trained on different datasets (consider the difference between CIFAR and brain MRIs). This shows, at least empirically, that our defaults are generalizable to different image domains. Also, we have included an analysis on FashionMNIST vs MNIST in the appendix.\n\n In Section 2.1, we present a contrived toy example for illustrative purposes. We chose a significantly higher noise scale so that the difference in score norms was exaggerated and clearly identifiable. Further note that in this section we restricted ourselves to $L=3$ in order to plot each noise dimension. For all our main experiments, we kept the hyperparameters $L=10$ and $\\sigma_H=1$. We advocate for the use of these defaults as they empirically seem to generalize well to many OOD settings. For two of our auxiliary models (GMM and Flows), we do have access to likelihoods as an easy measure to tune performance. For KD trees, one could choose the Kth neighbour cutoff point according to the largest tolerable false positive rate for the application (which would require an inlier validation set only). We hope to evaluate MSMA on non-image domains in a future work.\n\n#2. You raise a valid concern about comparisons to a baseline in our MRI experiment. We have updated the paper with results comparing MSMA to the canonical OOD detection baseline introduced by Hendrycks and Gimpel (2017). We observe that MSMA generally outperforms it and observed it to be more stable across multiple runs.\n\nWhile we acknowledge Glow's generative capabilities on higher resolution images, we would like to emphasize that our goal is to extend the method to high resolution 3D MRIs, those that can reach 256x256x256. We have updated the paper to reflect this intention. Under this light, it is unclear whether models such as Glow can be easily extended to those regimes with reasonable engineering and computational costs. However, our (unreported) preliminary results show that MSMA works just as well on 3D samples as it did on the 2D MRI slices reported in the paper, with the same hyperparameters. More importantly, generative models like Glow already struggle to detect out-of-distribution samples in low-resolution domains like CIFAR vs SVHN (as shown by Nalisnick et al. 2018). It is difficult to say whether the situation would improve when looking at much higher resolution 3D images. \n\n#3. We apologize for lack of clarity in Section 2. $p(x)$ is indeed important in identifying outliers but NCSN outputs gradients of $\\log p(x)$ (the score). It is unclear how to remove the numerator as it is only implicitly contained in our scores. Your idea of training likelihood models at different scales is a useful comparison and we plan to pursue that research direction in the future.\n\nYou raise a good question about why multiple scales are important, which may not have come across in the paper. We are not guaranteed that one scale will work for all outliers. Consider outliers close to inlier modes e.g. outliers between Low-Density outliers and Inliers in Fig 2. Our large scale results in an overlap in the score distribution of inliers and Low-Density outliers. This makes it difficult to detect the aforementioned \"in-between\" outliers from the inliers. However, this large scale was necessary to get a big enough neighborhood context in order to capture the further away Local-Mode outliers. Thus, all three scales in the range would be necessary. We have updated Section 2 to clarify this intuition. The idea of using multiple scales for detecting local inlier modes is indeed very interesting. We leave such an analysis for future work.\n\n#4. We are fully committed to open sourcing the code, the paper will be updated with the GitHub repo in the final version. Unfortunately, we are not allowed to redistribute the medical data. However, it is all publicly available at nda.nih.gov. As a compromise, we plan on making the model checkpoints available in the GitHub repo once the review period is over.\n\nFinally, thank you for the minor comments, we have corrected our paper accordingly. Note that we highlighted the changed sections in the revised manuscript with a colored sidebar.\n\nReferences\n\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\nexamples in neural networks. ICLR, 2017.\n\nNalisnick, Eric, et al. \"Do Deep Generative Models Know What They Don't Know?.\" International Conference on Learning Representations. 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "xoHdgbQJohv", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3465/Authors|ICLR.cc/2021/Conference/Paper3465/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837272, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Comment"}}}, {"id": "tX5fX0b2N4", "original": null, "number": 2, "cdate": 1603840833828, "ddate": null, "tcdate": 1603840833828, "tmdate": 1605023996098, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review", "content": {"title": "Official Blind Review", "review": "Summary: \nThey proposed a new  method  of OOD detection, MSMA which uses a new generative model [NCSN] and a 2nd phase of fitting a simple density model on the vector of likelihoods at the various scales.\nThey showed empirically good results on standard OOD image datasets (CIFAR10 vs OOD, SVHN vs OOD etc.).  They were  able to achieve perfect separation at most settings, and much improved results for CIFAR10 vs SVHN compared to previous unsupervised methods\nThey showed interesting application for detecting OOD in medical  images where the inliears are scans for 9-11 years of  age, and OOD  are <9 years of age. \n\nStrength:\nMSMA is straightforward, and clearly described.  Since it\u2019s based on a fairly well tested generative model, the part of getting multi-scale likelihood from  NCSN should be fairly robust, and reproducible. \nApplication on medical images is novel, could potentially benefit the ICLR audience if the dataset is released.\n\nConcerns:\n#1 Robustness of method (i.e. sensitivity to hyperparameters) \nMSMA introduces an auxiliary model, which introduces extra hyperparameters, e.g. number of components in GMMs.  \nAlso, as the authors pointed out, choosing different noise scales for NCSN gives vastly different  results in terms of OOD detection. \nIn the multi-scale case, there is a high degree of freedom in how to choose the various noise scales.  \nIn Figure 1b, even in the multi-scale case MNIST and FashionMNIST seemed to have overlapping score vectors. It would be good if thorough results for this pair are included in the experiment section.  \nA.2 presents somewhat contradicting descriptions to Section 2.1. A.2 states that all experiments are done with the largest noise scale of 10, whereas in S2.1 they said it\u2019s only effective at a noise scale of 20. \nThis raises the concern of how applicable this method is to domains not studied in [NCSN].  E.g., on non-image OOD tasks e.g. those in [SEBM]. How would one choose the scale schedule in general?  \nUnlike Flows, VAEs,  and GANs where likelihood can be used to do model selection (e.g. using AIS), it\u2019s unclear how to do model selection with NCSN.  This makes me wonder if the range of hyperparameters used  for  the auxiliary models is generally applicable, in the case that the base NCSN model is trained with very different hyperparameters.  \n\n#2 A somewhat restricted coverage of existing methods\nBoth in the introduction and conclusion the authors emphasize how MSMA is developed  with the application of medical image OOD  detection in mind.  They dismiss comparison to density methods by saying they cannot be used with their high-resolution images. This is simply not true. [Glow] can easily learn images at 256x256, whereas the images here are only 110x90.  Also, another very popular family of methods for OOD detection in medical images are those related to [fAnoGAN]. GANs are more than capable of learning images of these scales.  \n\nProviding a new and meaningful application of OOD detection such as the MRI dataset provided here is a good contribution, but it seems to me that the authors did not attempt to compare to other methods, but only tried to show MSMA somewhat works.  \n\n#3 Incomplete understanding of the method\nSection 2 tries to provide some intuition about the effectiveness of the method. However, the analysis is quite brief.  Here I try to list a few questions:\n Most of the reasoning of how the  \u201cscore\u201d is  intuitively useful is  based on how the \u201cdensity\u201d appears in the denominator.  This makes me wonder if the numerator (\u201cgradient of the density\u201d) is of any importance, or maybe we can improve the method if that term is removed. One obvious thing to compare to here is just train a Flow, or VAE at different noise scales and compare to them.\nFigure 2 and  section 2.1 kind of explains why a large noise scale is useful, but not why using multiple scale is useful.  Why not show using a single best scale in the experiment section?  \nFigure 2 uses the construction of a local-model outlier to justify why large-scale is needed, but does this construction really translate to the real  world scenario?   Is it possible to show the  difference in prediction on the real datasets when using different scales, much  like in the toy setting?  If so, this method can also be  useful for selecting the local-mode outliers in the image  setting, which could inspire new applications\n\nMinor comments:\nIn Section 5.4. \u201c \u2026 is not tackled by classifier based OOD detectors \u2026 \u201c, this is wrong.  [Lee] and many works after does study this.  \nTable 2 caption is not describing Table 2\n\nOverall, the method is simple and effective on the CIFAR10 benchmark.  It\u2019s possible that this method is a worthy contribution. However, I\u2019m not sure about how generally applicable this method is because I don\u2019t see experiments in different settings, ablations studies, and/or adequate understanding of why MSMA is better than other unsupervised methods.  For the MRI task, the author  did not compare  to relevant  baselines. Lastly, the authors show no intention in open sourcing their code/dataset, which undermines the value of an empirical study.   \n\nReferences:\n[NCSN] Song, Yang, and Stefano Ermon. \"Generative modeling by estimating gradients of the data distribution.\" Advances in Neural Information Processing Systems. 2019.\n[SEBM] Shuangfei Zhai, Yu Cheng, W. Lu, and Zhongfei Zhang. Deep structured energy based models for anomaly detection. In ICML, 2016.\n[Glow] Kingma, Durk P., and Prafulla Dhariwal. \"Glow: Generative flow with invertible 1x1 convolutions.\" Advances in neural information processing systems. 2018.\n[LikelihoodRatio] J. Ren, Peter J. Liu, E. Fertig, Jasper Snoek, Ryan Poplin, Mark A. DePristo, Joshua V. Dillon, and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In NeurIPS, 2019.\n[fAnoGAN] Schlegl, Thomas, et al. \"f-anogan: Fast unsupervised anomaly detection with generative adversarial networks.\" Medical image analysis 54 (2019): 30-44.\n[Lee] Lee, Kimin, et al. \"A simple unified framework for detecting out-of-distribution samples and adversarial attacks.\" Advances in Neural Information Processing Systems. 2018.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075359, "tmdate": 1606915769797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3465/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review"}}}, {"id": "ZPivueMgy-S", "original": null, "number": 3, "cdate": 1603906174889, "ddate": null, "tcdate": 1603906174889, "tmdate": 1605023996029, "tddate": null, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "invitation": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review", "content": {"title": "Very good paper on a highly relevant topic", "review": "#### Summary:\nThe authors leveraged and repurposed Noise Conditioned Score Network (NCSN) that was originally introduced by Song & Ermon (2019) for generative modeling to be used for detection out-of-distribution (OOD) images. The authors unfold the intuition and rationale behind score matching followed by the equivalence of denoising autoencoder (DAE) to derive NCSN as a score estimator and provide an analysis to demonstrate the value of multiscale score analysis. In an experimental analysis on SVHN and CIFAR datasets they demonstrate superiority of their method (MSMA) over previously reported findings in the literature using state-of-the-art models (ODIN, JEM, Likelihood Ratios) on OOD task.\n\n##########################################################################\n#### Reasons for score: \nI vote for accepting. While the objective foundation of the methodology is adapted from previous work, I find the repurposing of it for fast and effective OOD detection novel and meaningful. The authors have structured and communicated their findings remarkably and provided a well designed experimental evidence to support the methodology for the detection of OOD images task. \n \n##########################################################################\n#### Pros: \n \n1. The paper addresses a relevant issue of OOD images detection using norms of score estimates and is highly relevant to the ICLR community. \n \n2.  The multiscale score analysis was very well done and very well communicated. The visualizations captured very well the essence of the findings and were well highlighted in in the discussion. It was clear, useful and it well justified the following method development.   \n \n3. This paper provides comprehensive experiments, well related to the scientific context, to show the effectiveness of the proposed method. The additional performance metrics in the appendix provide a well complementary supprot. \n \n##########################################################################\n#### Major comment: \nWhile the paper is overall very well written, structured and communicated, I found the final discussion and conclusion quite lacking. 1) The claim that autoencoding taks better suits deep CNNs should be a bit more elaborated/ demonstrated. 2) The sentence on the \u201cpeculiar phenomenon exhibited by multiscale score estimates\u201d is also not fully clear. It would be better if the authors explicitly mention to which phenomenon they relate. 3) I would find it important to add to the discussion a paragraph on the paper limitations, for example, any limitations the datasets present, limitations on the applied comparisons, limitations of the method application or others. 4) While the authors mentioned their plan to apply the methodology on a specific task, I think the discussion on future directions quite lacking. Are there other potential next steps that can be done on top of the proposed method? The analysis on range of scales mentioned in the end of section 2.1 could be an example of that. 5) As a minor suggestion, the authors may consider to relate to any wider impact of their work.\n\n#### Minor comments:\n\nAt two points in the manuscript the authors mentioned a future application of the method to identify atypical morphometry in early brain development. Since this experimental analysis was not actually done, I found it quite distracting and out of the scope of this paper. I would therefore suggest removing it from both introduction and discussion. \nSection 5.3, I would suggest to briefly mention what preprocessing was done on *_all_images_.\n \n##########################################################################\n#### Questions during rebuttal period: \n \nPlease address and above comments.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3465/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3465/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multiscale Score Matching for Out-of-Distribution Detection", "authorids": ["~Ahsan_Mahmood1", "~Junier_Oliva1", "~Martin_Andreas_Styner1"], "authors": ["Ahsan Mahmood", "Junier Oliva", "Martin Andreas Styner"], "keywords": ["out-of-distribution detection", "score matching", "deep learning", "outlier detection"], "abstract": "We present a new methodology for detecting out-of-distribution (OOD) images by utilizing norms of the score estimates at multiple noise scales. A score is defined to be the gradient of the log density with respect to the input data. Our methodology is completely unsupervised and follows a straight forward training scheme. First, we train a deep network to estimate scores for $L$ levels of noise. Once trained, we calculate the noisy score estimates for $N$ in-distribution samples and take the L2-norms across the input dimensions (resulting in an $N$x$L$ matrix). Then we train an auxiliary model (such as a Gaussian Mixture Model) to learn the in-distribution spatial regions in this $L$-dimensional space. This auxiliary model can now be used to identify points that reside outside the learned space. Despite its simplicity, our experiments show that this methodology significantly outperforms the state-of-the-art in detecting out-of-distribution images. For example, our method can effectively separate CIFAR-10 (inlier) and SVHN (OOD) images, a setting which has been previously shown to be difficult for deep likelihood models.", "one-sentence_summary": "Using score estimates at multiple noise scales outperforms state-of-the-art in out-of-distribution detection.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "mahmood|multiscale_score_matching_for_outofdistribution_detection", "pdf": "/pdf/639279c160eb93e79cf2ee33db8f9dc5b040f345.pdf", "supplementary_material": "/attachment/8daede8688564212322b3e8e04ad2e927f12b2a7.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nmahmood2021multiscale,\ntitle={Multiscale Score Matching for Out-of-Distribution Detection},\nauthor={Ahsan Mahmood and Junier Oliva and Martin Andreas Styner},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=xoHdgbQJohv}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xoHdgbQJohv", "replyto": "xoHdgbQJohv", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3465/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075359, "tmdate": 1606915769797, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3465/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3465/-/Official_Review"}}}], "count": 13}