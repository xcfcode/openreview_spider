{"notes": [{"id": "Db4yerZTYkz", "original": "fHqoJ7pmoiG", "number": 194, "cdate": 1601308030308, "ddate": null, "tcdate": 1601308030308, "tmdate": 1615855778148, "tddate": null, "forum": "Db4yerZTYkz", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "XFtdGdpA93i", "original": null, "number": 1, "cdate": 1610040458096, "ddate": null, "tcdate": 1610040458096, "tmdate": 1610474060950, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "After the rebuttal stage, three of four reviewers recommend acceptance, and one gives a borderline score but argues they lean positive. Concerns seem well addressed; the method is simple yet effective."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040458082, "tmdate": 1610474060935, "id": "ICLR.cc/2021/Conference/Paper194/-/Decision"}}}, {"id": "4IvNnYISE-g", "original": null, "number": 13, "cdate": 1606116507914, "ddate": null, "tcdate": 1606116507914, "tmdate": 1606125346181, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "U97u2kwAvcY", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "Glad to see we addressed your concerns! We will update the final version based on your suggestions.", "comment": "Thanks for your suggestions which help us improve the quality of this paper. We are glad to see your concerns are addressed. We additionally comment on Q3 & Q7 as below:\n\nRe Q3: Thanks for your great recommendations! This dataset seems to be a promising candidate for extending our method in the future!\n\nRe Q7: Thanks! We will definitely include the comparison to Geirhos et al and the state-of-the-art results in our final version. The table is expected to be like below (SOTA results are found via https://paperswithcode.com/):\n\n|                                    | FLOPs | #param | ImageNet | ImageNet-A | ImageNet-C | S-ImageNet | FGSM |\n|------------------------------------|-------|--------|----------|------------|------------|------------|------|\n| ResNet-50                          | 4G    | 98M    | 76.4     | 2.0        | 75.0         | 7.4        | 17.1 |\n| ResNet-50 in Geirhos et al. 2019   | 4G    | 98M    | 76.7     | 2.3        | 73.8       | 10.4       | 21.3 |\n| Ens. Shape- & Texture-Biased       | 8G    | 196M   | 77.2     | 2.0        | 68.6       | 16.3       | 20.4 |\n| Debiased-ResNet-50                 | 4G    | 98M    | 76.9     | 3.5        | 67.5       | 17.4       | 27.4 |\n| State-of-the-art (using ResNet-50) | 4G    | 98M    | 78.4 [1] | 8.4 [2]     | 60.4 [3]    | 55.8 [4]    | *    |\n\n\n*still literature searching\n\n[1] Yun, Sangdoo, et al. \"Cutmix: Regularization strategy to train strong classifiers with localizable features.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n\n[2] Li, Boyi, et al. \"On Feature Normalization and Data Augmentation.\" arXiv preprint arXiv:2002.11102 (2020).\n\n[3] Hendrycks, Dan, et al. \"The many faces of robustness: A critical analysis of out-of-distribution generalization.\" arXiv preprint arXiv:2006.16241 (2020).\n\n[4] Geirhos, Robert, et al. \"ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.\" ICLR, 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "DJ3tO-GTYAx", "original": null, "number": 9, "cdate": 1605938272743, "ddate": null, "tcdate": 1605938272743, "tmdate": 1606115051858, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "QzTMgnaPtFU", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "No similar work before, experiments about three fc layers and gamma=1 for segmentation", "comment": "We thank the reviewer for the detailed comments and the appreciation of our work. We address the concerns below:\n \n---\nQ1: Has anybody done any similar work before?\n\nA1: To our best knowledge, our work is indeed the first one in this direction!\n\nIt is possible that not using auxiliary BN (which just published in CVPR 2020) prevents others from observing the benefits of the proposed shape-texture debiased training. For example, without auxiliary BN, the proposed method on ResNet-50 can only achieve 75.1% top-1 ImageNet accuracy, which is much lower (1.8%) than the one with auxiliary BN. We will make this part more clear for facilitating the future research in this direction.\n\n---\nQ2: use three different final fc layers to classify training images based on shape, synthetic images based on texture, or natural images\n\nA2: Thanks for your suggestion. \n\nFirst, we want to clarify that, when using the CrossEntroyLoss, our method is equivalent to using three **weight-shared** fc layers to classify training images based on shape, synthetic images based on texture, or natural images. For example, as clarified in the implementation from mixup\u2019s author (https://github.com/hongyi-zhang/mixup/issues/5), applying the cross entropy loss to the softly constructed label ($\\widetilde{y}$ in Equation (1) in our paper), is equivalent to compute cross entropy loss for the shape label $y_s$ and the texture label $y_t$ separately, and then weighted average them.\n\nGiven the only difference between our method and the reviewer\u2019s suggestion is whether the fc layer should be shared or not (no other algorithmic changes), we expect the corresponding impacts will be small. Currently we are running this comparison, and will report the results once available.\n\n\\###### update ######\n\nTo reduce the computational cost, we first conducted the suggested experiment on ImageNet-200 using ResNet-18. As shown below, the performance gap between the weight-shared version (our proposed version) and the non-weight-shared version (the reviewer\u2019s suggestion using 3 fully connected layers) is quite small (~0.3%). This result may suggest that using three different final fc layers in training will not have significant impacts on the final performance. We will update the results again once the experiments on ImageNet using ResNet-50 are done.\n\n\n|              Models              | Top-1 Accuracy |\n|----------------------------------|----------------|\n| ResNet-18                        |           71.3 |\n| Debiased-ResNet-18 (original)    |           73.6 |\n| Debiased-ResNet-18 (3 fc layers) |           73.3 |\n\n\n---\nQ3: segmentation results when gamma=1?\n\nA3: The segmentation result with gamma=1 is 77.5 mIOU. We note this result is slightly worse than gamma=0.95 but much better than the vanilla training strategy (possibly due to that shape cue is crucial for the segmentation task). We will update the paper accordingly.\n\n---\nResponse to other comments: \n\nThank you so much for the suggestions on our writing. We will polish the paper accordingly.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "U97u2kwAvcY", "original": null, "number": 12, "cdate": 1606068536646, "ddate": null, "tcdate": 1606068536646, "tmdate": 1606070778019, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "n7_hdtMRpzq", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "Thanks for the great level of details in addressing the questions.", "comment": "First, I would like to express thanks to the authors for the level of details in addressing all the reviewers' comments and questions.\n\nRegarding Q3 -- thank you for the clarification with respect to the necessity of labels being from the same dataset - an interesting candidate, for future datasets could be the MetaDataset [1]\n\nFor Q4/A4:  Kylberg Texture dataset seems a saturated dataset, i.e. all results are in the 99.+%. Flickr Material Dataset behaves as expected, i.e. shape would not be very informative, as the images are of textures/materials, covering the entire image. It is interesting to note that the debiased model performs closer to the texture than shape-biased method.\n\nRe. Q7: Thank you for comparing with Geirhos et al -- adding one row in Tab2. would make it easier for readers to compare results with other methods. \nPlease also include one row in the final version, with the state-of-the-art results for each of the benchmarks, i.e.\nImageNet  ImageNet-A \tImageNet-C \tS-ImageNet \tFGSM\nVanilla ResNet-50 \t76.4 \t2.0 \t75.0 \t7.4 \t17.1\nState-of-the-art  70.5(1)   2.1(2) ....\n--- \nIn table caption: (1) XXYYZZ et al, (2) AABBCC et al, ...  \n\nPlease also include the baseline suggested by Rev. 2 (ensemble of T- and S-), which clearly shows the advantage of the proposed method.\n\n[1] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, E. Triantafillou et al, ICLR 2020\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "DHwLcyN8ed7", "original": null, "number": 11, "cdate": 1605938562311, "ddate": null, "tcdate": 1605938562311, "tmdate": 1605938679959, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "blW4lRCluWM", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "\u201cSimple but effective\u201d is a **merit** and we add more in depth analyses (part 1)", "comment": "We first thank the reviewer for the valuable comments, which help us to improve the quality of this paper.  We address the concerns below:\n\n---\nQ1: the method is a bit too simple & the contribution seems to consist in changing the domain of the texture source\n\nA1: FIrst of all, we would like to argue that \u201csimple but effective\u201d is a merit (rather than a weak point) of this paper! Given the simplicity of our method, we believe (1) it can be easily integrated into different learning frameworks (e.g., semi-supervised learning, few-shots learning) or applied to different computer vision tasks (e.g., object detection), for additional improvements; and (2) it can be easily implemented by other researchers using different deep learning libraries. \n\nSecondly, we want to clarify that, in addition to \u201c changing the domain of the texture source, with same dataset images.\u201d,  our most important contribution over Geirhos et al. is the proposed label assignment strategy. As shown in our ablation study, this simple (but definitely non-trivial) strategy successfully and substantially improves model performance on several image recognition benchmarks and adversarial robustness. And conversely, if we fail to compose the labels with information from both the shape source and the texture source, the learned models will NOT yield additional improvements (or even cause performance degradation) over the vanilla training baseline. \n\nWe will revise our paper accordingly to make these points more clear.\n\n\n---\nQ2: difference between the \"Vanilla\" training and shape-biased training\n\nA2: The main difference between these two methods lies in the training images. For vanilla training, the training images are the original dataset (e.g., ImageNet); for shape-biased training, the training images are the original dataset and its stylized version (e.g., ImageNet + Stylized-ImageNet). We will make it clear in the paper.\n\n\n---\nQ3: the method would benefit from a more principled way of choosing textures\n\nA3: Thanks for the suggestion. Currently, our algorithm randomly chooses images as either the shape source or the texture source in style transfer. We have trained the same model for multiple times, and find that such randomness in style transfer will not result in any training instability or have any impacts on the final model performance. The training framework is released here for the purpose of reproducing: \nhttps://anonymous.4open.science/r/678bd5dd-836a-45ba-b9ff-aacd007f4f89/\n\nAdditionally, we would like to clarify that extending our method to use a texture dataset (e.g., Materials in Context) as a source of texture may be not applicable at the current stage. As stated in Equation (1), we need the texture dataset and the shape dataset to have a compatible label space, for constructing the new soft label. But we do agree that, with more fine-grained texture selection, it is possible to further improve the representation learning of our method. We leave it as a future direction for our work.\n\n\n---\nQ4: lack of evaluation of impact on other tasks, such as transfer learning\n\nA4: First, we would like to clarify that our experiment on the Kylberg Texture dataset (Section 4.3) belongs to transfer learning: we only finetune the last fc-layer of the pretrained models on the Kylberg Texture dataset. The results are as shown below:\n\n\n|        | vanilla | S-biased | T-biased | Debiased |\n|--------|---------|----------|----------|----------|\n| Res50  |   99.48 |    99.09 |    99.61 |    99.53 |\n| Res101 |   99.74 |    99.61 |    99.72 |    99.79 |\n| Res152 |   99.78 |    99.63 |    99.76 |     99.80 |\n\n\nAs expected, we can observe that our debiased models are comparable to the texture-biased models and the vanilla training models (as they are texture-biased), and get better performance than the shape-biased models. \n\nAdditionally, we provide new results on Flickr Material Database (FMD). Similar to the experiment on the Kylberg Texture dataset, we only finetune the last fc-layer on FMD. The results are as shown below:\n\n\n\n|       | vanilla | S-biased | T-biased | Debiased |\n|-------|---------|----------|----------|----------|\n| Res50 |    74.6 |     73.3 |     79.2 |     75.8 |\n\n\n\nIf the reviewer has any particular requests on the transfer learning dataset, please let us know.\n\nBesides performing additional experiments for the transfer learning tasks, we would like to highlight that our current results already show the proposed method substantially improves the models\u2019 representation learning. For example, by training models on ImageNet (and no further fine-tuning), our method shows much better generalization on ImageNet-C, ImageNet-A and on defending against FGSM attack, than other baselines.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "n7_hdtMRpzq", "original": null, "number": 10, "cdate": 1605938429504, "ddate": null, "tcdate": 1605938429504, "tmdate": 1605938650296, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "blW4lRCluWM", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "\u201cSimple but effective\u201d is a **merit** and we add more in depth analyses (part 2)", "comment": "Q5: style transfer seems to be performed on entire image, how about on objects\n\nA5: Thanks for the suggestion. For the segmentation experiments in Section 4.4, we have tried to utilize the mask to perform style transfer exclusively insides the objects. Nonetheless, we found that training quickly gets overfitted. We hypothesize that replacing texture patterns inside an object probably will introduce some artifacts (e.g., more obvious object boundaries), therefore let models exploit shortcut features in training and fail to learn features with good generalization. But we do agree that designing more sophisticated style transfer algorithms is a promising direction to further improve our method, and we leave it as a future work.\n\n---\nQ6: the improvement against FGSM\n\nA6: We provide two possible interpretations here:\n\n1) As argued in [1], adversarial training (which is an effective way to improve adversarial robustness)  alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. For our method, it also helps models gain more shape-representations and be less biased towards texture, which potentially explains why we see improvements for defending against the FGSM attacker.\n\n2) As argued in previous works [2,3], adversarial examples shift the distribution of the original data. Given that our method improves models\u2019 representation learning, it is expected that it can also improve robustness against the FGSM attacker. We will make it clear in our paper that the purpose of evaluating robustness against FGSM attack is to serve as an evidence of demonstrating models\u2019 domain generalization ability.\n\n\n\n\n[1] Zhang, Tianyuan, and Zhanxing Zhu. \"Interpreting adversarially trained convolutional neural networks.\" ICML, 2019\n\n[2] Jacobsen, J\u00f6rn-Henrik, et al. \"Excessive invariance causes adversarial vulnerability.\" ICLR, 2019\n\n[3] Xie, Cihang, and Alan Yuille. \"Intriguing properties of adversarial training at scale.\" ICLR, 2020\n\n---\nQ7: Please add the benchmarks \nA7: Thanks for the suggestions. The comparison to Geirhos et al. 2019 and the vanilla training is shown below (will merge these results in Table 2),\n\n\n|                                  | ImageNet | ImageNet-A | ImageNet-C | S-ImageNet | FGSM |\n|----------------------------------|----------|------------|------------|------------|------|\n| Vanilla ResNet-50                |     76.4 |        2.0 |       75.0 |        7.4 | 17.1 |\n| ResNet-50 in Geirhos et al. 2019 |     76.7 |        2.3 |       73.8 |       10.4 | 21.3 |\n| Debiased-ResNet-50               |     76.9 |        3.5 |       67.5 |       17.4 | 27.4 |\n\n\n\n\n\nBesides directly comparing our method to other state-of-the-art methods, we would like to highlight that our method potentially is compatible with others, as specifically guiding CNNs to learn better shape and texture representations is largely ignored in previous works.  For example, as reported in Section 4.3, by building upon the advanced data augmentation CutMix, our method can obtain additional improvements on ImageNet-A (+1.4%), ImageNet-C (-5.9%, the lower the better) and Stylized ImageNet (+7.5%).\n\n\n\n[1] Jacobsen, J\u00f6rn-Henrik, et al. \"Excessive invariance causes adversarial vulnerability.\" ICLR, 2019\n\n[2] Xie, Cihang, and Alan Yuille. \"Intriguing properties of adversarial training at scale.\" ICLR, 2020\n\n[3] Taori, Rohan, et al. \"Measuring robustness to natural distribution shifts in image classification.\" Advances in Neural Information Processing Systems 33 (2020).\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "KQ32n5w-XU-", "original": null, "number": 8, "cdate": 1605938082245, "ddate": null, "tcdate": 1605938082245, "tmdate": 1605938121387, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "pq0HNGLN2z1", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "Benefits come more from **shape-texture debiased model** rather than data augmentation, and we show attention visualizations and better results than model ensemble", "comment": "We thank the reviewer for the detailed comments and the appreciation of our work. We address the concerns below:\n\n---\nQ1: the benefits may simply come from data augmentation? \n\nA1: We want to clarify that we use **exactly the same images** to train shape-biased, texture-biased or shape-texture debiased models. The only difference between them is how to assign labels (rather than how many shape/texture images are used) during training. As illustrated in Figure 2, for the shape-texture cue conflict inputs, if we assign the labels exclusively based on the shape/texture information, the learned models will be shape/texture biased; if we assign the labels by considering both shape and texture information, the learned models will enjoy both shape and texture representations. Our results show that the proposed shape-texture debiased model shows significantly better performance than other biased models (even if their training images are the same). Besides, as demonstrated in Section 4.3 & Appendix B (in the supplementary material), both our quantitative and qualitative results suggest that the proposed shape-texture debiased training strategy indeed obtains better shape and texture representations. We hope these clarifications can alleviate the reviewer\u2019s concern on \u201cthe benefits of this method may simply come from data augmentation\u2019. \n\n\n\n---\nQ2: Attention visualization for the debiased model.\n\nA2: Thanks for the suggestion. We have uploaded the appendix (in the supplementary material) for including this visualization and the corresponding analysis.\n\n---\nQ3 (a): the baseline of ensembling two models\n\nA3 (a): Thanks for the suggestion. We include the ensembling results below\n\n\n|       Model1       |        Model2        | FLOPs | #params | ImageNet | ImageNet-A | S-ImageNet | FGSM |\n|--------------------|----------------------|-------|---------|----------|------------|------------|------|\n| Debiased Res50     | -                    | 4G    | 98M     |     76.9 |        3.5 |       17.4 | 27.4 |\n| Shape-biased Res50 | Texture-biased Res50 | 8G    | 196M    |     77.2 |          2.0 |       16.3 | 20.4 |\n\n\n\n\nWe note that, compared to the proposed shape-texture debiased model, ensembling a shape-biased model and a texture biased model achieve similar performance on ImageNet classification (77.2% vs. 76.9%), but much worse performance on ImageNet-A (2.0% vs. 3.5%), S-ImageNet (16.3% vs. 17.4%) and on defending against FGSM (20.4% vs. 27.4%). Moreover, this ensemble model is 2X expensive at the inference time, i.e., FLOPs have increased from 4G to 8G. These evidences clearly demonstrate the effectiveness of the proposed shape-texture debiased training. We will update this baseline in the paper accordingly.\n\n\nQ3 (b): the cost of searching shape-texture coefficient\n\nA3 (b): Firstly, we want to clarify that the budget of grid-search here is very cheap---we use the shallow ResNet-18 on the ImageNet-200 (with 100,000 images) to find the shape-texture coefficient. Besides, our experiments show that this searched hyperparameter (gamma = 0.8) can successfully transfer to a wide range of network architectures on the ImageNet dataset. These observations possibly suggest that searching this hyperparameter just could be a one-time job (for a specific visual task like classification), and can be done cheaply by using shallow models and a small subset of the dataset. \n\nWe agree with the reviewer that our method can be further benefited if a more efficient search algorithm is designed. But designing such search algorithms is non-trivial and is irrelevant to the main purpose of this paper (shape-texture debiased training improves recognition models), we tend to leave it as a future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "8-Hm2HA81M", "original": null, "number": 7, "cdate": 1605937760177, "ddate": null, "tcdate": 1605937760177, "tmdate": 1605937760177, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "LtPy2_SWrql", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment", "content": {"title": "Interpretations of good adversarial defense results", "comment": "Thanks for your great appreciation of our work. We address the concerns below:\n\nQ: how shape-texture debiasing can help against adversarial attacks?\n\nA: We provide two possible interpretations here:\n\n1) As argued in [1], adversarial training (which is an effective way to improve adversarial robustness)  alleviates the texture bias of standard CNNs when trained on object recognition tasks, and helps CNNs learn a more shape-biased representation. For our method, it also helps models gain more shape-representations and be less biased towards texture, which potentially benefits model robustness for defending against the FGSM attacker.\n\n2) As argued in previous works [2,3], adversarial examples shift the distribution of the original data. Given that our method improves models\u2019 representation learning, it is expected that it can also improve robustness against the FGSM attacker. We will make it clear in our paper that the purpose of evaluating robustness against FGSM attack is to serve as evidence of demonstrating models\u2019 domain generalization ability.\n\n\n[1] Zhang, Tianyuan, and Zhanxing Zhu. \"Interpreting adversarially trained convolutional neural networks.\" ICML, 2019\n\n[2] Jacobsen, J\u00f6rn-Henrik, et al. \"Excessive invariance causes adversarial vulnerability.\" ICLR, 2019\n\n[3] Xie, Cihang, and Alan Yuille. \"Intriguing properties of adversarial training at scale.\" ICLR, 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Paper194/Reviewers", "ICLR.cc/2021/Conference/Paper194/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Db4yerZTYkz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper194/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper194/Authors|ICLR.cc/2021/Conference/Paper194/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873586, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Comment"}}}, {"id": "QzTMgnaPtFU", "original": null, "number": 1, "cdate": 1603736106351, "ddate": null, "tcdate": 1603736106351, "tmdate": 1605023933969, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Review", "content": {"title": "very simple paper, good IN results, could use more detailed analysis", "review": "Note that I don't follow this research area closely, this is thus more an educated impression of the paper.\n\nThe paper is extremely simple. It builds on the idea of Geirhos et al. 2019 to use style transfer to augment IN training data by mixing shape and texture information but when using a training image resulting from mixing the shape of an image with label A and the texture of an image with label B, they use 0.8*A + 0.2*B as label (which is similar to mixup/cutmix, except the merging is done using style transfer). This alone seem to provide a clear boost on IN and different variants.\n\nMy main worry/question, since Geirhos paper has had a lot of attention (as well as mixup/cutmix), is why nobody has done this yet? The paper is very clear and there don't seem to be any hidden difficulty (except maybe using an auxiliary BN?) If indeed there are no similar work, I think the clear performance boost on IN with a simple and clearly explained method justifies publication.\n\nI would however complain the analysis is not very strong. In particular, a natural alternative to the proposed approach would be to jointly train the convolutional layers but use three different final fc layers to classify synthetic images based on shape, synthetic images based on texture, or natural images. This experiment might help to clarify why the method works.\n\nOther comments:\n- I am not sure the \"debiaised\" term is really appropriate\n- I would prefer to see the results of 4.3 in tables (some could actually be in table 1), they would be more readable, more exhaustive and allow more effective analysis (with the current presentation I have a hard time believing the current approach peforms better than cutmix) + I would like to actually see the curve corresponding to the sensitivity to gamma\n- for the segmentation results, the results are really not sufficient: what are the results with gamma=1?\n- end of p.7 \"which can BE cropped\"\n- personally, I think it's a shame one has to go to p.5 to understand what the method is actually doing when it is so simple. In general, a lot could be shortened in the first 4 pages, and dedicated to better analysis and more exhaustive evaluation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069324, "tmdate": 1606915788916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper194/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Review"}}}, {"id": "blW4lRCluWM", "original": null, "number": 2, "cdate": 1603833390583, "ddate": null, "tcdate": 1603833390583, "tmdate": 1605023933899, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Review", "content": {"title": "Promising results, but could benefit from more in depth analysis.", "review": "Summary:\n\nThe authors propose a method to mitigate the bias towards either texture or shape, in convolutional network training. The method follows the idea from Geirhos et al (2019), but use images randomly sampled from the same dataset, instead of style transfer from paintings. Then, depending on a manually selected hyperparameter, the weights of conflicting labels are blended by weighted average of the one-hot encoding.\n\n###################\n\nReasons for score\n\nThe method proposed is simple, yet effective; however, parts of the method can be more principled, for example, the sampling of textures used for style transfer and debiasing - instead of using random pairs, random sampling from certain classes or external texture or material dataset would help understand better how debiasing works.\nSince the model claims that the method is debiasing the model (or can bias it towards shape or texture), it would be interesting to validate this claim on a texture database (e.g. Materials in Context, Flickr Material Database (FMD) or Describable Textures Dataset (DTD) ). The expected outcome would be - shape biased model underperforms; texture biased model, improves texture recognition. The last two datasets are small (1K images; 6K images), so it would not increase the computational overhead. \n\n###################\n\nStrong points of the paper:\n\n- visualization of the cues of the two models (texture and shape), using CAM (Class Activation Mapping);\n- showing the orthogonality of shape and texture-biased models (Fig. 4), with examples of tasks where each model is more accurate or less acurate.\n- significant improvement on Stylized ImageNet \n- usecase of the method to mitigate an adversarial attacker (FGSM).\n\n###################\n\nWeaker aspects:\n\n- method is a  bit too simple (however, it is effective).\n The whole paper can be summarized as: pick a random (texture) image, style transfer and blend the labels via weighted average.\nThe contribution in addition to Geirhos et al seems to consist in changing the domain of the texture source, with same dataset images. \n\n- What is the difference between the \"Vanilla\" training and shape biased training? My understanding from Fig. 2 and equation (1) - the two seem the same -- however they have separate columns in Table1 -- with similar results.\nPlease clarify the distinction.\n\n- the method would benefit from a more principled way of choosing textures.\nThe proposed method consists of  (Sec 2.1., Data Generation: \"... first select a pair of images from the training set uniformly at random, and then apply style transfer to blend their shape and texture\".  How the algorithm decides which of the images in the pair is a texture and which one contains shape information? Can the authors make public a list of such fixed pairs, for other researchers to evaluate the methods on?\nJust using style transfer (please also cite the method used for style transfer -- or if it is novel, please describe it) is very similar to the approach in Geirhos et al (2019), and it is not clear what the contribution is apart from changing the domain of the conflicting texture information.\nIt would be interesting to use a texture or material dataset (e.g. Materials in Context, Flickr Material Database, Describable Textures Dataset) as a source of texture for style transfer, or use class-specific textures (e.g. fur-like texture, to bias non-animal classes towards animal classes, e.g. cat, dog). \n\n- lack of evaluation of impact on other tasks, such as transfer learning:\nHow does debiasing affect the features on transfer learning tasks? For example, using the features from the debiased model (pretrained) on some external dataset, e.g. with a linear classifier on top of the features.\n\n- style transfer seems to be performed on entire image -- e.g. Figure 2 - could the authors please clarify this?\nThe example in Fig. 1 shows using the shape of the object, while Fig. 2  There are more recent datasets, such as MS-COCO or LVIS which provide image segmentation which could be used for training and evaluation - and also for guiding the style transfer, e.g. restricting the texture style transfer or retexturing only to the object mask.\nThere is an evaluation on Semantic segmentation in Sec. 4.4; however, it is not clear from Fig 5 why the background is replaced with the texture source.\n\n\n- not too much information on how / why the proposed method improves against FGSM.\n\n- Please add the benchmarks (state-of-the-art and methods compared with, e.g. Geirhos et al in Table 2). It's difficult to chase the numbers in the paper to compare the method with existing work.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069324, "tmdate": 1606915788916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper194/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Review"}}}, {"id": "pq0HNGLN2z1", "original": null, "number": 3, "cdate": 1604568852495, "ddate": null, "tcdate": 1604568852495, "tmdate": 1605023933828, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Review", "content": {"title": "Good paper with noticeable limitations", "review": "The paper tackles the problem of an existing bias in the classification networks, which makes them focus on a particular set of features (either local, \"textures\", or global, \"shapes\"). While it is clear how to train shape- and texture-biased networks (by preparing datasets with items easier to recognize using global or local features respectively), the authors claim that a combined training on both of these datasets would improve the performance of the network.\n\nPros.\n\n1. The proposed method tackles an important problem and improves over some baselines (when compared to plain training or other similar regularizations, like Mixup and CutMix) on ImageNet-based datasets. Also, it can be applied to different problems like segmentation and improve the results, which shows the generality of the approach.\n\n2. The authors try to explain the behavior of the method by providing extensive motivation and some experimental confirmations (unfortunately, they are quite limited).\n\n3. The paper is extremely well-written and concise.\n\nCons.\n\n1. The authors claim that whether or not the trained model will have bias depends on the statistics of items in the dataset: the more items easier to recognize using shape features, the more shape-biased the model will be. A simple test for that hypothesis would be to mix shape- and texture-biased datasets and train a model or their concatenation. If the authors' hypothesis is correct, this would aid in the final performance of the model, but I did not find such an experiment or any discussion of it in the experiments section. It seems to me that a major part of the performance of this method could simply come from data augmentation (basically, the same principle as described in the Mixup paper).\n\n2. Following on the previous point, it would be really helpful to see attention visualizations for the proposed model, similar to Figure 4. Right now it is not possible to discern whether or not it has any noticeable differences from the baseline shape model in terms of attention maps, and it could reinforce the proposed explanation of the method.\n\n3. Another important baseline that is missing is evaluating an ensemble of two models: one texture- and one shape-biased. It is a useful baseline, since it is hyperparameter-free, while the proposed method requires a grid-search to find an optimal parameter for blending the labels. Importantly, while the authors state that there is a \"sweet spot\" in this continuous hyperparameter, they do not propose a way to efficiently search for it other than grid-search. While they show that for the ImageNet dataset the network is quite robust to this hyperparameter, the same result may not be true for other datasets. Searching for this hyperparameter may prove to be significantly more costly than training an ensemble of two models.\n\nIn general, the proposed method itself is quite simple and has a lot of familiarities to the previous approaches, which the authors note. The main distinguishing feature of the proposed approach is the mechanism by which it works, suggested by the authors. And, in my opinion, there is not enough numerical and qualitative justification that this approach works the way it is explained.\n\nYet, assuming that some additional clarifications for the mechanisms behind the proposed approach will be provided in the rebuttal, I place my initial rating as \"accept\".", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069324, "tmdate": 1606915788916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper194/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Review"}}}, {"id": "LtPy2_SWrql", "original": null, "number": 4, "cdate": 1604807176260, "ddate": null, "tcdate": 1604807176260, "tmdate": 1605023933752, "tddate": null, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "invitation": "ICLR.cc/2021/Conference/Paper194/-/Official_Review", "content": {"title": "interesting observation, simple experiments!", "review": "Hypothesis: Convolutional Neural Networks are biased towards either texture or shape according to the dataset used for training. If so, can we develop an approach that can do shape-texture debiased learning?\n\nThis hypothesis is in line with Geihros et al., 2019 who observed that ImageNet trained models are texture-biased. \n\nVerification of the hypothesis: The paper qualitatively and quantitatively shows the influence of a shape-biased and a texture-biased model.\n\nHow to solve the problem?: The authors make two observations - (1). the model with shape-biased representations and the models with texture-biased representations are highly complementary to each other; and (2) being biased towards either shape cues or texture cues may hurt the performance but there exists a sweet-spot along the interpolation path of shape-texture debiased model that allows acquiring both shape and texture representations and achieve superior performance than vanilla models. \n\nThe rest of the paper consists of experiments to demonstrate this property and show analysis for different aspects of the model. \n\nQuestion: One thing that I did not understand is how shape-texture debiasing can help against adversarial attacks? The authors present some quantitative analysis demonstrating it but I still don't get the rationale behind it.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper194/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper194/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shape-Texture Debiased Neural Network Training", "authorids": ["~Yingwei_Li4", "~Qihang_Yu1", "~Mingxing_Tan3", "~Jieru_Mei2", "~Peng_Tang1", "~Wei_Shen2", "~Alan_Yuille1", "~cihang_xie1"], "authors": ["Yingwei Li", "Qihang Yu", "Mingxing Tan", "Jieru Mei", "Peng Tang", "Wei Shen", "Alan Yuille", "cihang xie"], "keywords": ["data augmentation", "representation learning", "debiased training"], "abstract": "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (eg, an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. \n\nExperiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A  (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, eg, Mixup, and CutMix. The code is available here: https://github.com/LiYingwei/ShapeTextureDebiasedTraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|shapetexture_debiased_neural_network_training", "one-sentence_summary": "Training CNNs to acquire a debiased shape-texture representation improves image recognition.", "pdf": "/pdf/95feebd9ddd0cc554ef18bf3b5cfce3f76bb9dd6.pdf", "supplementary_material": "", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021shapetexture,\ntitle={Shape-Texture Debiased Neural Network Training},\nauthor={Yingwei Li and Qihang Yu and Mingxing Tan and Jieru Mei and Peng Tang and Wei Shen and Alan Yuille and cihang xie},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Db4yerZTYkz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Db4yerZTYkz", "replyto": "Db4yerZTYkz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper194/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538069324, "tmdate": 1606915788916, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper194/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper194/-/Official_Review"}}}], "count": 13}