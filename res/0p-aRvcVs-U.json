{"notes": [{"id": "0p-aRvcVs-U", "original": "Q5Agt3TCapW", "number": 3543, "cdate": 1601308393374, "ddate": null, "tcdate": 1601308393374, "tmdate": 1614985649520, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HCKOxAUQdlb", "original": null, "number": 1, "cdate": 1610040513320, "ddate": null, "tcdate": 1610040513320, "tmdate": 1610474121369, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes \\alphaVIL, a method for weighting the task-specific losses in a multi-task setting in order to optimize the performance on a particular target task. The idea is to first collect gradient updates for the model based on all the separate tasks, and then re-weight those updates in order to optimize the loss on a held-out development set for the target task. In practice, this meta-optimization is performed with gradient descent. Experiments on multi-MNIST and several tasks that are part of GLUE and SuperGLUE show that \\alphaVIL is close in performance to a baseline multitask method and discriminative importance weighting.\n\nStrengths:\n- The idea is intuitively appealing. Directly reweighting tasks as a meta-optimization step is straightforward and appears to not be proposed previously in the literature.\n- The paper is clear in its presentation.\n\nWeaknesses:\n- The reviewers agree that the main weakness is that the experimental results do not show that \\alphaVIL offers any substantial benefits over existing methods. On the multi-MNIST task, while \\alphaVIL tends to have the highest mean performance, the difference is small (less than a standard deviation). On the GLUE/SuperGLUE tasks, it outperforms other methods on only 1 out of 10 experiments. There are also no confidence intervals/standard deviations provided to assess the significance of the results."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040513306, "tmdate": 1610474121353, "id": "ICLR.cc/2021/Conference/Paper3543/-/Decision"}}}, {"id": "oFi-R_p3Tz1", "original": null, "number": 6, "cdate": 1606226828218, "ddate": null, "tcdate": 1606226828218, "tmdate": 1606226828218, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "33OOEexVtIv", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment", "content": {"title": "Review-specific Response", "comment": "We would like to thank you, your comments are very helpful in our efforts to improve our work.\nPlease refer to our general comment, where we believe we answered the points raised in \"Cons\" and \"Overall\".\n\nFor specific questions:\nThe task weights are there to 'accumulate' in time the relative task weighing that has been calculated using the alpha optimization. The intuition is that if a task's change in the model has been always applied with a weight < 1, we probably want to collect future changes by this task with a scale in its gradients that reflects this downweighting. However, it is correct that it's possible for a single metaparameter to decide at the end how to weight a task delta without the need for extra scaling during collection. We have not done any experiments to compare the two methods yet but it'd be a good addition for an updated version of the paper.\n\nWe chose MultiMNIST for two reasons. First, we knew that multitask learning for this dataset helps, as this had been established in prior work (in particular, the MultiMNIST paper itself). Second, as the tasks overlap in terms of their classification space, but concentrate on different parts of the image, we could reasonably expect the task weights to eventually go towards 1.0 and 0.0 for the main and auxiliary tasks respectively (see also Figure 3, which shows they actually do). This would add as a sort of sanity check. We should definitely have made this point clearer in the paper, and will do so in the updated version.\n\nEnsembles were chosen for the test set as GLUE and superGLUE only allow a very limited number of submissions to be tested, leaving us the choice between ensemble or single-best models per method. While we believe that average scores per methods are more meaningful, we agree that we should also include ensemble results for the development set to be consistent with the test setting.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0p-aRvcVs-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3543/Authors|ICLR.cc/2021/Conference/Paper3543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836512, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment"}}}, {"id": "mjBGRH9S9Dy", "original": null, "number": 5, "cdate": 1606226789131, "ddate": null, "tcdate": 1606226789131, "tmdate": 1606226789131, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "he0-t4t4_YF", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment", "content": {"title": "Review-specific Response", "comment": "Thank you for your helpful comments.\n\nIn addition to our general response, we would like to answer your specific questions here.\n\n\"For example, in line 11 of Algorithm 1, I don\u2019t understand the intuition arbitrarily subtracting 1 from all weights.\" -- We see how this is unclear and should be pointed out in the text. We are not subtracting 1 from all weights, but from the newly found alpha parameters. Alpha parameters tell us which way to adjust the weights, i.e., whether to increase or decrease a task's importance. Since alphas are initialised to 1 before being optimised to weigh the tasks (lines 8--10, Algorithm 1), an alpha-value >1 entails that the corresponding task importance weight should be greater, while an optimized alpha < 1 indicates that the task should be down-weighted. Accordingly in line 11 of Algorithm 1, the term (\\alpha - 1) will be positive if alpha > 1 and the new weight according to w+(\\alpha-1) will be increased. Conversely, if alpha < 1, the overall term will be negative, and thus substracted from the task weight, decreasing it.\n\nOn the novelty of our method, while dynamic task weighting is not new, we believe that determining task weights through direct metaoptimization is. While DIW aims to optimize the weights with a numerical estimate of the gradient, we see our work as a more general framework which is compatible with any optimization method (e.g. Adam). We see how this could seem similar to meta-learning algorithms like MAML or Reptile. However, the metalearning objective is different i.e., instead of searching for a model initialization that can be used for rapid finetuning, we are looking for task weights during training, skipping this step. Other Meta-weight approaches like Shu et al. (2019) do adjust sample-specific weights for a single task during training by learning a complex weighting function. This differs from our approach as we are looking at data accross different tasks rather than within the same task, and our weighting is a very simple interpolation step of different task-specific updates.\n\nAdditional notes/questions:\n\n1. In NLU, for standard multitask, due to the fact that task datasets are not balanced, we sample 25% of all data for each task and train with this to calculate a task delta.\n\n2. While the weights eventually go to 1.0 and 0.0 respectively, we observed in Figure 3 that they do so gradually over the course of training to about epoch 25. We conjecture that initially, there is at least some benefit conveyed by the auxiliary task, which has implications for the final trained model.\n\n3. In part, MultiMNIST was meant to provide this sanity check, in combination with Figure 3. We should have made this more clear in the text. We tried more sanity checks, e.g., splitting the datasets into parts and looking to see if the algorithm will pick the same task's splits (that are guaranteed to positively transfer), but we were short of space in the paper. \n\n4. We can add standard deviations for NLU in the camera ready version however, for the submitted version they are collected over only 4 random seeds, so are less meaningful than for MNIST where they are based on a much larger 20 runs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0p-aRvcVs-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3543/Authors|ICLR.cc/2021/Conference/Paper3543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836512, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment"}}}, {"id": "chg0_tXsrUA", "original": null, "number": 4, "cdate": 1606226711512, "ddate": null, "tcdate": 1606226711512, "tmdate": 1606226711512, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "y6RkFNGKTm", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment", "content": {"title": "Review-specific Response", "comment": "Thank you for your feedback and suggestions. We hope we have replied to your concerns in our general response.\n\nWe would just like to add that our baselines include standard MT learning, and a single-task oriented approach (each task trained in isolation) as pointed out, as well as Discriminative Importance Weighting. DIW is a very competitive and strong target-task oriented MT algorithm, and in its formulation close to aVIL, with the difference of aVIL tuning task-specific weights through additional optimization. The experiments show aVIL on average outperforms DIW on the tested domains, and in particular, does seem to suffer less from overfitting on the development set(s)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0p-aRvcVs-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3543/Authors|ICLR.cc/2021/Conference/Paper3543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836512, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment"}}}, {"id": "ooUtIp6cKAf", "original": null, "number": 3, "cdate": 1606226650784, "ddate": null, "tcdate": 1606226650784, "tmdate": 1606226650784, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment", "content": {"title": "General Response to Reviewers", "comment": "We would like to thank the reviewers for their genuinely helpful comments. We are very glad there is consensus that the paper is well written and easy to understand.\n\nWe will address issues raised by multiple reviewers here, and reviewer-specific questions in their own comments.\n\nThere was a general concern that results are relatively weak and only marginally improve over the compared methods, in particular Discriminative Importance Weighting. We agree that the numbers leave this impression especially in the tested NLP domain.\nTo put the numbers in perspective, we would like to point out that improvements yielded by multitask learning on NLU tasks are often small. This phenomenon is also observed in other multitask settings, for example in the survey of [1] where MT leads to very mixed results.\nLooking at SuperGLUE (of which we used a subset of tasks), the overall average scores of the RoBERTa_large model in single and multitask setup differ by only 1.1 points (due to computational constraints we used the smaller _base variant in this work). Furthermore, on CommitmentBank, CoPA, and RTE the accuracy differences yielded by multitask training are +0.4, +0.6, and -0.1. This goes to show that in general achieving a substantial accuracy improvement is tough with the given model on these tasks.\n\nOn the other hand, on the more 'artificial' task of MultiMNIST (less noise, larger, cleaner and more consistent data), aVIL consistently outperforms the compared methods wrt. mean performance, including the very strong DIW which is close in its formulation to aVIL, but relies on a more aggressive weight tuning approach.\nOn the same note, we believe that one advantage of aVIL over DIW is that is less prone to this overfitting on the development data, as we briefly point in the last paragraph of Section 4.\n\nAnother common criticism is a lack of theoretical motivation/justification of the proposed algorithm. We have to concede that the work at present is lacking in this regard, and intend to remedy this for the camera-ready version.\n\nThe final common suggestion between reviews was the addition of more experiments and/or ablation studies to show the efficacy of our method. As we were hard-pressed to fit the algorithm along with the existing MultiMNIST and NLP experiments into the page limits, we had to cut out additional experiments and analysis. We will add this to the main text of a camera-ready version, space permitting, or add respective appendices.\n\n\n[1] Vandenhende et al. (2020) \"Multi-Task Learning for Dense Prediction Tasks: A Survey\""}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0p-aRvcVs-U", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3543/Authors|ICLR.cc/2021/Conference/Paper3543/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836512, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Comment"}}}, {"id": "33OOEexVtIv", "original": null, "number": 1, "cdate": 1603823484555, "ddate": null, "tcdate": 1603823484555, "tmdate": 1605023982660, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review", "content": {"title": "Multi-task learning with gradient-based meta-optimization for learning task-specific weights", "review": "Summary:\nThis paper proposes a new approach for multi-task learning that estimates the individual task weights through gradient-based meta-optimization on a weighted accumulation of task-specific model updates. Evaluations are performed in a multi-task learning setup on tasks related to computer vision (Multi-MNIST) and natural language understanding (tasks from GLUE and SuperGLUE).\n\nPros:\n1) The paper is easy to follow. \n\n2) Empirical evaluation is performed on vision and NLU domains.\n\nCons:\n1) I am not completely convinced with the proposed alpha-Variable Importance Learning algorithm. It is not very clear in the discussion how the alpha is different from task-specific weights. For example, in algorithm-1, if you replace deltas in line-10 with line-6, then there is no need to have separate alphas and task-specific weights, where line-9 can calculate the task-specific weights directly. \n\n2) In general, for a multi-task setup, I would expect to show the multi-task learning with multiple auxiliary tasks (that\u2019s the main motivation of this paper as well). However, the choice of the experimental setup is convincing, especially for the vision domain there is only one auxiliary task. \n\n3) Both results in Table-1 and Table-2 suggest that the proposed algorithm is not superior over the baselines and previous approaches. The improvements are minor and sometimes lower, and I believe most of the results fall within the statistically insignificant range. \n\n\nOverall: \nI think the paper can be made stronger with more thorough discussion on the algorithm and its properties. Further, the experimental results suggest that the proposed algorithm performs more or less similar to previous methods. Hence, there is a lot of scope for further improvement and I would suggest rejecting this paper. I would also suggest the authors to perform more experiments and ablations. \n\n\nQuestions:\n1) How is the alpha different from task-specific weights. Please discuss more on this. In algorithm-1, if you replace deltas in line-10 with line-6, then there is no need to have separate alphas and task-specific weights?\n\n\n2) Please provide statistical significant scores for all the results. \n\n3)  What's the reason behind choosing a multi-MNIST dataset with only one auxiliary task? Aren\u2019t there other datasets in a MTL setup with more auxiliary tasks? \n\n4) Table-2 results for the development set are based on the average of multiple runs, but for test you reported the ensemble, so why don\u2019t you report ensemble for the development set as well?\n\n5) Can you also present some ablations/discussion on the learned importance of an auxiliary task (based on task-specific weight over the training trajectory) vs. any intuitive reason that makes sense of this importance of the axillary task for a given primary task? If there is not such correlation, then also it's good to discuss. \n\nOther comments: \n\n1) Please try to expand the introduction section. \n\n2) Please provide some more ablations. \n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074097, "tmdate": 1606915803870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review"}}}, {"id": "he0-t4t4_YF", "original": null, "number": 2, "cdate": 1603937932268, "ddate": null, "tcdate": 1603937932268, "tmdate": 1605023982593, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review", "content": {"title": "Ad hoc multitask learning via task weighting with unconvincing experiments", "review": "Summary: This paper presents an algorithm for multitask learning that learns task weights via an EM-like approach that alternates between updating the model parameters (using task weights) and updating the task weights (using current model parameters, based on the target task development set). \n\nExperiments: They compare against single task training, standard multitask training (though this isn\u2019t described very clearly but roughly is training jointly on tasks), and another method for learning task weights, Discriminative Importance Weighting (DIW).\nThey present experiments on MultiMNIST, where the two tasks are to predict the task in the top left and bottom right of two superimposed digits. The proposed algorithm has the beast mean performance, but results are within a standard deviation of the baselines. They also present experiments on 5 NLU tasks (CommitmentBank, COPA, MRPC, RTE, WNLI) with the same baselines. The results on these tasks are mixed, with all multitask methods outperforming single task training (except on WNLI, which is a bit degenerate).\n\nOverall, this paper needs a bit more work. The proposed is quite ad hoc, and with little justification, it\u2019s not clear why we should be doing any of the things the algorithm proposes. For example, in line 11 of Algorithm 1, I don\u2019t understand the intuition arbitrarily subtracting 1 from all weights. From a novelty perspective, I\u2019m not convinced the proposed method is different enough from existing methods. Dynamic task weighting is not particularly new (e.g. the baseline method, DIW, they compare against), and their method starts to look a lot like meta-learning of task weights (like MAML [1], [2], or [3]). The results from the experiments are not convincing to me. On MNIST, the results between all methods are fairly close together, and on the NLU tasks, there\u2019s no clear best algorithm.\n\nAdditional notes and questions\n1. For the \u201cstandard multitask baseline\u201d, are the tasks balanced in size? Do you deterministically train on a batch from both or is it stochastic? This is mostly relevant for the NLU tasks, which have fairly different sizes.\n2. On MNIST, given that the algorithm sets the weight of one task to 1.0 and the other to 0.0, why is this algorithm outperforming single-task training?\n3. On a similar note, it'd be nice to see a sanity check experiment that the learned weights are sensible (e.g. one task has random labels) or an examples of where the learned weights are binary.\n4. I appreciate that the authors report min/max/mean/std of 20 runs on MNIST. It would be nice to see the standard deviations for the NLU tasks for consistency and given the fact that the standard deviations on the MNIST task were important in differentiating significant differences. Similarly, it would be nice to see how the task weights evolve.\n\nStyle notes\n* Huggingface Transformers now has a citation\n* Multitask Learning; Computer Vision; Natural Language Processing/Understanding: lowercase\n* \u201cSingletask\u201d should probably be hyphenated or two words.\n* \u201c10.000\u201d \u2192 \u201c10,000\u201d\n* Table 2 could really use headers over the two columns within each task.\n\n[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\" ICML. 2017.\n[2] Shu, Jun, et al. \"Meta-weight-net: Learning an explicit mapping for sample weighting.\" Advances in Neural Information Processing Systems. 2019.\n[3] Wang, Xinyi, Yulia Tsvetkov, and Graham Neubig. \"Balancing training for multilingual neural machine translation.\" arXiv preprint 2004.06748 (2020).", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074097, "tmdate": 1606915803870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review"}}}, {"id": "y6RkFNGKTm", "original": null, "number": 3, "cdate": 1603961058665, "ddate": null, "tcdate": 1603961058665, "tmdate": 1605023982520, "tddate": null, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "invitation": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review", "content": {"title": "The proposed methodology is intuitive and flexible, the experimental results are not convincing enough.", "review": "This paper proposes a novel multi-task learning method which adjusts task weights dynamically during training, by exploiting task-specific updates of the model parameters between training epochs. Specifically, the proposed model takes the differences between the model\u2019s parameters before and after the singletask update, after that the mixing factors of the model updates are found based on the differences to minimize the loss on the target task\u2019s development data. Empirical studies are performed on tasks of computer vision and natural language understanding.\n\nThe paper is well written and easy to follow, the authors summarize the related work in a clear manner. The proposed methodology is intuitive and well-motivated, in the meantime, it is flexible and can be generalized to other variations in terms of models and tasks. \n\nMy major concern about the paper include the following:\n1)\tAlthough the proposed method is intuitive and straightforward, it would be necessary to provide some theoretical justification or a formal analysis of the proposed methodology.\n\n2)\tConsidering the lack of a theoretical justification, the experimental results are not convincing enough to justify the proposed method. The baselines chosen include standard multi-task learning and one single task-oriented approach, which is somewhat limited. Even so, on both of the computer vision and natural language understanding tasks, the proposed method doesn\u2019t consistently outperform the baselines in most cases. The authors did provide sufficient analysis, nevertheless, it doesn\u2019t justify the effectiveness of the algorithm. \n\nBased on the concerns above, the paper can be improved from both the theoretical and empirical perspectives. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3543/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3543/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "$\\alpha$VIL: Learning to Leverage Auxiliary Tasks for Multitask Learning", "authorids": ["rafael.kourdis@gmail.com", "ggordonhall@gmail.com", "~Philip_John_Gorinski1"], "authors": ["Rafael Kourdis", "Gabriel Gordon-Hall", "Philip John Gorinski"], "keywords": ["multitask learning", "meta-optimization", "deep learning"], "abstract": "Multitask Learning is a Machine Learning paradigm that aims to train a range of (usually related) tasks with the help of a shared model. While the goal is often to improve the joint performance of all training tasks, another approach is to focus on the performance of a specific target task, while treating the remaining ones as auxiliary data from which to possibly leverage positive transfer towards the target during training. In such settings, it becomes important to estimate the positive or negative influence auxiliary tasks will have on the target. While many ways have been proposed to estimate task weights before or during training they typically rely on heuristics or extensive search of the weighting space. We propose a novel method called $\\alpha$-Variable Importance Learning ($\\alpha$VIL) that is able to adjust task weights dynamically during model training, by making direct use of task-specific updates of the underlying model's parameters between training epochs. Experiments indicate that $\\alpha$VIL is able to outperform other Multitask Learning approaches in a variety of settings. To our knowledge, this is the first attempt at making direct use of model updates for task weight estimation.", "one-sentence_summary": "We use a metaoptimization approach to tweak task-specific weights in multitask learning settings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kourdis|\\alphavil_learning_to_leverage_auxiliary_tasks_for_multitask_learning", "pdf": "/pdf/d436adb8e5fa9411dfcf86ea8332cb8468240602.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=yG3aY6_GD9", "_bibtex": "@misc{\nkourdis2021alphavil,\ntitle={{\\$}{\\textbackslash}alpha{\\$}{\\{}VIL{\\}}: Learning to Leverage Auxiliary Tasks for Multitask Learning},\nauthor={Rafael Kourdis and Gabriel Gordon-Hall and Philip John Gorinski},\nyear={2021},\nurl={https://openreview.net/forum?id=0p-aRvcVs-U}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0p-aRvcVs-U", "replyto": "0p-aRvcVs-U", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3543/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538074097, "tmdate": 1606915803870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3543/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3543/-/Official_Review"}}}], "count": 9}