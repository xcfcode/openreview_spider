{"notes": [{"id": "oeHTRAehiFF", "original": "jiQjzn6npKV", "number": 3350, "cdate": 1601308371626, "ddate": null, "tcdate": 1601308371626, "tmdate": 1614985648037, "tddate": null, "forum": "oeHTRAehiFF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tj19qe-d-zk", "original": null, "number": 1, "cdate": 1610040514546, "ddate": null, "tcdate": 1610040514546, "tmdate": 1610474122699, "tddate": null, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "invitation": "ICLR.cc/2021/Conference/Paper3350/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors propose a new dataset, ChemistryQA which has complex questions requiring scientific and mathematical reasoning. They show that existing SOTA models do not perform well on this dataset thereby establishing the complexity of the dataset. \n\nThe reviewers raised several concerns as summarised below:\n\n1) Writing is not very clear\n2) The quality of the dataset is hard to judge as some crucial information about the dataset creation process is missing \n3) The size of the dataset is small\n4) some stronger QA baselines need to be included\n\nUnfortunately the authors did not provide a rebuttal. Hence, its current form this paper cannot be accepted. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040514533, "tmdate": 1610474122684, "id": "ICLR.cc/2021/Conference/Paper3350/-/Decision"}}}, {"id": "GEWPfnRR8Zs", "original": null, "number": 4, "cdate": 1603882621335, "ddate": null, "tcdate": 1603882621335, "tmdate": 1605061483596, "tddate": null, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "invitation": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review", "content": {"title": "Interesting dataset, but evaluation can be improved", "review": "This paper proposes a new dataset for the chemistry domain as a real-world QA task.\nThe authors collected chemistry questions from a web page and used crowdsourcing to annotate the questions with labels and conditions required for solving them.\nAs baseline models, the authors propose an end-to-end neural network model and a symbolic solver that uses pre-defined rules. They demonstrate that the neural model struggles to solve the task but the symbolic solver outperforms it with the pre-defined rules that cover only a part of predicates in the dataset, arguing that their dataset is challenging and can foster the research of real-world problems in the chemical domain.\n\nPros:\n\n- The paper proposes a new dataset, ChemistryQA, that consists of 4,500 questions with labels of target variables and conditions provided in the questions.\n- The paper proposes a graph-search based solver with an extraction mechanism which is carefully implemented for this task.\n- Appendix has thorough lists of the predicates and units in the dataset and functions used in the baseline solver.\n\nCons:\n\n- Overall, I think that the writing of the paper can be improved more. There are some typos and formatting issues, which reduce the paper strength. Besides in Sections 2.1, 2.2, and 3.2.1, the paragraphs refer to figures and tables in Appendix. This seems to violate the submission instructions.\n- My primary concern is on the quality of collected questions. In Section 2.2, the authors say that they performed some check-and-verify mechanism in the first batch, which should be described in detail. Some related questions:\n  + Did the author compute the inter-annotator agreement on sampled questions?\n  + What kind of rules did you define for the verification?\n  + How many workers did work on the annotation in total?\n  + Did the same pool of workers work on the annotation for the first batch and the subsequent batches?\n  + Is there actual human performance on the collected questions? Seemingly, it is not guaranteed that posted questions on the web page are reasonably answerable.\n- The purpose of employing two baseline models is not explained well. In the introduction, the authors say that \"to verify the dataset is consistent with the purpose of evaluating AI' comprehensive capability\". However, their hypotheses for the experiments are not clearly stated.\n- It seems that the authors stopped implementing the predefined functions for the symbolic solver at the point where the solver outperforms the neural network model. The authors could have implemented more, but it is not clearly explained why they only implemented the predefined functions for the 35.5% predicate coverage. What would happen if there are a larger number of functions implemented?\n- Comparison with existing datasets can be elaborated more. For example, because the Option answer type should include different types of entities and the option is just a form, directly comparing it with value, formula, and equation does not make sense to me.\n- In the error analysis, 18% of cases are classified into Other Reason, which does not look like a small number to me. Can the authors break this down into more detailed categories?\n\nTypos:\n\n- Section 2.1, It this website -> In this website\n- Cite Devlin et al. (2019) for using BERT.\n- There is an ill-formatted cell in Table 4.\n- There is inconsistent use of decimal commas (4,500 vs 4418)\n- Add whitespace before citations (e.g., in Section 2.4).\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3350/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3350/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077556, "tmdate": 1606915804197, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3350/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review"}}}, {"id": "DgHMh7jcBdw", "original": null, "number": 1, "cdate": 1603608438387, "ddate": null, "tcdate": 1603608438387, "tmdate": 1605024017135, "tddate": null, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "invitation": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review", "content": {"title": "Official Blind Review #2 ", "review": "Paper Summary:\n* This paper presents a question answering dataset called ChemistryQA.  It is different from existing datasets in that ChemistryQA requires open knowledge and complex solving processes.  It provides triplet like extraction annotation which isolates language understanding and domain knowledge. Experimental results show that a neural encoder-decoder model and an extractor-plus-solver do not work well.\n\nStrengths:\n* The dataset contains real-world QA that requires the ability to perform complex chemistry calculation and reasoning. It is difficult for crowdsourcing workers to generate such complex questions.\n* The authors proposed a novel annotation method that target variables and conditions are labeled in a triple-like format.\n\nWeaknesses:\n* The dataset seems small to acquire the ability to perform complex calculation and reasoning. The training, validation, and testing datasets consist of 3,433, 485, and 500 questions, respectively.\n* The paper does not show statistics of the dataset such as the average length of questions and answers and the unique number of answers.\n* The paper does not show the performances broken down by question types. Although the end-to-end solver achieves an answer accuracy of 0.164, I think it is important to show more detail on what it can and cannot do.\n* The authors uses a pre-trained BERT as the encoder of the end-to-end solver and trained the decoder from scratch.  I think pre-trained encoder-decoder models such as T5 and BART are better as the baselines of the end-to-end solver than the model used in this paper.  \n\nReview Summary:\n* The paper is well motivated. ChemistryQA can be a useful dataset to evaluate the ability of chemistry calculation and reasoning, while the dataset seems small to acquire the ability.   I think it can benefit a lot with a more comprehensive analysis of evaluation results of baselines.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3350/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3350/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077556, "tmdate": 1606915804197, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3350/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review"}}}, {"id": "iBkRwn-zaS8", "original": null, "number": 2, "cdate": 1603667270901, "ddate": null, "tcdate": 1603667270901, "tmdate": 1605024016901, "tddate": null, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "invitation": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review", "content": {"title": "Lofty motivations and goals, but final dataset may not meaningfully accelerate progress toward such goals.", "review": "*Summary*: This paper proposes a new dataset based on textbook / classroom chemistry questions for complex knowledge retrieval and aggregation. The authors scrape several thousands questions from online repositories and add additional natural language annotations signifying the quantities to be solved for in each question, as well as the declarative knowledge. Two baselines, one end-to-end neural and another symbolic, both fail at this dataset.\n\n*Strengths*: The dataset targets the important question of how to build models that can retrieve knowledge while performing complex reasoning.\n\n*Weaknesses*: As-is, the dataset fails to target the knowledge retrieval component---models are either expected to magically know how to calculate the answer, or use hard-coded functions that complete a graph of values. The neural baseline also seems a bit non-standard, raising questions of how well modern systems can actually do on the task; furthermore, the end-to-end neural system is disadvantaged in that it likely has not seen much chemistry-related content during fine-tuning, whereas the symbolic baseline has access to a host of human-defined functions. Furthermore, dataset quality is a bit difficult to assess without more samples.\n\n*Recommendation*: 3 . This benchmark is motivated by the lofty goals of encouraging the development of models that can combine knowledge retrieval, complex reasoning, and language understanding. However, it\u2019s unclear to this reviewer whether it will prove useful in making progress towards such goals---they\u2019re too conflated to be meaningfully evaluated within this context. To improve the benchmark and make it more amenable toward advancing those research goals (versus just being a difficult datasets that current models cannot handle), I\u2019d recommend explicitly targeting and evaluating this knowledge retrieval component as well. For instance, given a specific knowledge-base that\u2019s guaranteed to span the facts necessary to answer the questions, how well can a model (1) retrieve relevant information and (2) use such relevant information to answer questions?\n\nQuestions:\n\n\u201cChemical Calculation Problems cannot be solved by end-to-end neural networks since complex symbolic calculations are required\u201d: this is a hypothesis---there are many tasks where \u201ccomplex symbolic calculations are required\u201d, but end-to-end networks excel.\n\nWhat extent of knowledge is required to solve this task? For instance, many old semantic parsing datasets came with databases, and it was guaranteed that within the database, an answer would occur. What would a corresponding knowledge graph for this case look like, and how complex would it be?\n\n\u201cUnlike similar tasks\u2019 annotation, we cannot collect all the atomic operations needed before starting annotation, since the set of chemical operators is not closed.\u201d The set of mathematical operators is also not closed (e.g., in math word problems). Why is this approach better than collecting all the operations represented in the dataset (even if it doesn\u2019t cover all of the operations that one could conceivably see)?\n\nThe annotation interface / process looks quite regular---you aren\u2019t expecting too much variation from the templates given. Given that you can help crowdworkers with these templates, why not just use these templates as the baseline for a formal meaning representation that would encompass the knowledge needed for the task?\n\nCan you give more details about the annotation process, beyond the short paragraph near the end of section 2.2? (\u201cWe employed crowdsourcing for this annotation work...around 336 hours)\u201d. I\u2019d be surprised if any crowdworker could label this sort of data well. What quality control filters did you put in place? Can we see more (random) samples of the dataset, so we can better assess its quality?\n\nEnd-To-End Solver: where did you get this model architecture from, such that \u201cThis method represents a class of powerful neural networks, which achieved state-of-the-art performance on many QA tasks.\u201d? I\u2019ve never seen BERT used in a seq2seq setting like this (instead, people tend to use models trained on input/output pairs, like BART or T5). I\u2019d like to see how this compares to using BART or T5, since it\u2019s not clear that the BERT initialization would be good for generation.\n\nGraph-Search based Solver: the need to implement specific functions (78, in this case) is significant, and undermines the point of this dataset, in my opinion. There\u2019s no inherent value in learning to solve chemical equations well---the hope is that, in the process of doing so, we\u2019ll get modeling insights into what methods work well and can be generally applied to other knowledge-intensive tasks. This graph-search based solver seems narrowly scoped to ChemistryQA and difficult to adapt to other tasks, and it\u2019s not entirely clear why we should value its results.\n\nToken-level accuracy: Is it guaranteed that the output of the graph-search based solver will be the same length as the gold output? How? Else, how is token-level accuracy computed?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3350/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3350/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077556, "tmdate": 1606915804197, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3350/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review"}}}, {"id": "lIkX2AkkEay", "original": null, "number": 3, "cdate": 1603864573008, "ddate": null, "tcdate": 1603864573008, "tmdate": 1605024016741, "tddate": null, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "invitation": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review", "content": {"title": "Summary:  The paper propose a new complex dataset for QA, Chemistry QA, for the task of answering chemistry questions requiring mathematical and symbolic reasoning. They also provide two simple baselines. They claim that end-to-end neural networks cannot solve this task easily and efficiently. ", "review": "Strengths:  A new QA dataset for chemistry QA consisting of 4500 questions and covering 200 topics. Crowdsourced annotation of variable and conditions. \n\nWeaknesses: -  Strong baseline results are missing. Intermediate steps is missing in the annotations which are really helpful in training an end-to-end model.  The topic distribution is missing from the paper. Experimental results and analysis is not enough. \n\nOverall: The idea of curating and annotating a new dataset for Chemistry QA dataset is good. I feel a stronger baseline would have helped much in understanding and analysing the quality dataset and annotations. Also the question complexity analysis/topic distribution is missing. Overall the paper writing could be improved a lot, in the current version it is difficult to follow.\n\nQuestion: If the whole problem can be converted into a set of triples and conditions they why not use graph based QA techniques?  It will be interesting to see how neural symbolic machines/Neural module network perform on this dataset ? Topic distribution, question type distribution etc are missing.  Any specific reasons for using 12 layers of transformer encoders and 6 layers of transformer decoders in Extractor plus Graph Search Based Solver?  Which graph search algorithm is used in section 3.2.2 and Table 5?\n\n\nTypos:\n It this website,  -> On this website\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3350/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3350/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ChemistryQA: A Complex Question Answering Dataset from Chemistry", "authorids": ["~Zhuoyu_Wei1", "jiwe@microsoft.com", "xiubo.geng@microsoft.com", "yining.chen@microsoft.com", "baihua.chen@microsoft.com", "~Tao_Qin1", "djiang@microsoft.com"], "authors": ["Zhuoyu Wei", "Wei Ji", "Xiubo Geng", "Yining Chen", "Baihua Chen", "Tao Qin", "Daxin Jiang"], "keywords": [], "abstract": "Many Question Answering (QA) tasks have been studied in NLP and employed to evaluate the progress of machine intelligence. One kind of QA tasks, such as Machine Reading Comprehension QA,  is well solved by end-to-end neural networks; another kind of QA tasks, such as Knowledge Base QA,  needs to be translated to a formatted representations and then solved by a well-designed solver. We notice that some real-world QA tasks are more complex, which cannot be solved by end-to-end neural networks or translated to any kind of formal representations. To further stimulate the research of QA and development of QA techniques, in this work, we create a new and complex QA dataset, ChemistryQA,  based on real-world chemical calculation questions. To answer chemical questions, machines need to understand questions, apply chemistry and Math knowledge, and do calculation and reasoning. To help researchers ramp up, we build two baselines: the first one is BERT-based sequence to sequence model, and the second one is an extraction system plus a graph search based solver. These two methods achieved 0.164 and 0.169 accuracy on the development set, respectively, which clearly demonstrate that new techniques are needed for complex QA tasks. ChemistryQA dataset will be available for public download once the paper is published.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wei|chemistryqa_a_complex_question_answering_dataset_from_chemistry", "pdf": "/pdf/532ee6316c613095dea67afd41077305d28539bc.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=mAMvPMWT0D", "_bibtex": "@misc{\nwei2021chemistryqa,\ntitle={Chemistry{\\{}QA{\\}}: A Complex Question Answering Dataset from Chemistry},\nauthor={Zhuoyu Wei and Wei Ji and Xiubo Geng and Yining Chen and Baihua Chen and Tao Qin and Daxin Jiang},\nyear={2021},\nurl={https://openreview.net/forum?id=oeHTRAehiFF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oeHTRAehiFF", "replyto": "oeHTRAehiFF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3350/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077556, "tmdate": 1606915804197, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3350/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3350/-/Official_Review"}}}], "count": 6}