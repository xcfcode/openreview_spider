{"notes": [{"id": "BJlc6iA5YX", "original": "BkeX4Nw9YX", "number": 827, "cdate": 1538087873833, "ddate": null, "tcdate": 1538087873833, "tmdate": 1545355435469, "tddate": null, "forum": "BJlc6iA5YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJgdAnPBg4", "original": null, "number": 1, "cdate": 1545071823752, "ddate": null, "tcdate": 1545071823752, "tmdate": 1545354481370, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Meta_Review", "content": {"metareview": "The reviewers have agreed this work is not ready for publication at ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper827/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper827/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353071463, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353071463}}}, {"id": "SylhtNBFkN", "original": null, "number": 6, "cdate": 1544275075698, "ddate": null, "tcdate": 1544275075698, "tmdate": 1544275075698, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "rylYb5iqCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "content": {"title": "Thank you for the suggestion", "comment": "Finding the constraint on which our model is robust is crucial. Thank you for the suggestion.\n\nFor now, from our experiments we find that our model is robust against  L0-based attacks. Our method works well for attacks that are not bounded within an epsilon ball, but are bounded in terms of the number of pixels perturbed.\n\nWe will come up with a more general measure of robustness to backup the performance of our model."}, "signatures": ["ICLR.cc/2019/Conference/Paper827/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624730, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper827/Authors|ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624730}}}, {"id": "rylYb5iqCQ", "original": null, "number": 5, "cdate": 1543318017112, "ddate": null, "tcdate": 1543318017112, "tmdate": 1543318017112, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "HJl5PX0-T7", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "content": {"title": "Rating unchanged ", "comment": "I thank the authors for their detailed response. Unfortunately my assessment remains unchanged:\n\nRegarding robustness versus successful defense. Any attack should obey a set of constraints (which define the sense in which the attack is adversarial). This could be a bounded norm, or the set of rotations of the image, or a number of pixels that can be changed, etc. A meaningful defense should be robust, in that all points which obey the constraints are correctly classified. If the defense is not robust, then its success represents the limitations of the attacks used. (one counter-case would be if you could prove that the remaining adversarial points inside the constraints are computationally hard to find).\n\nI would change my assessment, if the authors provided convincing evidence that the defense is robust. I acknowledge that the authors considered an attacker which is aware of the defense, but it is not clear to me that this attacker successfully exploited this information."}, "signatures": ["ICLR.cc/2019/Conference/Paper827/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper827/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624730, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper827/Authors|ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624730}}}, {"id": "SyePSFiSAQ", "original": null, "number": 4, "cdate": 1542990143267, "ddate": null, "tcdate": 1542990143267, "tmdate": 1542990143267, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BkgTgd4Tam", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "content": {"title": "Our response to reviewer4", "comment": "Thank you for reviewing our paper with valuable comments. We will revise the terminology to be more precise in the paper as you suggested. We will present the definition of key ideas as a formula, and the specifications of experiments will be documented in our code for the ease of reproduction.\n\nC1) The meaning of \u201cthey were reproduced by [the authors themselves]\u201d\n-> We intended to show clearly that all the experiments are reproduced by ourselves. This is just to stress that our results are reproducible and to present that we will release our code so that everyone can easily reproduce our results.\n\nC2) How shifting the pixel makes it harder to attack in the adaptive case.\n-> In Appendix H, the ACE module \u201crandomly\u201d shifts the pixel so that the adversary knows the distribution of shift, but does not know the exact direction for a single image. For attack algorithms bounded by l_0, l_1, or l_2-norm (i.e., except for the l_inf-norm-based attacks), this random shift averages out the perturbation within the neighborhood of each pixel. Then, the intensity of perturbation towards the decision boundary is reduced, therefore we have the increased probability of classifying the attacked image correctly, which is identical to increasing the defense rate.\n\nHowever in the l_inf-norm case, random shift does not necessarily reduce the intensity of perturbation imposed on each pixel. In this case, shifting the pixel may not have any influence on defense, thus the attack should be mitigated by the use of adversarial training. Therefore, we performed the adversarial training by combining our pixel shifting method as shown in Appendix H..\n\nC3) Why would ACE enhance the checkerboard pattern?\n-> This is because the ACE module makes our model learn through the checkerboard pattern with respect to our intensity parameter lambda. Let us assume that the ACE module has the autoencoder structure stated in the second paragraph in Section 5. If \u03bb=0, the gradient clearly has no checkerboard pattern as shown in Figure 3(c). If \u03bb=1, the gradient must be distributed in a checkerboard pattern as Figure 3(b). This is because these pixels are the only pixels that are connected to the output. For \u03bb \u2208 (0,1), the gradient can be interpreted as the interpolation between the case of \u03bb=0 and \u03bb=1. As \u03bb approaches 1 from 0, the checkerboard pattern becomes more clear. Since the original network without the ACE module is the same as when \u03bb=0 with the ACE module, using the ACE module with \u03bb > 0 will enhance the checkerboard pattern in gradient. Please refer to Section 4.1 for details about the structure of ACE.\n\nC4) Why wouldn't an adversary remove the padded pixels before generating the attack?\n-> Yes it would. \u201cThe adversary in the adaptive case\u201d in Appendix H does try to remove the padded pixels. We have set the direction of shift to random in order to avoid this. "}, "signatures": ["ICLR.cc/2019/Conference/Paper827/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624730, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper827/Authors|ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624730}}}, {"id": "BkgTgd4Tam", "original": null, "number": 3, "cdate": 1542436853414, "ddate": null, "tcdate": 1542436853414, "tmdate": 1542436853414, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "content": {"title": "Needs further clarifications", "review": "I have to emphasize first that this is not my area of expertise so I am going to review it as an outsider. \n\nThe authors argue that the checkerboard phenomenon can be exploited to make neural networks robust against adversarial attacks. They propose to enhance the checkerboard pattern by first adding a layer, called Artificial Checkerboard Enhancer (ACE), and then evading the attacks by zero-padding the image. The authors\u2019 argument is that enhancing the checkerboard phenomenon will make attacks more targeted towards certain pixels, which can be evaded by shifting the image. \n\nOverall, I think the paper is difficult to read and is not suitable for publication. In terms of clarity, the authors do not use precise terminology that would allow the reader to reproduce their work. They allude to vague statements. For example, they introduce two KEY terminologies that are repeatedly used throughout the paper but are not properly defined (see for instance the \u201cdefinition\u201d of \u201cGradient Overlap\u201d in Appendix C). \n\nIn addition, in terms of the experiements, it certainly does not help to say that they were \u201creproduced by [the authors themselves]\u201d. What does this mean?\n \nIn terms of originality, I agree with the first reviewer that the defense strategy seems to be easily breakable. The authors propose that they enhance the checkerboard phenomenon so that adversarial attacks become easier to implement by targeting individual pixels (the pixels in the checkerboard artifacts). Then, they pad the image with zero pixels to shift it to the right. I don\u2019t understand how shifting the pixels would make it harder to attack (especially when the adversary knows the system). \n\nIt would be really appreciated if the authors elaborate on the following points to help me understand their contribution: \n- The entire discussion about ACE in Section 4.1 is ad-hoc and not well-motivated. Why would ACE enhance the checkerboard patterm? Can you please explain why it works? This is not mentioned anywhere in the paper. The experiment in Section 4.2 helps a bit but it does not answer this question. \n\n- What wouldn't an adversary remove the padded pixels before generating the attack? In defense strategies, it is often assumed that the adversary knows the system. Can you please explain why that is not possible in this setting? \n\n- \n\nIn Figure 4, the axes are \\bar i and \\bar j in the main body, but they are x and y in the figure. Please use the same notation. ", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper827/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "cdate": 1542234367828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335811469, "tmdate": 1552335811469, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJl5PX0-T7", "original": null, "number": 2, "cdate": 1541690210452, "ddate": null, "tcdate": 1541690210452, "tmdate": 1541739894463, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "rkeyuETr3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "content": {"title": "Our response to reviewer3", "comment": "We thank the detailed review and valuable comments. First of all, we believe that our defense will not be easy to break even if the adversary knows in advance about our method. We hope that this response can resolve all of your concerns. Plus, we will revise the overall paper organization for better clarity.\n\nBefore responding to the comments (from C1 to C6 below), we first point out the relationship between building a \u201crobust model\u201d and creating a \u201csuccessful defense method\u201d. The Euclidean distance between the fooled images and the originals (in C2 below), and the attack-ability inside an epsilon ball (in C6 below) are good measures for model robustness in metric spaces with the l_2-norm and its equivalent. The attack methods that are formulated within the l_2-equivalent norms, such as Carlini&Wagner and PGD, can be defended if a model is robust concerning the measures above. However, for the attacks including OnePixel and JSMA, which are not formulated within the l_2-equivalent, the model can be fooled even when the model is robust concerning those measures. This is due to the fact that the constraint these attacks utilize cannot be bounded by the l_2-norm. Therefore, a \u201crobust model\u201d is robust to l_2-equivalent attacks and can be considered as a subset of \u201csuccessful defense methods\u201d. This can be demonstrated by evaluating the classification accuracy of the models adversarially-trained with PGD, when attacking with an l_0-norm-based attack.\n\n\nC1) The attacks considered did not uncover the defense strategy.\n-> At the beginning of Section 5, we propose three types of threat models for attack algorithms. Among those attack scenarios, we considered the \u201cwhite-box scenario\u201d where the attacks uncover our defense strategy. To show that our method is not vulnerable in the scenario, we conducted the experiment against the PGD attack method, which is the strongest attack, combined with adversarial training. As shown in Appendix H, our method can successfully defend the attacks, thus we can guarantee our method can successfully defend in the white-box scenario as well.\n\nC2) Evidence to suggest that the method can reduce the probability that a misclassified example lies close to the training or test examples was not presented.\n-> Using the suggested measure concerning the probability change could be suitable to identify the l_2-equivalent-robustness of a model. However, as mentioned above, since the probability cannot reflect the robustness against attacks that are not formulated within the l_2-equivalent such as OnePixel and JSMA attacks, our method did not aim to reduce this probability.\n\nC3) The specific kinds of attacks the method was intended to defend against were unclear.\n-> Our goal is to propose a method that can be utilized in both black-box and white-box attack scenarios as explicitly mentioned in Section 5. Specifically, we targeted three scenarios: 1) the vanilla attack scenario, the adversary can access the target model but not our proposed defense method, 2) the transfer attack scenario, the adversary generates adversarial perturbations from a source model, which is different from the target model, and finally 3) the adaptive attack scenario (white-box attack scenario), the adversary knows every aspect of the model and the defense method so that it can directly exploit our defense.\n \nC4)  In section 2.2, key definitions are relegated to Appendix C\n-> We will revise our paper for better readability as you suggested.\n\nC5) Could the authors provide a baseline with a random 30% of pixels?\n-> Yes, we have been conducting an experiment based on the suggested setting. We will post the result as soon as possible. \n\nC6) Recommendation about including a measure of the attack-ability under random noise in the epsilon ball. \n-> Thank you for the suggestion. It seems that we need to verify the robustness of our method against the attacks that do not utilize the gradients. Our method would be robust against such attacks because of the following three reasons. First, for a random noise in the epsilon ball, the probability that the noise can directly affect the labels is very low (according to Section 4 in [1]). Furthermore, the probability that it matches the \u201crandom\u201d direction of shift (i.e., in the adaptive scenario) as mentioned in Appendix H is low as well. Finally, the random noise itself can be reduced through the autoencoder structure of the ACE module (according to Section 5 in [2]). We will conduct experiments by following your suggestion to verify these claims and strengthen our proposed defense method. \n\n\n[1] Xiaoyu Chao and Neil Zhenqiang Gong. \u201cMitigating Evasion Attacks to Deep Neural Networks via Region-based Classification\u201d. ACSAC 2017.\n\n[2] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. \u201cExtracting and Composing Robust Features with Denoising Autoencoders\u201d. ICML 2008.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper827/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624730, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper827/Authors|ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624730}}}, {"id": "rkeyuETr3Q", "original": null, "number": 2, "cdate": 1540899942827, "ddate": null, "tcdate": 1540899942827, "tmdate": 1541533657897, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "content": {"title": "I worry that this defense will be easy to break", "review": "The authors propose that the \"checkerboard phenomenon\", whereby the gradients exhibit a repeating pattern over the pixel space, is a source of vulnerability to adversarial examples. They propose to first enhance this vulnerability for pre-trained models with a pre-conditioning layer, and then to evade it by zero padding the image to offset the pattern.\n\nClarity: I found the work difficult to follow in places, and I felt that some material crucial to the paper was relegated to appendices.\n\nOriginality: To my knowledge, the idea is original.\n\nQuality and significance:\nI feel the significance of this work is likely to be low. While the authors report positive \"defense\" results, I strongly suspect this is simply because the attacks considered did not uncover the defense strategy. I expect that this defense would be broken relatively quickly if the paper is accepted. The authors did not present evidence to suggest that their method reduces the probability that a misclassified example lies close to the training or test examples. As such, the defense seems to rely on the attacker being \"tricked\".\n\nSpecific comments:\n\n1) Throughout the paper, I was unclear what specific kinds of attacks the method was intended to defend against.\n2) In section 2.2, key definitions are relegated to appendix C.\n3) Section 3.2-> p = 0.3 is still 30% of the pixels. Could the authors provide a baseline with a random 30% of pixels?\n4) Adaptive attack scenario: I would recommend that the authors also included a measure of the attack-ability under random noise in the epsilon ball. This would demonstrate whether the defense actually removes adversarial examples or just \"attacks the attackers\".", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper827/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "cdate": 1542234367828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335811469, "tmdate": 1552335811469, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gqDA65jX", "original": null, "number": 1, "cdate": 1540181601911, "ddate": null, "tcdate": 1540181601911, "tmdate": 1541533657474, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "content": {"title": "I am not right person to review", "review": "I am a researcher in NLP and know little about vision, so I cannot review this paper. I have contacted general chair about this situation. ", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper827/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Review", "cdate": 1542234367828, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335811469, "tmdate": 1552335811469, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1ec3QQz3Q", "original": null, "number": 1, "cdate": 1540662194399, "ddate": null, "tcdate": 1540662194399, "tmdate": 1541174735916, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "S1gYmGQAsX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "content": {"title": "Explanation on Figure 8", "comment": "Thank you for your interest in our work. How Figure 8 was generated has been explained in Section 4.2, but we would like to elaborate more on this. We will explain about the axes in the figure first, and then give some detailed explanation on why the classified label map has such shape.\n\nFor a classified label map (Figure 4 and 8), we have an input image x without any perturbation at (0, 0), where X-axis is the gradient direction vector of checkerboard artifacts (C), and Y-axis is the gradient direction vector of non-checkerboard artifacts (X\\C). Formally, this is expressed as \\hat{e}_C and \\hat{e}_{X\\C} in Section 4.2. Note that C is the checkerboard pixels with high absolute gradients designed by our ACE module with 1 x1 conv and stride 2 (Figure 3.(b) shows the pixels in C that absolute gradients turn out to be in general greater than those in X\\C). Then, each point in the classified label map is generated by perturbing the original image to the direction of (x, y) coordinates from -100 to 100 respectively. We have the classified label map after pixel perturbations using an example image of soup bowl as shown in Figure 4.\n\nIn Figure 4, we have empirically demonstrated the effect of this imbalance on gradients by creating a classified label map. This is an empirical backup of showing that our ACE module induces the vulnerable domain to the checkerboard. As \\lambda increases, our model becomes more vulnerable on the perturbation on C, while the opposite behavior is observed on the perturbation on X\\C. Therefore, the labels easily change on y-axis of classified label map with large lambda and the opposite on x-axis. You can think of this as an extension of Section 3 where we have shown that one-pixel attack success rates have checkerboard shape (Figure 2) due to the difference in the number of associated parameters on each pixel. We designed C to be the vulnerable domain which induces attacks on our known C. Thus, we successfully defended attacks with one-pixel padding.\n\nWe hope this explanation is clear enough."}, "signatures": ["ICLR.cc/2019/Conference/Paper827/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624730, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlc6iA5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper827/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper827/Authors|ICLR.cc/2019/Conference/Paper827/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624730}}}, {"id": "S1gYmGQAsX", "original": null, "number": 1, "cdate": 1540399648842, "ddate": null, "tcdate": 1540399648842, "tmdate": 1540399648842, "tddate": null, "forum": "BJlc6iA5YX", "replyto": "BJlc6iA5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper827/Public_Comment", "content": {"comment": "Can you explain Figure 8? How are the X and Y axis selected?\n\nIt is confusing that traveling +/- 100 along the X axis does not change the class label, but traveling +/- along the Y axis quickly does.", "title": "Figure 8 question"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper827/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ACE: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks", "abstract": "The checkerboard phenomenon is one of the well-known visual artifacts in the computer vision field. The origins and solutions of checkerboard artifacts in the pixel space have been studied for a long time, but their effects on the gradient space have rarely been investigated. In this paper, we revisit the checkerboard artifacts in the gradient space which turn out to be the weak point of a network architecture. We explore image-agnostic property of gradient checkerboard artifacts and propose a simple yet effective defense method by utilizing the artifacts. We introduce our defense module, dubbed Artificial Checkerboard Enhancer (ACE), which induces adversarial attacks on designated pixels. This enables the model to deflect attacks by shifting only a single pixel in the image with a remarkable defense rate. We provide extensive experiments to support the effectiveness of our work for various attack scenarios using state-of-the-art attack methods. Furthermore, we show that ACE is even applicable to large-scale datasets including ImageNet dataset and can be easily transferred to various pretrained networks.", "keywords": ["Adversarial Examples", "Neural Network Security", "Deep Neural Network", "Checkerboard Artifact"], "authorids": ["jeshwang92@uchicago.edu", "snu13dlx@snu.ac.kr", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "genesis.kim@navercorp.com", "dongyoon.han@navercorp.com", "jungwoo.ha@navercorp.com"], "authors": ["Jisung Hwang", "Younghoon Kim", "Sanghyuk Chun", "Jaejun Yoo", "Ji-Hoon Kim", "Dongyoon Han", "Jung-Woo Ha"], "TL;DR": "We propose a novel aritificial checkerboard enhancer (ACE) module which guides attacks to a pre-specified pixel space and successfully defends it with a simple padding operation.", "pdf": "/pdf/0a044815727fef5d73d3f921c6b233499620845e.pdf", "paperhash": "hwang|ace_artificial_checkerboard_enhancer_to_induce_and_evade_adversarial_attacks", "_bibtex": "@misc{\nhwang2019ace,\ntitle={{ACE}: Artificial Checkerboard Enhancer to Induce and Evade Adversarial Attacks},\nauthor={Jisung Hwang and Younghoon Kim and Sanghyuk Chun and Jaejun Yoo and Ji-Hoon Kim and Dongyoon Han and Jung-Woo Ha},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlc6iA5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper827/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311743388, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJlc6iA5YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper827/Authors", "ICLR.cc/2019/Conference/Paper827/Reviewers", "ICLR.cc/2019/Conference/Paper827/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311743388}}}], "count": 11}