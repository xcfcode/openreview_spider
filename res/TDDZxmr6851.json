{"notes": [{"id": "TDDZxmr6851", "original": "ucmu9y4rSAS", "number": 2875, "cdate": 1601308319092, "ddate": null, "tcdate": 1601308319092, "tmdate": 1614985643745, "tddate": null, "forum": "TDDZxmr6851", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Rsg83YT1SPc", "original": null, "number": 1, "cdate": 1610040518232, "ddate": null, "tcdate": 1610040518232, "tmdate": 1610474126624, "tddate": null, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "I agree with the reviewers that said that this paper has valuable insights. However, all reviewers ultimately recommended rejection. I think the main reason was that the reviewers did not feel these insights don't accumulate together to a message that would justify a paper. I hope the authors can address these concerns and resubmit. There were additional concerns, like the fact a very simplistic toy model is being used, but I agree with the authors that it makes sense to first explore such phenomena in the simplest model that produces them."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040518218, "tmdate": 1610474126607, "id": "ICLR.cc/2021/Conference/Paper2875/-/Decision"}}}, {"id": "qKGtK8i7MHg", "original": null, "number": 3, "cdate": 1604182718113, "ddate": null, "tcdate": 1604182718113, "tmdate": 1607376203972, "tddate": null, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review", "content": {"title": "Valuable insights on a highly relevant problem, contributions are somewhat limited", "review": "The paper studies the effect of the learning rate's magnitude when training neural networks. I believe this to be an extremely relevant problem since large learning rates are widely adopted in practice due to the their positive impact on the model's generalization, even though we don't understand the reason behind this.\n\nThe main contributions come from the analysis of a 2-layer linear network trained on a single data point, which shows the existence of three regimes that are governed by the learning rate's magnitude and the local curvature at initialization. Unlike prior work that also identifies different regimes governed by the learning rate, this submission does not point at noise as the main responsible for the regimes.\n\nPresented experiments are interesting and clearly show the different regimes and how they are connected to the learning rate, but do not seem to provide significant new insights compared to previous work. I believe the plots showing how the curvature behaves as the learning rate varies are novel to the best of my knowledge, but are not surprising given the previous discussions on how the learning rate affects the solution's sharpness.\n\nWhile the theoretical result does provide a precise characterization of the phenomena, it is restricted to a very constrained setting (2-layer linear net trained on one data point), making it hard to evaluate how the result translates to more complex settings (although the authors do a reasonable job at exploring this via experimental analysis).\n\nI also find the discussion on the presented theory to be somewhat lacking. For example, while the paper seems to downplay the importance of noise in characterizing the three regimes (compared to prior work), the curvature inevitably changes if noise is added to the sample gradients, e.g. adding isotropic noise will decrease \\eta_crit and shift the phase transitions, which is also expected to occur if as we decrease the batch size when doing mini-batch SGD. The fact that different batch sizes have been adopted across experiments (even for the same dataset/model) makes the experimental results harder to evaluate when compared to the developed theory. Except for a footnote on page 2, there is no discussion on how the initialization scale affects the phase transitions, which would be useful to better understand how the theory relates to prior work that focuses on the init scale instead of noise or width.\n\nOverall I believe the problem to be very relevant and the approach to be promising, but the theoretical results are overly limited and the contributions are below what I'd expected to suggest acceptance. Since extending the analysis to non-linear networks would seem to be a considerable technical leap, the authors could instead extend it to a less limited training setting where gradient variance is non-zero, in which case \\eta_crit would have a dependence on the batch size assuming mini-batch SGD updates.\n\n----\n\nI have read the response and am keeping my score. I agree that the simplicity of the results/model is valuable, but additional theoretical results (even extensions to Theorem 1, with more involved but stronger claims) would greatly improve the paper and make its contributions closer to what is expected of a ICLR submission. Extending the result to consider noise should be straightforward and yield a reasonably simple claim, which can be further verified empirically by adding synthetic noise to the gradients, adding label noise, and/or adopting extremely small batch sizes. As it stands now, the submission is still lacking in terms of contributions.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086868, "tmdate": 1606915805244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2875/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review"}}}, {"id": "MKQFm2jaJC", "original": null, "number": 5, "cdate": 1605637861540, "ddate": null, "tcdate": 1605637861540, "tmdate": 1605637861540, "tddate": null, "forum": "TDDZxmr6851", "replyto": "H5lsQgDLMX", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for their detailed comments.\n\n> Cons: This paper lacks theoretical evidence for supporting the raised claims. The model studied in Theorem 1 is very simple. Moreover, the effect of choosing the step-size and batch-size on the sharpness of the computed solution has been observed in the literature. For instance, Keskar et al. (2017) justified the improved generalization achieved when using small batch sizes by the ability of such methods to converge to flat minima. The choice of the batch-size was formally related to the choice of the step-size in the work of Patel (2017) which provides a learning-rate lower bound threshold for the divergence of batch SGD. This threshold is also a function of the curvature. The higher the curvature around a critical point, the lower the threshold required for divergence. Hence choosing a large step-size tend to escape sharp minima and potentially converge to flatter minimizers. The last paper was not cited in the submitted manuscript.\n\nWhile the relation between step size and curvature has been studied before, to our knowledge our paper is the first to point out a sharp transition between the small and large learning regimes, which happens independently of SGD noise, as well as to explain the mechanism underlying the curvatures reached in these two regimes. We thank the reviewer for bringing Patel (2017) to our attention; we would like to note that the nonconvexity in our problem is particular to the structure of (wide) neural networks, and does not correspond to a landscape with multiple, separated quadratic minima, like the case studied in that work. Instead, what we find is that the landscape has a set of connected global minima of varying curvatures. Therefore, our results do not follow from the papers mentioned in the comment.\n\n> The paper does not provide any intuition on how to compute the thresholds $\\mu_{\\rm crit}$ and $\\mu_{\\rm max}$, and how is this related to the choice of non-linearity, structure of the network, \u2026\n\nWe believe the reviewer is referring to the thresholds $\\eta_{\\rm crit}$ and $\\eta_{\\rm max}$. $\\eta_{\\rm crit}$ is a simple function of the curvature at initialization (defined precisely in the paper). Understanding how the curvature relates to choices in network architecture and training data is a hard question and the subject of ongoing research in the community; however, operationally we can measure this curvature in the standard way by computing the top eigenvalue of a relatively small matrix. As for $\\eta_{\\rm max}$, based on our empirical results we suggest a heuristic for ReLU networks (given by $12/\\lambda$), again in terms of the curvature $\\lambda$ at initialization.\n\n> For the gradient descent update rule in equation (1): why do we have $f_t$ \n\nEquation (1) lists the gradient descent update equations. The factor $f_t$ appears due to the loss function being $L=f_t^2/2$, and the fact that we have a factor of $dL/df_t = f_t$ when computing the gradient using the chain rule.\n\n> What is meant by \u201ccompute budget\u201d?\n\nBy this we mean the number of epochs used to train the model.\n\n> In the definition of Theta in Page 4: I think it should by (sic) $\\sum_\\alpha$ instead of $\\sum_\\mu$\n\nThe NTK is defined by summing over the parameter index $\\mu$, not the sample index $\\alpha$. \n\n> Expression (S12) is missing a term.\n\nCan you please clarify which term is missing?\n\n\n> (S16), what is $u_{ia}$.\n\nIt is the first layer weight matrix in the full model setting of Appendix B.3, where $u \\in R^{n \\times d}$ and the inputs $x \\in R^{d}$. \n\n>Figure S10, the colors in the label do not match the plots.\n\nIn this figure, different colors refer to different configurations, while dashed vs. solid patterns refer to what is being measured. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TDDZxmr6851", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2875/Authors|ICLR.cc/2021/Conference/Paper2875/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment"}}}, {"id": "g2KxE-M24hH", "original": null, "number": 4, "cdate": 1605637502118, "ddate": null, "tcdate": 1605637502118, "tmdate": 1605637502118, "tddate": null, "forum": "TDDZxmr6851", "replyto": "L7gebsreHY", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for their detailed review.\n\n> The paper defines the curvature as the max eigenvalue of the Fisher information matrix. The large learning rate values are determined according to that value. Fig 2 c and f show the cross over of the regimes. What would make the cross over sharper? Increasing width? Larger batch size?\n\nAccording to our theoretical analysis, the cross-over can be made sharper by increasing the width.\n\n> The large increase in loss value is huge! It is surprising that training doesn\u2019t just diverge from there. What controls the growth and the subsequent decay? How consistent is it? Would the result hold in all random seeds or what fraction would still converge?\n\nIndeed, the increase in loss value in the catapult phase is huge. In our experiments this result is consistent across runs, as long as the learning rate is not chosen to be too large (i.e. is not too close to the maximum learning rate). Our theoretical analysis indicates that the loss reaches a value proportional to the width, and also clearly explains why training does not diverge: while the loss grows, the curvature shrinks, and once the curvature is small enough the loss starts to decrease. In the theoretical model this result holds deterministically. Empirically we have also found this to consistently hold across random seeds, unless one is close to the maximum learning rate.\n\n> What\u2019s the max tr loss for the wide res net? (Why not consistent plots for fully connected and resnets in fig2?) (Also, for clarity, Fig 2 label of B would be Max tr loss)\n\nIn Figure 2 we wanted to show several different empirical effects related to loss and curvature early on in training, which is why we plot slightly different data for the two networks. However, in Figure 2.d the catapult effect is clearly visible in the Wide ResNet, and Figure 2.f shows that the curvature undergoes the phase transition at the predicted critical learning rate.\n\nThank you for pointing out the typo in the label of Figure 2.b; we will correct it.\n\n> Fig 3 would be clearer if the test error axis of (a,b) would be aligned, same for (c,d). It appears as if decay actually makes the search for the correct large lr rate redundant. (Perhaps at the expense of searching for the right decay rate). In general, what can we do to maximize the range of hyperparameters that works for the model (to minimize search)? The paper proposes a heuristic based on the initial local curvature yet the findings for practice seem somewhat preliminary.\n\nThe errors-axis of figure 3 is chosen so that the performance for the different learning rates is displayed in the plot. As the reviewer mentions, performance of small learning rates can be competitive with large learning rates if they are evolved for a time proportional to 1/eta (what we call physical time), which tends to be a very long time for practical learning rates. However, in the Wide ResNet model, the best performance happens for learning rates in the catapult phase even when training for this amount of time.\n\n> The comparison between lazy training and how large learning rate deviates from lazy training is a very interesting and active area of research. I think the paper would be more complete if the experiments would verify it in a more robust way. Measuring the deviation of the activations or the movement of the kernel would be helpful. Here is one of the references with clear observables to measure in the experimental framework https://arxiv.org/abs/1906.08034\n\nWe thank the reviewer for this suggestion and the reference. We would like to point out that in the paper we focus on presenting results for the maximum eigenvalue of the Neural Tangent Kernel (equivalently of the Fisher Information Matrix), for consistency with the theoretical model. The fact that this eigenvalue changes considerably in the large learning rate phase implies that the NTK changes considerably, and therefore shows that lazy training does not occur in this phase. However, we have also empirically measured the kernel movement, and it displays the same sharp behavior as a function of learning rate when networks are made wider.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TDDZxmr6851", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2875/Authors|ICLR.cc/2021/Conference/Paper2875/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment"}}}, {"id": "KND_9Jzf0ta", "original": null, "number": 3, "cdate": 1605637471912, "ddate": null, "tcdate": 1605637471912, "tmdate": 1605637471912, "tddate": null, "forum": "TDDZxmr6851", "replyto": "qKGtK8i7MHg", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for their thoughtful and detailed comments.\n\nAs the reviewer points out, our theoretical results are shown in the simple setting of a 2-layer linear network. However, we show empirically that the detailed behavior of this model matches that of deep networks with common fully-connected and convolutional architectures. \n\nIn light of this, we argue that the simplicity of the model is an advantage rather than a drawback. Our model is able to explain quantitative phenomena -- the sharp transition in loss and curvature dynamics that occurs at the critical learning rate. If we share the goal of developing a better understanding of deep learning phenomena, then the best model is the simplest model that can explain the phenomena. Indeed, thanks to the simplicity of the model, we were able to develop a detailed understanding of the underlying mechanism that gives rise to these effects. That being said, we do extend the model to the case of multiple data points in supplemental section B.3.\n\nWhile the sharp dependence of curvature on learning rate is one contribution of our work, there are others we discuss. (1) We show the sharp drop in curvature, which occurs in the catapult regime, always occurs simultaneously with a large spike in the loss. (2) We discuss how the (i) maximum loss and (ii) step at which the catapult effect occurs grow with width (respectively, linearly and logarithmically). (3) We give a theoretical prediction for the maximum learning rate beyond which gradient descent diverges and show empirically that this holds for networks with non-ReLU nonlinearities. (4) Network width is an important ingredient which controls the sharpness of the phenomenon we study. Our work gives evidence for a regime of nonlinear (non-NTK) dynamics that exists in networks that are arbitrarily wide.\n\nRegarding the effect of SGD noise, we agree that the noise shifts the positions of the phase transitions. However, in practice we find that the shift is mild, as can be seen in the empirical results: even though we train most networks using SGD rather than full-batch gradient descent, we observe the phase transitions close to the critical learning rate predicted by the full-batch analysis. We therefore chose not to explore the shift due to noise in great detail in this paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TDDZxmr6851", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2875/Authors|ICLR.cc/2021/Conference/Paper2875/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843571, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Comment"}}}, {"id": "H5lsQgDLMX", "original": null, "number": 1, "cdate": 1603953686196, "ddate": null, "tcdate": 1603953686196, "tmdate": 1605024112716, "tddate": null, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review", "content": {"title": "The Large Learning Rate Phase of Deep Learning", "review": "This paper analyzes the effect of choosing a large step-size on the generalization of deep networks. They suggest that starting with a large learning rate, the loss initially increases before converging to a flatter minimum with improved generalization (catapult effect). When the learning rate is above a certain threshold, the authors observed this catapult effect along with a decrease in the curvature of the landscape. These observations were empirically demonstrated through various experiments and analytically studied for a two-layer linear network.\n\nPros:\nThe paper tackles an interesting problem in a vibrant field of research.\n\nSeveral experiments that demonstrate the authors claims were presented in the paper. These results can be further used to propose algorithms designed to converge to flatter solutions. \n\nThe related material is referenced and well-discussed in the paper.\n\nCons:\nThis paper lacks theoretical evidence for supporting the raised claims. The model studied in Theorem 1 is very simple. Moreover, the effect of choosing the step-size and batch-size on the sharpness of the computed solution has been observed in the literature. For instance, Keskar et al. (2017) justified the improved generalization achieved when using small batch sizes by the ability of such methods to converge to flat minima. The choice of the batch-size was formally related to the choice of the step-size in the work of Patel (2017) which provides a learning-rate lower bound threshold for the divergence of batch SGD. This threshold is also a function of the curvature. The higher the curvature around a critical point, the lower the threshold required for divergence. Hence choosing a large step-size tend to escape sharp minima and potentially converge to flatter minimizers. The last paper was not cited in the submitted manuscript.\n\nThe paper does not provide any intuition on how to compute the thresholds $\\mu_{\\mbox{crit}}$ and $\\mu_{\\max}$, and how is this related to the choice of non-linearity, structure of the network, \u2026\n\nComments:\n1.\tFor the gradient descent update rule in equation (1): why do we have $f_t$? \n\nMinor Comments:\n1.\tWhat is meant by ``compute budget\u2019\u2019?\n2.\tIn the definition of $\\Theta$ in Page 4: I think it should by $\\sum_{\\alpha = 1}^m$ instead of $\\sum_{\\mu=1}^p$.\n3.\tIn Figure S1, the images are small and not very clear.\n4.\tExpression (S12) is missing a term.\n5.\tS(16), what is $u_{ia}$.\n6.\tFigure S10, the colors in the label do not match the plots.  \n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086868, "tmdate": 1606915805244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2875/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review"}}}, {"id": "L7gebsreHY", "original": null, "number": 2, "cdate": 1603991610745, "ddate": null, "tcdate": 1603991610745, "tmdate": 1605024112650, "tddate": null, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "invitation": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review", "content": {"title": "Limits of large learning rate training at fixed learning rate SGD", "review": "The paper is a detailed account of how large of a learning rate a given mode can take when it is trained by constant step size SGD. Many papers investigated the effect of learning rate (and batch size and width etc\u2026) to the final accuracy. In general, the findings indicate increased accuracy up to a certain threshold (where the algorithm doesn\u2019t converge any more). Such findings are abundant in the literature. However, the present paper investigates the path that SGD takes to get to find those \u2018good\u2019 performing points in the weight space. Two phases emerge: monotonic decay in loss vs catapult regime and the latter appears to perform the best. \nThe paper has interesting findings but also some shortcomings, in more detail:\n- Similar curve as in Figure 14 of https://arxiv.org/abs/1905.03776 as Fig 1 of the present paper, in that sense the findings of large lr better are certainly not novel, but the authors are well aware of this\n- The paper defines the curvature as the max eigenvalue of the Fisher information matrix. The large learning rate values are determined according to that value. Fig 2 c and f show the cross over of the regimes. What would make the cross over sharper? Increasing width? Larger batch size? \n- The large increase in loss value is huge! It is surprising that training doesn\u2019t just diverge from there. What controls the growth and the subsequent decay? How consistent is it? Would the result hold in all random seeds or what fraction would still converge? \n- What\u2019s the max tr loss for the wide res net? (Why not consistent plots for fully connected and resnets in fig2?) (Also, for clarity, Fig 2 label of B would be Max tr loss)\n- Fig 3 would be clearer if the test error axis of (a,b) would be aligned, same for (c,d). It appears as if decay actually makes the search for the correct large lr rate redundant. (Perhaps at the expense of searching for the right decay rate). In general, what can we do to maximize the range of hyperparameters that works for the model (to minimize search)? The paper proposes a heuristic based on the initial local curvature yet the findings for practice seem somewhat preliminary. \n- The comparison between lazy training and how large learning rate deviates from lazy training is a very interesting and active area of research. I think the paper would be more complete if the experiments would verify it in a more robust way. Measuring the deviation of the activations or the movement of the kernel would be helpful. Here is one of the references with clear observables to measure in the experimental framework https://arxiv.org/abs/1906.08034 \n\nThe characteristic difference in the early time dynamics is claimed to be related to the performance gains. There is a growing body of evidence that points to the role of the initial dynamics of SGD when it comes to identifying the final performance. The paper is further evidence in that direction. Therefore I think it\u2019ll be useful for the ICLR community. Overall, the paper has very interesting bits and pieces but it fails to come together as a coherent whole to provide a consistent story.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2875/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2875/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The large learning rate phase of deep learning", "authorids": ["~Aitor_Lewkowycz2", "~Yasaman_Bahri1", "~Ethan_Dyer1", "~Jascha_Sohl-Dickstein2", "~Guy_Gur-Ari1"], "authors": ["Aitor Lewkowycz", "Yasaman Bahri", "Ethan Dyer", "Jascha Sohl-Dickstein", "Guy Gur-Ari"], "keywords": ["deep learning", "wide networks", "training dynamics"], "abstract": "The choice of initial learning rate can have a profound effect on the performance of deep networks. We present empirical evidence that networks exhibit sharply distinct behaviors at small and large learning rates. In the small learning rate phase, training can be understood using the existing theory of infinitely wide neural networks. At large learning rates, we find that networks exhibit qualitatively distinct phenomena that cannot be explained by existing theory: The loss grows during the early part of training, and optimization eventually converges to a flatter minimum. Furthermore, we find that the optimal performance is often found in the large learning rate phase. To better understand this behavior we analyze the dynamics of a two-layer linear network and prove that it exhibits these different phases. We find good agreement between our analysis and the training dynamics observed in realistic deep learning settings. \n\n", "one-sentence_summary": "The loss grows early on in training if the learning rate is large, and understanding this in full requires new theory.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lewkowycz|the_large_learning_rate_phase_of_deep_learning", "supplementary_material": "/attachment/cd955008b2009c3bb0bf38f6c21b465c77fd1a55.zip", "pdf": "/pdf/5998402f64ea1647129d9e4f0a8b3c905572e877.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GV8UiIPmEg", "_bibtex": "@misc{\nlewkowycz2021the,\ntitle={The large learning rate phase of deep learning},\nauthor={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},\nyear={2021},\nurl={https://openreview.net/forum?id=TDDZxmr6851}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TDDZxmr6851", "replyto": "TDDZxmr6851", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2875/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538086868, "tmdate": 1606915805244, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2875/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2875/-/Official_Review"}}}], "count": 8}