{"notes": [{"id": "rkeUcRNtwS", "original": "BkeijiudPr", "number": 1284, "cdate": 1569439374260, "ddate": null, "tcdate": 1569439374260, "tmdate": 1577168229821, "tddate": null, "forum": "rkeUcRNtwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Salient Explanation for Fine-grained Classification", "authors": ["Kanghan Oh", "Sungchan Kim", "Il-Seok Oh"], "authorids": ["blastps@gmail.com", "s.k@jbnu.ac.kr", "iosh@jbnu.ac.kr"], "keywords": ["Visual explanation", "XAI", "Constitutional Neural Network"], "abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ", "pdf": "/pdf/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "paperhash": "oh|salient_explanation_for_finegrained_classification", "original_pdf": "/attachment/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "_bibtex": "@misc{\noh2020salient,\ntitle={Salient Explanation for Fine-grained Classification},\nauthor={Kanghan Oh and Sungchan Kim and Il-Seok Oh},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeUcRNtwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OuhbDBefb", "original": null, "number": 1, "cdate": 1576798719294, "ddate": null, "tcdate": 1576798719294, "tmdate": 1576800917229, "tddate": null, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "invitation": "ICLR.cc/2020/Conference/Paper1284/-/Decision", "content": {"decision": "Reject", "comment": "This paper is interested in finding salient areas in a deep learning image classification setting. The introduced method relies on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output.\n\nAs noted by all reviewers, the paper is too weak for publication in its current form:\n- Novelty is very low.\n- Experimental section not convincing enough, in particular some metrics are missing.\n- The writing should be improved.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Salient Explanation for Fine-grained Classification", "authors": ["Kanghan Oh", "Sungchan Kim", "Il-Seok Oh"], "authorids": ["blastps@gmail.com", "s.k@jbnu.ac.kr", "iosh@jbnu.ac.kr"], "keywords": ["Visual explanation", "XAI", "Constitutional Neural Network"], "abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ", "pdf": "/pdf/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "paperhash": "oh|salient_explanation_for_finegrained_classification", "original_pdf": "/attachment/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "_bibtex": "@misc{\noh2020salient,\ntitle={Salient Explanation for Fine-grained Classification},\nauthor={Kanghan Oh and Sungchan Kim and Il-Seok Oh},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeUcRNtwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727188, "tmdate": 1576800279409, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1284/-/Decision"}}}, {"id": "Ske5I-NPFH", "original": null, "number": 1, "cdate": 1571402066172, "ddate": null, "tcdate": 1571402066172, "tmdate": 1574345942387, "tddate": null, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "invitation": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Summary:\nThis paper proposes a simple procedure to display the salient areas that determine classification decisions from deep networks. In practice, these area are computed by shadowing the image using a Gaussian and measuring the network contribution at every location of the image. Results in terms of \"Pointing game\" metric (that measure similarity between computed saliency result and ground truth) are provided on the Imagenet validation dataset.\u00a0\n\n1 Positive aspects:\n- Experiments are conducted on many datasets and show visually convincing results.\u00a0\u00a0\n- state-of-the-art on ImageNet compared to the RICE approach.\u00a0\n\n2 Negative aspects:\u00a0\n- Very simple approach\n- Depends on many parameters (t1, t2, sigma_s, sigmal_l,\u00a0 sigma_spatial)\u00a0\n- Improvement of PT w.r.t. Rice is not very high: 0.912 compared to 0.907.The variants Light, shadow, fusion approaches are not described in the text.\u00a0\u00a0\n- The paper is badly written.\u00a0\n-- Many sentences\u00a0are uninformative (e.g. abstract :\u00a0 \"We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset\")\n-- There is copy-pasted text everywhere (e.g. fea - tures , formu - lated, etc.), showing there has not been proof-reading of the paper, which is pretty disconcerting to reviewers.\u00a0\n- There are multiple typos including misspelled names in the bib.\n- blurry figures.\u00a0\u00a0\n\nMy initial rating is mainly motivated by the minor experimental improvement wrt RICE that is not clearly explained. There would also be a lot of work for improving the writing of the paper.   \n\nMinor :\u00a0lambda is 10^-5, not 10^5 ?\u00a0\nis sigma_s used in the algo or any equation? Where is it defined?\u00a0", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Salient Explanation for Fine-grained Classification", "authors": ["Kanghan Oh", "Sungchan Kim", "Il-Seok Oh"], "authorids": ["blastps@gmail.com", "s.k@jbnu.ac.kr", "iosh@jbnu.ac.kr"], "keywords": ["Visual explanation", "XAI", "Constitutional Neural Network"], "abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ", "pdf": "/pdf/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "paperhash": "oh|salient_explanation_for_finegrained_classification", "original_pdf": "/attachment/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "_bibtex": "@misc{\noh2020salient,\ntitle={Salient Explanation for Fine-grained Classification},\nauthor={Kanghan Oh and Sungchan Kim and Il-Seok Oh},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeUcRNtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661532866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1284/Reviewers"], "noninvitees": [], "tcdate": 1570237739618, "tmdate": 1575661532882, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review"}}}, {"id": "Hkeo4WgCYr", "original": null, "number": 2, "cdate": 1571844402536, "ddate": null, "tcdate": 1571844402536, "tmdate": 1572972488866, "tddate": null, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "invitation": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a new method in the family of local perturbation-based interpretations for deep networks and more specifically for fine-grained classification tasks. Compared to other saliency map methods (gradient-based, propagation-based, etc), this family has the advantage of needing only black-box access to the model. The introduced method GLAS, scans over an image and lights/shadows each part of the image to assign an importance score to different regions of the image based on the change in model's prediction. The motivation of this work is to increase the inherently low speed of (some) methods in this family and to give better explanations by \n\n\n\nI vote for rejecting this paper as the contributions to what already exists in the literature are not clear and the provided experimental results are not convincing.\n\nCompared to previous perturbation-based methods, the main advantage seems to be speed. There are perturbation-based methods that do not suffer from low speed e.g. Dabkowski & Gal and give real-time perturbation-based saliency maps. The authors do not mention this work (and similar works) and do not compare both their speed and their performance against it.\n\nOne important problem (probably the most important) with the perturbation-based saliency maps is the fact that the perturbations might push a given image out of the true data manifold and therefore give an invalid interpretation of the model. The authors do not discuss the matter and how their method would address this issue. Intuitively, the introduced algorithm, more specifically the RGLAS algorithm, seems to suffer from this issue not any less than other existing methods.\n\nThe experimental results seek to demonstrate the superiority of the introduced method over rival methods. The justification behind the provided visual examples is their focus on more discriminative features (in human eyes). This does not necessarily mean that a given saliency map is more \"truceful\"; i.e. having a more visually appealing saliency map has nothing to do with a more truthful explanation of a model's decision making. The results focused on the mistakes of the model seem more convincing and interesting.\n\n The objective results first focus on the target localization metric which has traditionally been used in the literature. Although it is much faster to execute, the introduced method is only marginally superior to other methods. The most important problem, however, is that as mentioned above, there are fast methods in the literature and therefore a fair objective comparison is should include other methods as well. Secondly, the IOU measure is used. GLAS is not compared to other methods in this metric.\n\nThe authors mention the effectiveness of their work for \"fine-grained\" classification tasks while throughout the paper there is no convincing evidence or discussion that the method is curated for such tasks. As mentioned above, changing the scale parameter for getting more visuall appealing saliency maps is not enough evidence.\n\nAll in all, the true contribution of this work to other existing methods in this family is not enough for this venue.\n\n\nA few questions and suggestions:\n* How should one adjust the scale parameter? In other words, what is the hyper-parameter search scheme for this method which would make it robust against the human-biased choice of hyper-parameters which would result in visually more appealing saliency maps but not necessarily explain the model?\n* Explanation of RGLAS is not clear.\n* For a general reader, metrics such as IOU should be explained more clearly.\n* The paper has many many typing errors."}, "signatures": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Salient Explanation for Fine-grained Classification", "authors": ["Kanghan Oh", "Sungchan Kim", "Il-Seok Oh"], "authorids": ["blastps@gmail.com", "s.k@jbnu.ac.kr", "iosh@jbnu.ac.kr"], "keywords": ["Visual explanation", "XAI", "Constitutional Neural Network"], "abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ", "pdf": "/pdf/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "paperhash": "oh|salient_explanation_for_finegrained_classification", "original_pdf": "/attachment/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "_bibtex": "@misc{\noh2020salient,\ntitle={Salient Explanation for Fine-grained Classification},\nauthor={Kanghan Oh and Sungchan Kim and Il-Seok Oh},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeUcRNtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661532866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1284/Reviewers"], "noninvitees": [], "tcdate": 1570237739618, "tmdate": 1575661532882, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review"}}}, {"id": "ByeZgnsWcr", "original": null, "number": 3, "cdate": 1572088809335, "ddate": null, "tcdate": 1572088809335, "tmdate": 1572972488821, "tddate": null, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "invitation": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a simple and effective method for pinpointing salient features contributing to discriminating different classes in classification. It is based on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output. The authors also develop an iterative method to identify multiple instances. Experiments quantitatively evaluate the proposed method on pointing game using ILSVRC validation set, where the proposed method outperforms recent related methods. \n\nThe technical novelty of this paper is marginal and the experimental evaluation is not convincing. \n \n1) Marginal novelty compared to related work\nThere have been several masking-based black-box methods. Considering the method of RISE (Petsiuk et al., BMVC19) in particular, the novelty of the proposed method is marginal; the proposed method uses Gaussian light and shadow at individual positions while RISE uses random masks. \n\n2) Hyperparameter sensitivity \nThe proposed method includes several, at least four, hyperparameters to be tuned: T, sigma_l, sigma_ss, sigma_spatial. These may affect the results significantly, but the experimental section does not discuss their sensitivity. Are they robust across different datasets? It is not clear from the experiments. \n\n3) Experimental comparison\nThe comparisons are not convincing. The results on fine-grained classification are presented qualitatively, and the quantitative comparison is done only on a single dataset, ILSVRC validation set, which is very limited considering experiments in the related work, e.g., three benchmark experiments of RISE RISE (Petsiuk et al., BMVC19) on PASCAL VOC, MSCOCO, and ImageNet. Furthermore, in the ILSVRC experiments, the gap from Grad-CAM and RISE is not significant in terms of accuracy. Considering several hyperparameters of the proposed method to be tuned, these results appear less appealing. I hope the authors provide more convincing experiments, e.g., on the same benchmarks of RISE.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1284/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Salient Explanation for Fine-grained Classification", "authors": ["Kanghan Oh", "Sungchan Kim", "Il-Seok Oh"], "authorids": ["blastps@gmail.com", "s.k@jbnu.ac.kr", "iosh@jbnu.ac.kr"], "keywords": ["Visual explanation", "XAI", "Constitutional Neural Network"], "abstract": "Explaining the prediction of deep models has gained increasing attention to increase its applicability, even spreading it to life-affecting decisions. However there has been no attempt to pinpoint only the most discriminative features contributing specifically to separating different classes in a fine-grained classification task. This paper introduces a novel notion of salient explanation and proposes a simple yet effective salient explanation method called Gaussian light and shadow (GLAS), which estimates the spatial impact of deep models by the feature perturbation inspired by light and shadow in nature. GLAS provides a useful coarse-to-fine control benefiting from scalability of Gaussian mask. We also devised the ability to identify multiple instances through recursive GLAS. We prove the effectiveness of GLAS for fine-grained classification using the fine-grained classification dataset. To show the general applicability, we also illustrate that GLAS has state-of-the-art performance at high speed (about 0.5 sec per 224$\\times$224 image) via the ImageNet Large Scale Visual Recognition Challenge. ", "pdf": "/pdf/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "paperhash": "oh|salient_explanation_for_finegrained_classification", "original_pdf": "/attachment/a49e6bf374df1d051719b4ed0a1564a57fed1990.pdf", "_bibtex": "@misc{\noh2020salient,\ntitle={Salient Explanation for Fine-grained Classification},\nauthor={Kanghan Oh and Sungchan Kim and Il-Seok Oh},\nyear={2020},\nurl={https://openreview.net/forum?id=rkeUcRNtwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkeUcRNtwS", "replyto": "rkeUcRNtwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575661532866, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1284/Reviewers"], "noninvitees": [], "tcdate": 1570237739618, "tmdate": 1575661532882, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1284/-/Official_Review"}}}], "count": 5}