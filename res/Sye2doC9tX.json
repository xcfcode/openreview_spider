{"notes": [{"id": "Sye2doC9tX", "original": "B1gfHxP5Ym", "number": 388, "cdate": 1538087795597, "ddate": null, "tcdate": 1538087795597, "tmdate": 1545355383580, "tddate": null, "forum": "Sye2doC9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Exploration by Uncertainty in Reward Space", "abstract": "Efficient exploration plays a key role in reinforcement learning tasks. Commonly used dithering strategies, such as\u000f-greedy, try to explore the action-state space randomly; this can lead to large demand for samples. In this paper, We propose an exploration method based on the uncertainty in reward space. There are two policies in this approach, the exploration policy is used for exploratory sampling in the environment, then the benchmark policy try to update by the data proven by the exploration policy. Benchmark policy is used to provide the uncertainty in reward space, e.g. td-error, which guides the exploration policy updating. We apply our method on two grid-world environments and four Atari games. Experiment results show that our method improves learning speed and have a better performance than baseline policies", "keywords": ["Policy Exploration", "Uncertainty in Reward Space"], "authorids": ["nju_qwy@163.com", "yuy@nju.edu.cn", "hzlvtangjie@corp.netease.com", "chenyingfeng1@corp.netease.com", "fanchangjie@corp.netease.com"], "authors": ["Wei-Yang Qu", "Yang Yu", "Tang-Jie Lv", "Ying-Feng Chen", "Chang-Jie Fan"], "TL;DR": "Exploration by Uncertainty in Reward Space", "pdf": "/pdf/5d0758db0c8a5ab2f365bfee891eba7010a67d2f.pdf", "paperhash": "qu|exploration_by_uncertainty_in_reward_space", "_bibtex": "@misc{\nqu2019exploration,\ntitle={Exploration by Uncertainty in Reward Space},\nauthor={Wei-Yang Qu and Yang Yu and Tang-Jie Lv and Ying-Feng Chen and Chang-Jie Fan},\nyear={2019},\nurl={https://openreview.net/forum?id=Sye2doC9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BylAX-UbxV", "original": null, "number": 1, "cdate": 1544802598022, "ddate": null, "tcdate": 1544802598022, "tmdate": 1545354526436, "tddate": null, "forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper388/Meta_Review", "content": {"metareview": "The paper has some nice ideas for efficient exploration, but reviewers think more work is needed before it is ready for publication.  In particular, the paper should have an improved discussion of state-of-the-art work on exploration, compare the difference and benefits of the proposed approach, and then conduct proper experiments to validate the claims.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Nice ideas, but need a better comparison to previous work"}, "signatures": ["ICLR.cc/2019/Conference/Paper388/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper388/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploration by Uncertainty in Reward Space", "abstract": "Efficient exploration plays a key role in reinforcement learning tasks. Commonly used dithering strategies, such as\u000f-greedy, try to explore the action-state space randomly; this can lead to large demand for samples. In this paper, We propose an exploration method based on the uncertainty in reward space. There are two policies in this approach, the exploration policy is used for exploratory sampling in the environment, then the benchmark policy try to update by the data proven by the exploration policy. Benchmark policy is used to provide the uncertainty in reward space, e.g. td-error, which guides the exploration policy updating. We apply our method on two grid-world environments and four Atari games. Experiment results show that our method improves learning speed and have a better performance than baseline policies", "keywords": ["Policy Exploration", "Uncertainty in Reward Space"], "authorids": ["nju_qwy@163.com", "yuy@nju.edu.cn", "hzlvtangjie@corp.netease.com", "chenyingfeng1@corp.netease.com", "fanchangjie@corp.netease.com"], "authors": ["Wei-Yang Qu", "Yang Yu", "Tang-Jie Lv", "Ying-Feng Chen", "Chang-Jie Fan"], "TL;DR": "Exploration by Uncertainty in Reward Space", "pdf": "/pdf/5d0758db0c8a5ab2f365bfee891eba7010a67d2f.pdf", "paperhash": "qu|exploration_by_uncertainty_in_reward_space", "_bibtex": "@misc{\nqu2019exploration,\ntitle={Exploration by Uncertainty in Reward Space},\nauthor={Wei-Yang Qu and Yang Yu and Tang-Jie Lv and Ying-Feng Chen and Chang-Jie Fan},\nyear={2019},\nurl={https://openreview.net/forum?id=Sye2doC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper388/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353233980, "tddate": null, "super": null, "final": null, "reply": {"forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper388/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper388/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper388/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353233980}}}, {"id": "BJeZDDMgaQ", "original": null, "number": 3, "cdate": 1541576536938, "ddate": null, "tcdate": 1541576536938, "tmdate": 1541576536938, "tddate": null, "forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "content": {"title": "An interesting form of \"imitation learning\".", "review": "This paper considered the idea of accelerating sampling process by exploring uncertainty of rewards. The authors claimed more efficient sampling by building a reference policy and an exploration policy. Algorithm was tested on grids and Atari games.\n\nThe authors proposed eDQN, which is more like exploring the uncertainty of Q values instead of rewards. The reviewer is also expecting to see the convergence guarantee of eDQN.\n\nThe paper is well-organized and easy to understand. Written errors didn't influence understanding.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper388/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploration by Uncertainty in Reward Space", "abstract": "Efficient exploration plays a key role in reinforcement learning tasks. Commonly used dithering strategies, such as\u000f-greedy, try to explore the action-state space randomly; this can lead to large demand for samples. In this paper, We propose an exploration method based on the uncertainty in reward space. There are two policies in this approach, the exploration policy is used for exploratory sampling in the environment, then the benchmark policy try to update by the data proven by the exploration policy. Benchmark policy is used to provide the uncertainty in reward space, e.g. td-error, which guides the exploration policy updating. We apply our method on two grid-world environments and four Atari games. Experiment results show that our method improves learning speed and have a better performance than baseline policies", "keywords": ["Policy Exploration", "Uncertainty in Reward Space"], "authorids": ["nju_qwy@163.com", "yuy@nju.edu.cn", "hzlvtangjie@corp.netease.com", "chenyingfeng1@corp.netease.com", "fanchangjie@corp.netease.com"], "authors": ["Wei-Yang Qu", "Yang Yu", "Tang-Jie Lv", "Ying-Feng Chen", "Chang-Jie Fan"], "TL;DR": "Exploration by Uncertainty in Reward Space", "pdf": "/pdf/5d0758db0c8a5ab2f365bfee891eba7010a67d2f.pdf", "paperhash": "qu|exploration_by_uncertainty_in_reward_space", "_bibtex": "@misc{\nqu2019exploration,\ntitle={Exploration by Uncertainty in Reward Space},\nauthor={Wei-Yang Qu and Yang Yu and Tang-Jie Lv and Ying-Feng Chen and Chang-Jie Fan},\nyear={2019},\nurl={https://openreview.net/forum?id=Sye2doC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "cdate": 1542234472854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper388/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712847, "tmdate": 1552335712847, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper388/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJg5oYDUhX", "original": null, "number": 2, "cdate": 1540942242334, "ddate": null, "tcdate": 1540942242334, "tmdate": 1541534037703, "tddate": null, "forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "content": {"title": "Nice paper", "review": "The authors develop a new algorithm for reinforcement learning based on adding another agent rewarded by both the extrinsic environment reward and the TD-errors of the original agent, and use the state-action pairs visited by the added agent as data for the original.\n\nThis co-evolutionary process could be more broadly described as a student agent that learns from the trajectories of a teacher agent, which visits trajectories with high reward and/or high student TD-error.\n\nThe algorithm is not proved to converge to the optimal Q.\nAlgorithm (1) by not using epsilon-greedy on Q_hat\nhas an initialization-based counter-example in the tabular bandit case\ne.g.\n  MDP being a bandit with two arms with rewards 100 and 1000 respectively\n  Q_hat that initially is X for first arm and Y for second arm, with X > 100 > Y\nThis could be solved by, for example, adopting epsilon greedy.\n\nPrioritized Experience Replay (Schaul et al. https://arxiv.org/pdf/1511.05952.pdf), which suggests using the TD-error to change the data distribution for Q-learning, should be also a baseline to evaluate against.\n[Speculative: It feels like a way to make prioritized experience replay that instead of being based on time-steps (state, action, reward, new state) is based on trajectories, and this is done by doubling the number of model parameters.]\n\nOn quality:\nThe evaluation needs different experience replay baselines.\n\nOn clarity/naming:\nHere 'uncertainty in reward space' is used to refer to TD-error (temporal difference), I found that confusing.\nHere 'intrinsic motivation' is used but the 'intrinsic motivation' proposed depends on already having an externally defined reward.\n\nPros:\n+ \"Prioritized Experience Replay for Trajectories with Learning\"\nCons:\n- Not evaluated against experience replay methods.\n- No plots showing number of gradient descent steps (as the proposed method has double gradient descent updates than the baselines)\n- No proof of correctness (nor regret bounds).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper388/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploration by Uncertainty in Reward Space", "abstract": "Efficient exploration plays a key role in reinforcement learning tasks. Commonly used dithering strategies, such as\u000f-greedy, try to explore the action-state space randomly; this can lead to large demand for samples. In this paper, We propose an exploration method based on the uncertainty in reward space. There are two policies in this approach, the exploration policy is used for exploratory sampling in the environment, then the benchmark policy try to update by the data proven by the exploration policy. Benchmark policy is used to provide the uncertainty in reward space, e.g. td-error, which guides the exploration policy updating. We apply our method on two grid-world environments and four Atari games. Experiment results show that our method improves learning speed and have a better performance than baseline policies", "keywords": ["Policy Exploration", "Uncertainty in Reward Space"], "authorids": ["nju_qwy@163.com", "yuy@nju.edu.cn", "hzlvtangjie@corp.netease.com", "chenyingfeng1@corp.netease.com", "fanchangjie@corp.netease.com"], "authors": ["Wei-Yang Qu", "Yang Yu", "Tang-Jie Lv", "Ying-Feng Chen", "Chang-Jie Fan"], "TL;DR": "Exploration by Uncertainty in Reward Space", "pdf": "/pdf/5d0758db0c8a5ab2f365bfee891eba7010a67d2f.pdf", "paperhash": "qu|exploration_by_uncertainty_in_reward_space", "_bibtex": "@misc{\nqu2019exploration,\ntitle={Exploration by Uncertainty in Reward Space},\nauthor={Wei-Yang Qu and Yang Yu and Tang-Jie Lv and Ying-Feng Chen and Chang-Jie Fan},\nyear={2019},\nurl={https://openreview.net/forum?id=Sye2doC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "cdate": 1542234472854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper388/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712847, "tmdate": 1552335712847, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper388/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkxhq0z83Q", "original": null, "number": 1, "cdate": 1540923028203, "ddate": null, "tcdate": 1540923028203, "tmdate": 1541534037501, "tddate": null, "forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "content": {"title": "A well-intentioned piece of work... but the understanding of prior work / the exploration problem is lacking", "review": "This paper suggests an exploration driven by uncertainty in the reward space.\nIn this way, the agent receives a bonus based on its squared error in reward estimation.\nThe resultant algorithm is then employed with DQN, where it outperforms an e-greedy baseline.\n\nThere are several things to like about this paper:\n- The idea of exploration wrt uncertainty in the reward is good (well... actually it feels like it's uncertainty in *value* that is important).\n- The resultant algorithm, which gives bonus to poorly-estimated rewards is a good idea.\n- The algorithm does appear to outperform the basic DQN baseline on their experiments.\n\nUnfortunately, there are several places where the paper falls down:\n- The authors wrongly present prior work on efficient exploration as \"exploration in state space\" ... rather than \"reward space\"... in fact prior work on provably-efficient exploration is dominated by \"exploration in value space\"... and actually I think this is the one that makes sense. When you look at the analysis for something like UCRL2 this is clear, the reason we give bonus on rewards/transitions is to provide optimistic bounds on the *value function*... now for tabular methods this often degenerates to \"counts\" but for analysis with generalization this is not the case: https://arxiv.org/abs/1403.3741, https://arxiv.org/abs/1406.1853\n\n- So this algorithm falls into a pretty common trope of algorithms of \"exploration bonus\" / UCB, except this time it is on the squared error of rewards (why not take the square root of this, so at least the algorithm is scale-invariant??)\n\n- Once you do start looking at taking a square root as suggested (and incorporating a notion of transition uncertainty) I think this algorithm starts to fall back on something a lot more like lin-UCB *or* the sort of bonus that is naturally introduced by Thompson (posterior) sampling... for an extension of this type of idea to value-based learning maybe look at the line of work around \"randomized value functions\"\n\n- I don't think the experimental results are particularly illuminating when comparing this method to other alternatives for exploration. It might be helpful to distill the concept to simpler settings where the superiority of this method can be clearly demonstrated.\n\nOverall, I do like the idea behind this paper... I just think that it's not fully thought through... and that actually there is better prior work in this area.\nIt could be that I am wrong, but in this case I think the authors need to include a comparison to existing work in the area that suggests \"exploration by uncertainty in value space\"... e.g. \"deep exploration via randomized value functions\"", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper388/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploration by Uncertainty in Reward Space", "abstract": "Efficient exploration plays a key role in reinforcement learning tasks. Commonly used dithering strategies, such as\u000f-greedy, try to explore the action-state space randomly; this can lead to large demand for samples. In this paper, We propose an exploration method based on the uncertainty in reward space. There are two policies in this approach, the exploration policy is used for exploratory sampling in the environment, then the benchmark policy try to update by the data proven by the exploration policy. Benchmark policy is used to provide the uncertainty in reward space, e.g. td-error, which guides the exploration policy updating. We apply our method on two grid-world environments and four Atari games. Experiment results show that our method improves learning speed and have a better performance than baseline policies", "keywords": ["Policy Exploration", "Uncertainty in Reward Space"], "authorids": ["nju_qwy@163.com", "yuy@nju.edu.cn", "hzlvtangjie@corp.netease.com", "chenyingfeng1@corp.netease.com", "fanchangjie@corp.netease.com"], "authors": ["Wei-Yang Qu", "Yang Yu", "Tang-Jie Lv", "Ying-Feng Chen", "Chang-Jie Fan"], "TL;DR": "Exploration by Uncertainty in Reward Space", "pdf": "/pdf/5d0758db0c8a5ab2f365bfee891eba7010a67d2f.pdf", "paperhash": "qu|exploration_by_uncertainty_in_reward_space", "_bibtex": "@misc{\nqu2019exploration,\ntitle={Exploration by Uncertainty in Reward Space},\nauthor={Wei-Yang Qu and Yang Yu and Tang-Jie Lv and Ying-Feng Chen and Chang-Jie Fan},\nyear={2019},\nurl={https://openreview.net/forum?id=Sye2doC9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper388/Official_Review", "cdate": 1542234472854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Sye2doC9tX", "replyto": "Sye2doC9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper388/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712847, "tmdate": 1552335712847, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper388/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}