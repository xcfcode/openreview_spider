{"notes": [{"id": "b4Phn_aTm_e", "original": "MimvjOUGt2OT", "number": 3408, "cdate": 1601308378346, "ddate": null, "tcdate": 1601308378346, "tmdate": 1614985648870, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "bbNFL0a58M", "original": null, "number": 1, "cdate": 1610040513845, "ddate": null, "tcdate": 1610040513845, "tmdate": 1610474121904, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All reviewers agree that the paper overclaims its contributions both in the main text and in the title, and given also the limited novelty  and scope it is not suggested for publication."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040513832, "tmdate": 1610474121888, "id": "ICLR.cc/2021/Conference/Paper3408/-/Decision"}}}, {"id": "MFcZO95s5am", "original": null, "number": 4, "cdate": 1605585580989, "ddate": null, "tcdate": 1605585580989, "tmdate": 1605585580989, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "zkWSLA5i8gP", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "1. Scope: It seems the paper works specific on the left-right warping consistency of semantic label and depth, while the major scope told in the title and introduction is about pseudo label for general multiple task learning, which is byfar not shown in the worked experiments. It needs to be adjusted. \n\n\u2192 I agree with your opinion about the title. We will revise the title so as to reflect the main contribution of this work. As mentioned above, the cross consistency loss used in the paper can be applied to all tasks in which stereo image pairs are available. We will conduct more experiments by applying it to all tasks.\n\n2. Method: The major methodology is using obtain consistency losses by warping depth and semantic with respect to stereo output. The warped loss containing 6 terms each through enumeration, are all of them useful ? Is there a lot of redundency, what happened if droping half of it. The ablation shows using concistency is useful, while the usefulness of each term and how balance between these losses has not been proven. \n\n\u2192 Thank you for your suggestion. The results of dropping some losses were attached to the ablation study, and hyper-parameters for six losses are described in experiments. As you suggested, we will investigate the usefulness of each term in more details.\n\n3. Experiments Comparing to other SoTA algorithms, it seems for depth, the results are comparable to many existing algorithms, and for semantic it is hard to compare against other SoTA semantic algorithms such as HRNet etc.. In my opinion, MTL has two benefits either differet tasks can help the output results, another is unifying tasks into single network for more efficient inference. It might be better to also compare about the running speed and Flops for performing multiple tasks to better support the idea. \n\n\u2192 While SoTA monocular depth estimation methods usually rely on simple architectures according to monocular depth estimation literatures, the SoTA segmentation methods are based on very complicated architectures. We guess our segmentation performance is not as good as SoTA segmentation algorithms since our multi-task learning architecture rely on the simple encoder-decoder architecture. As you suggested, we will compare the runtime and Flops for performing multiple tasks.\n\n4. Writting Overall, it is easy to follow, however the figures are too small making it hard to diagnose the difference between multiple predictions. \n\n\u2192 Thank you for your suggestion. We had to put the figures small due to a page limit. Instead, more results with a relatively large size are provided in the Appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "b4Phn_aTm_e", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3408/Authors|ICLR.cc/2021/Conference/Paper3408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837832, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment"}}}, {"id": "MLT1MwsI8yr", "original": null, "number": 3, "cdate": 1605585474759, "ddate": null, "tcdate": 1605585474759, "tmdate": 1605585474759, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "R9Iqc8HgmuR", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "1. The problem setting in this work, which requires stereo image pair for learning network, is different from the prior work (e.g., Liu et al 2019). The proposed method also uses a pre-trained stereo-matching networks and confidence estimation network, which essentially included additional prior information/training data. Therefore, it is not surprising to see the performance improvement over the prior work. \n\n\u2192 Your comment is right. Indeed, we attempted to show the effectiveness of the cross-consistency loss using pseudo depth labels and confidence maps. When comparing \u2018MTAN\u2019 (Liu et al 2019) of Table 3 and \u2018Baseline\u2019 in Table 5, we can see that even the baseline of using the pseudo depth labels only outperforms 'MTAN\u2019 (Liu et al 2019). The performance gain becomes higher by leveraging the cross-consistency loss as reported in Table 5.\n\n2. While the proposed cross-view loss improves the segmentation, the overall design is quite complicated. There are many hyper-parameters in the loss functions, and it is unclear how their values would generalize to other datasets that are not road scenes. Moreover, based on the ablative study, the improvement over the noisy depth setting is marginal (Table 4 and 5). Also, it is unclear whether all those terms make significant contributes to the performance improvements, and sometimes it even hurts the performance. \n\n\u2192 I agree that the hyper-parameter tuning may not be easy. But, we found that the proposed loss is not much sensitive to hyper-parameters in the datasets used in experiments. We will investigate the generalization capability in diverse scenes.\nAs shown in the ablation study of Table 4 and 5, it was observed that when adding the depth cross consistency view loss, the monocular depth accuracy is improved over the baseline. It was also seen that when the segmentation cross consistency view loss leads to the performance improvement in the segmentation. However, under the simple multi-task architecture sharing the encoder, boosting the depth and segmentation accuracy significantly is quite challenging. We will investigate to use more sophisticate multi-task architectures provided in recent works to address this issue.\n\n3. The experimental evaluation is a bit lacking in the following aspects.\nThis work only uses two road-scene datasets for evaluation, but those two datasets are quite similar to each other, and hence do not have sufficient diversity. The other work typically also use NYU-v2, which is an indoor dataset. Can the author also report their method's performance on NYU-v2?\nThe evaluation on the Cityscapes dataset seems unconvincing due to two issues: First, the depth performance in Table 3 seems very different from the prior literature, and in particular, the Abs values are much worse than the SOTA results. Secondly, it lacks comparisons with Jha et al. 2020, which achieves better performance than the results shown in Table 3.\nThe improvement from the proposed loss seems very marginal in the ablative study. Different combinations of proposed components typically give minor or mixed improvement on segmentation or depth estimation. It is unclear how effective of the confidence weighting or using multiple consistency constraints.\n\n\u2192 Thank you for your valuable suggestion. Unfortunately, it is infeasible to apply the cross consistency loss to NYU-v2 dataset that provides only a single image, not stereo image pairs. We will seek various datasets for conducting experiments for ensuring the diversity, as you suggested.\nIn the prior literatures, the performance in the Cityscapes dataset was usually measured with disparity maps obtained using the hand-crafted stereo matching method, semi-global matching (SGM) [Hirschmuller, 2008]. We found that the SGM disparity map used for the performance evaluation contains disparity values that are 0 or close to 0 at many parts. Since these values are meaningless, we excluded these values in the performance evaluation. Note that for a fair comparison, we measured the performance of all methods under the same setup. The code will be publicly available soon. As suggested, we will include the comparison with Jha et al. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "b4Phn_aTm_e", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3408/Authors|ICLR.cc/2021/Conference/Paper3408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837832, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment"}}}, {"id": "K8s1O2GJ8U-", "original": null, "number": 2, "cdate": 1605585108909, "ddate": null, "tcdate": 1605585108909, "tmdate": 1605585108909, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "vd8T39GylIi", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment", "content": {"title": "Response to Reviewer1 ", "comment": "1. Overall, the paper does not have much novelty in my opinion. Joint learning of depth and semantic segmentation is clearly not new, and the paper does not provide new or particular insight towards this combined learning.\nThe use of pseudo label itself is nowadays quite common in the vision community. And, the pseudo labels are used in the paper in a pretty trivial way in my opinion.\n2.The cross-view consistency across two views in a stereo setup is not new neither. It has been intensively used in the monocular depth estimation. In addition, this constraint is applicable to any individual task and does not seem to fit into the multi-task learning context, which is the main focus of this paper. I would expect specific insights in making use of pseudo labels towards solving the depth and semantics predictions; otherwise, any other tasks such as moving objects segmentation\n\n\u2192 Thank you for your analysis and suggestions. In our humble opinion, while existing works [Godard et al., 2017; 2019; Watson et al., 2019; Chen et al., 2019] for imposing the cross-view consistency across two views use predicted disparity maps, our method attempts to impose the cross-view consistency by making use of pseudo disparity maps and their associated confidences. We showed the effectiveness of the cross-view consistency based on the pseudo label and its confidence through the ablation study in Table 4. We could see that when the cross-consistency loss was applied by using (incomplete) predicted disparity, it does not improve performance. We believe this is a difference from the existing papers [Godard et al., 2017; 2019; Watson et al., 2019; Chen et al., 2019]. Nevertheless, we will continue to investigate the applicability of using pseudo labels in the depth and semantics predictions, as you suggested.\n\n3. The current title is too general, so much so that the main arguments made by the paper are not reflected in the title; I believe that the left-right consistency brought about by the pseudo ground truth depth is the main claim of the paper.\n\n\u2192 I agree with your opinion about the title. We will revise the title so as to reflect the main contribution of this work. As mentioned above, the cross consistency loss used in the paper can be applied to all tasks in which stereo image pairs are available. We will conduct more experiments by applying it to all tasks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "b4Phn_aTm_e", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3408/Authors|ICLR.cc/2021/Conference/Paper3408/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837832, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Comment"}}}, {"id": "zkWSLA5i8gP", "original": null, "number": 1, "cdate": 1603914758346, "ddate": null, "tcdate": 1603914758346, "tmdate": 1605024005733, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review", "content": {"title": "too large scope for a relative limited experiments", "review": "This paper propose to use depth pseudo ground truth (generated with a pretrained stereo network) as augmented information to help a joint prediction network for depth and segment estimation. \n\nPros:\nMulti task learning is an important direction to explore, and left-right consistency has shown to be very useful in depth estimation Godard et.al 2017. The extension using similiar idea to depth and semantic is reasonable, and experiments verify the effectiveness of proposed strategies. \n\nCons: \n1) Scope: \nIt seems the paper works specific on the left-right warping consistency of semantic label and depth, while the major scop told in the title and introduction is about pseudo label for general multiple task learning, which is byfar not shown in the worked experiments. It needs to be adjusted.  \n\n\n2) Method:\nThe major methodology is using obtain consistency losses by warping depth and semantic with respect to stereo output. The warped loss containing 6 terms each through enumeration, are all of them useful ? Is there a lot of redundency, what happened if droping half of it. The ablation shows using concistency is useful, while the usefulness of each term and how balance between these losses has not been proven. \n\n\n3) Experiments\nComparing to other SoTA algorithms, it seems for depth, the results are comparable to many existing algorithms, and for semantic it is hard to compare against other SoTA semantic algorithms such as HRNet etc.. In my opinion, MTL has two benefits either differet tasks can help the output results, another is unifying tasks into single network for more efficient inference. It might be better to also compare about the running speed and Flops for performing multiple tasks to better support the idea. \n\n4) Writting\nOverall, it is easy to follow, however the figures are too small making it hard to diagnose the difference between multiple predictions. \n\n ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076338, "tmdate": 1606915804001, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review"}}}, {"id": "R9Iqc8HgmuR", "original": null, "number": 2, "cdate": 1603927547043, "ddate": null, "tcdate": 1603927547043, "tmdate": 1605024005670, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review", "content": {"title": "Several concerns on the problem setting and experimental evaluation", "review": "The paper presents a joint learning strategy for simultaneous semantic segmentation and monocular depth estimation. The main idea is to exploit stereo pairs in training and introduce pseudo-depth label estimated from pre-trained stereo-matching networks. Given the pseudo-depth with confidence estimation, the method proposes a cross-view consistency loss for both depth and semantic predictions, which augments the standard segmentation loss. The proposed method is evaluated on KITTI and Cityscapes datasets with comparisons to prior work and ablative study.   \n\nStrengths:\n- The proposed cross-view loss on semantic segmentation seems interesting and effective on two benchmarks, which improves the segmentation performance. \n- The overall method achieves competitive performance on semantic segmentation and monocular depth estimation on the KITTI and Cityscapes. \n\nConcerns:\n- The contribution of this work to the multi-task learning is a bit overclaimed. The targeted problem of the paper is solely on joint semantic segmentation and monocular depth estimation. Based on the model and loss design, it is non-trivial to extend them to other scene understanding tasks. \n\n- The problem setting in this work, which requires stereo image pair for learning network, is different from the prior work  (e.g., Liu et al 2019). The proposed method also uses a pre-trained stereo-matching networks and confidence estimation network, which essentially included additional prior information/training data. Therefore, it is not surprising to see the performance improvement over the prior work. \n\n- While the proposed cross-view loss improves the segmentation, the overall design is quite complicated. There are many hyper-parameters in the loss functions, and it is unclear how their values would generalize to other datasets that are not road scenes. Moreover, based on the ablative study, the improvement over the noisy depth setting is marginal (Table 4 and 5). Also, it is unclear whether all those terms make significant contributes to the performance improvements, and sometimes it even hurts the performance. \n\n- The experimental evaluation is a bit lacking in the following aspects. \n  + This work only uses two road-scene datasets for evaluation, but those two datasets are quite similar to each other, and hence do not have sufficient diversity. The other work typically also use NYU-v2, which is an indoor dataset. Can the author also report their method's performance on NYU-v2?\n\n  + The evaluation on the Cityscapes dataset seems unconvincing due to two issues: First, the depth performance in Table 3 seems very different from the prior literature, and in particular, the Abs values are much worse than the SOTA results. Secondly, it lacks comparisons with Jha et al. 2020, which achieves better performance than the results shown in Table 3. \n\n  + The improvement from the proposed loss seems very marginal in the ablative study. Different combinations of proposed components typically give minor or mixed improvement on segmentation or depth estimation. It is unclear how effective of the confidence weighting or using multiple consistency constraints. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076338, "tmdate": 1606915804001, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review"}}}, {"id": "vd8T39GylIi", "original": null, "number": 3, "cdate": 1603942828338, "ddate": null, "tcdate": 1603942828338, "tmdate": 1605024005611, "tddate": null, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "invitation": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review", "content": {"title": "This paper presents a framework which leverages pseudo depth ground truth to train monocular depth and semantic segmentation networks. ", "review": "The paper presents a framework to learn depth prediction and semantic segmantation jointly; the key idea lies in making use of the pseudo depth label from stereo to provide supervision and as a means to enforce cycle consistency between the left and right views of the stereo. \n\nReasons for scores: overall the paper is rather incremental and the idea is neither novel nor significant in my opiniont. I do not see interesting or deep insight from the paper towards the depth and semantic segmantation tasks.  \n\nPros:\n+ First, the paper is clearly written and easy to follow. The proposed framework is pretty straightforward. \n+ The idea of joint learning depth and semantic segmentation is good considering their tightly coupled nature. \n+ The use of cross-view consistency as a constraint is good.\n\nCons:\n- Overall, the paper does not have much novelty in my opinion. Joint learning of depth and semantic segmentation is clearly not new, and the paper does not provide new or particular insight towards this combined learning.\n- The use of pseudo label itself is nowadays quite common in the vision community.  And, the pseudo labels are used in the paper in a pretty trivial way in my opinion. \n-  The cross-view consistency across two views in a stereo setup is not new neither. It has been intensively used in the monocular depth estimation. In addition, this constraint is applicable to any individual task and does not seem to fit into the multi-task learning context, which is the main focus of this paper. I would expect specific insights in making use of pesudo labels towards solving the depth and semantics predictions; otherwise, any other tasks such as moving objects segmentation \n- The current title is too general, so much so that the main arguments made by the paper are not reflected in the title; I believe that the left-right consistency brought about by the pseudo ground truth depth is the main claim of the paper. ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3408/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3408/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Pseudo Label-Guided Multi Task Learning for Scene Understanding", "authorids": ["~Sunkyung_Kim1", "~Hyesong_Choi1", "~Dongbo_Min3"], "authors": ["Sunkyung Kim", "Hyesong Choi", "Dongbo Min"], "keywords": ["Multi-task learning", "monocular depth estimation", "semantic segmentation", "pseudo label", "cross-view consistency"], "abstract": "Multi-task learning (MTL) for scene understanding has been actively studied by exploiting correlation of multiple tasks. This work focuses on improving the performance of the MTL network that infers depth and semantic segmentation maps from a single image. Specifically, we propose a novel MTL architecture, called Pseudo-MTL, that introduces pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks. The pseudo ground truth depth maps, generated from pretrained stereo matching methods, are leveraged to supervise the monocular depth estimation. More importantly, the pseudo depth labels serve to impose a cross-view consistency on the estimated monocular depth and segmentation maps of two views. This enables for mitigating the mismatch problem incurred by inconsistent prediction results across two views. A thorough ablation study validates that the cross-view consistency leads to a substantial performance gain by ensuring inference-view invariance for the two tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kim|pseudo_labelguided_multi_task_learning_for_scene_understanding", "one-sentence_summary": "This paper proposes a novel multi-task learning (MTL) architecture, called Pseudo-MTL, that leverages pseudo labels for joint learning of monocular depth estimation and semantic segmentation tasks.", "pdf": "/pdf/59be1c152eb2cc9f77ada4b6a5e640ff33ae9cee.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PAtmMy1zuW", "_bibtex": "@misc{\nkim2021pseudo,\ntitle={Pseudo Label-Guided Multi Task Learning for Scene Understanding},\nauthor={Sunkyung Kim and Hyesong Choi and Dongbo Min},\nyear={2021},\nurl={https://openreview.net/forum?id=b4Phn_aTm_e}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "b4Phn_aTm_e", "replyto": "b4Phn_aTm_e", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3408/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538076338, "tmdate": 1606915804001, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3408/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3408/-/Official_Review"}}}], "count": 8}