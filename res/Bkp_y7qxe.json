{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396419808, "tcdate": 1486396419808, "number": 1, "id": "Sk2bhGI_e", "invitation": "ICLR.cc/2017/conference/-/paper186/acceptance", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors apply an already-published method for state representation learning in a very simple experimental scenario. They give no additional contribution or comparison, nor do they offer any empirical or analytical study."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396420260, "id": "ICLR.cc/2017/conference/-/paper186/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396420260}}}, {"tddate": null, "tmdate": 1484815514038, "tcdate": 1484815514038, "number": 4, "id": "r1MsneALx", "invitation": "ICLR.cc/2017/conference/-/paper186/public/comment", "forum": "Bkp_y7qxe", "replyto": "HJbRfpeEl", "signatures": ["~Timothee_LESORT1"], "readers": ["everyone"], "writers": ["~Timothee_LESORT1"], "content": {"title": "About unsupervised versus reinforcement", "comment": "Dear Reviewer,\n\nAs written in the paper we use rewards information to train our neural network. But we don't aim to maximize those rewards during training and that why it is not reinforcement learning. We call our training unsupervised because we don't give any label and we only impose some priors to the representation in order to learn it.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694875, "id": "ICLR.cc/2017/conference/-/paper186/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkp_y7qxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper186/reviewers", "ICLR.cc/2017/conference/paper186/areachairs"], "cdate": 1485287694875}}}, {"tddate": null, "tmdate": 1484814609901, "tcdate": 1484814609901, "number": 3, "id": "By9GFgC8x", "invitation": "ICLR.cc/2017/conference/-/paper186/public/comment", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["~Timothee_LESORT1"], "readers": ["everyone"], "writers": ["~Timothee_LESORT1"], "content": {"title": "Answer to reviewers", "comment": "Dear reviewers,\n\nwe agreed that comparisons with other works need to be done and that other tasks have to be tested to improve evaluation and validation.\nIt will be done in future experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694875, "id": "ICLR.cc/2017/conference/-/paper186/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkp_y7qxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper186/reviewers", "ICLR.cc/2017/conference/paper186/areachairs"], "cdate": 1485287694875}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484814111054, "tcdate": 1478270837010, "number": 186, "id": "Bkp_y7qxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bkp_y7qxe", "signatures": ["~Timothee_LESORT1"], "readers": ["everyone"], "content": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481926988995, "tcdate": 1481926988995, "number": 3, "id": "SySIFJGVg", "invitation": "ICLR.cc/2017/conference/-/paper186/official/review", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/conference/paper186/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper186/AnonReviewer2"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "This paper implements the method of Jonschkowski & Brock to learn a low-dimensional state representation represented as the last layer of a neural network. The experiments apply the method for learning a one-dimensional state representation of a simulated robot\u2019s head position from synthetic images.\n\nLearning state representations is an active and useful area of research for learning representations in interactive domains such as robotics. However, there seems to be no novelty in the method, over Jonschkowki & Brock. The primary contribution is the experimental evaluation performed on one task, where the paper evaluates the correlation between the learned state representation and the ideal state representation for the task (which is the robot\u2019s head position).\n\nAs acknowledged by the authors, the experiments are very preliminary, only showing one simple task with a one-dimensional learned representation and a two-dimensional discrete action space. To make the experiments compelling, there need to be comparisons to prior methods such as Lange et al. \u201912, Watter et al. NIPS \u201915, and Finn et al. ICRA \u201916 which also learn state representations from raw images. PCA on the images would also be a useful comparison, especially for simple tasks. Without these comparisons, it is impossible to evaluate the effectiveness of the method.\n\nLastly, as mentioned in the pre-review questions, the related work should include a discussion of other state representation learning methods such as Watter et al. NIPS \u201915, Finn et al. ICRA \u201916, and van Hoof et al. IROS \u201916.\n\nIn summary, this paper lacks novelty and significance, as the paper implements an existing method and demonstrates results on only one simple task. Without comparisons, the results are impossible to interpret. More challenging tasks and experimental comparisons would significantly improve the paper. Additionally, this paper does not introduce any novel contributions to state representation learning for solving challenges in this domain. One pro is that the paper is generally written clearly.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671521, "id": "ICLR.cc/2017/conference/-/paper186/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper186/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper186/AnonReviewer3", "ICLR.cc/2017/conference/paper186/AnonReviewer1", "ICLR.cc/2017/conference/paper186/AnonReviewer2"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671521}}}, {"tddate": null, "tmdate": 1481911628986, "tcdate": 1481911447464, "number": 2, "id": "SJyjhjZVg", "invitation": "ICLR.cc/2017/conference/-/paper186/official/review", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/conference/paper186/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper186/AnonReviewer1"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The paper proposes to use the representation learning approach of [Jonschkowski & Brock, 2015] with a deep network as function approximator. The general task and approach are interesting, but contribution of this work is limited, and experimental evaluation is absolutely unsatisfactory, so the paper cannot be accepted for publications. \n\nThe approach is tested on a simple synthetic task with very small training and test sets and very little variation in the data. The authors admitted themselves that the results are preliminary. The proposed method is not compared with existing approaches or simple hand-crafted baselines. It is impossible to judge if the proposed method is useful and/or performs well compared to existing approaches. This makes the paper unfit for publication. \n\nWith proper experiments, and if the method works in interesting realistic scenarios, this could become a good paper.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671521, "id": "ICLR.cc/2017/conference/-/paper186/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper186/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper186/AnonReviewer3", "ICLR.cc/2017/conference/paper186/AnonReviewer1", "ICLR.cc/2017/conference/paper186/AnonReviewer2"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671521}}}, {"tddate": null, "tmdate": 1481851593475, "tcdate": 1481851593475, "number": 1, "id": "HJbRfpeEl", "invitation": "ICLR.cc/2017/conference/-/paper186/official/review", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/conference/paper186/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper186/AnonReviewer3"], "content": {"title": "", "rating": "3: Clear rejection", "review": "This paper proposed to use unsupervised learning to learn features in a reinforcement learning setting. It is unclear what \"unsupervised\" means here since the \"causality prior\" uses reward signals for training. This is reinforcement learning, not unsupervised learning.\n\nThe experiments are also very premature. The task is as simple as moving the head of the robot left or right. There is also no comparison to baselines.\n\nIn conclusions section, the authors claim the proposed method can be used for transfer learning without experiments to backup the claim.\n\nOverall this paper is confusing and premature.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512671521, "id": "ICLR.cc/2017/conference/-/paper186/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper186/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper186/AnonReviewer3", "ICLR.cc/2017/conference/paper186/AnonReviewer1", "ICLR.cc/2017/conference/paper186/AnonReviewer2"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512671521}}}, {"tddate": null, "tmdate": 1481216544477, "tcdate": 1481216445718, "number": 2, "id": "BkLabzPQx", "invitation": "ICLR.cc/2017/conference/-/paper186/public/comment", "forum": "Bkp_y7qxe", "replyto": "HkSlSfy7g", "signatures": ["~Timothee_LESORT1"], "readers": ["everyone"], "writers": ["~Timothee_LESORT1"], "content": {"title": "Answer to AnonReviewer1", "comment": "Dear reviewer, \n\nthank you for your questions, here are the answers: \n\n1. Why do you use so few sequences for training and only one (!) for validation? Why do you not simulate more data? Are you sure the training and the validation sets are sufficiently different?\n\nThis is quite preliminary work, and we did not had time to generate more data before people working on this had to leave. However, we are quite confident that the validation set is different enough from the training set because the arms of the robot are in a position never seen during training. We expect the trained neural network to generalize to new configurations of the environment which means in this case new arms position. We will update our results with cross-validation results. \nBeside these limitations, our longer term goal is to learn with reasonably limited data so as to be able to work with demonstration made by humans on a real robot, thus working with a few tenth of training data is a reasonable size given these constraints. The data augmentation we use, aims to bring good generalization without more data.\n\n2. You argue deep network is better because it can learn invariance to illumination and noise. This is known very well, and moreover, could probably be achieved with some simple hand-designed features. Have you tried introducing more variation in the task and the visual input to better make use of deep network's capabilities?\n\nYou are right that hand-designed features could be used to solve this, however, while obviously not new, it remain interesting for us to learn this from data. Furthermore one of the side goals of the paper is to show that a deep neural network can be trained using the robotics priors introduce in (Jonschkowski & Brock, 2015). The visual perturbations introduced where chosen in order to simulate real world perturbations. That is why some techniques of data augmentation where not adapted, like non-linear deformations of images. Furthermore, the perturbation must not change the head angle estimation possibility. For example, cropping the images can bring uncertainty because two different images can become identical after cropping. Therefore we did not introduced more variations in the image, but even in the current setting, the network has basically to learn to ignore the robot arm in arbitrary configuration so as to be not perturbed by its position for the head direction estimation, which is not completely trivial (even if once again could be possible with other techniques). The image variations are currently limited to the random arms position but we are planning to work on real image with dynamic background in the next steps. Regarding tasks variations, we tried a push-button task (see 4. below).\n\n\n3. Have you compared to any baselines? For example, E2C (Watter et al., 2015).\n\nNo, we did not had time to do that, but the reference you mention is indeed very relevant. See the answers to the other reviewers for some element of comparison.\n\n4. Have you tried other tasks?\n\nYes, we tried a button-pushing task with the goal of learning a three dimensional state space corresponding to the robot arm end-effector position, but as stated in the paper, we did not achieve success on this task yet. This is probably in part because the evaluation procedure by comparison with the ground truth is not relevant, so we are planning to integrate reinforcement learning to have a coherent evaluation system.\n\nThanks"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694875, "id": "ICLR.cc/2017/conference/-/paper186/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkp_y7qxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper186/reviewers", "ICLR.cc/2017/conference/paper186/areachairs"], "cdate": 1485287694875}}}, {"tddate": null, "tmdate": 1481216526046, "tcdate": 1481215873795, "number": 1, "id": "By5YJzv7x", "invitation": "ICLR.cc/2017/conference/-/paper186/public/comment", "forum": "Bkp_y7qxe", "replyto": "By0AY8kQe", "signatures": ["~Timothee_LESORT1"], "readers": ["everyone"], "writers": ["~Timothee_LESORT1"], "content": {"title": "Answer to AnonReviewer2", "comment": "Dear Reviewer,\n\nthanks for your questions, here are the answers :\n\n- As discussed in the paper, the experiments are preliminary. Do you have plans to add more complex, higher-dimensional tasks?\n\nYes, in the short term, we plan to learn a 3-dimensional state space representation corresponding to the robot arm end-effector position with a button-pushing task, but as stated in the paper, we did not achieve success on this task yet. On the longer term, we plan to extend to tasks where the state are relative, for example object grasping where the state should encode distance form effector to a randomly located object.\n\n- How does this approach compare to other state representation learning approaches (e.g. [1], [2], [3], [4])?\n[1] Lange, Sascha, et al. \"Autonomous reinforcement learning on raw visual input data in a real world application.\" IJCNN, 2012.\n[2] Watter, Manuel, et al. \"Embed to control: A locally linear latent dynamics model for control from raw images.\" NIPS, 2015.\n[3] Finn, Chelsea, et al. \"Deep Spatial Autoencoders for Visuomotor Learning.\" ICRA, 2016.\n[4] van Hoof, Herke, et al. \"Stable reinforcement learning with autoencoders for tactile and visual data.\" IROS, 2016.\n\nThose papers are relevant to our approach and we will update our related work.\n\nWe have the same overall objective as [1],[2],[3] and [4]: Learning a low dimension state representation in an unsupervised way which could be used for RL. \nA first difference is that, in all these approaches, training is done with auto-encoders (AE, DAE, VAE, ...) to learn a representation able to reconstruct the input images when we learn a representation which only respect the priors we impose. We assume that, depending on the environment and tasks, some information learned for reconstructing the images may be not relevant to the task. For example in our head direction task estimation with random arm positions, reconstructing an image would require to encode arm position, which is quite complex and not directly necessary for the head direction estimation. \nAnother distinctive point is the use of the dynamics or of the action performed by the robot as constraints for learning a relevant state representation. [1] does not use this information, and [3] use it in an indirect way by imposing the states to be positions in the images (additionally filtered by a Kalman filter). We use this informations in our priors, which like is shown in [2] and [4] is important for learning a relevant representation.\nAdditionally, the causality prior (eq (5)) of our approach require the use of the reward for state learning (which is not the case in [1-4]. While this is more dependent on the particular task that images and actions, this is a good way to bias representation toward relevant states (as shown in (Jonschkowski & Brock, 2015)) that could be advantageous in some tasks.\n\nWe did not had time to experiment these approaches to compare results, it will be done in future work with integration with reinforcement learning.\n\n- Can this approach be used with continuous action spaces?\nWe think the approached can be extended to continuous actions by modifying the priors (3) (4) and (5) to have a loss function that takes action differences into account instead of finding states with identical actions. For example, (2) would become (|Delta_s1|-|Delta_s2|)^2.exp(-|a1-a2|^2) . However, this would raise the problem of sampling enough similar actions so that the priors could be applied on enough pairs or quadruples of images, which would be problematic with actions of higher dimensions.\n\n- What is the correlation if you simply use PCA on the images?\nWe did not achieve this experiment, but in the case we study, we assume that the PCA would not find a relevant representation in just one dimension from the image given that an important part of the image variations is due to the arm movement. The goal of the approach is to find a better comprehension of the data than a PCA could achieve. However, it is a baseline we will consider in future work.\n\nThanks"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287694875, "id": "ICLR.cc/2017/conference/-/paper186/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkp_y7qxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper186/reviewers", "ICLR.cc/2017/conference/paper186/areachairs"], "cdate": 1485287694875}}}, {"tddate": null, "tmdate": 1480710614241, "tcdate": 1480710614234, "number": 2, "id": "By0AY8kQe", "invitation": "ICLR.cc/2017/conference/-/paper186/pre-review/question", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/conference/paper186/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper186/AnonReviewer2"], "content": {"title": "Pre-Review Questions", "question": "Some questions:\n- As discussed in the paper, the experiments are preliminary. Do you have plans to add more complex, higher-dimensional tasks?\n- How does this approach compare to other state representation learning approaches (e.g. [1], [2], [3], [4])?\n- Can this approach be used with continuous action spaces?\n- What is the correlation if you simply use PCA on the images?\n\n[1] Lange, Sascha, et al. \"Autonomous reinforcement learning on raw visual input data in a real world application.\" IJCNN, 2012.\n[2] Watter, Manuel, et al. \"Embed to control: A locally linear latent dynamics model for control from raw images.\" NIPS, 2015.\n[3] Finn, Chelsea, et al. \"Deep Spatial Autoencoders for Visuomotor Learning.\" ICRA, 2016.\n[4] van Hoof, Herke, et al. \"Stable reinforcement learning with autoencoders for tactile and visual data.\" IROS, 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959418232, "id": "ICLR.cc/2017/conference/-/paper186/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper186/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper186/AnonReviewer1", "ICLR.cc/2017/conference/paper186/AnonReviewer2"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959418232}}}, {"tddate": null, "tmdate": 1480692973173, "tcdate": 1480692973167, "number": 1, "id": "HkSlSfy7g", "invitation": "ICLR.cc/2017/conference/-/paper186/pre-review/question", "forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "signatures": ["ICLR.cc/2017/conference/paper186/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper186/AnonReviewer1"], "content": {"title": "pre-review questions", "question": "Hello,\n\nA few questions:\n\n1. Why do you use so few sequences for training and only one (!) for validation? Why do you not simulate more data? Are you sure the training and the validation sets are sufficiently different?\n\n2. You argue deep network is better because it can learn invariance to illumination and noise. This is known very well, and moreover, could probably be achieved with some simple hand-designed features. Have you tried introducing more variation in the task and the visual input to better make use of deep network's capabilities?\n\n3. Have you compared to any baselines? For example, E2C (Watter et al., 2015).\n\n4. Have you tried other tasks?\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Deep Learning of State Representation Using Robotic Priors ", "abstract": "Our understanding of the world depends highly on how we represent it.  Using background knowledge about its complex underlying physical rules, our brain can produce intuitive and simplified representations which it can easily use to solve problems. The approach of this paper aims to reproduce this simplification process using a neural network to produce a simple low dimensional state representation of the world from images acquired by a robot. As proposed in Jonschkowski & Brock (2015), we train the neural network in an unsupervised way, using the \"a priori\" knowledge we have about the world as loss functions called \"robotic priors\" that we implemented through a siamese network. This approach has been used to learn a one dimension representation of a Baxter head position from raw images. The experiment resulted in a 97,7% correlation between the learned representation and the ground truth, and show that relevant visual features form the environment are learned.", "pdf": "/pdf/f96e9d307271e9f77d2a1df049bd9f0b8198987a.pdf", "TL;DR": "This paper introduces a method for training a deep neural network to learn a  representation of a robot's environment state using a priori knowledge.", "paperhash": "lesort|unsupervised_deep_learning_of_state_representation_using_robotic_priors_", "conflicts": ["ensta-paristech.fr", "cpe.fr", "umontreal.ca"], "keywords": ["Deep learning", "Computer vision", "Unsupervised Learning"], "authors": ["Timothee LESORT", "David FILLIAT"], "authorids": ["timothee.lesort@ensta-paristech.fr", "david.filliat@ensta-paristech.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959418232, "id": "ICLR.cc/2017/conference/-/paper186/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper186/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper186/AnonReviewer1", "ICLR.cc/2017/conference/paper186/AnonReviewer2"], "reply": {"forum": "Bkp_y7qxe", "replyto": "Bkp_y7qxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper186/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959418232}}}], "count": 11}