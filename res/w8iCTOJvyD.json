{"notes": [{"id": "w8iCTOJvyD", "original": "Z7XXr0Fhm6D", "number": 2539, "cdate": 1601308280737, "ddate": null, "tcdate": 1601308280737, "tmdate": 1614985730383, "tddate": null, "forum": "w8iCTOJvyD", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4NWuHZ8Dhry", "original": null, "number": 1, "cdate": 1610040409074, "ddate": null, "tcdate": 1610040409074, "tmdate": 1610474006201, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes new learning algorithms that fine-tune (\"tailor\") a model at test-time using unsupervised objectives. This formulation allows for introducing an inductive bias into the model that might improve generalization on unseen data. The proposed algorithm is demonstrated on two example tasks.\n\nThe reviewers like the topic and also find the proposed approach to be interesting. However, they are unconvinced by the current empirical evaluation of the method. Additional experimental evaluation could improve our understanding of the proposed method and help contrast it to previously proposed techniques. Given these reviews I recommend rejecting the paper at this time."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040409060, "tmdate": 1610474006181, "id": "ICLR.cc/2021/Conference/Paper2539/-/Decision"}}}, {"id": "REvI6CLYi_C", "original": null, "number": 3, "cdate": 1606212624818, "ddate": null, "tcdate": 1606212624818, "tmdate": 1606262355097, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "epS5O7i3Zeq", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment", "content": {"title": "clarifications on affine parameters + pointing to general answer", "comment": "Thanks for your review.\n\n**\u201cWhat is the formulation of the affine parameters? Is there any study about adapting the entire model versus the affine parameters?\u201d**\n\n- Inspired by conditional normalization, our method CNGrad adds affine layers $\\vec{y} = \\vec{\\gamma}\\cdot \\vec{x} + \\vec{\\beta}$ and only optimizes these parameters in the inner loop. These layers can be added in every hidden layer of most deep architectures.\n- The closest method that we know is CAVIA (Zintgraf et al.), which makes a neural network predict these affine parameters from a trainable vector. Then, instead of directly optimizing the affine parameters, they optimize the trainable vector. We show this extra complexity is unnecessary.\n- In theorem 2 of section 4 we prove that optimizing the affine parameters alone has the same capacity as optimizing the entire network. This theorem holds for a wide variety of losses and realistic network requirements, but requires evaluating the loss only at a finite number of data-points. As a result, the affine parameters have enough capacity for the inner loop (which uses a finite number of samples), but not for the outer loop (where we optimize all parameters to express arbitrary functions).\n\n**Comparison to test-time training, missing contrastive results and new model-based RL results:** \n\nWe address these points in the general answer, where we also explain a new application of meta-tailoring to model-based RL, added in section 5.3 in the main text.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w8iCTOJvyD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2539/Authors|ICLR.cc/2021/Conference/Paper2539/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847239, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment"}}}, {"id": "lawgTT7wpPJ", "original": null, "number": 4, "cdate": 1606213010437, "ddate": null, "tcdate": 1606213010437, "tmdate": 1606262228039, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "Y921m80PMc", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment", "content": {"title": "Extra experiments and standard benchmarks on adversarials", "comment": "Thank you for your comments.\n\n**Standard benchmarks on adversarial examples and model-based RL**\n\n\"The results of widely used benchmarks are not provided\"\n\nAs you point out, the physics experiment is not a standard benchmark. We chose it for instructive purposes and to show potential useful applications of our method in science. However, the adversarial experiments build on arguably the most standard certifiable defense (Randomized Smoothing) and evaluate on the two most important benchmarks for adversarial examples: CIFAR-10 & Imagenet. For conciseness (and because we believe the value of the paper lies in its novelty and general applicability rather than matching state-of-the-art) we put the tables including the SOA methods in the appendix. We also mentioned both works in the penultimate paragraph of section 5.2 (Zhai et al., Salman et al.). There, we detail that our approach matches SOA on ImageNet (not CIFAR-10) despite our framework not being specialized to adversarial examples and that meta-tailoring could also potentially improve the other approaches.\n\nIn the general response we detail new experiments improving on PDDM (Nagabandi et al.), a popular method, on a complex MuJoCo benchmark, with results added to section 5.3 in the main text. We believe these further show the wide range of applicability of meta-tailoring.\n\n**Extra experiments comparing to TTT**\n\n\"There is no direct comparison with closely related methods like TTT.\"\n\nIn the general answer to all reviewers we discuss differences from test time training (Sun et al.), and show that it is a sub-optimal approach for same-distribution generalization by providing extra experiments. We hope this addresses your concerns, both in terms of novelty and experimental performance."}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w8iCTOJvyD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2539/Authors|ICLR.cc/2021/Conference/Paper2539/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847239, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment"}}}, {"id": "-aA78DI4AMl", "original": null, "number": 5, "cdate": 1606213752012, "ddate": null, "tcdate": 1606213752012, "tmdate": 1606261475263, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "1AO51IDpSR", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment", "content": {"title": "More details on theory and pointing to general response for new experiments", "comment": "Thank you for your comments.\n\n\u201cI struggled to understand the importance or necessity of the provided theorems\u201d & \u201cI see no bold green in theorem 1 and definition 1\u201d\n\n**The theory section formalizes and provides guarantees to the claims and intuitions made in the introduction.** As you mention, the bold green in the theoretical guarantees is indeed hardly noticeable, we have made it brighter. It was also hard to see because the differences in guarantees between meta-tailoring and classic (inductive) learning are very small (text-wise, not result-wise!), affecting only the subscript of a subscript. In particular, $f_{\\theta_{x,S}}$ turns to $f_{\\theta_{S}}$ for inductive learning guarantees, because meta-tailoring adapts the parameters to the input $x$, but regular inductive learning doesn\u2019t. \nBy having two extremely similar bounds, we can understand the effect of meta-tailoring more clearly.\n\n**Eliminating generalization gap for tailoring loss** Concretely, we show that we can upper bound the task loss using two terms. The first term is the key to our approach, depending solely on the tailoring loss at the query point. Because we tailor a custom $\\theta_x$ to minimize the tailoring loss at test time, as well as training time, we can make the term small, eliminating the generalization gap that arises in classic ML training. In contrast, classic (inductive) ML cannot adapt $\\theta$ to each $x$ and thus has a generalization gap. The second term involves a classic stability bound, and we also provide arguments why meta-tailoring can help on that term as well.\nIn the general response to all reviewers, we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees. There, we also discuss new results on model-based RL.\n\n**Empirial validation**: please see the general response where we detail new baselines with related work and a totally new application in model-based RL, added on a new section 5.3 in the main text."}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w8iCTOJvyD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2539/Authors|ICLR.cc/2021/Conference/Paper2539/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847239, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment"}}}, {"id": "tDR4UCG5Wfg", "original": null, "number": 2, "cdate": 1606211817508, "ddate": null, "tcdate": 1606211817508, "tmdate": 1606258036903, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment", "content": {"title": "General response: New experiments on RL application + test-time training baseline with discussion on differences", "comment": "Thanks for your reviews. Here we go over common comments and explain new results:\n\n**Differences w.r.t. prior work and extra experiments to elucidate these differences**\n\nFine-tuning has long been a popular idea to adapt to new distributions or new tasks. To adapt to new distributions or tasks at test time, when supervision is not available, multiple methods have proposed to fine-tune using unsupervised objectives. In particular, in \u201cTest time training(TTT) with self-supervised objectives for out-of-distribution(OOD) generalization\u201d, Sun et al. address the case of distribution shift between training and testing. Tailoring also optimizes an unsupervised objective, but on a different problem and for a different reason. We study the standard ML setting in which test and training share both task and distribution. There, one would think fine-tuning is not necessary because there is nothing to adapt. However, inspired by transductive learning, we show that it is useful to tailor our models to each input to encode inductive biases at prediction-time.\n\nThese conceptual differences have three practical consequences:\n\n1. **Whereas TTT only makes sense at test-time, when there is a different distribution, tailoring also makes sense at training time: meta-tailoring.** Moreover, this work shows theoretically and empirically that meta-tailoring is a better paradigm than tailoring. This is because, in meta-tailoring, training and test use the same predictive mechanism, which includes the tailoring step. \n\n2. **In contrast to prior work, which cares about adaptation, tailoring improves the standard ML setting; being widely applicable.** In experiments explained in point 3, we show TTT is not good in this same-distribution setting. Moreover, the theoretical guarantees of meta-tailoring are much stronger than TTT\u2019s. In particular,TTT has to make a posteriori assumptions about the gradients of the model once trained (which may not be true in the actual model). In contrast, ours are a priori: we ensure training will go in such a way that guarantees will be met. Note that, conversely, meta-tailoring is probably not the right approach for the out-of-distribution generalization, the setting of the TTT paper(see app. F).\n\n3. TTT proposes to adapt the model by optimizing unsupervised objectives on available samples from the test distribution. Tailoring instead proposes to customize the model to each individual input. **We made two experiments comparing both hypothesis:**\n    - In TTT, we care about adapting to the test distribution with any available samples. Therefore, the sample we use to adapt should be mostly irrelevant. However, we find this is not the case in the standard ML setting: **using the same sample $x$ to adapt and evaluate(tailoring) is significantly better than adapting on another test sample** $x'\\neq x$ before evaluating on $x$ (2.9% vs 7.6% improvement).\n    - In TTT, given more test samples, performance should increase and outperform tailoring. We show this is not the case when the distributions are the same: **even adapting with 6400 samples (a full batch, including the evaluated sample), performance of TTT is worse than tailoring(3.5% vs 7.6%  improvement)**. Meta-tailoring (36%) further increases this gap. We have updated table 1 and figure 1 to include it, although note that it is not a valid approach because test samples should be evaluated independently.\n\n**Lack of contrastive learning results:**\nSome reviewers pointed out the original text wasn\u2019t clear about the lack of contrastive experiments, we are sorry for the confusion. We used the word \u201capplications\u201d when we should have used \u201cdomains\u201d and over-compressed some sentences; we have now improved the wording. Making real experiments out of the theory has two big hurdles:\n- We carefully chose the contrastive loss to make the theory as clean and insightful as possible. In particular, the meta-tailoring bound is the same as the inductive bound just changing $\\theta_{S}$ to $\\theta_{x,S}$, easing comparisons. However, the crafted contrastive loss is hard to implement and computationally inefficient.\n- Contrastive semi-supervised learning is dominated by methods requiring big servers (MoCo, SimCLR) just to train one model, making experiments very expensive.\n\n**Addition of model-based RL results to the main text (section 5.3):** \nWe show the applicability of meta-tailoring to model based reinforcement learning (MBRL). We improve PDDM (Nagabandi et al.), a popular MBRL method. We reimplemented it from tensorflow to pytorch and tried the two complex domains from that paper. We only report one domain (dclaw), because we were unable to reproduce the results in the other domain using the base PDDM (both the original tensorflow and ours). We have been working with the PDDM authors to reproduce the other domain in order to add extra results in case of acceptance.\n\nWe believe these MBRL results further broaden the range of applicability of the tailoring paradigm."}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w8iCTOJvyD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2539/Authors|ICLR.cc/2021/Conference/Paper2539/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847239, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment"}}}, {"id": "dET5CBWAD05", "original": null, "number": 6, "cdate": 1606216255518, "ddate": null, "tcdate": 1606216255518, "tmdate": 1606216272383, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "tzXCvboDF-", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment", "content": {"title": "Details on runtime and inductive learning paradigm, more results in general answer", "comment": "Thanks for your review. We address your points in order, and note that the answer to all reviewers includes general comments on recurrent topics as well as new experiments.\n\n**Inefficient meta-tailoring vs CNGrad** Compute-wise the naive implementation of meta-tailoring (using MAML or similar algorithms) is significantly slower than CNGrad by a factor between 128 (batch for ImageNet) and 6400 (batch for planet experiments), making it far too slow to run (on the order of weeks to months). This is because it has to be run sequentially and the affine layers are very cheap computationally compared to linear layers. Result-wise, our theorem in section 4 guarantees that we have enough capacity to optimize the inner loop with the affine parameters alone. However, CNGrad still needs to optimize all parameters in the outer loop.\n\n**Classical inductive baseline & setting** Because inductive learning is classic machine learning (train model, freeze parameters, test), section 5.2 is comparing vanilla (inductive) RandomizedSmoothing vs. its meta-tailoring counterpart. We have added \u201cInductive\u201d in the table to make it clearer. This is also relevant to one of your comments, suggesting we add a paragraph on inductive learning in the intro. The paragraph was there (starting with \u201cIn classic supervised learning\u201c), but also didn\u2019t mention inductive learning. We have changed it to start with\u201cIn classic inductive supervised learning\u201d to make it clear.\n\n**Contrastive experiments & theoretical guarantees** In the general response to all reviewers, we address the confusion regarding the contrastive experiments and add more details on the theoretical guarantees. There, we also discuss new results on model-based RL.\n\n**BatchNorm statistics** \u201cDo you keep a running average of the batch-statistics for CNGrad at test time?\u201d CNGrad per se does not apply any normalization, only an affine transformation. Therefore, there is no need to keep any statistics. One can combine BatchNorm and CNGrad, handling the batch statistics of BatchNorm as per usual.\nTable 1 used MSE loss; we\u2019ve added that to the description.\n\n**\u201cRun-time implications of tailoring both at training and test-time\u201d**\nGood question. The detailed answer can be found on \u201cComputational cost\u201d of appendix D. In short, first, it depends on the number of tailoring steps linearly (often 1 is enough). Then, it depends on which layers are adapted in the inner loop: if we add affine transformations to all the layers in the network and do one tailoring step, performance will be 3x the usual prediction time (initial forward pass, tailoring adaptation, final forward pass). One can speed this up (which we did in the adversarial experiments) by only adapting the higher layers (which are usually the ones needing adaptation). This reduces the computational factor to $1+\\frac{2a}{L}$, where \u2018a\u2019 is the number of adapted layers and L the number of total layers. For instance, if we tailor the last 5 layers of a resnet-110 then we would get an increase only of $5\\cdot2/110 \\approx 9$ percent, (1.09x factor).\n\n**Color-blind-friendly plots** Good point on the colors for the plots, we have changed the red to light orange and made the green darker. We also checked with a color-blind simulator that they are distinguishable. Thanks for the suggestion."}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "w8iCTOJvyD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2539/Authors|ICLR.cc/2021/Conference/Paper2539/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847239, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Comment"}}}, {"id": "tzXCvboDF-", "original": null, "number": 1, "cdate": 1603795715204, "ddate": null, "tcdate": 1603795715204, "tmdate": 1605024188766, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review", "content": {"title": "Tailoring - An interesting approach for more powerful inductive biases", "review": "The paper proposes tailoring and meta-tailoring, learning processes that fine-tune the model parameters during test-time using unsupervised objectives. This allows for designing and integrating powerful inductive biases into the model, leading to an improved test-time performance in two example tasks.\n\nStrengths:\n* The proposed approaches are well-motivated, and the paper is written clearly.\n* CNGrad provides an elegant solution to implement tailoring efficiently.\n* The experimental results show the benefits of the proposed approach convincingly.\n\nWeaknesses:\n* While CNGrad provides an efficient implementation of tailoring, it would be interesting to see how it compares to its \u201cinefficient counterpart\u201d, in which no additional parameters are introduced, but the parameters w are optimized for each individual sample using the supervised and the tailoring objective simultaneously.\n\n* It would be nice to include the inductively trained model as a baseline to the experiment in section 5.2. This could highlight more clearly the benefit of tailoring when applied to adversarial examples.\n\n* I find the statement that the paper showed \u201cthe applicability of tailoring on three domains\u201d slightly misleading. The paper shows its applicability experimentally in two domains, and shows theoretical results for the third. Additionally, building on these theoretical results, it would be interesting to see how the contrastive loss might be used for tailoring in an experimental setting. \n\nQuestions:\n* How is the element-wise normalization in CNGrad performed at test-time? Do you keep a running average of the batch-statistics as is done in batch normalization?\n\n* Which loss was used for the results in Table 1? \n\n* What are the run-time implications of tailoring, both at train and test time?\n\nAdditional Comments:\n* Fig 1: Try to avoid using red and green as distinguishing colors to improve the paper\u2019s accessibility.\n\n* It might be nice to add a paragraph on inductive learning to the intro.\n\n\nDisclaimer: I did not check the provided proofs in detail.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094185, "tmdate": 1606915775157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2539/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review"}}}, {"id": "1AO51IDpSR", "original": null, "number": 2, "cdate": 1603854152669, "ddate": null, "tcdate": 1603854152669, "tmdate": 1605024188704, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review", "content": {"title": "A very interesting idea, but needs more empirical validation of its claims.", "review": "The authors propose a learning method called tailoring, inspired\nby transductive learning, which works\nby fine tuning a model on an unsupervised loss given a test time example\ninput. The benefit of this approach is that arbitrary constraints\non predictions can be imposed without a suffering from a generalization gap\nif the constraints were used as an auxiliary objective during training.\nThe authors also propose meta-tailor, which includes the tailoring process\nas an inner optimization loop during training. In a sense, meta-tailoring\nis like meta-learning but with  each training example considered a separate\ntask.\n\nThe authors provided extensive theoretical justification for tailoring\nand meta tailoring, as well as an efficient means to implement it through\nconditional normalization parameters.\nThey then perform two experiments, one where (meta-) tailoring\nis used to impose physical constraints on a neural network physics simulator\nand another where meta-tailoring is used to improve the robustness of an\nimage classifier to adversarial attacks.\n\n\nThe idea of (meta-)tailoring is quite intriguing and I could see it\napplied to scenarios in structured prediction or as an alternative to\nposterior regularization. However, this paper was very theory heavy and I must admit,\nI struggled to understand the import or necessity of the provided theorems.\nOne of the claims of this paper is that they demonstrate that improving\nprediction quality with contrastive learning, but this is only done\ntheoretically. As this claim is included in the list of experimental results,\nI find that somewhat disingenuous. I would have much preferred a \ncontrastive learning experiment.\n\nI lean to reject this paper.\n\n\n\nMiscellaneous note:\n\nIn the last paragraph of page 4, it is mentioned that parts of \ndefinition 1 and theorem 1 are in bold green, but I see no bold green in\nthis copy of the paper.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094185, "tmdate": 1606915775157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2539/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review"}}}, {"id": "Y921m80PMc", "original": null, "number": 3, "cdate": 1603900338901, "ddate": null, "tcdate": 1603900338901, "tmdate": 1605024188634, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review", "content": {"title": "Interesting idea, but the experiments are weak", "review": "This paper presents tailoring and meta-tailoring to eliminate the generalization gap by optimizing at test time. This paper combines the idea of test time optimization and meta-learning and provides some theoretical analysis of the proposed methods. Experiments show the proposed method is effective in solving the machine learning problem and improving model robustness.\n\nStrengths:\n- The idea of using meta-learning to improve tailoring is interesting. Some theoretical justification for the proposed method is provided.\n- The explanation of different learning settings is insightful.\n\nWeaknesses:\n- The proposed tailoring method is very close to Test Time Training (Sun et al. , 2019). However, this paper fails to clearly show the differences between tailoring and TTT. There is also no theoretical or empirical evidence to show that tailoring/meta-tailoring is better than TTT.\n- The experiments in this paper are quite weak. The descriptions of the experiment settings are unclear. There is no direct comparison with closely related methods like TTT. The results of widely used benchmarks are not provided. \n\nOverall, although I think the idea is interesting, the proposed method is not well verified, which makes the effectiveness of the method is still unclear. Some closely related work is not sufficiently discussed. Therefore, I lean to recommend rejection for this paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094185, "tmdate": 1606915775157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2539/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review"}}}, {"id": "epS5O7i3Zeq", "original": null, "number": 4, "cdate": 1603954587668, "ddate": null, "tcdate": 1603954587668, "tmdate": 1605024188570, "tddate": null, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "invitation": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review", "content": {"title": "Very good paper with strong theoretical analysis", "review": "==========\nSummary \n\nThis paper proposes a meta-tailoring method where the auxiliary tailor loss is used to adapt the model parameters at test time.  The paper also provides a theoretical analysis of the advantages of the proposed tailoring/meta-tailoring.  The proposed method is evaluated on various application tasks and obtain significant improvements over baselines. \n\n==========\nPros\n\n1. Test-time training is getting increasing attention recently, however, the theoretical analysis of the advantages of test time training is still behind. This paper provides a reasonable theoretical analysis of tailoring and meta-tailoring. \n2. The proposed CNGrad approach is simple but effective, which provides strong empirical improvements on various tasks. \n3. The paper is well written and clearly organized. \n\n===========\nConcerns/confusions\n\n1.  what is the formulation of the affine parameters?  Is there any study about adapting the entire model versus the affine parameters only?\n\n2. Missing contrastive learning experiments on images?  In the last sentence of the abstract, it says \"..., and using contrastive losses on the query image to improve generalization\". However, I didn't find a discussion on it in the main paper.  Given that previous work (e.g., Sun et al.) has done experiments on image classification tasks, it would be more compelling to provide a comparison in the same setting. \n\n\n======\n\nIn general, this paper proposes an interesting approach and insightful analysis of the proposed method.  I'm willing to upgrade my score with comparable experiments with prior work on the image classification task. \n\n\n\n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2539/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2539/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time", "authorids": ["~Ferran_Alet1", "~Kenji_Kawaguchi1", "~Maria_Bauza_Villalonga1", "ngkuru@mit.edu", "~Tomas_Perez1", "~Leslie_Pack_Kaelbling1"], "authors": ["Ferran Alet", "Kenji Kawaguchi", "Maria Bauza Villalonga", "Nurullah Giray Kuru", "Tomas Perez", "Leslie Pack Kaelbling"], "keywords": ["inductive biases", "meta-learning", "semi-supervised learning", "adversarial learning", "representation learning", "transductive learning"], "abstract": "From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.", "one-sentence_summary": "New framework to encode inductive biases as unsupervised losses optimized at prediction time and show, theoretically and empirically, its usefulness in a wide range of domains(contrastive learning, physics, adversarial robustness, and MBRL).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "alet|tailoring_encoding_inductive_biases_by_optimizing_unsupervised_objectives_at_prediction_time", "supplementary_material": "/attachment/b3479357e3ac1ff49b5a179c4b6731bfab909404.zip", "pdf": "/pdf/282821c941bc5b0d3a400c4cf3ac26de89202246.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=nolV0Wxfy", "_bibtex": "@misc{\nalet2021tailoring,\ntitle={Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time},\nauthor={Ferran Alet and Kenji Kawaguchi and Maria Bauza Villalonga and Nurullah Giray Kuru and Tomas Perez and Leslie Pack Kaelbling},\nyear={2021},\nurl={https://openreview.net/forum?id=w8iCTOJvyD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "w8iCTOJvyD", "replyto": "w8iCTOJvyD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538094185, "tmdate": 1606915775157, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2539/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2539/-/Official_Review"}}}], "count": 11}