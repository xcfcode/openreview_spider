{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458150880676, "tcdate": 1458150880676, "id": "vlpOAAvVMh7OYLG5inyv", "invitation": "ICLR.cc/2016/workshop/-/paper/125/comment", "forum": "91EowxONgIkRlNvXUVog", "replyto": "4QygYX3XwhBYD9yOFqMA", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"title": "Response to review 12", "comment": "Thank you for your helpful comments.\nWe would like to clarify that for the results in Table 1, row 1 is all unidirectional, row 3 is all bidirectional, and row 2 is all unidirectional except for the last layer.\nThank you for your suggestion of an additional type of networks (all lookahead convolution), we will consider this.\nWe note that this network architecture would introduce additional delays for deep networks and large tau compared to our proposed architecture (all unidirectional except for the last layer).\nSince each lookahead layer needs to wait tau steps, we can only compute the first output after waiting tau + (tau-1)(depth-2) steps (instead of tau steps)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been shown to be very effective for many\nsequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others.\nThe best variant is typically a bidirectional RNN that learns\nrepresentation for a sequence by performing a forward and a backward pass through the entire sequence.\nHowever, unlike unidirectional RNNs, bidirectional RNNs\nare challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system),\nbecause they need to see an entire sequence before making a prediction.\nWe introduce a lookahead convolution layer that incorporates information from future subsequences\nin a computationally efficient manner to improve unidirectional recurrent neural networks.\nWe evaluate our method on speech recognition tasks for two languages---English and Chinese.\nOur experiments show that the proposed method outperforms vanilla unidirectional\nRNNs and is competitive with bidirectional RNNs in terms of character and word error rates.", "pdf": "/pdf/91EowxONgIkRlNvXUVog.pdf", "paperhash": "wang|lookahead_convolution_layer_for_unidirectional_recurrent_neural_networks", "conflicts": ["baidu.com"], "authorids": ["dyogatama@cs.cmu.edu"], "authors": ["Chong Wang", "Dani Yogatama", "Adam Coates", "Tony Han", "Awni Hannun", "Bo Xiao"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824058805, "ddate": null, "super": null, "final": null, "tcdate": 1455824058805, "id": "ICLR.cc/2016/workshop/-/paper/125/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "91EowxONgIkRlNvXUVog", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/125/reviewer"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458068507740, "tcdate": 1458068507740, "id": "4QygYX3XwhBYD9yOFqMA", "invitation": "ICLR.cc/2016/workshop/-/paper/125/review/12", "forum": "91EowxONgIkRlNvXUVog", "replyto": "91EowxONgIkRlNvXUVog", "signatures": ["~Navdeep_Jaitly1"], "readers": ["everyone"], "writers": ["~Navdeep_Jaitly1"], "content": {"title": "Simple idea, some details are unclear that make it difficult to assess gains.", "rating": "6: Marginally above acceptance threshold", "review": "Instead of computing the output for a unidirectional RNN at time point t using only the hidden state of the layer below at the same time, the paper proposes to use point-wise multiplication and addition from future states of a unidirectional RNN to compute the hidden states at the next layer (this is akin to a separate convolution on each of the feature dimensions). This, the authors argue gets it closer to bidirectional RNNs.\n\nThe idea is simple, and the paper seems to show results that there are gains from using the approach, but important details are missing that make it hard to judge whether the gains come from the model or not. \n\nSpecifically, the authors say on page that \"The next five layers are either all unidirectional (forward) or all bidirectional recurrent layers\" From this I would assume that row 1 of the paper is all unidirectional, and row 3 is all bidirectional, while row 2 is all unidirectional, except for the last layer which is a \"look-ahead convolution\" If that's the case the results are good. \n\nHowever, the next lines \"We also compare with two baselines constructed by replacing the second-to-last layer with either a unidirectional recurrent layer or bidirectional recurrent layer\", make we wonder if this is really the case; the statement leaves open the possibility that Row 1 is bidirectional all the way, and then unidirectional, followed by the softmax, while Row 2 is bidirectional all the way and then a look-ahead convolutional layer etc... this result would be less convincing since it does get bidirectional inputs to the top layer..\n\nAn obvious comparison would have been unidirectional all the way, look-ahead convolutional all the way and bidirectional all the way. I'm surprised this isn't the one that is offered. And if it is indeed the one that is offered, the paper should writh the model section in such a way that its clearer.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been shown to be very effective for many\nsequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others.\nThe best variant is typically a bidirectional RNN that learns\nrepresentation for a sequence by performing a forward and a backward pass through the entire sequence.\nHowever, unlike unidirectional RNNs, bidirectional RNNs\nare challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system),\nbecause they need to see an entire sequence before making a prediction.\nWe introduce a lookahead convolution layer that incorporates information from future subsequences\nin a computationally efficient manner to improve unidirectional recurrent neural networks.\nWe evaluate our method on speech recognition tasks for two languages---English and Chinese.\nOur experiments show that the proposed method outperforms vanilla unidirectional\nRNNs and is competitive with bidirectional RNNs in terms of character and word error rates.", "pdf": "/pdf/91EowxONgIkRlNvXUVog.pdf", "paperhash": "wang|lookahead_convolution_layer_for_unidirectional_recurrent_neural_networks", "conflicts": ["baidu.com"], "authorids": ["dyogatama@cs.cmu.edu"], "authors": ["Chong Wang", "Dani Yogatama", "Adam Coates", "Tony Han", "Awni Hannun", "Bo Xiao"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580011037, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580011037, "id": "ICLR.cc/2016/workshop/-/paper/125/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "91EowxONgIkRlNvXUVog", "replyto": "91EowxONgIkRlNvXUVog", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/125/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458066643770, "tcdate": 1458066643770, "id": "K1VM5mjK2C28XMlNCVGP", "invitation": "ICLR.cc/2016/workshop/-/paper/125/review/11", "forum": "91EowxONgIkRlNvXUVog", "replyto": "91EowxONgIkRlNvXUVog", "signatures": ["ICLR.cc/2016/workshop/paper/125/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/125/reviewer/11"], "content": {"title": "Simple and useful concept, clear writeup, limited experiments", "rating": "6: Marginally above acceptance threshold", "review": "A clear description of the so called \"convolutional lookahead\" for RNNs in order to incorporate small windows of future context information in a similar fashion to bidirectional RNN, but in a way amenable to streaming decode.\n\nThe primary drawback of this paper is the limited experimental section - it would have been great to see more comparison over various settings of `tau`, ideally showing convergence to the full bidirectional RNN solution with larger and larger settings - the authors mention other experiments (\"increasing future context size did not close the gap\", conclusion), but fail to show them here. One other experiment of interest would be to see the performance limitations of only using the convolutional lookahead, either by making the network use a single recurrent layer (bidirectional vs lookahead) in a fashion similar to Deep Speech 1, or making *all* layers use convolutional lookahead. Also showing the experiments in which \"using a regular\nconvolution layer with multiple filters resulted in poor performance\" due to overfitting would be useful - perhaps the gap between them is due to capacity limitations in the lookahead?\n\nAdditionally, the paper mentions \"We note that much better performance can be\nobtained for both datasets by using a more powerful language model or more training data. We have\nobserved that in both cases the improvements from the lookahead convolution layer are consistent\nwith the smaller scale experiments shown here.\" - it would be good to actually *see* these experiments in a table or description, rather than an offhand comment.\n\nMore experiments are always interesting, and actually showing the experiments mentioned in passing in the text would be even better, but the paper as it stands is already a \"minimum viable paper\" for workshop purposes. It clearly displays a particular technique, its uses, and some drawbacks and performance issues in application. Some of the mentioned experiments above are also described as \"future work\", so it is clear the authors know these are interesting directions of exploration, and ideally a subset of those results can make the workshop paper.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been shown to be very effective for many\nsequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others.\nThe best variant is typically a bidirectional RNN that learns\nrepresentation for a sequence by performing a forward and a backward pass through the entire sequence.\nHowever, unlike unidirectional RNNs, bidirectional RNNs\nare challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system),\nbecause they need to see an entire sequence before making a prediction.\nWe introduce a lookahead convolution layer that incorporates information from future subsequences\nin a computationally efficient manner to improve unidirectional recurrent neural networks.\nWe evaluate our method on speech recognition tasks for two languages---English and Chinese.\nOur experiments show that the proposed method outperforms vanilla unidirectional\nRNNs and is competitive with bidirectional RNNs in terms of character and word error rates.", "pdf": "/pdf/91EowxONgIkRlNvXUVog.pdf", "paperhash": "wang|lookahead_convolution_layer_for_unidirectional_recurrent_neural_networks", "conflicts": ["baidu.com"], "authorids": ["dyogatama@cs.cmu.edu"], "authors": ["Chong Wang", "Dani Yogatama", "Adam Coates", "Tony Han", "Awni Hannun", "Bo Xiao"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580011712, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580011712, "id": "ICLR.cc/2016/workshop/-/paper/125/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "91EowxONgIkRlNvXUVog", "replyto": "91EowxONgIkRlNvXUVog", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/125/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455824057916, "tcdate": 1455824057916, "id": "91EowxONgIkRlNvXUVog", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "91EowxONgIkRlNvXUVog", "signatures": ["~Dani_Yogatama1"], "readers": ["everyone"], "writers": ["~Dani_Yogatama1"], "content": {"CMT_id": "", "title": "Lookahead Convolution Layer for Unidirectional Recurrent Neural Networks", "abstract": "Recurrent neural networks (RNNs) have been shown to be very effective for many\nsequential prediction problems such as speech recognition, machine translation, part-of-speech tagging, and others.\nThe best variant is typically a bidirectional RNN that learns\nrepresentation for a sequence by performing a forward and a backward pass through the entire sequence.\nHowever, unlike unidirectional RNNs, bidirectional RNNs\nare challenging to deploy in an online and low-latency setting (e.g., in a speech recognition system),\nbecause they need to see an entire sequence before making a prediction.\nWe introduce a lookahead convolution layer that incorporates information from future subsequences\nin a computationally efficient manner to improve unidirectional recurrent neural networks.\nWe evaluate our method on speech recognition tasks for two languages---English and Chinese.\nOur experiments show that the proposed method outperforms vanilla unidirectional\nRNNs and is competitive with bidirectional RNNs in terms of character and word error rates.", "pdf": "/pdf/91EowxONgIkRlNvXUVog.pdf", "paperhash": "wang|lookahead_convolution_layer_for_unidirectional_recurrent_neural_networks", "conflicts": ["baidu.com"], "authorids": ["dyogatama@cs.cmu.edu"], "authors": ["Chong Wang", "Dani Yogatama", "Adam Coates", "Tony Han", "Awni Hannun", "Bo Xiao"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}