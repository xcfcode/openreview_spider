{"notes": [{"id": "VwU1lyi5nzb", "original": "18PrWPG27A", "number": 1193, "cdate": 1601308133784, "ddate": null, "tcdate": 1601308133784, "tmdate": 1614985630003, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IwMZB2qW5MI", "original": null, "number": 1, "cdate": 1610040530425, "ddate": null, "tcdate": 1610040530425, "tmdate": 1610474139907, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "invitation": "ICLR.cc/2021/Conference/Paper1193/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper is not ready for a publication at ICLR, as agreed unanimously by the reviewers. \n\nThere are three main reasons for that:\n1. Novelty: it is mentioned in the paper that \u201cTo the best of our knowledge, a multi-span QA architecture has not been proposed\", which is certainly incorrect. See the multiple references provided by the reviewers.\n2. Evaluation: there is no evaluation in multi-span setting on a public dataset. SQuAD being single span, As stated by R1, \"Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation.\" Several public datasets could be used like DROP, Quoref, or Natural Questions.\n3. Motivation: the reviewers also note that the clarity and motivation behind the work could be improved. Some choices of the architecture or the model should be more clearly justified.\n\nWe encourage the authors to look into the multiple comments in the reviews in order to improve the paper and the research project overall.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040530412, "tmdate": 1610474139891, "id": "ICLR.cc/2021/Conference/Paper1193/-/Decision"}}}, {"id": "IAU0cnsAFB1", "original": null, "number": 1, "cdate": 1603386876246, "ddate": null, "tcdate": 1603386876246, "tmdate": 1605024506616, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "invitation": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review", "content": {"title": "Blind Review", "review": "#### Summary\nThe paper proposes a novel method for predicting multiple answer spans in question-answering (QA) tasks. When the Span-Image technique is applied to a base BERT model, the authors show performance gains on a single-span dataset (SQuAD) and substantial improvements on a multi-span dataset (an internal Amazon dataset). The authors propose that the method is both model-agnostic and can eliminate common post-processing steps via built-in architecture design choices.\n#### Positives\n- The authors show significant improvement in top-K accuracy on the SQuAD dataset, highlighting their technique\u2019s ability to hone in on multiple relevant spans.\n- The authors show that the method works (either does not degrade performance or indeed improves performance) for both single-span and multi-span datasets.\n- The technique is not too complex and is model agnostic (does not require BERT models to function)\n- The paper is well-written, with clear descriptions of baseline architectures and their new Span-Image architecture and appropriate accompanying diagrams and results tables.\n\n#### Negatives\n- The authors only compare to one baseline, BERT-QA, and do not fully motivate how it is state-of-the-art on the respective datasets considered; a few other observations call into question the generalizability of the method:\n    - The authors claim that the technique is modular and can be applied to other architectures, however only one architecture is considered.\n    - The authors also claim that the method is amenable to specifically designed channels to eliminate post-processing, however only one such channel is considered.\n- A central claim is that the method works better for multi-span question answering, however the authors only consider one true multi-span dataset (an internal dataset). There are other multi-span \u201creading comprehension\u201d datasets in the literature (DROP[1], Quoref[2]) that would probably have been appropriate datasets to which the authors could have applied their technique.\n- The significant performance gains of the authors\u2019 technique on the multi-span dataset are mitigated almost entirely by adding an additional length penalty to the base BERT-QA model, raising the question about whether the authors\u2019 technique itself is what effects better performance (rather than, say, the explicit length penalty channel in the architecture)\n\n#### Decision\nI think the paper is marginally below the acceptance threshold. The method is novel and explainable, and appears to improve performance on the two datasets over the BERT QA baseline. However, the method is not compared to other QA architectures (is BERT QA SOTA on SQuAD?), the gains do not appear to be entirely significant (especially on the multi-span dataset, where the performance gap is reduced significantly by adding a length penalty to the baseline model), and various proposed contributions are not expanded upon (e.g. the modularity of the architecture is not demonstrated). Ultimately, the paper could be improved by further exploration of the method, whether via application to new datasets or more in depth analysis of the generalizability of the method.\n\n#### Questions\nMostly curious about the author\u2019s responses to the questions posited above, and in addition:\n1. Did the authors consider the RC datasets specified above (DROP, Quoref), or is there a reason they were omitted.\n2. Did the authors try applying their method to other DNN architectures?\n3. Did the authors consider other image channels to eliminate post-processing?\n4. Did the authors try adding a specific span length penalty term to the Span-Image network as well? \n\n#### Cites\n[1] Dua, M. (2019). DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 2368\u20132378). Association for Computational Linguistics.\n[2] Pradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, & Matt Gardner (2019). Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning. In Proc. of EMNLP-IJCNLP.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1193/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1193/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1193/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124505, "tmdate": 1606915808571, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1193/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review"}}}, {"id": "fEzTo8YBIkR", "original": null, "number": 2, "cdate": 1603781176772, "ddate": null, "tcdate": 1603781176772, "tmdate": 1605024506552, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "invitation": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes a new framework to inherently break the independence assumption of start-end decision making in extractive question answering models. The general idea is to expand the current point-to-point product to a 3-D convolution. Since each point absorbs information within the sliding window, the probability (i, j) does not solely depend on $h_i, h_j$. The basic motivation makes sense to me, but I still have the following concerns about the paper:\n1) the proposed convolutional network though incorporates more information, it's constrained to be the nearest information in its sliding window, let's say 2 or 3, and the probability of p(i, j) thus depends on $h_{i-2:i+2}, h_{j-2:j+2}$. However, I don't think this modification will help the model make a better decision since the nearest surrounding words' information is already well encoded in the lower-layers of the transformer. The errors the model made are mainly due to a lack of global reasoning, the more further-away context is actually a critical issue for breaking the bottleneck. The proposed span-image framework doesn't seem to touch on the core problem in the current QA datasets.\n2) the proposed method obtains similar or even worse performance on top-answer selection, this point kind of reflects my previous concern. Adding more neighboring word context does not help the model make a wiser decision, and thus the final scores remain similar to the standard BERT QA model.\n3) the results in in-house datasets are not quite convincing. I think you can add some heuristics to the BERT QA answer span selection to improve its top-k results like adding diversity, prevent overlapping, etc. I'm not sure whether the gap will still remain as significant as reported in the paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1193/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1193/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1193/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124505, "tmdate": 1606915808571, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1193/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review"}}}, {"id": "rO6dl5aZwid", "original": null, "number": 3, "cdate": 1603927578915, "ddate": null, "tcdate": 1603927578915, "tmdate": 1605024506485, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "invitation": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review", "content": {"title": "Not good enough for ICLR standard", "review": "This paper proposes a neural model called Span-Image Network that allows the model to output multiple spans as answers (existing BERT-based QA models usually only output two endpoints of the answer span). The model is evaluated on SQuAD (top-K answer prediction) and an internal Amazon dataset.\n\nUnfortunately, it is clear that this paper doesn\u2019t meet the standard of the ICLR publications. I can\u2019t recommend the acceptance of the paper.\n\nFirst of all, it is mentioned in the paper that \u201cTo the best of our knowledge, a multi-span QA architecture has not been proposed.\u201d This is certainly incorrect. There have been many models proposed recently to tackle the multi-span QA problemm.  Some examples include:\n\n- (Hu et al., EMNLP 2019): A Multi-type Multi-span Network for Reading Comprehension that Requires Discrete Reasoning\n- (Segal et al, EMNLP 2020): A Simple and Effective Model for Answering Multi-span Questions\n- (Andor et al., EMNLP 2019): Giving BERT a Calculator: Finding Operations and Arguments with Reading Comprehension => This one is not straightforward but they support merging single spans at the end\n\nIt is very natural to cast a multi-span QA problem as a sequence tagging task and this paper doesn\u2019t have such a baseline to compare with.\n\nSecond, I think the model is not motivated well and the writing of the paper needs to be improved. Why is it called a span-image network? Is it because there is a CNN layer applied? What is the rationale behind that? It is really not an image of pixels. It is just an N by N matrix, and each entry is an elementwise-multiplication of the BERT hidden vectors.\n\nThird, the evaluation of the paper also should be improved. SQuAD is a single-span QA dataset and using top-K prediction looks quite artificial and unnatural. Also, the performance on SQuAD 2 is also lower than expected. For a BERT-base-uncased model, it is expected to achieve at least ~75 F1 on SQuAD 2 so even the baselines don\u2019t seem to be right. The internal Amazon dataset lacks details so I can\u2019t comment much on that.  There are indeed several multi-span QA datasets (e.g., DROP, Quoref, Natural Questions) and I think the paper should experiment with those datasets and compare to previous approaches. \n", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1193/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1193/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1193/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124505, "tmdate": 1606915808571, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1193/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review"}}}, {"id": "S3YsLI8JsaC", "original": null, "number": 4, "cdate": 1603952116905, "ddate": null, "tcdate": 1603952116905, "tmdate": 1605024506416, "tddate": null, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "invitation": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review", "content": {"title": "Not ready for publication - missing important literatures, limited empirical justification, etc", "review": "This paper introduces a new QA model based on BERT, which is called Span-Image Network. The paper first points out that previous span extraction models model independent probability of the start and the end of the span, making the extension to multi-span extraction harder. Span-Image Network model the joint probability of the start and the end by deploying a 2-D convolution, enabling multi-span extraction.\n\nI think the paper is not ready for publication for a few reasons.\n\nFirst, the limitation of independent modeling of the start and the end of the span has been reported in the literature, and there have been a few works that use joint probability instead. I will only list some of them: work in QA [1] [2] as well as QA baselines for pretrained LMs [3][4][5]; [6], which is concurrent work to thsi work, includes detailed survey. Such literature has not been mentioned or discussed in this manuscript.\n\nSecond, although \u201cmulti\u201d span extraction is the main motivation for this work, as described in the Introduction, there is no evaluation on public dataset. Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation. The only experiment on public dataset is SQuAD which does not require multi-answer extraction and the proposed model does not show superior results compared to the baseline. (The top-k experiment in the paper is not convincing - it is synthetic and does not align with the goal of predicting multiple answers.) If the authors want to include public datasets for multi-span extraction, they can consider multi-answer portions of Natural Questions dataset [7] or AmbigQA dataset [8].\n\nThird, as mentioned above, calculating alignment of the start and end position has been already incorporated in the previous work (such as [2]). The convolutional component is the one I have not seen in the previous literature - however, as there is no ablation with and without convolution, its effect is not shown in the paper.\n\nFourth, even with Amazon internal dataset, the gains are not significant compared to bert baseline with postprocessing. Based on Table 5, the baseline\u2019s best EM is 88.5, whereas the best performance of the proposed model in Table 4 is 89.1. In fact, the baseline number in Table 4 is not the best number of the baseline, which will mislead the audience to believe that the gap is significant - another major issue of this paper.\n\nLastly, this is a minor point, but I believe \u201cSpan-Image\u201d in the name of the model is largely misleading. There is no \u201cimage\u201d involved in the model architecture or training. Something like \u201c2D\u201d or \u201cconvolution\u201d might be a better term.\n\n\n[1] Lee et al. Learning recurrent span extractions for extractive question answering. 2016. \n[2] Seo et al. Real-time open-domain question answering with dense-sparse phrase index. 2019.\n[3] Yang et al. Xlnet: Generalized autoregressive pretraining for language understanding. 2019.\n[4] Lan et al. Albert: a lite bert for self-supervised learning of language representation. 2019.\n[5] Clark et al. Electra: pre-training text encoders as discriminators rather than generators. 2020.\n[6] Fajcik et al. Rethinking the objectives of extractive question answering. 2020.\n[7] Kwiatkowski et al. Natural questions: a benchmark for question answering research. 2019.\n[8] Min et al. Ambigqa: answering ambiguous open-domain questions. 2020.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1193/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1193/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MULTI-SPAN QUESTION ANSWERING USING SPAN-IMAGE NETWORK", "authorids": ["aricit@amazon.com", "~Hayreddin_Ceker1", "ismailt@amazon.com"], "authors": ["Tarik Arici", "Hayreddin Ceker", "Ismail Baha Tutar"], "keywords": ["BERT", "deep learning", "multi-span answer", "question-answering", "SQuAD", "transformers"], "abstract": "Question-answering (QA) models aim to find an answer given a question and con- text. Language models like BERT are used to associate question and context to find an answer span. Prior art on QA focuses on finding the best answer. There is a need for multi-span QA models to output the top-K likely answers to questions such as \"Which companies Elon Musk started?\" or \"What factors cause global warming?\" In this work, we introduce Span-Image architecture that can learn to identify multiple answers in a context for a given question. This architecture can incorporate prior information about the span length distribution or valid span patterns (e.g., end index has to be larger than start index), thus eliminating the need for post-processing. Span-Image architecture outperforms the state-of-the-art in top-K answer accuracy on SQuAD dataset and in multi-span answer accuracy on an Amazon internal dataset.\n", "one-sentence_summary": "We build multi-span question-answering models to output the top-N likely answers to questions instead of one answer using SQuAD dataset and Amazon internal dataset.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "arici|multispan_question_answering_using_spanimage_network", "pdf": "/pdf/70e8f9b5c609c97d049eb13dd42127ca42720d4e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QxGaN_qJM6", "_bibtex": "@misc{\narici2021multispan,\ntitle={{\\{}MULTI{\\}}-{\\{}SPAN{\\}} {\\{}QUESTION{\\}} {\\{}ANSWERING{\\}} {\\{}USING{\\}} {\\{}SPAN{\\}}-{\\{}IMAGE{\\}} {\\{}NETWORK{\\}}},\nauthor={Tarik Arici and Hayreddin Ceker and Ismail Baha Tutar},\nyear={2021},\nurl={https://openreview.net/forum?id=VwU1lyi5nzb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VwU1lyi5nzb", "replyto": "VwU1lyi5nzb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1193/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124505, "tmdate": 1606915808571, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1193/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1193/-/Official_Review"}}}], "count": 6}