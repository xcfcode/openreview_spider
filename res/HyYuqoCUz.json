{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582948959, "tcdate": 1520241423770, "number": 1, "cdate": 1520241423770, "id": "HydGoF9uG", "invitation": "ICLR.cc/2018/Workshop/-/Paper90/Official_Review", "forum": "HyYuqoCUz", "replyto": "HyYuqoCUz", "signatures": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer3"], "content": {"title": "Interesting model for few-shot learning but partly unclear; Comparison to other few-shot learning models missing.", "rating": "6: Marginally above acceptance threshold", "review": "The authors present a simple neural network architecture inspired by the moth's olfactory system. They show that, when combined with Hebbian type learning rules in a supervised setup it can perform few-shot learning. \nIn particular, they show that it can learn to classify MNIST digits with an accuracy of 75 % to 85 % on test digits after training on 1 to 20 samples per class. Performance is compared to standard classifiers (SVMs, nearest neighbor classifiers, and convolutional neural networks)\n\nIn general, the paper is interesting, showing that a quite simple neural network can achieve reasonable performance in few-shot learning. It is hard to judge how well the network performs, as it is only compared to methods that are not optimized for few-shot learning. A comparison with few-shot learning models or meta-learning methods seems necessary.\nAlso, while the model is described in quite some detail, the description is unclear at several places, including one detail that seems important to judge the achievements of the model.\nThe writing could be improved regarding mathematical notation and clarity.\nA more detailed analysis of the model is missing. It would be good to study what components of the model are essential in order to achieve the reported results.\n\nSome details:\np.3:\n- What is the reference set V? This seems important. Does it consist of examples that have been used for training or is it an additional set. If the latter, how large is it? If it is the latter, the conclusions are questionable, as one actually needs more examples from the dataset than stated.\np.2:\n- I would not call (1) \"integrate-and-fire dynamics\" as there is no firing. It is rather a (stochastic) leaky integrator dynamics.\n- Indicate the variable over which the sum runs in (1). \n- Is the range of the sigmoid S non-negative or are negative outputs possible?\n- In (1), weights have a single index, in (2) and (3) they have two indices.\n- Training data: The transformation of the training data is unclear. What does \"killed negative values\" mean?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A moth brain learns to read MNIST", "abstract": "We seek to characterize the learning tools (ie algorithmic components) used in biological neural networks, in order to port them to the machine learning context. In particular we address the regime of very few training samples.\nThe Moth Olfactory Network is among the simplest biological neural systems that can learn. We assigned  a computational model of the Moth Olfactory Network the  task of classifying the MNIST digits. The moth brain successfully learned to read given very few training samples (1 to 20 samples per class). In  this few-samples regime the moth brain substantially outperformed standard ML methods such as Nearest-neighbors, SVM, and CNN.\nOur experiments elucidate biological mechanisms for fast learning that rely on cascaded networks, competitive inhibition, sparsity, and Hebbian plasticity. These biological algorithmic components represent a novel, alternative toolkit for building neural nets that may offer a valuable complement to standard  neural nets. ", "pdf": "/pdf/9228a75e1b76bd8c88d85856ccf60cf45930ac22.pdf", "TL;DR": "A moth brain model can learn the MNIST digits, and out-performs ML methods in the few-training-samples regime.", "paperhash": "delahunt|a_moth_brain_learns_to_read_mnist", "keywords": ["machine learning", "neural nets", "neuroscience", "sparsity"], "authors": ["Charles B. Delahunt", "J. Nathan Kutz"], "authorids": ["delahunt@uw.edu", "kutz@uw.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582948729, "id": "ICLR.cc/2018/Workshop/-/Paper90/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper90/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper90/AnonReviewer2"], "reply": {"forum": "HyYuqoCUz", "replyto": "HyYuqoCUz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper90/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper90/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582948729}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582825113, "tcdate": 1520609799846, "number": 2, "cdate": 1520609799846, "id": "HkezqQlYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper90/Official_Review", "forum": "HyYuqoCUz", "replyto": "HyYuqoCUz", "signatures": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer2"], "content": {"title": "Review: A moth brain learns to read MNIST", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors present an application of a recently developed firing rate model of the Moth Olfactory Network (MothNet) to MNIST. The authors introduce the MothNet model, plasticity, training data, and their classifier. They then go on to show that the MothNet model can classify MNIST digits well in the low training samples regime.\n\nOverall, the results are interesting but incomplete. Why does the accuracy testing for the MothNet stop at 20 samples? If the other ML methods (e.g. CNN) outperform this algorithm, it is important to show this directly and explain the results. The fast learning performance of the two MothNet versions tested is interesting, but the fact that the learning stops after essentially one sample is a problem for the algorithm. A more general understanding of the model and the results would perhaps lead to a more clear workshop proposal.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A moth brain learns to read MNIST", "abstract": "We seek to characterize the learning tools (ie algorithmic components) used in biological neural networks, in order to port them to the machine learning context. In particular we address the regime of very few training samples.\nThe Moth Olfactory Network is among the simplest biological neural systems that can learn. We assigned  a computational model of the Moth Olfactory Network the  task of classifying the MNIST digits. The moth brain successfully learned to read given very few training samples (1 to 20 samples per class). In  this few-samples regime the moth brain substantially outperformed standard ML methods such as Nearest-neighbors, SVM, and CNN.\nOur experiments elucidate biological mechanisms for fast learning that rely on cascaded networks, competitive inhibition, sparsity, and Hebbian plasticity. These biological algorithmic components represent a novel, alternative toolkit for building neural nets that may offer a valuable complement to standard  neural nets. ", "pdf": "/pdf/9228a75e1b76bd8c88d85856ccf60cf45930ac22.pdf", "TL;DR": "A moth brain model can learn the MNIST digits, and out-performs ML methods in the few-training-samples regime.", "paperhash": "delahunt|a_moth_brain_learns_to_read_mnist", "keywords": ["machine learning", "neural nets", "neuroscience", "sparsity"], "authors": ["Charles B. Delahunt", "J. Nathan Kutz"], "authorids": ["delahunt@uw.edu", "kutz@uw.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582948729, "id": "ICLR.cc/2018/Workshop/-/Paper90/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper90/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper90/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper90/AnonReviewer2"], "reply": {"forum": "HyYuqoCUz", "replyto": "HyYuqoCUz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper90/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper90/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582948729}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573579080, "tcdate": 1521573579080, "number": 153, "cdate": 1521573578745, "id": "Sk7A0AAKM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyYuqoCUz", "replyto": "HyYuqoCUz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A moth brain learns to read MNIST", "abstract": "We seek to characterize the learning tools (ie algorithmic components) used in biological neural networks, in order to port them to the machine learning context. In particular we address the regime of very few training samples.\nThe Moth Olfactory Network is among the simplest biological neural systems that can learn. We assigned  a computational model of the Moth Olfactory Network the  task of classifying the MNIST digits. The moth brain successfully learned to read given very few training samples (1 to 20 samples per class). In  this few-samples regime the moth brain substantially outperformed standard ML methods such as Nearest-neighbors, SVM, and CNN.\nOur experiments elucidate biological mechanisms for fast learning that rely on cascaded networks, competitive inhibition, sparsity, and Hebbian plasticity. These biological algorithmic components represent a novel, alternative toolkit for building neural nets that may offer a valuable complement to standard  neural nets. ", "pdf": "/pdf/9228a75e1b76bd8c88d85856ccf60cf45930ac22.pdf", "TL;DR": "A moth brain model can learn the MNIST digits, and out-performs ML methods in the few-training-samples regime.", "paperhash": "delahunt|a_moth_brain_learns_to_read_mnist", "keywords": ["machine learning", "neural nets", "neuroscience", "sparsity"], "authors": ["Charles B. Delahunt", "J. Nathan Kutz"], "authorids": ["delahunt@uw.edu", "kutz@uw.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518414448849, "tcdate": 1518414448849, "number": 90, "cdate": 1518414448849, "id": "HyYuqoCUz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyYuqoCUz", "signatures": ["~Charles_B_Delahunt1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "A moth brain learns to read MNIST", "abstract": "We seek to characterize the learning tools (ie algorithmic components) used in biological neural networks, in order to port them to the machine learning context. In particular we address the regime of very few training samples.\nThe Moth Olfactory Network is among the simplest biological neural systems that can learn. We assigned  a computational model of the Moth Olfactory Network the  task of classifying the MNIST digits. The moth brain successfully learned to read given very few training samples (1 to 20 samples per class). In  this few-samples regime the moth brain substantially outperformed standard ML methods such as Nearest-neighbors, SVM, and CNN.\nOur experiments elucidate biological mechanisms for fast learning that rely on cascaded networks, competitive inhibition, sparsity, and Hebbian plasticity. These biological algorithmic components represent a novel, alternative toolkit for building neural nets that may offer a valuable complement to standard  neural nets. ", "pdf": "/pdf/9228a75e1b76bd8c88d85856ccf60cf45930ac22.pdf", "TL;DR": "A moth brain model can learn the MNIST digits, and out-performs ML methods in the few-training-samples regime.", "paperhash": "delahunt|a_moth_brain_learns_to_read_mnist", "keywords": ["machine learning", "neural nets", "neuroscience", "sparsity"], "authors": ["Charles B. Delahunt", "J. Nathan Kutz"], "authorids": ["delahunt@uw.edu", "kutz@uw.edu"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 4}