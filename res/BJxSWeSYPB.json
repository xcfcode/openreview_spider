{"notes": [{"id": "BJxSWeSYPB", "original": "rJln4egYvr", "number": 2135, "cdate": 1569439741268, "ddate": null, "tcdate": 1569439741268, "tmdate": 1577168252752, "tddate": null, "forum": "BJxSWeSYPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "FbM9YFgsS1", "original": null, "number": 1, "cdate": 1576798741425, "ddate": null, "tcdate": 1576798741425, "tmdate": 1576800894809, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Decision", "content": {"decision": "Reject", "comment": "This work proposes a self-supervised segmentation method: building upon Crawford and Pineau 2019, this work adds a Monte-Carlo based training strategy to explore object proposals.\nReviewers found the method interesting and clever, but shared concerns about the lack of a better comparison to Crawford and Pineau, as well as generally a lack of care in comparisons to others, which were not satisfactorily addressed by authors response.\nFor these reasons, we recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706728, "tmdate": 1576800254859, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Decision"}}}, {"id": "B1lr_hNniH", "original": null, "number": 4, "cdate": 1573829741160, "ddate": null, "tcdate": 1573829741160, "tmdate": 1573829741160, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "H1gAXMJFjB", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment", "content": {"title": "Categorical reparameterization vs importance sampling", "comment": "We tried replacing the importance sampling part in our method with the categorical reparameterization used in (Crawford and Pineau 2019). Since both strategies approximate the same objective, they should lead to very similar outcomes with a possible difference in the convergence speed. To this end we used Gumbel-Softmax distribution and tried this estimator with several different temperature values. Our experiments show that Gumbel-Softmax based categorical reparameterization didn\u2019t lead to faster convergence (see page 18 in SectionA7, Fig. 13). This might be partly due to the high value of Gumbel noise added to the log probabilities. However, to have a more solid claim, a principled grid search for hyperparameters such as the temperature is necessary. We will provide the exact comparison in our final version after conducting an extensive hyperparameter search.\n\nOverall our importance sampling approach is simpler compared to their approach and has the advantage of being an unbiased estimator. In addition to that it does not need custom layers that behave differently in the forward and backwards passes during optimization, which is the case for the Gumbel-Softmax categorical reparameterization. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxSWeSYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2135/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2135/Authors|ICLR.cc/2020/Conference/Paper2135/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145818, "tmdate": 1576860546263, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment"}}}, {"id": "S1lT7LJYoS", "original": null, "number": 3, "cdate": 1573611044570, "ddate": null, "tcdate": 1573611044570, "tmdate": 1573611044570, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "BylC3XOcYS", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment", "content": {"title": "Initial Response to Review #3", "comment": "Thank you for the constructive feedback. We are glad that you find the idea original. We address your concerns below. \n\nThe changes in the main text are highlighted in red.\n\nReviewer 3: Precision/recall scores\n\nWe agree with your point on the compromise between precision and recall. Therefore, we will rewrite our comparison to focus on J and F measures; precision and recall will only be used to explain where additional improvements are needed.\n\nReviewer 3: Training of multiple images of the same scene\n\nThis comment is related to Reviewer 1\u2019s: \u201c\u2026 assumption that the background follows relatively consistent textures\u201d. Therefore, we restate our response here.\n\nIndeed, using an off-the-shelf inpainting network performed poorly because our images differ significantly from those in the datasets it had been trained on. \nTherefore, we trained our own inpainting model (see Page 11, Section A1 Implementation Details, The inpainting network). It can deal with diverse backgrounds as well as non-consistent ones given enough data. Although in the Ski-PTZ dataset the scene looks homogeneous, in HandHeld190k the scene is cluttered with different textured objects such the houses, fences and trees. It requires multiple images of the same scene, or videos. Note, that we can leverage the input videos directly; we don\u2019t need additional background images without persons. In the revised version we added a more detailed discussion about the inpainting network."}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxSWeSYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2135/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2135/Authors|ICLR.cc/2020/Conference/Paper2135/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145818, "tmdate": 1576860546263, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment"}}}, {"id": "BygQUXkKoH", "original": null, "number": 2, "cdate": 1573610315386, "ddate": null, "tcdate": 1573610315386, "tmdate": 1573610315386, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "HklAE1a2Fr", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment", "content": {"title": "Initial Response to Review #2", "comment": "Thank you for your constructive comments. We address your concerns in detail below.\n\nThe changes in the main text are highlighted in red.\n\nReviewer 2: Comparison to [R1] \u201cLearning deep features for discriminative localization\u201d and ImageNet pre-training\n\nThe only component where we use a pre-trained model is the ResNet50 module in the encoder component of S (see Page 11, Section A1 Implementation Details, The synthesis network). This component does not perform localization, and only encodes the selected patch of the image. In the last two rows of Table 3 in the Appendix we report the performance of our model using a randomly initialized ResNet and also an unsupervised pre-trained model (from Wu et al. 2018). Comparing the ImageNet pre-trained model (in the fourth row) with the unsupervised pre-trained model (in the last row), the unsupervised setup shows only a small performance drop; pre-training helps but is not essential. \n\nWe also compare the detection accuracy of [R1] in the revised  version, note, however, that [R1] does not provide accurate segmentation masks but rather heatmaps.\n\nTo compare our method to [R1] we first obtain the heatmaps and the corresponding bounding box predictions for H36M S9 and S11 test images using the official code and their GoogLeNet-CAM pre-trained model on ImageNet. As the output heatmaps of [R1] illustrate (see page 17, section A7 Additional Comparisons to Related Work, Fig. 12), there can be several highly activated regions in the heatmaps leading to very big bounding boxes. In some cases, the model highlights other objects in the scene since ImageNet contains a wide range of object classes. Among all the bounding box predictions that [R1] generates, we select the one yielding the highest IOU score against the ground-truth bounding box corresponding to that frame. We report that [R1] has an mAP_{0.5} score of 0.1 which is significantly lower than our mAP_{0.5} score of 0.58. This gap indicates that performing detection using only their pre-trained model is not reliable and selection of the best bounding box among the candidates is infeasible in a self-supervised setting. Our model can accurately detect the human subject without needing ImageNet pre-training in the detection component. \n\nReviewer 2: Comparison to [R2] \u201cUnsupervised learning of depth and ego-motion from video\u201d and using motion as the supervision.\n\nThis comment is related to Reviewer 1\u2019s: \u201coptical flow and boundary detection, which I thought are OK cues to be used\u201d. Therefore, we restate our response here.\n\nWhile one can use optical flow and other motion cues from unsupervised techniques to boost performance, the independence of such cues makes our proposed approach applicable to single images and, most importantly, avoids failure cases of optical flow. Motion-based approaches are prone to failure when there is no or too complex motion information to separate the foreground and background and in textureless areas. These failure cases occur in all our scenarios, as shown in the attached optical flow images (see page 17, section A7 Additional Comparisons to Related Work, Fig. 11) generated from our datasets  using FlowNet2.0. Depth and ego-motion prediction suffers from similar problems. There are always multiple ways of addressing the same problem and only in retrospect the preferred strategy becomes clear. In this study, we investigated how far (quite far!) one can get without motion cues. Nevertheless, as we state in the outlook section, we will combine the complementary merits of motion-based strategies in the future; using [R2] will be a viable addition. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxSWeSYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2135/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2135/Authors|ICLR.cc/2020/Conference/Paper2135/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145818, "tmdate": 1576860546263, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment"}}}, {"id": "H1gAXMJFjB", "original": null, "number": 1, "cdate": 1573610021529, "ddate": null, "tcdate": 1573610021529, "tmdate": 1573610021529, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "HyloanZEcr", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment", "content": {"title": "Initial Response to Review #1", "comment": "Thank you for your insightful comments. We address your concerns in detail below.\n\nThe changes in the main text are highlighted in red.\n\nReviewer 1: \u201c\u2026 the framework directly came from (Crawford & Pineau 2019), and the only change is from variational inference to an importance-sampling (MC) approach\u201d\n\nWe would like to clarify that the novelty of our method with respect to (Crawford & Pineau 2019) does not only come from the Monte-Carlo based strategy for handling the discrete proposals through an unbiased estimator. In addition to changing from a variational inference using Gumbel-Softmax to a Monte-Carlo sampling approach, the model in (Crawford and Pineau 2019) is designed for multi-object detection in images with monochrome background, while our model tackles single object detection and segmentation in non-static backgrounds, making our approach easily applicable to outdoor scenes captured with moving cameras. Considering this, a direct comparison of the two models is not possible. In particular, there is no proposed mechanism in (Crawford and Pineau 2019) to deal with non-static backgrounds, as acknowledged in their paper. By contrast, the inpainting model in our approach, through the proposal based bounding box selection framework, favors the locations that have high inpainting loss, thus guiding us to the salient object region in non-static backgrounds. We would like to point out that the key novelty here is casting the segmentation problem as an inpainting and search task which has not been attempted before. To this end, we come up with an efficient training strategy that allows a stable training using the adversarial foreground and background losses.\n\nA different comparison can be made by replacing the importance sampling part in our method with the categorical reparameterization used in (Crawford and Pineau 2019), which we will report before the end of the rebuttal period.\n\nReviewer 1: \u201cHow does the method perform in images where multiple objects are in the view?\u201d\n\nWhile we show in Fig. 4 of the paper that our model can tackle multi-object detection and segmentation by sampling more than once at test time, multi-object detection is not the focus of our current work. The algorithm can be extended for multi-object detection and segmentation, but since it was out of the scope of the current model, we did not pursue that particular line of research.\n\nReviewer 1: \u201c\u2026 assumption that the background follows relatively consistent textures\u201d\n\nIndeed, using an off-the-shelf inpainting network performed poorly because our images differ significantly from those in the datasets it had been trained on. \nTherefore, we trained our own inpainting model (see Page 11, Section A1 Implementation Details, The inpainting network). It can deal with diverse backgrounds as well as non-consistent ones given enough data. Although in the Ski-PTZ dataset the scene looks homogeneous, in HandHeld190K the scene is cluttered with different textured objects such the houses, fences and trees. It requires multiple images of the same scene, or videos. Note, that we can leverage the input videos directly; we don\u2019t need additional background images without persons.\n\nReviewer 1: \u201coptical flow and boundary detection, which I thought are OK cues to be used\u201d\n\nWhile one can use optical flow and other motion cues from unsupervised techniques to boost performance, the independence of such cues makes our approach applicable to single images and, most importantly, avoids failure cases of optical flow. Motion-based approaches are prone to failure when there is no or too complex motion information to separate the foreground and background and in textureless areas. These failure cases occur in all our scenarios, as shown in the attached optical flow images (see page 17, section A7 Additional Comparisons to Related Work, Fig. 11) generated from our datasets using FlowNet2.0. Depth and ego-motion prediction suffers from similar problems. There are always multiple ways of addressing the same problem and only in retrospect the preferred strategy becomes clear. In this study, we investigated how far (quite far!) one can get without motion cues. Nevertheless, as we state in the outlook section, we will combine the complementary merits of motion-based strategies in the future; using [R2, mentioned by Reviewer 2] will be a viable addition. \n\nReviewer 1: Typo in Section 3.2 page 5\n\nThanks for pointing out the wrong equation number. It should be \u201cthe foreground objective O of Eq. (4)\u201d."}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxSWeSYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2135/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2135/Authors|ICLR.cc/2020/Conference/Paper2135/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145818, "tmdate": 1576860546263, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Authors", "ICLR.cc/2020/Conference/Paper2135/Reviewers", "ICLR.cc/2020/Conference/Paper2135/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Comment"}}}, {"id": "BylC3XOcYS", "original": null, "number": 1, "cdate": 1571615670222, "ddate": null, "tcdate": 1571615670222, "tmdate": 1572972378653, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This submission proposes a self-supervised segmentation method, that learns from single-object videos by finding the region where it can segment an object, remove the entire bounding box around it, inpaint it, then finally put the object back. The loss is a balance between a reconstruction error and a negative inpainting error (high error means object is probably present, due to the weak correlation with the background).\n\nMy decision is Weak Accept. I like the method very much and think it\u2019s a clever and well-executed algorithm. The reason for being Weak is because the experimental evidence could be stronger, especially comparing with Croitoru et al. and Rhodin et al. The paper leaves some open problems, inviting future work to be built on top of it (i.e. leveraging time more and handling multiple objects better). I think it should be accepted to promote such future work.\n\nMethod:\n\nThe method is interesting and clever. Similar efforts have been made, such as Bielski & Favaro and Crawford & Pineau. However, key contributions relax some of the contrived requirements of these past methods (e.g. simple foreground translation over background; requirement of plain background). Thanks to the inpainter, the importance sampler, and avoidance of collapsing into trivial solutions, this paper is able to put together a method that works on moving cameras and fairly complex scene semantics (still limited to one object though). Actually getting this to converge with only two loss terms balanced against each other is impressive. Having certain heads of the network trained only on one of the two terms seems to be a key contribution.\u2028\n\nExperimental results:\n\nThe comparison with Rhodin et al. on H36M is characterized as \u201cslightly\u201d lower, but I would call 71% vs. 58% a significant difference. Of course, I understand that Rhodin et al. relies on the static background, so this is not a fair comparison.\n\nSki-PTZ-Dataset should then offer a better comparison, but here the method struggles to compete with Croitoru et al. 2019, which has a 11-point higher F-measure. On Handheld190k, the dataset proposed in this paper, is where the method finally shines, but still only offers a 1-point F-measure improvement over Croitoru et al. That being said, considering how different the methods are, and how Croitoru et al. requires two-stage training, there are many benefits to this method. Croitoru et al. also relies on video to extract the object features, and this requirement is not as explicit in this work. Actually, that brings me to one question I had. The paper states that \u201cas long as videos or picture collections of a single object in front of the same scene are available.\u201d I didn\u2019t quite understand why this must be trained on multiple images of the same scene. If the inpainter is general to any background scenery, couldn\u2019t it work on single-images as well? The conclusion even says you do not use temporal cues.\n\nOther:\n\nI don\u2019t think it\u2019s that meaningful to include precision/recall in the tables. It is also not that meaningful to point out that your method\u2019s precision is higher than that of Croitoru et al., when the F-measure is shy 11 points. The reason is because many points on the precision/recall could be constructed simply by applying for instance a gamma curve on the segmentation predictions. The high precision is clearly at the cost of a low recall, and another point on this tradeoff curve could be presented. This is why F-measure and average precision are much better."}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575865748933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Reviewers"], "noninvitees": [], "tcdate": 1570237727206, "tmdate": 1575865748946, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review"}}}, {"id": "HklAE1a2Fr", "original": null, "number": 2, "cdate": 1571766070311, "ddate": null, "tcdate": 1571766070311, "tmdate": 1572972378615, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper introduces a method to self-supervised train a model for object detection/segmentation. The idea is that the background is easy to reconstruct while the foreground/object is hard. Experiments demonstrate the effectiveness of the proposed methods.\n\nHere are some high-level concerns.\n\n1. As mentioned in the \"Implementation details\", 'naive end-to-end training is difficult... we use ImageNet-trained weights for initialization'. This is worrisome to justify the effectiveness. It may be possible the imagenet-trained model has already captured salient objects. To justify the claims and effectiveness of the method, it should include a comparison with [R1], which demonstrates the possibility of doing detection with a pretrained model. Other work along this line should be also good reference.\n\n2. As a moving camera is available, it is also possible to segment background with frames through a 6DoF prediction on the camera, rotation and translation, e.g., [R2]. The supervision signal is from frame reconstruction through learning to predict both camera pose and pixel-level depth. This is also self-supervised learning. At least such a self-supervised trained model can act as an initialization.\n\nConsidering the above points, the paper does not appear compelling, due to lack of either careful claims or justification.\n\n\n[R1] Learning deep features for discriminative localization\n[R2] Unsupervised learning of depth and ego-motion from video"}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575865748933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Reviewers"], "noninvitees": [], "tcdate": 1570237727206, "tmdate": 1575865748946, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review"}}}, {"id": "HyloanZEcr", "original": null, "number": 3, "cdate": 1572244675110, "ddate": null, "tcdate": 1572244675110, "tmdate": 1572972378570, "tddate": null, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "invitation": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper provides a new self-supervised proposal-based approach for object detection and segmentation. The author introduces a Monte Carlo-based optimization to solve the inefficiency problem in the discrete proposal-based forward process defined in (Crawford and Pineau 2019). Also, the paper redefines the decoder part for self-supervised from minimizing reconstruction loss with background segmentation to maximize reconstruction error with learning a foreground segmentation. \u00a0The method is then verified with a suite of experiments for people-detection on video datasets.\n\nThe main benefit over many previous unsupervised object detection/segmentation approaches is that they did not make use of optical flow or other readily available cues during training. However, given that the framework directly came from (Crawford & Pineau 2019), and the only change is from variational inference to an importance-sampling (MC) approach. This would be fine if it is verified in experiments, however, the experiments did not show any comparison w.r.t. (Crawford & Pineau 2019) hence we have no way of understanding what is the relative performance w.r.t. that baseline approach.\n\nBesides, in all the experiments a single object is in the view. How does the method perform in images where multiple objects are in the view?\n\nA little bit of a philosophical question is whether this a problem worth pursuing as well. For self-supervised motion estimation (e.g. optical flow), it is clear why we want to do that. However, the current type of algorithm is dependent on the assumption that the background follows relatively consistent textures, this may not necessarily be true in practice, and hence the application could be quite limited. Many previous unsupervised video object segmentation methods make use of optical flow and boundary detection, which I thought are OK cues to be used, especially when both can be learned in a self-supervised manner. This is not entirely related to the assessment, but I would still like to hear what the authors think.\n\nMinor:\nIn the paragraph after 'Training strategy'(Section 3.2, Page 5), is it 'the foreground objective O of Eq. (2)' or 'Eq.(4)'?\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2135/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["isinsu.katircioglu@epfl.ch", "rhodin@cs.ubc.ca", "victor.constantin@epfl.ch", "joerg.spoerri@balgrist.ch", "mathieu.salzmann@epfl.ch", "pascal.fua@epfl.ch"], "title": "Self-supervised Training of Proposal-based Segmentation via Background Prediction", "authors": ["Isinsu Katircioglu", "Helge Rhodin", "Victor Constantin", "J\u00f6rg Sp\u00f6rri", "Mathieu Salzmann", "Pascal Fua"], "pdf": "/pdf/e88079182cb5248f8aa3398d395d00fc9bc75170.pdf", "abstract": "While supervised object detection and segmentation methods achieve impressive accuracy, they generalize poorly to images whose appearance significantly differs from the data they have been trained on. To address this in scenarios where annotating data is prohibitively expensive, we introduce a self-supervised approach to detection and segmentation, able to work with monocular images captured with a moving camera. At the heart of our approach lies the observations that object segmentation and background reconstruction are linked tasks, and that, for structured scenes, background regions can be re-synthesized from their surroundings, whereas regions depicting the object cannot.\nWe encode this intuition as a self-supervised loss function that we exploit to train a proposal-based segmentation network. To account for the discrete nature of the proposals, we develop a Monte Carlo-based training strategy that allows the algorithm to explore the large space of object proposals. We apply our method to human detection and segmentation in images that visually depart from those of standard benchmarks, achieving competitive results compared to the few existing self-supervised methods and approaching the accuracy of supervised ones that exploit large annotated datasets.", "keywords": [], "paperhash": "katircioglu|selfsupervised_training_of_proposalbased_segmentation_via_background_prediction", "original_pdf": "/attachment/b3e50ccc4d75ea402d54a136a30574163ff36c43.pdf", "_bibtex": "@misc{\nkatircioglu2020selfsupervised,\ntitle={Self-supervised Training of Proposal-based Segmentation via Background Prediction},\nauthor={Isinsu Katircioglu and Helge Rhodin and Victor Constantin and J{\\\"o}rg Sp{\\\"o}rri and Mathieu Salzmann and Pascal Fua},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxSWeSYPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxSWeSYPB", "replyto": "BJxSWeSYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2135/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575865748933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2135/Reviewers"], "noninvitees": [], "tcdate": 1570237727206, "tmdate": 1575865748946, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2135/-/Official_Review"}}}], "count": 9}