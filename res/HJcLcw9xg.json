{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396568004, "tcdate": 1486396568004, "number": 1, "id": "HJgshGIOg", "invitation": "ICLR.cc/2017/conference/-/paper413/acceptance", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper studies the invertibility properties of deep rectified networks, and more generally the piecewise linear structure that they implicitly define. The authors introduce a 'pseudocode' to compute preimages of (generally non-invertible) half-rectified layers, and discuss potential implications of their method with manifold-type models for signal classes. \n \n The reviewers agreed that, while this is an interesting and important question, the paper is currently poorly organized, and leaves the reader a bit disoriented, since the analysis is incomplete. The AC thus recommends rejection of the manuscript. \n \n As an addendum, the AC thinks that the authors should make an effort to reorganize the paper and clearly state the contributions, and not expect the reader to find them out on their own. In this field of machine learning, I see contributions as being either (i) theoretical, in which case we expect to see theorems and proofs, (ii) algorithmical, in which case an algorithmic is presented, studied, extensively tested, and laid out in such a way that the interested reader can use it, or (iii) experimental, in which case we expect to see improved numerical performance in some dataset. Currently the paper has none of these contributions. \n Also, a very related reference seems to be missing: \"Signal Recovery from Pooling Representations\", Bruna ,Szlam and Lecun, ICML'14, in which the invertibility of ReLU layers is precisely characterized (proposition 2.2)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396568546, "id": "ICLR.cc/2017/conference/-/paper413/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396568546}}}, {"tddate": null, "tmdate": 1484926171080, "tcdate": 1484926171080, "number": 3, "id": "BJmJToywl", "invitation": "ICLR.cc/2017/conference/-/paper413/public/comment", "forum": "HJcLcw9xg", "replyto": "B13A9k4Vl", "signatures": ["~stefan_carlsson1"], "readers": ["everyone"], "writers": ["~stefan_carlsson1"], "content": {"title": "To all reviewers", "comment": "I appreciate the general positive tone of all the reviews that all seem to boil down to \"note ready yet\". The paper is definitely just a first in a series where implications of it will be investigated. I suppose by \"being ready\" the reviewers mean: 1. A novel sofar unnoticed way of directly computing preimages to rectifier networks (which we have) 2. An algorithm to actually perform the computation (which we don't have yet but could easily write down in terms of finding nullspaces of linear mappings) and 3. \"Empirical results\", i.e. testing the algorithm on real cases of trained rectifier networks (which we don't have but intend to produce). \n\nWhat we do have is a previously unknown mathematical demonstration of how to directly compute exact preimages as opposed to previous work (Mahendran, vedaldi 2015, Dosovitskiy, Brox \tarXiv:1506.02753 where heuristic methods for computing preimages are presented. We also give a clear demonstration of the fact that rectifier networks are not in general invertable which seem to contradict the claims given in recent submissions to ICLR, Towards Understanding the Invertibility of Convolutional Neural Networks (ICLR 2017), Why are deep nets reversible ?: A simple theory, with implications for training (ICLR ws 2016) although these papers seem to make some assumptions that are not made by us.\n\nThe paper Montufar et al. mentions the fact that multiple inputs are mapped to the same output but does not contain any extensive discussion of preimages or their computation. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588428, "id": "ICLR.cc/2017/conference/-/paper413/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJcLcw9xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper413/reviewers", "ICLR.cc/2017/conference/paper413/areachairs"], "cdate": 1485287588428}}}, {"tddate": null, "tmdate": 1482196937771, "tcdate": 1482196937771, "number": 2, "id": "BkfADZU4l", "invitation": "ICLR.cc/2017/conference/-/paper413/official/comment", "forum": "HJcLcw9xg", "replyto": "r1EIWZLEx", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer2"], "content": {"title": "Related work", "comment": "Also, I think there is more related work out there than cited. For instance, https://arxiv.org/abs/1402.1869 and https://arxiv.org/abs/1606.05336 look at preimages."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588304, "id": "ICLR.cc/2017/conference/-/paper413/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers", "ICLR.cc/2017/conference/paper413/areachairs"], "cdate": 1485287588304}}}, {"tddate": null, "tmdate": 1482195276122, "tcdate": 1482195276122, "number": 3, "id": "r1EIWZLEx", "invitation": "ICLR.cc/2017/conference/-/paper413/official/review", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer2"], "content": {"title": "Nice premise but somewhat incomplete?", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\nThis paper looks at the structure of the preimage of a particular activity at a hidden layer of a network. It proves that any particular activity has a preimage of a piecewise linear set of subspaces.\n\nPros:\nFormalizing the geometry of the preimages of a particular activity vector would increase our understanding of networks\n\nCons:\nAnalysis seems quite preliminary, and no novel theoretical results or clear practical conclusions.\n\nThe main theoretical conclusion seems to be the preimage being this stitch of lower dimensional subspaces? Would a direct inductive approach have worked? (e.g. working backwards from the penultimate layer say?) This is definitely an interesting direction, and it would be great to see more results on it (e.g. how does the depth/width, etc affect the division of space, or what happens during training) but it doesn't seem ready yet.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595404, "id": "ICLR.cc/2017/conference/-/paper413/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper413/AnonReviewer3", "ICLR.cc/2017/conference/paper413/AnonReviewer1", "ICLR.cc/2017/conference/paper413/AnonReviewer2"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595404}}}, {"tddate": null, "tmdate": 1482178333302, "tcdate": 1482178333302, "number": 2, "id": "rJrQkpHVg", "invitation": "ICLR.cc/2017/conference/-/paper413/official/review", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer1"], "content": {"title": "review of ``THE PREIMAGE OF RECTIFIER NETWORK ACTIVITIES''", "rating": "4: Ok but not good enough - rejection", "review": "I have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595404, "id": "ICLR.cc/2017/conference/-/paper413/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper413/AnonReviewer3", "ICLR.cc/2017/conference/paper413/AnonReviewer1", "ICLR.cc/2017/conference/paper413/AnonReviewer2"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595404}}}, {"tddate": null, "tmdate": 1482058452200, "tcdate": 1482058452200, "number": 1, "id": "B13A9k4Vl", "invitation": "ICLR.cc/2017/conference/-/paper413/official/review", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer3"], "content": {"title": "not ready yet", "rating": "4: Ok but not good enough - rejection", "review": "I really appreciate the directions the authors are taken and I think something quite interesting can come out of it. I hope the authors continue on this path and are able to come up with something quite interesting soon. However I feel this paper right now is not quite ready. Is not clear to me what the results of this work are yet. The preimage construction is not obviously (at least not to me) helpful. It feels like the right direction, but it didn't got to a point where we can use it to identify the underlying mechanism behind our models. We know relu models need to split apart and unite different region of the space, and I think we can agree that we can construct such mechanism (it comes from the fact that relu models are universal approximators) .. though this doesn't speak to what happens in practice.  All in all I think this work needs a bit more work yet. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512595404, "id": "ICLR.cc/2017/conference/-/paper413/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper413/AnonReviewer3", "ICLR.cc/2017/conference/paper413/AnonReviewer1", "ICLR.cc/2017/conference/paper413/AnonReviewer2"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512595404}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481659614184, "tcdate": 1478290001675, "number": 413, "id": "HJcLcw9xg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJcLcw9xg", "signatures": ["~Hossein_Azizpour1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481033544025, "tcdate": 1481033544019, "number": 2, "id": "HyxUPSEXe", "invitation": "ICLR.cc/2017/conference/-/paper413/public/comment", "forum": "HJcLcw9xg", "replyto": "r15S0r17l", "signatures": ["~stefan_carlsson1"], "readers": ["everyone"], "writers": ["~stefan_carlsson1"], "content": {"title": "Answer and explanation of figure 1", "comment": "Figure 1 illustrates the transformation occuring between layer (l) and layer (l+1) for a two node\nmulti layer network. In math terms this transformation is described in eq. 5 and 6 as  affine transformations\nrelating   (x^(l)_1, x^(l)_2) with (x^(l+1)_1, x^(l+2)_2).\n\nIn detail for x^(l+1)_1:\n\nx(l+1)_1 = w_1,1 x^(l)_1 + w_1,2 x^(l)_2  + w_1,3\nx(l+1)_2 = w_2,1 x^(l)_1 + w_2,2 x^(l)_2  + w_2,3\n\nIn order to get a more compact notation I introduce the vector notation\nx^T = (x_1, x_2,  1) which always has a 1 as the last element.\nThis allows us to write the transformations in the compact way: x^(l+1) = w^T x^(l)\nwhich would be  a hyperplane passing through the origin\nif x was a general 3D vector. However since the 3:rd element of x is always 1 it\nis just a compact way of writing the affine transformations of eq. 5 and 6, i.e as\ntwo general lines in (x^(l)_1, x^(l)_2) space\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588428, "id": "ICLR.cc/2017/conference/-/paper413/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJcLcw9xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper413/reviewers", "ICLR.cc/2017/conference/paper413/areachairs"], "cdate": 1485287588428}}}, {"tddate": null, "tmdate": 1481023668479, "tcdate": 1481023668471, "number": 1, "id": "SJ23gQN7l", "invitation": "ICLR.cc/2017/conference/-/paper413/public/comment", "forum": "HJcLcw9xg", "replyto": "ByXo3xEXl", "signatures": ["~stefan_carlsson1"], "readers": ["everyone"], "writers": ["~stefan_carlsson1"], "content": {"title": "Answers to questions 1-4 and general question about revision", "comment": "Q1: I am aware of that paper and I thought we had referenced it but I was wrong. The ref. Glorot et. al which we have, is a more original reference of rectifier networks. I will update the reference list adding the suggested paper. That paper and the work we cited notes that the input space will be divided into piecewise linear regions that map to the same output. We give an more detailed and explicit way of constructing these regions (preimages) that map to the same output.\n\nQ2: Yes, this is standard MLP if by that is meant Multi Layer Perceptrons of which convnets are a special case. \n\nQ3: In fig2 we have a coordinate system associated eith each layer. This coordinate system refers to the concatenated linear mappings from the first layer to layer (l). This means it illustrates how  the input space is mapped to layer (l). The ReLU elements mean that we only consider the positive quadrant of the coordinate systems for the linear mappings. The effect of this is that we get the preimages of a certain layer's activity in the input space.\n\nQ4: For any activity at any layer we give a procedure of finding the preimage of the previous layer. This procedure involves computing a set of nullspaces and constructing a coordinate system, i.e. just standard linear algebra procedures. We have not yet studied the actual complexity of this but we believe that it should not offer any major problems. By concatenating this procedure we can get the preimage of any  layer expressed in any previous layer as well as the input space\n\nGeneral question: Yes we will update a revised version wednesday 7 december"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287588428, "id": "ICLR.cc/2017/conference/-/paper413/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJcLcw9xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper413/reviewers", "ICLR.cc/2017/conference/paper413/areachairs"], "cdate": 1485287588428}}}, {"tddate": null, "tmdate": 1481014426921, "tcdate": 1481014426915, "number": 2, "id": "ByXo3xEXl", "invitation": "ICLR.cc/2017/conference/-/paper413/pre-review/question", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer3"], "content": {"title": "Pre-review questions", "question": "Hi, \nSorry for the delay. The paper reads as unfinished. I'm wondering if the authors meant to submit a revision and didn't get a chance to do it? I mean, is not that the paper is not carefully written, is more that it feels not ready for ICLR in terms of content. \n\nFirst question I have is if the authors are aware of this work which seems relevant to current work (https://arxiv.org/abs/1402.1869). It was accepted at NIPS if I'm not mistaken. The papers don't overlap, but use the same kind of constructions so I thought it makes sense to point it out. \n\nAfter eq 4, you say this is a generalization of convnets. Isn't this just a standard MLP ? Also you mention this construction will be described for convolutional nets as well, though is never done in the paper. By later you mean in a different paper? (this are the kind of signs that make me think that the real intend was to upload a revision that I'd love to read). \n\nFinally why I understand what the authors are going for, their construction of a new coordinate system happens at each layer. How do you put all of these together? It feels you don't get something easy to work with when you do this (e.g. given an arbitrary MLP can I easily characterize the preimage  at some intermediary layer in a closed form and maybe reason about it?). Is not clear to me what the results of this work are and what implications it has. Any comments of the authors explaining their intend will be useful. I think is a very interesting (and full of potential) direction, it just seems to be the authors might still need some time to figure out what is their main result. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481014427539, "id": "ICLR.cc/2017/conference/-/paper413/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper413/AnonReviewer1", "ICLR.cc/2017/conference/paper413/AnonReviewer3"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481014427539}}}, {"tddate": null, "tmdate": 1480707649784, "tcdate": 1480707649780, "number": 1, "id": "r15S0r17l", "invitation": "ICLR.cc/2017/conference/-/paper413/pre-review/question", "forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "signatures": ["ICLR.cc/2017/conference/paper413/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper413/AnonReviewer1"], "content": {"title": "terminology", "question": "Kernel is referring to the nullspace? If so, should it not pass through the origin and what is a transformation kernel? \nWhat is the considered x^{(l+1)} in Figure 1? \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "The Preimage of Rectifier Network Activities", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "pdf": "/pdf/d292514377df32b560d6fcf877c13f34dbaac64f.pdf", "paperhash": "carlsson|the_preimage_of_rectifier_network_activities", "conflicts": ["kth.se"], "keywords": [], "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481014427539, "id": "ICLR.cc/2017/conference/-/paper413/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper413/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper413/AnonReviewer1", "ICLR.cc/2017/conference/paper413/AnonReviewer3"], "reply": {"forum": "HJcLcw9xg", "replyto": "HJcLcw9xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper413/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481014427539}}}], "count": 11}