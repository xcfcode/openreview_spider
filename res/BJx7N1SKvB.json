{"notes": [{"id": "BJx7N1SKvB", "original": "r1lqBj3dPS", "number": 1647, "cdate": 1569439530553, "ddate": null, "tcdate": 1569439530553, "tmdate": 1577168214020, "tddate": null, "forum": "BJx7N1SKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "7kJxOtZ2JW", "original": null, "number": 1, "cdate": 1576798728729, "ddate": null, "tcdate": 1576798728729, "tmdate": 1576800907800, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Decision", "content": {"decision": "Reject", "comment": "In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.\n\nUnfortunately, the reviewers could not reach a consensus as to whether this paper had sufficient novelty to merit acceptance at this time. Incorporating their feedback would move the paper closer towards the acceptance threshold.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721318, "tmdate": 1576800272314, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Decision"}}}, {"id": "HyxOtaQviB", "original": null, "number": 1, "cdate": 1573498239663, "ddate": null, "tcdate": 1573498239663, "tmdate": 1573499129118, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment", "content": {"title": "Thanks to the reviewers and AC", "comment": "We are grateful to all reviewers for their constructive feedback and for the time they took to review our work. We have uploaded a new version of the paper and added detailed responses to their comments below. In particular, we address some technical questions about our paper, and explain the overall merits of our work. We hope this encourages reviewers 2 and 3 to reconsider their scores."}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJx7N1SKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1647/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1647/Authors|ICLR.cc/2020/Conference/Paper1647/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152934, "tmdate": 1576860561520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment"}}}, {"id": "rkxjQeEwiH", "original": null, "number": 5, "cdate": 1573498914569, "ddate": null, "tcdate": 1573498914569, "tmdate": 1573498914569, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "SJgdT5JY5S", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment", "content": {"title": "Response to comments", "comment": "2. As we outline in Subsection 1.1, our work generalizes substantially over previous work as we analyze nontrivial data distributions and NNs with biases. In particular, previous formulae for the spectrum would agree poorly on real datasets like MNIST and CIFAR, whereas we get good agreement as evidenced in Fig. 2. Moreover, to achieve these generalizations, we had to introduce a new method of proof rather than the moment method employed in Pennington & Worah (2017). \n\n3. While Fig. 4 is indeed important and illustrates a main conclusion of our paper about mixtures of nonlinearities, the other figures are also significant as they demonstrate the validity of the mathematical machinery we use to predict the spectrum on complex datasets. To help provide context on these other figures, we have reworded their captions and made sure that each figure is referenced in the main text.\n\n4, 5, and 6. We have rewritten these sentence to clarify our meaning and fix any grammatical errors."}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJx7N1SKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1647/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1647/Authors|ICLR.cc/2020/Conference/Paper1647/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152934, "tmdate": 1576860561520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment"}}}, {"id": "Byg3S1NPjH", "original": null, "number": 4, "cdate": 1573498692308, "ddate": null, "tcdate": 1573498692308, "tmdate": 1573498692308, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "rklsQVQCtS", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment", "content": {"title": "Response to minor comments", "comment": "1. The formula for E_train in Eq. (17) is exact, since the training error is a random variable that converges in probability to a deterministic quantity. We have updated the statement of Cor. 1 to clarify this.\n\n2. The only assumption on f is that it leads to finite moments up to third order under Gaussian expectation. There is also some discussion of the differentiability of f in Remark 4: while we assume f is differentiable for convenience, it is in fact not a necessary assumption as the derivative is only used in a Gaussian expectation, which inherently smooths the function. This implies in particular that ReLU activations are included in the behavior we identify. See Figs. 1 and 2, for example, where we find excellent agreement when f is ReLU.\n\n3. The object G(\\gamma) is indeed the same as G defined in Eq. (1); the only difference is that the function is evaluated at z=-\\gamma."}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJx7N1SKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1647/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1647/Authors|ICLR.cc/2020/Conference/Paper1647/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152934, "tmdate": 1576860561520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment"}}}, {"id": "BylmyJNwjB", "original": null, "number": 3, "cdate": 1573498587162, "ddate": null, "tcdate": 1573498587162, "tmdate": 1573498587162, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "rklsQVQCtS", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment", "content": {"title": "Response to cons.", "comment": "1. This is a natural question and one we want to respond to in detail. Augmenting X as [X, 1] does not allow us to address both models in a unified manner. In fact, if it did, our theorem would be a special case of earlier work -- in particular, the final spectral distribution would depend on only the two parameters \\eta and \\zeta. However, the class of spectral distributions we find is nonparametric and thus cannot be expressed in this way.\n\nThe issue is that the previous work only applies if the augmented coordinate [--, 1] is transformed by weights of the same order of magnitude as the other features in X, i.e. the bias is of order O(1/\\sqrt{n}) per output feature -- an effect that disappears in the large m limit. Instead, our formulation f(WX+b) allows for an O(1) bias term per output feature. One can certainly reformulate this algebraically as f( [W, b] . [X, 1] ), but the scaling assumptions of prior work are (significantly) broken since W and b are on different scales. Addressing these issues is a main contribution of our derivation.\n\nWe have included a brief discussion of this point in the new version of the paper.\n\n2. In our setting the training error is a way to quantify the capacity of the function class. Indeed the test error is also interesting, but analyzing it requires specifying a model for the joint distribution between the data points and labels, and substantially more analysis. Unfortunately, this analysis is outside of the scope of this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJx7N1SKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1647/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1647/Authors|ICLR.cc/2020/Conference/Paper1647/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152934, "tmdate": 1576860561520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment"}}}, {"id": "rylKZCXDiS", "original": null, "number": 2, "cdate": 1573498369172, "ddate": null, "tcdate": 1573498369172, "tmdate": 1573498369172, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "Bkl2KwYTtH", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment", "content": {"title": "Response to minor comments", "comment": "(1) We focused on the linear model because it is the simplest (nontrivial) learning task. Moreover, the analysis follows directly from the paper\u2019s main theorem, whereas more complex learning tasks require substantially more calculation\u2014generally beyond the scope of this initial work.\n\n(2) We are happy to include experiments on additional datasets in a final version of the paper. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJx7N1SKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1647/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1647/Authors|ICLR.cc/2020/Conference/Paper1647/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152934, "tmdate": 1576860561520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Authors", "ICLR.cc/2020/Conference/Paper1647/Reviewers", "ICLR.cc/2020/Conference/Paper1647/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Comment"}}}, {"id": "Bkl2KwYTtH", "original": null, "number": 1, "cdate": 1571817348203, "ddate": null, "tcdate": 1571817348203, "tmdate": 1572972441364, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors focus on the high-dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations. I think it is a solid work and vote for acceptance. \nPros: \n(1) This paper has a solid theoretical foundation. Although I have not checked in detail, I think the deduction is clear and the contribution is well-established.\n(2) It extends some traditional bounds to more general cases. I think it will provide useful guidance to real applications, such as the network design in deep learning.\n(3) The authors have explained the results in a clear way. Thus, it will benefit the following readers and give deep insights about the related research areas.\nMinor comments:\n(1) I think some assumptions should be explained. For example, why the authors focus only on linear model. Due to the simplicity or the requirement from real applications?\n(2) More experimental results on large data sets should be added to validate the effectiveness."}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Reviewers"], "noninvitees": [], "tcdate": 1570237734330, "tmdate": 1574723076096, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review"}}}, {"id": "rklsQVQCtS", "original": null, "number": 2, "cdate": 1571857443360, "ddate": null, "tcdate": 1571857443360, "tmdate": 1572972441321, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper investigates the asymptotic spectral density of a random feature model F(Wx + B).  This is an extension of existing result that analyzed a model without the bias term, i.e., F(WX). This extension requires a modification of the proof technique. In addition to that, it analyzed a mixture of linear and non-linear activation functions, and show that mixture is better than single nonlinearity in terms of expected training error for ridge regression estimators.\n\nPros:\n- This paper investigates an interesting problem and it successfully extends the existing work. The theoretical curve well matches the simulated curve.\n- The finding that mixture of nonlinearities gives better expected training error is interesting.\n\nCons:\n- The extension to the model with bias seems a bit incremental. In practice, we may consider an input with additional constant feature, X <- [X,1], to deal with both models in a unified manner. There should be more discussion about why this kind of trivial argument cannot be applied in the analysis.\n- The effect of mixture of activation functions is investigated in the \"training error,\" but I don't see much significance on investigating the training error thoroughly. Instead, people are interested in the test error. I guess there does not appear such a trade-off for the test error and the linear activation function would be always better because the true function is the linear model. Hence, more expositions about why the training error is investigated should be provided.\n\nMore minor comment:\n- I guess the definition of Etrain  (Eq.(17)) requires an expectation with respect to the training data.\n- Assumptions of the activation function f should be provided; is it just assumed to be differentiable?, ReLU is included?\n- The definition of G(\\gamma) in page 6 had better to be consistent to that in previous pages.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Reviewers"], "noninvitees": [], "tcdate": 1570237734330, "tmdate": 1574723076096, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review"}}}, {"id": "SJgdT5JY5S", "original": null, "number": 3, "cdate": 1572563648160, "ddate": null, "tcdate": 1572563648160, "tmdate": 1572972441277, "tddate": null, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "invitation": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks.\n\nComments:\n1.The paper is well written and provides sound derivation for the theories.\n\n2. Since this area is out of my expertise, I\u2019m not sure whether merely extending the work of Pennington & Worah (2017) to non-Gaussian data distributions is significant enough or not.\n\n3. Except for Fig 4, the other figures seem out of the context. There is no explanation for the purpose of those figures in the main contents. It is a bit hard for the audience to figure out what to look at in the figures or what the figures try to prove. \n\n4. In \u201c..., and our analysis actually extends to general such distributions, ... \u201d, \u201cgeneral\u201d should be \u201cgeneralize\u201d.\n\n5. In \u201cAnd whether these products generate a medical diagnosis or a navigation decision or some other important output, ..\u201d, \u201cwhether\u201d should be \u201cno matter\u201d.\n\n6. \u201c..., they may not be large in comparison to the number of constraints they are designed asked satisfy.\u201d should be \u201c...  they are designed to satisfy\u201d.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1647/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions", "authors": ["Ben Adlam", "Jake Levinson", "Jeffrey Pennington"], "authorids": ["adlam@google.com", "jpennin@google.com", "jlev@google.com"], "keywords": [], "abstract": "One of the distinguishing characteristics of modern deep learning systems is that they typically employ neural network architectures that utilize enormous numbers of parameters, often in the millions and sometimes even in the billions. While this paradigm has inspired significant research on the properties of large networks, relatively little work has been devoted to the fact that these networks are often used to model large complex datasets, which may themselves contain millions or even billions of constraints. In this work, we focus on this high-dimensional regime in which both the dataset size and the number of features tend to infinity. We analyze the performance of a simple regression model trained on the random features $F=f(WX+B)$ for a random weight matrix $W$ and random bias vector $B$, obtaining an exact formula for the asymptotic training error on a noisy autoencoding task. The role of the bias can be understood as parameterizing a distribution over activation functions, and our analysis actually extends to general such distributions, even those not expressible with a traditional additive bias. Intruigingly, we find that a mixture of nonlinearities can outperform the best single nonlinearity on the noisy autoecndoing task, suggesting that mixtures of nonlinearities might be useful for approximate kernel methods or neural network architecture design.", "pdf": "/pdf/ce43d830186bc4455778324d5c578d134adaf67f.pdf", "paperhash": "adlam|a_random_matrix_perspective_on_mixtures_of_nonlinearities_in_high_dimensions", "original_pdf": "/attachment/fedc0f6df9dc04cfec2c2c219ed29af52bbacd37.pdf", "_bibtex": "@misc{\nadlam2020a,\ntitle={A Random Matrix Perspective on Mixtures of Nonlinearities in High Dimensions},\nauthor={Ben Adlam and Jake Levinson and Jeffrey Pennington},\nyear={2020},\nurl={https://openreview.net/forum?id=BJx7N1SKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJx7N1SKvB", "replyto": "BJx7N1SKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1647/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1647/Reviewers"], "noninvitees": [], "tcdate": 1570237734330, "tmdate": 1574723076096, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1647/-/Official_Review"}}}], "count": 10}