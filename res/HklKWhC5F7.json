{"notes": [{"id": "HklKWhC5F7", "original": "r1lOxz0qYQ", "number": 1190, "cdate": 1538087936698, "ddate": null, "tcdate": 1538087936698, "tmdate": 1545355434927, "tddate": null, "forum": "HklKWhC5F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkgGeBwSxE", "original": null, "number": 1, "cdate": 1545069802114, "ddate": null, "tcdate": 1545069802114, "tmdate": 1545354481902, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Meta_Review", "content": {"metareview": "The reviewers conclude the paper does not bring an important contribution compared to existing work. The experimental study can also be improved. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1190/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352932416, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352932416}}}, {"id": "HkeTVQV9RX", "original": null, "number": 5, "cdate": 1543287604838, "ddate": null, "tcdate": 1543287604838, "tmdate": 1543287604838, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HkeEEjMUCm", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "content": {"title": "Response to Authors", "comment": "I thank the authors for clarifying the contribution of the paper and for providing additional results with other measures of robustness. I have hence revised my rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1190/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614525, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1190/Authors|ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614525}}}, {"id": "S1gvPogc3m", "original": null, "number": 1, "cdate": 1541176158577, "ddate": null, "tcdate": 1541176158577, "tmdate": 1543287563629, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "content": {"title": "Empirical study of variation of accuracy and robustness of networks versus training data size", "review": "The paper presents an empirical study of how accuracy and robustness vary with increasing training data for four different data sets and CNN architectures. The main conclusion of the study is that while training accuracy generally increases with increasing training data, provided sufficient training data is available for training the network in the first place, the robustness on the other hand does not necessarily increase, and may even decrease.\n\nSimilar findings were presented previously in Su et al., 2018. Hence, the current paper contains incremental and marginal new findings versus the existing literature. The paper would also have been a lot stronger and significantly advanced our scientific understanding of the problem if the authors had made some attempt at trying to explain their findings theoretically. In its current form the paper does not contain sufficient contributions for acceptance.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "cdate": 1542234284730, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892595, "tmdate": 1552335892595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgBFpM8CX", "original": null, "number": 4, "cdate": 1543019901307, "ddate": null, "tcdate": 1543019901307, "tmdate": 1543020433986, "tddate": null, "forum": "HklKWhC5F7", "replyto": "S1xDWt79pm", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for your efforts reviewing our paper and providing helpful comments and suggestions.\n\nFirst, since the question about the contribution is the same as the one raised by Reviewer 3, we repeat our answer here.  We wish to clarify the contributions of our work in connection to Su et al., 2018.  Compared to Su et al., 2018, we study robustness from a very different perspective --- we study the effect of training data on the robustness of any given model, while Su et al., 2018 study how different model architectures affect robustness. The most important contribution of our work is that, from the experimental results, we have observed that training data can significantly affect the robustness of models. Our findings motivate further work, such as how to further analyze the relationship of robustness and the distribution of training data, and how to enhance the robustness of a model by transforming the training data. \n\nSecond, we add in the appendix results on comparing the robustness of models in a different way by taking the mistakes of models into consideration. From our experimental results, the test accuracy usually increases with more training data. The focus of our current study is how the accuracy and robustness of models change with respect to increased training data (by randomly partitioning the original training dataset into S1 \\subset S2,...\\subset Sn, all of which are balanced). As we commented in our response to one comment in Review 2, it would be interesting future research to study how to select training data to maintain test accuracy, but deteriorate robustness. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614525, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1190/Authors|ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614525}}}, {"id": "HkeEEjMUCm", "original": null, "number": 3, "cdate": 1543019308452, "ddate": null, "tcdate": 1543019308452, "tmdate": 1543019308452, "tddate": null, "forum": "HklKWhC5F7", "replyto": "S1gvPogc3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the suggestion to explore how to explain our findings theoretically, which we agree would be interesting.  \n\nWe wish to clarify the contributions of our work in connection to Su et al., 2018. Compared to Su et al., 2018, we study robustness from a very different perspective --- we study the effect of training data on the robustness of any given model, while Su et al., 2018 study how different model architectures affect robustness. The most important contribution of our work is that, from the experimental results, we have observed that training data can significantly affect the robustness of models. Our findings motivate further work, such as how to further analyze the relationship of robustness and the distribution of training data, and how to enhance the robustness of a model by transforming the training data. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614525, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1190/Authors|ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614525}}}, {"id": "SJxad5zUCX", "original": null, "number": 2, "cdate": 1543019125225, "ddate": null, "tcdate": 1543019125225, "tmdate": 1543019125225, "tddate": null, "forum": "HklKWhC5F7", "replyto": "rJxmqa5s3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for the helpful comments and suggestions. We will answer the questions as listed:\n\nQ1: For the motivation example I assume the following assessment holds true. Several linear functions are sampled and compose S_1, S_2, and T.\n\nA1: We apologize for the unclear description in the paper.  Only one linear function is used to generate natural training and testing data. In more detail, we generate the data using the function y = a * x + u, where a is a fixed value.  For example, let a = 2, we can sample the data using y = 2x + u. To generate each pair of (x, y), we randomly sample both x and u with x from the range (0, 10) and u from the range (-1, 1).  Then, we compute the corresponding y using the sampled x and u. \n\nQ2: A single linear regression model is used to fit all the data, either S_1 or (S_1 and S_2). If that is the case the experiment is not clear to me since the single linear model can only fit the data mean, mean slope (a) and constant (mu). Since the joint dataset better captures the mean of T the error for the joint training should be lower indeed.\n\nA2: Yes, in the example, the joint dataset better captures the mean of T, and the error for the joint training is lower, i.e., the mean squared error of M2 (computed on more training data) is smaller than that of M1 on T. \n\nQ3: However, to actually compare both values the same threshold theta should be used for both and not a percentage of their performance.\n\nA3: In the example, we do actually use the same threshold \\theta for both M1 and M2.  Our use of the symbol \\theta might have confused the reviewer, which we apologize --- it is the relative error, not a percentage. In more detail, the robustness of each of the models is measured by the average distortion it can tolerate to make correct predictions for the testing data in T. Here, we say a prediction is correct if the relative error of the prediction and the label is smaller than the given threshold, \\theta.  The same threshold is used for M1 and M2. For example, given a testing data t \\in T, we compute the robustness of M1 on t as follows. We continue adding bigger and bigger distortions to t by a tiny step.  For each t\u2019, we check if the model still predicts correctly on it (by checking if the relative error of the prediction on t\u2019 and the label exceeds the threshold \\theta). If no, we add additional distortions.  If yes, we return |t\u2019 - t| as the robustness value of M1 on t. \n\nQ4: I would argue that this very simple model does not provide any valuable insight into the problem due to its construction.\n\nA4: Through this simple example, we want to show analytically (rather than empirically) the existence of our observed phenomenon, i.e., models trained on more data can be more accurate but less robust.  In the example, we generate two datasets, S1 and S2 with S1 \\subset S2. Next, we compute two linear regression models M1 and M2 on S1 and S2 respectively based on a closed-form calculation. Then, we show that M2 is more accurate than M1 since the mean squared error of M2 on the testing set T is smaller than that of M1. And for each testing data t in T, we show that M1 tolerates more distortions on t to still make correct predictions than M2, which indicates that M2 is less robust than M1. The phenomenon, i.e, with more training data, the linear regression model can be more accurate but less robust, motivated us to conduct this extensive empirical analysis to investigate and understand if (and how much) this phenomenon holds for realistic neural network models for image classification tasks. \n\nThe reviewer\u2019s last question concerns how to measure and compare the robustness of models. In this paper, we use the average distortions on the commonly correctly predicted images to compare the robustness of models. We add further results in the appendix using a different metric (including the mistakes of the models), which confirm the same pattern. Usually the test accuracy increases with more training data. It would be interesting future research to study how to select training data to maintain test accuracy, but deteriorate robustness. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614525, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1190/Authors|ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614525}}}, {"id": "BJx-g_GU07", "original": null, "number": 1, "cdate": 1543018473101, "ddate": null, "tcdate": 1543018473101, "tmdate": 1543018473101, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "content": {"title": "Revision Summary", "comment": "We thank the reviewers for the helpful feedback, which has helped us refine our paper and guided us to clarify some important misinterpretations in the reviews, in particular, concerning the contributions of our work with respect to Su et al., 2018.  We have updated our paper accordingly. A major update is the extra appendix to provide a different measurement of robustness based on questions from the reviewers.  \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614525, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklKWhC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1190/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1190/Authors|ICLR.cc/2019/Conference/Paper1190/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers", "ICLR.cc/2019/Conference/Paper1190/Authors", "ICLR.cc/2019/Conference/Paper1190/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614525}}}, {"id": "S1xDWt79pm", "original": null, "number": 3, "cdate": 1542236414831, "ddate": null, "tcdate": 1542236414831, "tmdate": 1542236414831, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "content": {"title": "An empirical study of the influence of training data size on model robustness ", "review": "This paper conducts an empirical analysis of the effect of training data size on the model robustness to adversarial examples. The authors compared four different NN architectures using four different datasets for the task of image classification. Overall, the paper is easy to follow and clearly written. \n\nHowever, since Su et al., 2018, already presented similar findings, I do not see any major contribution in this paper. Additionally, I would expect the authors to conduct some more analysis of their results besides acc. and distortion levels. For examples, investigate the type of mistakes the models have made, compare models with the same test acc. but different amount of training data used to get there, some analysis/experiments to explain these findings (monitor models parameters/grads during training, etc.) \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "cdate": 1542234284730, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892595, "tmdate": 1552335892595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxmqa5s3Q", "original": null, "number": 2, "cdate": 1541283210768, "ddate": null, "tcdate": 1541283210768, "tmdate": 1541533346784, "tddate": null, "forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "content": {"title": "Clear structure and presentation of the empirical evaluation but the significance of the results is not clear.", "review": "This paper empirically evaluates the effect of the training dataset size on accuracy and robustness against adversarial attacks. The methodology of the paper is generally easy to assess and the overall idea well communicated.\n\nFor the motivation example I assume the following assessment holds true. Several linear functions are sampled and compose S_1, S_2, and T. A single linear regression model is used to fit all the data, either S_1 or (S_1 and S_2). If that is the case the experiment is not clear to me since the single linear model can only fit the data mean, mean slope (a) and constant (mu). Since the joint dataset better captures the mean of T the error for the joint training should be lower indeed. However, to actually compare both values the same threshold theta should be used for both and not a percentage of their performance. I would argue that this very simple model does not provide any valuable insight into the problem due to its construction.\n\nThe experimental setup presented in Section 4 only considers examples which are classified correctly by all data subsets. However, it is crucial to also consider the mistakes of these subsequent sets. For example, the learned model for the most restrictive dataset is most likely not exposed to a complex decision boundary, therefore it will exhibit a much smoother prediction at the cost that it will simply classify many more examples as the target class. In this case using data perturbations is not even the problem since completely different examples might be classified wrongly. Although not entirely clear, it would be very useful to consider the nearest negative neighbor in the dataset in the embedding space of the classifier to capture this problem at least partially. In general if the test accuracy is lower the learned classifier exhibits less performance, thus, adversary examples, distorted examples are not the main issue since it simply makes mistakes on visually different examples. Therefore, the overall analysis should be much more focused on models which achieve the same test performance but use require less data to achieve this performance.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1190/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification", "abstract": "Recent work has demonstrated the lack of robustness of well-trained deep neural networks (DNNs) to adversarial examples.  For example, visually indistinguishable perturbations, when mixed with an original image, can easily lead deep learning models to misclassifications.  In light of a recent study on the mutual influence between robustness and accuracy over 18 different ImageNet models, this paper investigates how training data affect the accuracy and robustness of deep neural\nnetworks. We conduct extensive experiments on four different datasets, including CIFAR-10, MNIST, STL-10, and Tiny ImageNet, with several representative neural networks. Our results reveal previously unknown phenomena that exist between the size of training data and characteristics of the resulting models. In particular, besides confirming that the model accuracy improves as the amount of training data increases, we also observe that the model robustness improves initially, but there exists a turning point after which robustness starts to decrease.  How and when such turning points occur vary for different neural networks and different datasets.", "keywords": ["Adversarial attacks", "Robustness", "CW", "I-FGSM"], "authorids": ["sulei@ucdavis.edu", "huan@huan-zhang.com", "kewang@visa.com", "zhendong.su@inf.ethz.ch"], "authors": ["Suhua Lei", "Huan Zhang", "Ke Wang", "Zhendong Su"], "pdf": "/pdf/5b9bd31c5bd6f380776f65d90a087f4095be3c04.pdf", "paperhash": "lei|how_training_data_affect_the_accuracy_and_robustness_of_neural_networks_for_image_classification", "_bibtex": "@misc{\nlei2019how,\ntitle={How Training Data Affect the Accuracy and Robustness of Neural Networks for Image Classification},\nauthor={Suhua Lei and Huan Zhang and Ke Wang and Zhendong Su},\nyear={2019},\nurl={https://openreview.net/forum?id=HklKWhC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1190/Official_Review", "cdate": 1542234284730, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklKWhC5F7", "replyto": "HklKWhC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1190/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892595, "tmdate": 1552335892595, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1190/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}