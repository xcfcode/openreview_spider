{"notes": [{"id": "B1l8iiA9tQ", "original": "r1xm69dqKQ", "number": 624, "cdate": 1538087837857, "ddate": null, "tcdate": 1538087837857, "tmdate": 1545355406383, "tddate": null, "forum": "B1l8iiA9tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Backdrop: Stochastic Backpropagation", "abstract": "We introduce backdrop, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation.  Intuitively, inserting a backdrop layer after any  convolutional layer leads to stochastic  gradients corresponding to features of that scale.  Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure. Backdrop can also be applied to problems with non-decomposable loss functions where standard SGD methods are not well suited. We perform a number of experiments and demonstrate that backdrop leads to significant improvements in generalization.", "keywords": ["stochastic optimization", "multi-scale data analysis", "non-decomposable loss", "generalization", "one-shot learning"], "authorids": ["siavash.golkar@gmail.com", "kyle.cranmer@nyu.edu"], "authors": ["Siavash Golkar", "Kyle Cranmer"], "TL;DR": "We introduce backdrop, intuitively described as dropout acting on the backpropagation pipeline and find significant improvements in generalization for problems with non-decomposable losses and problems with multi-scale, hierarchical data structure.", "pdf": "/pdf/83dfc0524fa50888ad55198adbd0377e3f184130.pdf", "paperhash": "golkar|backdrop_stochastic_backpropagation", "_bibtex": "@misc{\ngolkar2019backdrop,\ntitle={Backdrop: Stochastic Backpropagation},\nauthor={Siavash Golkar and Kyle Cranmer},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l8iiA9tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Ske_97TJe4", "original": null, "number": 1, "cdate": 1544700816062, "ddate": null, "tcdate": 1544700816062, "tmdate": 1545354507256, "tddate": null, "forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper624/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers pointed out that the proximity with Dropout warranted special treatment and that the justification provided in the paper was not enough to understand why exactly the changes were important. In its current state, this work is not suitable for publication to ICLR.\n\nShould you decide to resubmit this work to another venue, please take the reviewers' comments into account.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Limited novelty compared to Dropout"}, "signatures": ["ICLR.cc/2019/Conference/Paper624/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper624/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Backdrop: Stochastic Backpropagation", "abstract": "We introduce backdrop, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation.  Intuitively, inserting a backdrop layer after any  convolutional layer leads to stochastic  gradients corresponding to features of that scale.  Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure. Backdrop can also be applied to problems with non-decomposable loss functions where standard SGD methods are not well suited. We perform a number of experiments and demonstrate that backdrop leads to significant improvements in generalization.", "keywords": ["stochastic optimization", "multi-scale data analysis", "non-decomposable loss", "generalization", "one-shot learning"], "authorids": ["siavash.golkar@gmail.com", "kyle.cranmer@nyu.edu"], "authors": ["Siavash Golkar", "Kyle Cranmer"], "TL;DR": "We introduce backdrop, intuitively described as dropout acting on the backpropagation pipeline and find significant improvements in generalization for problems with non-decomposable losses and problems with multi-scale, hierarchical data structure.", "pdf": "/pdf/83dfc0524fa50888ad55198adbd0377e3f184130.pdf", "paperhash": "golkar|backdrop_stochastic_backpropagation", "_bibtex": "@misc{\ngolkar2019backdrop,\ntitle={Backdrop: Stochastic Backpropagation},\nauthor={Siavash Golkar and Kyle Cranmer},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l8iiA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper624/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353148541, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper624/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper624/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper624/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353148541}}}, {"id": "S1lt8Yuan7", "original": null, "number": 3, "cdate": 1541405009030, "ddate": null, "tcdate": 1541405009030, "tmdate": 1541533833763, "tddate": null, "forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "content": {"title": "The proposed Backdrop is similar to the traditional Dropout method. Overall this paper lacks of novelty and the observed generalization performance does not have convincing justification. ", "review": "This paper proposes a stochastic based method, namely Backdrop, for updating the network structures via backpropagation type methods.  Backdrop inserts masking layers along the network; it acts as the identity in the forward pass, but as randomly masks parts of the backward gradient propagation. The paper claims this approach can significantly improves the overall generalization performance. \n\nAlthough some difference to Dropout is summarized in Section 2, I still feel these two methods have almost the same idea, with just different implementation. Actually this Backdrop seems to have one more limitation in the parameter complexity, as it introduces several mask layers but keep the dense structures from other intermediate layers. \n\nThe proposed Backdrop uses Bernoulli distribution to select active variables. This is the very fundamental way in the conventional Dropout method. On the other hand, the authors do not  provide convincing justification how this can guarantee the improvement in subsequent generalization. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper624/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Backdrop: Stochastic Backpropagation", "abstract": "We introduce backdrop, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation.  Intuitively, inserting a backdrop layer after any  convolutional layer leads to stochastic  gradients corresponding to features of that scale.  Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure. Backdrop can also be applied to problems with non-decomposable loss functions where standard SGD methods are not well suited. We perform a number of experiments and demonstrate that backdrop leads to significant improvements in generalization.", "keywords": ["stochastic optimization", "multi-scale data analysis", "non-decomposable loss", "generalization", "one-shot learning"], "authorids": ["siavash.golkar@gmail.com", "kyle.cranmer@nyu.edu"], "authors": ["Siavash Golkar", "Kyle Cranmer"], "TL;DR": "We introduce backdrop, intuitively described as dropout acting on the backpropagation pipeline and find significant improvements in generalization for problems with non-decomposable losses and problems with multi-scale, hierarchical data structure.", "pdf": "/pdf/83dfc0524fa50888ad55198adbd0377e3f184130.pdf", "paperhash": "golkar|backdrop_stochastic_backpropagation", "_bibtex": "@misc{\ngolkar2019backdrop,\ntitle={Backdrop: Stochastic Backpropagation},\nauthor={Siavash Golkar and Kyle Cranmer},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l8iiA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "cdate": 1542234417151, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper624/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766323, "tmdate": 1552335766323, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper624/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkgFP83Y3X", "original": null, "number": 1, "cdate": 1541158497431, "ddate": null, "tcdate": 1541158497431, "tmdate": 1541533833561, "tddate": null, "forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "content": {"title": "Interesting idea, lacking proper improvements and/or applications provided", "review": "This paper introduces a data dependent strategy to mask parts of the partial derivatives in the chain rule computation. \n\nTypically with papers proposing modifications of the training regime of the neural network one would expect one of three outcomes:\n - a well justified, mathematically sound method, well tested in simple cases and with some proof of concept results on proper tasks\n - a more heuristic, empirical driven research, where strong results on proper tasks\n - method, however justified, allows us to do something previously impossible, removing some limitations/constraints (like biologically plausible learning etc.)\n\nIn its current form paper seems to lack any of these characteristics. On one hand method lacks any guarantees and on the other paper does not present significant improvements under any approved metrics, nor it introduces new which can be properly quantified. In fact, authors explicitly claim that empirical section \"Note that in these experiments, the purpose is not to achieve state of the art performance, but to exemplify how backdrop can be used and what measure of performance gains one can expect.\".  \n\nWith methods like this it is almost obvious that resulting update is not an unbiased gradient estimator of any function. Consequently convergence/learning guarantees that we have for GD or SGD no longer apply. Do authors have any thoughts on how bad can it get? As noted in the text, other methods of \"dropping\" data (such as dropout) don't have this issue as they still estimate proper gradients. Here, since dropping is done inside the network only on backwards pass, resulting estimates could, in principle, lead to oscilations, divergence and other issues. If these are not encountered in practice it might be interesting to understand why. \n\nIf authors prefer to go through more empirical path, one would expect at least to see some baselines for tasks proposed, rather than comparing Backdrop to SGD. There are many methods that could be applied in scenarios like this, including dozens forms of dropout (which, as authors note, is not aimed at the same goals, but this does not mean that it will not shine under the metrics introduced, as they are non-standard and so - noone tested them in this exact regime).\n\nI am happy to revisit my rating given authors restructure paper towards one of these paths (or other one which is not listed here).", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper624/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Backdrop: Stochastic Backpropagation", "abstract": "We introduce backdrop, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation.  Intuitively, inserting a backdrop layer after any  convolutional layer leads to stochastic  gradients corresponding to features of that scale.  Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure. Backdrop can also be applied to problems with non-decomposable loss functions where standard SGD methods are not well suited. We perform a number of experiments and demonstrate that backdrop leads to significant improvements in generalization.", "keywords": ["stochastic optimization", "multi-scale data analysis", "non-decomposable loss", "generalization", "one-shot learning"], "authorids": ["siavash.golkar@gmail.com", "kyle.cranmer@nyu.edu"], "authors": ["Siavash Golkar", "Kyle Cranmer"], "TL;DR": "We introduce backdrop, intuitively described as dropout acting on the backpropagation pipeline and find significant improvements in generalization for problems with non-decomposable losses and problems with multi-scale, hierarchical data structure.", "pdf": "/pdf/83dfc0524fa50888ad55198adbd0377e3f184130.pdf", "paperhash": "golkar|backdrop_stochastic_backpropagation", "_bibtex": "@misc{\ngolkar2019backdrop,\ntitle={Backdrop: Stochastic Backpropagation},\nauthor={Siavash Golkar and Kyle Cranmer},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l8iiA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "cdate": 1542234417151, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper624/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766323, "tmdate": 1552335766323, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper624/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bke37P8q2Q", "original": null, "number": 2, "cdate": 1541199651825, "ddate": null, "tcdate": 1541199651825, "tmdate": 1541533833357, "tddate": null, "forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "content": {"title": "Small modification and not enough comparison to other methods", "review": "The authors propose to apply Dropout only in the backward pass, by applying a mask sampled from a Bernoulli distribution. They claim that this method can help in situations like optimizing non-decomposable losses where minibatch SGD is not viable. \n\nFirst and foremost, the paper has an acknowledgement paragraph that gives information violating, in my sense, the anonymity requirement. \n\nThis being said, I have other concerns with the paper, and this possible violation didn't effect much my rating. \n\nFirst, the authors claim that the proposed method \"is a flexible strategy for introducing data-dependent stochasticity into the gradient\". However, it doesn't seem to me that the sampled dropped nodes are data-dependent. \n\nIt is also not clear to me why the proposed method is better suited to non-decomposable losses and hierarchically structured data than the classical Dropout.\n\nMoreover, while the method is clearly related to Dropout, the paper lacks of comparison to this regularizer. \n\nThis being said, the idea is sound, and can have a good impact in for example combining the good aspects of batch-normalization and dropout. However, the authors structured the paper on a completely different argument that doesn't convince me for the reasons cited above.     ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper624/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Backdrop: Stochastic Backpropagation", "abstract": "We introduce backdrop, a flexible and simple-to-implement method, intuitively described as dropout acting only along the backpropagation pipeline. Backdrop is implemented via one or more masking layers which are inserted at specific points along the network. Each backdrop masking layer acts as the identity in the forward pass, but randomly masks parts of the backward gradient propagation.  Intuitively, inserting a backdrop layer after any  convolutional layer leads to stochastic  gradients corresponding to features of that scale.  Therefore, backdrop is well suited for problems in which the data have a multi-scale, hierarchical structure. Backdrop can also be applied to problems with non-decomposable loss functions where standard SGD methods are not well suited. We perform a number of experiments and demonstrate that backdrop leads to significant improvements in generalization.", "keywords": ["stochastic optimization", "multi-scale data analysis", "non-decomposable loss", "generalization", "one-shot learning"], "authorids": ["siavash.golkar@gmail.com", "kyle.cranmer@nyu.edu"], "authors": ["Siavash Golkar", "Kyle Cranmer"], "TL;DR": "We introduce backdrop, intuitively described as dropout acting on the backpropagation pipeline and find significant improvements in generalization for problems with non-decomposable losses and problems with multi-scale, hierarchical data structure.", "pdf": "/pdf/83dfc0524fa50888ad55198adbd0377e3f184130.pdf", "paperhash": "golkar|backdrop_stochastic_backpropagation", "_bibtex": "@misc{\ngolkar2019backdrop,\ntitle={Backdrop: Stochastic Backpropagation},\nauthor={Siavash Golkar and Kyle Cranmer},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l8iiA9tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper624/Official_Review", "cdate": 1542234417151, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l8iiA9tQ", "replyto": "B1l8iiA9tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper624/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335766323, "tmdate": 1552335766323, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper624/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}