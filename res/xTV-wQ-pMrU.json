{"notes": [{"id": "xTV-wQ-pMrU", "original": "vIfBiPZORC", "number": 3358, "cdate": 1601308372527, "ddate": null, "tcdate": 1601308372527, "tmdate": 1614985771649, "tddate": null, "forum": "xTV-wQ-pMrU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking", "authorids": ["~Andrew_N_Carr1", "~Quentin_Berthet2", "~Mathieu_Blondel1", "~Olivier_Teboul2", "~Neil_Zeghidour1"], "authors": ["Andrew N Carr", "Quentin Berthet", "Mathieu Blondel", "Olivier Teboul", "Neil Zeghidour"], "keywords": [], "abstract": "Self-supervised pre-training using so-called \"pretext\" tasks has recently shown impressive performance across a wide range of tasks. In this work we advance self-supervised learning from permutations, that consists in shuffling parts of input and training a model to reorder them, improving downstream performance in classification. To do so, we overcome the main challenges of integrating permutation inversions (a discontinuous operation) into an end-to-end training scheme, heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. These advances rely on two main, independent contributions. First, we use recent advances in differentiable ranking to integrate the permutation inversion flawlessly into a neural network, enabling us to use the full set of permutations, at no additional computing cost. Our experiments validate that learning from all possible permutations (up to $10^{18}$) improves the quality of the pre-trained representations over using a limited, fixed set. Second, we successfully demonstrate that inverting permutations is a meaningful pretext task in a diverse range of modalities, beyond images, which does not require modality-specific design. In particular, we also improve music understanding by reordering spectrogram patches in the frequency space, as well as video classification by reordering frames along the time axis. We furthermore analyze the influence of the patches that we use (vertical, horizontal, 2-dimensional), as well as the benefit of our approach in different data regimes. ", "one-sentence_summary": "We use recent advances in differentiable ranking to allow for self-supervised pre-training using the full set of permutations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "carr|shuffle_to_learn_selfsupervised_learning_from_permutations_via_differentiable_ranking", "pdf": "/pdf/2c3a505ad201e10cefa00cb6938a8c5c746a0293.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6F2JwWJ6In", "_bibtex": "@misc{\ncarr2021shuffle,\ntitle={Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking},\nauthor={Andrew N Carr and Quentin Berthet and Mathieu Blondel and Olivier Teboul and Neil Zeghidour},\nyear={2021},\nurl={https://openreview.net/forum?id=xTV-wQ-pMrU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8AJOxAYA6F1", "original": null, "number": 1, "cdate": 1610040360538, "ddate": null, "tcdate": 1610040360538, "tmdate": 1610473950672, "tddate": null, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "invitation": "ICLR.cc/2021/Conference/Paper3358/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "There is clear consensus on this submission. Reviewers cite a lack of comparison\nwith recent state-of-the-art methods and experiments on more realistic datasets.\nThough the reviewers find aspects of the approach interesting, the decision is\nto reject.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking", "authorids": ["~Andrew_N_Carr1", "~Quentin_Berthet2", "~Mathieu_Blondel1", "~Olivier_Teboul2", "~Neil_Zeghidour1"], "authors": ["Andrew N Carr", "Quentin Berthet", "Mathieu Blondel", "Olivier Teboul", "Neil Zeghidour"], "keywords": [], "abstract": "Self-supervised pre-training using so-called \"pretext\" tasks has recently shown impressive performance across a wide range of tasks. In this work we advance self-supervised learning from permutations, that consists in shuffling parts of input and training a model to reorder them, improving downstream performance in classification. To do so, we overcome the main challenges of integrating permutation inversions (a discontinuous operation) into an end-to-end training scheme, heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. These advances rely on two main, independent contributions. First, we use recent advances in differentiable ranking to integrate the permutation inversion flawlessly into a neural network, enabling us to use the full set of permutations, at no additional computing cost. Our experiments validate that learning from all possible permutations (up to $10^{18}$) improves the quality of the pre-trained representations over using a limited, fixed set. Second, we successfully demonstrate that inverting permutations is a meaningful pretext task in a diverse range of modalities, beyond images, which does not require modality-specific design. In particular, we also improve music understanding by reordering spectrogram patches in the frequency space, as well as video classification by reordering frames along the time axis. We furthermore analyze the influence of the patches that we use (vertical, horizontal, 2-dimensional), as well as the benefit of our approach in different data regimes. ", "one-sentence_summary": "We use recent advances in differentiable ranking to allow for self-supervised pre-training using the full set of permutations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "carr|shuffle_to_learn_selfsupervised_learning_from_permutations_via_differentiable_ranking", "pdf": "/pdf/2c3a505ad201e10cefa00cb6938a8c5c746a0293.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6F2JwWJ6In", "_bibtex": "@misc{\ncarr2021shuffle,\ntitle={Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking},\nauthor={Andrew N Carr and Quentin Berthet and Mathieu Blondel and Olivier Teboul and Neil Zeghidour},\nyear={2021},\nurl={https://openreview.net/forum?id=xTV-wQ-pMrU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040360522, "tmdate": 1610473950654, "id": "ICLR.cc/2021/Conference/Paper3358/-/Decision"}}}, {"id": "7QDcoieE_bF", "original": null, "number": 2, "cdate": 1603972946876, "ddate": null, "tcdate": 1603972946876, "tmdate": 1605024015524, "tddate": null, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "invitation": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review", "content": {"title": "Soft ranking like loss can make jigsaw like unsupervised learning framework more feasible.", "review": "The paper applied differentiable ranking operator on unsupervised learning framework that uses permutation based pretext task. They evaluated the proposed approach in audio, video, and image classification tasks. The results show that the proposed differentiable ranking operator is showing better performance than the pre assigned fixed permutation.\n\nOverall, the contribution of the paper is limited in applying the soft ranking loss (or perturbed FY) on permutation based unsupervised learning framework. I think there can be two factors that can improve the performance of the proposed approach to the baseline approach (fixed permutation). The first one can be related to the soft ranking (or perturbed FY) loss itself (unlike the classification of permutation cases or regression) and the second one can be related to the use of the number of permutation cases. By using the the soft ranking type loss, they could tackle the latter factor by using any permutation cases in the training phase. However, to verify where the performance gain come from, the authors can make an another comparison that is based on classification type loss (classification on N! permutation cases). It may be hard to deal with extremely large number of class labels, but at least some comparison table can be added with small sub dataset to verify the distinction between the two factors.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3358/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3358/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking", "authorids": ["~Andrew_N_Carr1", "~Quentin_Berthet2", "~Mathieu_Blondel1", "~Olivier_Teboul2", "~Neil_Zeghidour1"], "authors": ["Andrew N Carr", "Quentin Berthet", "Mathieu Blondel", "Olivier Teboul", "Neil Zeghidour"], "keywords": [], "abstract": "Self-supervised pre-training using so-called \"pretext\" tasks has recently shown impressive performance across a wide range of tasks. In this work we advance self-supervised learning from permutations, that consists in shuffling parts of input and training a model to reorder them, improving downstream performance in classification. To do so, we overcome the main challenges of integrating permutation inversions (a discontinuous operation) into an end-to-end training scheme, heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. These advances rely on two main, independent contributions. First, we use recent advances in differentiable ranking to integrate the permutation inversion flawlessly into a neural network, enabling us to use the full set of permutations, at no additional computing cost. Our experiments validate that learning from all possible permutations (up to $10^{18}$) improves the quality of the pre-trained representations over using a limited, fixed set. Second, we successfully demonstrate that inverting permutations is a meaningful pretext task in a diverse range of modalities, beyond images, which does not require modality-specific design. In particular, we also improve music understanding by reordering spectrogram patches in the frequency space, as well as video classification by reordering frames along the time axis. We furthermore analyze the influence of the patches that we use (vertical, horizontal, 2-dimensional), as well as the benefit of our approach in different data regimes. ", "one-sentence_summary": "We use recent advances in differentiable ranking to allow for self-supervised pre-training using the full set of permutations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "carr|shuffle_to_learn_selfsupervised_learning_from_permutations_via_differentiable_ranking", "pdf": "/pdf/2c3a505ad201e10cefa00cb6938a8c5c746a0293.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6F2JwWJ6In", "_bibtex": "@misc{\ncarr2021shuffle,\ntitle={Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking},\nauthor={Andrew N Carr and Quentin Berthet and Mathieu Blondel and Olivier Teboul and Neil Zeghidour},\nyear={2021},\nurl={https://openreview.net/forum?id=xTV-wQ-pMrU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3358/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077407, "tmdate": 1606915761357, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3358/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review"}}}, {"id": "-PTkDWBKfm", "original": null, "number": 3, "cdate": 1603992932471, "ddate": null, "tcdate": 1603992932471, "tmdate": 1605024015456, "tddate": null, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "invitation": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review", "content": {"title": "Missing comparisons to related methods", "review": "This paper presents a self-supervised learning task of shuffling input patches and demanding the network to learn to unshuffle. A related prior work, Noorozi and Favaro (2016) uses a fixed set of permutations to do this task for a given number of patches, and the current paper argues to expand this idea for the full set of permutations. To this end, the paper encodes a permutation as a number tuple, with the goal of the network to learn to produce the correct tuple that has the numbers in order. As the numbers are discrete and thus non-differentiable, the paper suggests differentiable soft-variants using stochastic perturbations and regularizations (Fenchel-Young loss). Experiments are provided on audio and video tasks and show promise over the method of Noorozi and Favaro (2016).\n\nPros\n1. A relatively simple self-supervised task with simple regularizations.\n2. Experiments show some promise. \n\nCons: \n1. Since the work of Noorozi and Favaro (2016), there have been several similar approaches to self-supervised learning that are not covered by the paper, either in the related work or in the experimental comparisons. A very reated work is \n[a] Visual Permutation Learning, Santa Cruz et al., TPAMI 2018\nthat attempts to tackle the very specific problem of fixed set of permutations used in Noorozi and Favaro (2016). They also propose an end-to-end neural network for solving the the permutation finding task. The paper should contrast against this work both technically and in experiments. \n\n[2] The experiments are rather weak in my opinion. As noted above, there are several prior papers (see the papers that cite the above paper) that attempts the unshuffling task in various ways, while the paper only makes comparison to the 2016 baseline paper. More recent baselines should be included in the comparisons and the advantages of the specific approach needs to be convincingly established. \n\nOverall, while the idea is interesting, the paper does not place its contributions within recent prior works.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3358/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3358/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking", "authorids": ["~Andrew_N_Carr1", "~Quentin_Berthet2", "~Mathieu_Blondel1", "~Olivier_Teboul2", "~Neil_Zeghidour1"], "authors": ["Andrew N Carr", "Quentin Berthet", "Mathieu Blondel", "Olivier Teboul", "Neil Zeghidour"], "keywords": [], "abstract": "Self-supervised pre-training using so-called \"pretext\" tasks has recently shown impressive performance across a wide range of tasks. In this work we advance self-supervised learning from permutations, that consists in shuffling parts of input and training a model to reorder them, improving downstream performance in classification. To do so, we overcome the main challenges of integrating permutation inversions (a discontinuous operation) into an end-to-end training scheme, heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. These advances rely on two main, independent contributions. First, we use recent advances in differentiable ranking to integrate the permutation inversion flawlessly into a neural network, enabling us to use the full set of permutations, at no additional computing cost. Our experiments validate that learning from all possible permutations (up to $10^{18}$) improves the quality of the pre-trained representations over using a limited, fixed set. Second, we successfully demonstrate that inverting permutations is a meaningful pretext task in a diverse range of modalities, beyond images, which does not require modality-specific design. In particular, we also improve music understanding by reordering spectrogram patches in the frequency space, as well as video classification by reordering frames along the time axis. We furthermore analyze the influence of the patches that we use (vertical, horizontal, 2-dimensional), as well as the benefit of our approach in different data regimes. ", "one-sentence_summary": "We use recent advances in differentiable ranking to allow for self-supervised pre-training using the full set of permutations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "carr|shuffle_to_learn_selfsupervised_learning_from_permutations_via_differentiable_ranking", "pdf": "/pdf/2c3a505ad201e10cefa00cb6938a8c5c746a0293.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6F2JwWJ6In", "_bibtex": "@misc{\ncarr2021shuffle,\ntitle={Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking},\nauthor={Andrew N Carr and Quentin Berthet and Mathieu Blondel and Olivier Teboul and Neil Zeghidour},\nyear={2021},\nurl={https://openreview.net/forum?id=xTV-wQ-pMrU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3358/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077407, "tmdate": 1606915761357, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3358/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review"}}}, {"id": "z-0dWMsSkki", "original": null, "number": 1, "cdate": 1603551256030, "ddate": null, "tcdate": 1603551256030, "tmdate": 1605024015392, "tddate": null, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "invitation": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review", "content": {"title": "The idea is interesting but it  lacks experimental support.", "review": "Summary:\n\nThe paper proposes to use differentiable ranking techniques to predict correct permutations. It can be applied to audio, video and images. It is quite obvious from the experiments that using differentiable ranking is more powerful to learn representations compared to fixed permutation in various applications.\n\nStrengths:\n\n- The idea is interesting and general to apply to different modalities.\n- It is well written and easy to follow.\n\nConcerns:\n\n- Lack of comparison with state-of-the-art methods. \n- In downstream tasks, there are always labels available, it is common to test on unsupervised setting as well.\n- In Section 3.3, the experiment is conducted on MNIST, it would be more convincing to show results on larger datasets, since it is quite common to apply it on ImageNet. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3358/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3358/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking", "authorids": ["~Andrew_N_Carr1", "~Quentin_Berthet2", "~Mathieu_Blondel1", "~Olivier_Teboul2", "~Neil_Zeghidour1"], "authors": ["Andrew N Carr", "Quentin Berthet", "Mathieu Blondel", "Olivier Teboul", "Neil Zeghidour"], "keywords": [], "abstract": "Self-supervised pre-training using so-called \"pretext\" tasks has recently shown impressive performance across a wide range of tasks. In this work we advance self-supervised learning from permutations, that consists in shuffling parts of input and training a model to reorder them, improving downstream performance in classification. To do so, we overcome the main challenges of integrating permutation inversions (a discontinuous operation) into an end-to-end training scheme, heretofore sidestepped by casting the reordering task as classification, fundamentally reducing the space of permutations that can be exploited. These advances rely on two main, independent contributions. First, we use recent advances in differentiable ranking to integrate the permutation inversion flawlessly into a neural network, enabling us to use the full set of permutations, at no additional computing cost. Our experiments validate that learning from all possible permutations (up to $10^{18}$) improves the quality of the pre-trained representations over using a limited, fixed set. Second, we successfully demonstrate that inverting permutations is a meaningful pretext task in a diverse range of modalities, beyond images, which does not require modality-specific design. In particular, we also improve music understanding by reordering spectrogram patches in the frequency space, as well as video classification by reordering frames along the time axis. We furthermore analyze the influence of the patches that we use (vertical, horizontal, 2-dimensional), as well as the benefit of our approach in different data regimes. ", "one-sentence_summary": "We use recent advances in differentiable ranking to allow for self-supervised pre-training using the full set of permutations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "carr|shuffle_to_learn_selfsupervised_learning_from_permutations_via_differentiable_ranking", "pdf": "/pdf/2c3a505ad201e10cefa00cb6938a8c5c746a0293.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=6F2JwWJ6In", "_bibtex": "@misc{\ncarr2021shuffle,\ntitle={Shuffle to Learn: Self-supervised learning from permutations via differentiable ranking},\nauthor={Andrew N Carr and Quentin Berthet and Mathieu Blondel and Olivier Teboul and Neil Zeghidour},\nyear={2021},\nurl={https://openreview.net/forum?id=xTV-wQ-pMrU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "xTV-wQ-pMrU", "replyto": "xTV-wQ-pMrU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3358/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077407, "tmdate": 1606915761357, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3358/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3358/-/Official_Review"}}}], "count": 5}