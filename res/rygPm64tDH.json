{"notes": [{"id": "rygPm64tDH", "original": "Hkesgh1PvH", "number": 453, "cdate": 1569439007231, "ddate": null, "tcdate": 1569439007231, "tmdate": 1577168292967, "tddate": null, "forum": "rygPm64tDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ihmfUxggG", "original": null, "number": 1, "cdate": 1576798696907, "ddate": null, "tcdate": 1576798696907, "tmdate": 1576800938786, "tddate": null, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Decision", "content": {"decision": "Reject", "comment": "This work claims two primary contributions: first a new saliency method \"expected gradients\" is proposed, and second the authors propose the idea of attribution priors to improve model performance by integrating domain knowledge during training. Reviewers agreed that the expected gradients method is interesting and novel, and experiments such as Table 1 are a good starting point to demonstrate the effectiveness of the new method. However, the claimed \"novel framework, attribution priors\" has large overlap with prior work [1]. One suggestion for improving the paper is to revise the introduction and experiments to support the claim \"expected gradients improve model explainability and yield effective attribution priors\" rather than claiming to introduce attribution priors as a new framework. One possibility for strengthening this claim is to revisit experiments in [1] and related follow-up work to demonstrate that expected gradients yield improvements over existing saliency methods. Additionally, current experiments in Table 1 only consider integrated gradients as a baseline saliency method, there are many others worth considering, see for example the suite of methods explored in [2]. \n\nFinally, I would add that the current section on distribution shift provides an overly narrow perspective on model robustness by only considering robustness to additive Gaussian noise. It is known that it is easy to improve robustness to Gaussian noise by biasing the model towards low frequency statistics in the data, however this typically results in degraded robustness to other kinds of noise types. See for example [3], where it was observed that adversarial training degrades model robustness to low frequency noise and the fog corruption. If the authors wish to pursue using attribution priors for improving robustness to distribution shift, it is important that they evaluate on a more varied suite of corruptions/noise types [4]. Additionally, one should compare against strong baselines in this area [5].\n\n1. https://arxiv.org/abs/1703.03717\n2. https://arxiv.org/abs/1810.03292\n3. https://arxiv.org/abs/1906.08988\n4. https://arxiv.org/abs/1807.01697\n5. https://arxiv.org/abs/1811.12231\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729495, "tmdate": 1576800282094, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper453/-/Decision"}}}, {"id": "r1l-8Io2ir", "original": null, "number": 6, "cdate": 1573856841270, "ddate": null, "tcdate": 1573856841270, "tmdate": 1573856841270, "tddate": null, "forum": "rygPm64tDH", "replyto": "rkx_TWaojH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "Thanks for the pointer", "comment": "Thank you, AnonReviewer 1. I'm relative new to the field as claimed above. So, I may have not make the connection to that existing paper. I do find this paper is self-contained and well-written, which would is a good read to me. I totally respect your opinion that if there is nothing new compared to  https://arxiv.org/abs/1703.03717 then I would be happy to have my score ignored."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "SyxU6f_hiH", "original": null, "number": 5, "cdate": 1573843645740, "ddate": null, "tcdate": 1573843645740, "tmdate": 1573843645740, "tddate": null, "forum": "rygPm64tDH", "replyto": "BJx3xBU0tr", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3", "comment": "Thank you for discussing several strengths of our paper. We believe these strengths represent a significant contribution toward addressing the issue of incorporating human knowledge into neural network models. Below, we discuss the weaknesses described in the review: \n\n1) We first want to respond to the point that only a limited set of expert-invented human priors can be used in our approach. There are also only a limited number of expert-invented ways to regularize model parameters, yet parameter regularization (from parameter priors) is very important and widely studied in machine learning and statistics. In the same way we believe that regularizing feature attributions using expert-invented attribution priors promises to be a fundamentally new alternative to parameter regularization. Our paper significantly extends the pioneering work in this area by Ross et al (2017), and in doing so greatly expands the applicability of attribution priors.\n\n2) In the context of incorporating human knowledge into machine learning models, we believe that one critical evaluation metric is how well our models do on prediction tasks. Our experiments show that our method is successful: it leads to improved performance in all three domains by using penalties derived from human intuition about the data. In terms of learning more intuitive models, we chose three task-specific metrics to optimize: smoothness in images, capturing related genes in gene expression data, and sparsity on clinical data. In all cases, we achieve our stated goal as evidenced by Figures 1-3. Whether or not these goals represent the most human-intuitive goals to optimize for in their respective domain would be valuable future work. In general, what constitutes an \u201cinterpretable\u201d model is a challenging and open question. However, given the flexibility of our framework, we anticipate that it can be adapted to changing definitions of interpretability."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "B1xxifu2sB", "original": null, "number": 4, "cdate": 1573843607624, "ddate": null, "tcdate": 1573843607624, "tmdate": 1573843607624, "tddate": null, "forum": "rygPm64tDH", "replyto": "rkx_TWaojH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "On the issue of novelty", "comment": "Thank you for your concerns -- we hope that our comments in \"Response to Official Blind Review #1\" address some of these points."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "SylrvMdhsB", "original": null, "number": 3, "cdate": 1573843549426, "ddate": null, "tcdate": 1573843549426, "tmdate": 1573843549426, "tddate": null, "forum": "rygPm64tDH", "replyto": "Syer6jjDYH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "Official Blind Review #2", "comment": "Thank you for pointing out some strengths of our paper. Sundarajan et al. (2017) certainly provide motivation for our work. We make two substantial contributions beyond the 2017 paper. First, we provide a better solution for the problem they present -- axiomatic attributions in deep networks -- by showing in Table 1 that Expected Gradients provides better explanations across a range of metrics. Second, we solve a new problem that Integrated Gradients did not discuss at all -- we show how to use the output of axiomatic attributions to highlight problems in a model\u2019s behavior and use explicit priors to fix those problems. \n\nWe do not compare against Integrated Gradients in our attribution prior experiments for computational reasons. Without re-formulating IG as an expectation, it is necessary to approximate a path integral at every training step, which would have required up to 50 additional back-propagation calls per training step, which was intractable for larger datasets. The reformulation as an expectation allows us to draw as little as one additional back-propagation call per training step and rely on regularizing the true attributions in expectation. This difficulty was in fact part of the motivation for developing and using Expected Gradients in our paper.\n\nThanks for your comments on figure references; we will make the appropriate changes in the text."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "ryg-Sfdhsr", "original": null, "number": 2, "cdate": 1573843512662, "ddate": null, "tcdate": 1573843512662, "tmdate": 1573843512662, "tddate": null, "forum": "rygPm64tDH", "replyto": "H1x8LhQRKr", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1 ", "comment": "Thank you for the feedback. We acknowledge that Ross et al. (2017) proposed a specific instance of an attribution prior as discussed in Sections 1 and 2. However, we believe our contributions go significantly beyond the method proposed by Ross et al. (2017) and list our contributions below. We would appreciate any feedback on how we could better structure the manuscript to convey these points.\n\n1) We introduce expected gradient attributions based on a novel expected-value based formulation that is specifically designed to allow for the efficient integration of attribution priors into the training process. We demonstrate both that expected gradients is a more reliable attribution method than existing methods (input gradients and integrated gradients) and that there exists a close connection between training with expected gradients and stochastic gradient descent that allows for efficient training of attribution priors (Section \u201cTraining with expected gradients\u201d). \n\n2) We demonstrate that our novel, expected gradients-based regularization method outperforms the method proposed in Ross et al. (2017) significantly in all three of our chosen applications. This advancement is likely due to our attribution method better capturing network behavior than simply the gradients themselves - it is well-understood that gradients do not reliably capture the behavior of a neural network (https://arxiv.org/abs/1611.02639, https://arxiv.org/abs/1704.02685).\n\n3) We introduce 3 domain-specific attribution priors in three different domains that outperform unregularized baselines. The prior introduced in Ross et al. (2017) was limited to a binary penalty on features, meaning that for most applications, a user needed to know which features were important in advance. Our priors only require heuristic knowledge of the data and still improve performance.\n\nWith respect to having two different focal points in our paper: we see expected gradients and attribution priors as closely related due to contributions 1) and 2). In addition to being a more reliable method than existing feature attribution methods, expected gradients allows for efficient training of an attribution prior because it is defined as an expectation. This efficiency allows us to improve over the method proposed in Ross et al. (2017). Without it, attribution priors would be limited to penalizing the raw gradients alone, which we demonstrate to be comparatively ineffective (See the red curve in Figure 1 Right, the second bar in Figure 2 Left, and the second bars in both of the left plots in Figure 3).\n\nWith respect to our paper lacking evidence that the method expected gradients performs better than existing methods, we believe that Table 1 demonstrates that expected gradients is more effective at revealing model behavior than existing attribution methods, while Figures 1-3 demonstrate that training with expected gradients outperforms existing methods, including the one proposed in Ross et al. (2017) and several domain-specific baselines, such as the aforementioned graph convolutional networks.\n\nWe will clarify the above points in the final version, and look forward to any feedback you may have on how best to present the contributions we describe above."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "rkx_TWaojH", "original": null, "number": 1, "cdate": 1573798335521, "ddate": null, "tcdate": 1573798335521, "tmdate": 1573798335521, "tddate": null, "forum": "rygPm64tDH", "replyto": "Syer6jjDYH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment", "content": {"title": "Are you sure there is novelty?", "comment": "Given the existing paper https://arxiv.org/abs/1703.03717 what is significant about this paper? I don't see a significant contribution to justify an accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygPm64tDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper453/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper453/Authors|ICLR.cc/2020/Conference/Paper453/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171270, "tmdate": 1576860530051, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper453/Authors", "ICLR.cc/2020/Conference/Paper453/Reviewers", "ICLR.cc/2020/Conference/Paper453/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Comment"}}}, {"id": "Syer6jjDYH", "original": null, "number": 1, "cdate": 1571433405506, "ddate": null, "tcdate": 1571433405506, "tmdate": 1572972593495, "tddate": null, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The authors proposed the attribution priors framework to incorporate human domain knowledge as constraints when training deep neural networks. This is a general framework that the users can define different attribution priors for different tasks. For example, in this work, the authors proposed three reasonable priors for image input, graph data, and clinical medical data. For the image and the graph data, the prior is basically to have smoother attributions for nearby features; while for the clinical medical data, the authors used the Gini coefficient formula to encourage sparsity, which is of several practical benefits clinical practice. Moreover, the authors proposed the expected gradients algorithm which is a nice extension of the integrated gradients algorithm. The benefit of expected gradients is that it does not need a baseline input, which is usually arbitrary decided by the designer. The expected gradient method does indeed also performed better than the integrated gradient method in the benchmark (see Table 1.) The results in all three experiments are impressive. In the image domain, the model does generate models that paying more attention on the foreground objects, and is more tolerant to the Gaussian noise perturbation (though it does perform less well than a non-regularized baseline model in the no-noise test image, which is understandable.) More impressively, the model does outperform all other controls with a good margin in the anti-cancer drug prediction experiment, which is a nice demonstration of that domain knowledge could be incorporated in a neural network training to achieve better performance. Same to the healthcare mortality prediction data. The authors showed with a very limited amount of data, they can use sparsity prior constraints to get a model with good feature sparsity (Gini coef), and good performance (measured by ROC-AUC). Overall, I found the paper clearly written and the results are impressive. I am not super familiar with the field, and I am not sure how much progress is this paper compared to \"Axiomatic Attribution for Deep Networks\" (Sundararajan et. al. 2017),where integrated gradients is proposed. The experiments conducted in that paper seems to be similar to the ones that are done in this paper.\n\nMinor point:\n1. Even though the authors has shown in Table 1 benchmark that expected gradient is performing better than integrated gradient. Also, in Figure 5 showing that integrated gradient cannot highlight black pixels. It would be nice to see how integrated gradient method perform in the three experiments (image, drug data, mortality prediction), does the expected gradient method always outperform?\n2. When the authors refer to Figure 2 and Figure 3 multiple times in the main text, they are referring to either left or right panel. Would be nicer to do for example \"... as measured by R^2 (Figure 2 Left).  "}, "signatures": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575239891135, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper453/Reviewers"], "noninvitees": [], "tcdate": 1570237751906, "tmdate": 1575239891148, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Review"}}}, {"id": "BJx3xBU0tr", "original": null, "number": 3, "cdate": 1571869940441, "ddate": null, "tcdate": 1571869940441, "tmdate": 1572972593454, "tddate": null, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary.\nThe paper improves the existing feature attribution method by adding regularizers to enforce (human) expectations about a model\u2019s behavior. Three different datasets (i.e. image, gene expression, health-care) are chosen to evaluate the proposed model\u2019s effectiveness, while different regularizers (i.e. image prior, graph prior, and sparsity prior) are explored for the respective task. \n\nStrengths.\n1. Incorporating human knowledge into the model has a growing interest in ML / CV communities.\n2. Three datasets from different domains (i.e. image classification data, gene expression data, and health care data) are used to evaluate the effectiveness of the proposed approach. Data shows that the proposed approach shows better generalization performance (i.e. better performance in test dataset) than baselines.\n3. The paper provides well-documented supplemental materials that contain details of the experimental setting and additional supporting figures.\n\nWeaknesses.\n1. Task-specific heuristic human prior\nI agree (and personally like) the motivation that a method is needed to align a model\u2019s behavior with human knowledge or intuition -- model\u2019s behavior may be explained by feature attribution methods while making models accept human knowledge is challenging. However, such an ability is achieved by simply adding task-specific heuristic functions as a penalty or a regularizer. Also, the introduced human priors are similar to general regularization conventions, i.e. a penalty of smoothness over adjacent pixels is commonly used in the CV community. I am concerned that only a limited set of expert-invented human priors can be used in this approach.\n\nFurther, feature attribution methods aim to develop a richer notion of the contribution of a pixel to the output. However, the difficulty would be the lack of formal measures of how the network output is affected by spatially-extended features (rather than pixels). The explored priors (e.g. a total variation loss to make neighboring pixels have a similar impact on the final verdict) actually relieve this issue.\n\n2. Incorporating humans into the modeling process?\nA key motivation behind this work is \u201cincorporating humans into the modeling process\u201d. This would imply that (human-understandable) information needs first to be transferred from a model to humans. However, I am concerned about what information end-users are expected to obtain from the model. For example, Figure 1 (left) shows an attribution map that highlights multiple intermittent regions from which I cannot understand its behavior. Unless end-users cannot understand the model\u2019s behavior, how can we expect humans can provide knowledge to model? A user study would be needed to support that the proposed method can really provide a way to incorporate humans into the modeling process.\n\nMinor comments.\n1. Plots in Figure 3 are not intuitively understandable.\n2. There is no section Conclusion.\n3. A template for the reference section looks different from other ICLR papers."}, "signatures": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575239891135, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper453/Reviewers"], "noninvitees": [], "tcdate": 1570237751906, "tmdate": 1575239891148, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Review"}}}, {"id": "H1x8LhQRKr", "original": null, "number": 2, "cdate": 1571859534201, "ddate": null, "tcdate": 1571859534201, "tmdate": 1572972593402, "tddate": null, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "invitation": "ICLR.cc/2020/Conference/Paper453/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents expected gradients which is a method which looks at a difference from a baseline defined by the training data. \n\nThe structure of the paper is strange because it discusses attribution priors but then they are not used for the method. The paper should have a single focus.\n\nAttribution priors as you formalize it in section 2 (which seems like the core contribution of the paper) was introduced in 2017 https://arxiv.org/abs/1703.03717 where they use a mask on a saliency map to regularize the representation learned. \n\nIn section 2.2. I think a few papers to have a look at are a survey article about graph based biasing http://www.nature.com/articles/s41698-017-0029-7 as well as methods for using graph convolutions with biases based on graphs: https://arxiv.org/abs/1711.05859 and https://arxiv.org/abs/1806.06975 . Some of these should serve as baselines. It is not clear which model is used in Figure 2. It is also not clear from the literature if these models are really working so I think these results should be presented in a more detail. As I understand it, real improvements in predicting clinical variables has not been shown to be reproducible so this would be a significant claim of this paper.\n\nIt is not clear if the paper is presenting \"expected gradients\" or existing attribution priors. Most of the experiments revolve around existing attribution prior methods. So with that the paper positions itself not as a survey but as a method paper but lacks evidence that the method expected gradients performs better.\n\nI am also not clear on where the image attribution prior comes from for the image task. Where is this extra information? Is it just smoothing?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper453/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Explainable Models Using Attribution Priors", "authors": ["Gabriel Erion", "Joseph D. Janizek", "Pascal Sturmfels", "Scott M. Lundberg", "Su-In Lee"], "authorids": ["erion@cs.washington.edu", "jjanizek@cs.washington.edu", "psturm@cs.washington.edu", "slund1@cs.washington.edu", "suinlee@cs.washington.edu"], "keywords": ["Deep Learning", "Interpretability", "Attributions", "Explanations", "Biology", "Health", "Computational Biology"], "TL;DR": "A method for encouraging axiomatic feature attributions of a deep model to match human intuition.", "abstract": "Two important topics in deep learning both involve incorporating humans into the modeling process: Model priors transfer information from humans to a model by regularizing the model's parameters; Model attributions transfer information from a model to humans by explaining the model's behavior. Previous work has taken important steps to connect these topics through various forms of gradient regularization. We find, however, that existing methods that use attributions to align a model's behavior with human intuition are ineffective. We develop an efficient and theoretically grounded feature attribution method, expected gradients, and a novel framework, attribution priors, to enforce prior expectations about a model's behavior during training. We demonstrate that attribution priors are broadly applicable by instantiating them on three different types of data: image data, gene expression data, and health care data. Our experiments show that models trained with attribution priors are more intuitive and achieve better generalization performance than both equivalent baselines and existing methods to regularize model behavior.", "pdf": "/pdf/324969c79279a475801b24283635e72653367037.pdf", "code": "https://www.dropbox.com/sh/xvt3vqv8xjb5nwh/AACgt-0OxiefImjVXX5UJSuua?dl=0", "paperhash": "erion|learning_explainable_models_using_attribution_priors", "original_pdf": "/attachment/324969c79279a475801b24283635e72653367037.pdf", "_bibtex": "@misc{\nerion2020learning,\ntitle={Learning Explainable Models Using Attribution Priors},\nauthor={Gabriel Erion and Joseph D. Janizek and Pascal Sturmfels and Scott M. Lundberg and Su-In Lee},\nyear={2020},\nurl={https://openreview.net/forum?id=rygPm64tDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygPm64tDH", "replyto": "rygPm64tDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper453/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575239891135, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper453/Reviewers"], "noninvitees": [], "tcdate": 1570237751906, "tmdate": 1575239891148, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper453/-/Official_Review"}}}], "count": 11}