{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396500029, "tcdate": 1486396500029, "number": 1, "id": "Sk2UnG8ux", "invitation": "ICLR.cc/2017/conference/-/paper316/acceptance", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper analyzes the dynamics of learning under Gaussian input using dynamical systems theory. As two of the reviewers have pointed out, the paper is hard to read, and not written in a way which is accessible to the wider ICLR community. Hence, I cannot recommend its acceptance to the main conference. However, I recommend acceptance to the workshop track, since it has nice technical contributions that can lead to interesting interactions. I encourage the authors to make it more accessible for a future conference.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396500582, "id": "ICLR.cc/2017/conference/-/paper316/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hk85q85ee", "replyto": "Hk85q85ee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396500582}}}, {"tddate": null, "tmdate": 1484375713391, "tcdate": 1484375713391, "number": 4, "id": "S1tiUrvIx", "invitation": "ICLR.cc/2017/conference/-/paper316/public/comment", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["~Yuandong_Tian1"], "readers": ["everyone"], "writers": ["~Yuandong_Tian1"], "content": {"title": "Rebuttal", "comment": "We thanks the reviewers for their comments.\n\nAll reviewers agree that the paper propose a novel analysis with a different kind of assumption than independent assumption on the activations. Reviewer2 summarizes our contributions in details, and pointed out that the analysis is original, interesting and valuable.\n\nWe indeed assume that the input is drawn from zero-mean Gaussian. The technical motivation of the Gaussian assumption is to derive an analytic form of Eqn. 10 and Eqn. 11 (for expected gradient). The i.i.d Gaussian assumption gives a convenient tool for such analysis. Similar assumptions have been extensively applied to many branches of mathematics. In addition, the underlying intuition that \"the population gradient is smooth with a linear term and a nonlinear term dependent on the angle between w and w*\" is not restricted to i.i.d Gaussian input, but can be applied to general zero-mean distributions as long as the input space is properly covered (p(x) > 0 everywhere). In Fig. 3(d), we also show that for zero-mean uniform distribution, the analytic form of Eqn. 10 is still empirically correct.\n\nReviewer1 doubt that under this assumption, the hidden activation might also be Gaussian. This is not true. First, all activations are obviously not Gaussians since they are activations of ReLU and hence positive. I think what the reviewer suggests is that all the activations seem to be independent of each other since the inputs are i.i.d Gaussian. This is definitely not true as well. Given the same input, they are highly related to each other. E.g., for two dimensional case, if w1 = [1, 0] and w2 = [-1, 0], then obviously their responses are perfectly negatively correlated. In general, the responses of w1 and w2 are uncorrelated only if w1 and w2 are orthogonal. During optimization, in general wi and wj are not orthogonal at all, until they converge to wi* and wj*, which are assumed to be orthogonal (Section 4).\n\nReviewer1 and Reviewer3 mentioned that the motivation and notation are not generally clear in the paper, especially the second part of the analysis. The motivation of the second part is to show that the analytic form of expected (or population gradient) can be used to analyze multiple hidden units (K>=2) of two-layered network. In that setting, under the additional assumption that (1) teachers' weights are orthonormal and (2) we start from symmetric initialization w, then it is possible to prove the convergence to w*, even if the dynamics is highly nonlinear. Furthermore, the convergence result is not local, since the initialization w could be made in the epsilon-ball around origin with arbitrarily small epsilon. As mentioned in the introduction, to our knowledge, such result is very novel. We will make sure that the notation is clearly written, and intuitions are given first in the next version.\n\nAnswer to questions from Reviewer1:\n\n1. Yes all D are dependent on w.\n\n2. u and l is the same. Sorry for the notation issue here.\n\n3. F(e, w) is introduced to simplify the notation of gradient (as shown in the Appendix), since the gradient can be represented by the subtraction of two terms, F(e, w) and F(e, w*), where e is the normalized vector e of the current w, e = w/|w|. The gating matrix D does not depend on the magnitude of w, but only its direction. Therefore using the notation of D(e) is better.\n\n4. (Common question from Reviewer1 and Reviewer2). Theorem 3.3 is not inconsistent. It says as long as r <= O(sqrt(d)), then with 1/2-eps probability the sampling in the ball of radius r will lead to convergence to w*. So (1) if r is smaller than O(sqrt(d)), then the condition of the theorem still holds. That's the reason for \"initialization can be arbitrarily close to the origin\", and (2) Usually we just use largest r that still satisfies the condition, to avoid any issue of numerical instability. So r ~ sqrt(d).\n\n5. Basically Section 4 assumes that there is a symmetry among {w1, w2, ..., wK} in which w_i is a dimension-wise shifting of w_j by j - i. For example, w1 = [1, 2, 3], w2 = [3, 1, 2] and w3 = [2, 3, 1]. Because of this symmetry, the original dynamics of d*K (K weights) variables now becomes the dynamics of K variables (1 weights). The conclusion in Section 4 is based on this symmetry.\n\n6. a_j is the weight of each ReLU node. In the previous analysis (Section 4), a_j is 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287625112, "id": "ICLR.cc/2017/conference/-/paper316/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk85q85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper316/reviewers", "ICLR.cc/2017/conference/paper316/areachairs"], "cdate": 1485287625112}}}, {"tddate": null, "tmdate": 1484003569994, "tcdate": 1484003569994, "number": 3, "id": "BJqlFqZUl", "invitation": "ICLR.cc/2017/conference/-/paper316/public/comment", "forum": "Hk85q85ee", "replyto": "HkAvHKxNl", "signatures": ["~Yuandong_Tian1"], "readers": ["everyone"], "writers": ["~Yuandong_Tian1"], "content": {"title": "Answer", "comment": "Thank the reviewer for the comments. \n\nAll the activations are obviously not Gaussians since they are activations of ReLU and hence positive. I think what the reviewer suggests is that all the activations seem to be independent of each other since the inputs are i.i.d Gaussian. This is definitely not the case. Given the same input, they are highly related to each other. E.g., for two dimensional case, if w1 = [1, 0] and w2 = [-1, 0], then obviously their responses are perfectly negatively correlated. In general, the responses of w1 and w2 are uncorrelated only if w1 and w2 are orthogonal. During optimization, in general wi and wj are not orthogonal until they converge to wi* and wj*, which are assumed to be orthogonal (Section 4). \n\nAnswer to other questions:\n\n1. Yes all D are dependent on w. \n\n2. u and l is the same. Sorry for the notation issue here. \n\n3. F(e, w) is introduced to simplify the notation of gradient (as shown in the Appendix), since the gradient can be represented by the subtraction of two terms, F(e, w) and F(e, w*), where e is the normalized vector e of the current w, e = w/|w|. The gating matrix D does not depend on the magnitude of w, but only its direction. Therefore using the notation of D(e) is better.  \n\n4. Theorem 3.3 is not inconsistent. It says as long as r <= O(sqrt(d)), then with 1/2-eps probability the sampling in the ball of radius r will lead to convergence to w*. So (1) if r is smaller than O(sqrt(d)), then the condition of the theorem still holds. That's the reason for \"initialization can be arbitrarily close to the origin\", and (2) Usually we just use largest r that still satisfies the condition, to avoid any issue of numerical instability. So r ~ sqrt(d). \n\n5. Basically Section 4 assumes that there is a symmetry among {w1, w2, ..., wK} in which w_i is a dimension-wise shifting of w_j by j - i. For example, w1 = [1, 2, 3], w2 = [3, 1, 2] and w3 = [2, 3, 1]. Because of this symmetry, the original dynamics of d*K (K weights) variables now becomes the dynamics of K variables (1 weights). The conclusion in Section 4 is based on this symmetry. \n\n6. a_j is the weight of each ReLU node. In the previous analysis (Section 4), a_j is 1. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287625112, "id": "ICLR.cc/2017/conference/-/paper316/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk85q85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper316/reviewers", "ICLR.cc/2017/conference/paper316/areachairs"], "cdate": 1485287625112}}}, {"tddate": null, "tmdate": 1482358347850, "tcdate": 1482358347850, "number": 3, "id": "SJVUCuuNg", "invitation": "ICLR.cc/2017/conference/-/paper316/official/review", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["ICLR.cc/2017/conference/paper316/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper316/AnonReviewer3"], "content": {"title": "Potentially new analysis, but hard to read", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a convergence analysis of some two-layer NNs with ReLUs. It is not the first such analysis, but maybe it is novel on the assumptions used in the analysis, and the focus on ReLU nonlinearity that is pretty popular in practice. \n\nThe paper is quite hard to read, with many English mistakes and typos. Nevertheless, the analysis seems to be generally correct. The novelty and the key insights are however not always well motivated or presented. And the argument that the work uses realistic assumptions (Gaussian inputs for example) as opposed to other works, is quite debatable actually. \n\nOverall, the paper looks like a correct analysis work, but its form is really suboptimal in terms of writing/presentation, and the novelty and relevance of the results are not always very clear, unfortunately. The main results and intuition should be more clearly presented, and details could be moved to appendices for example - that could only help to improve the visibility and impact of these interesting results. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512625490, "id": "ICLR.cc/2017/conference/-/paper316/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper316/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper316/AnonReviewer1", "ICLR.cc/2017/conference/paper316/AnonReviewer2", "ICLR.cc/2017/conference/paper316/AnonReviewer3"], "reply": {"forum": "Hk85q85ee", "replyto": "Hk85q85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512625490}}}, {"tddate": null, "tmdate": 1482184016287, "tcdate": 1482184016287, "number": 2, "id": "HJO8S0BVl", "invitation": "ICLR.cc/2017/conference/-/paper316/public/comment", "forum": "Hk85q85ee", "replyto": "BkxN0nr4l", "signatures": ["~Yuandong_Tian1"], "readers": ["everyone"], "writers": ["~Yuandong_Tian1"], "content": {"title": "Answer", "comment": "Thanks reviewer for the comments! I really appreciate it. \n\nThe two paragraphs of page 2 are not inconsistent. \n\nTheorem 3.3 gives the relationship: as long as r <= O(sqrt(d)), then with 1/2-eps probability the sampling in the ball of radius r will lead to convergence to w*. \n\nSo (1) if r is smaller than O(sqrt(d)), then the condition of the theorem still holds. That's the reason for \"initialization can be arbitrarily close to the origin\", \nand (2) Usually we just use largest r that still satisfies the condition, to avoid any issue of numerical instability. So r ~ sqrt(d). \n\nI will make it more clear in the next revision. Thanks again. \n\nResponse to the minor comments:\n1. Yes. the Gaussians are zero-mean. \n2. Sorry for the spelling error and issues in theorem numbering, I will fix it. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287625112, "id": "ICLR.cc/2017/conference/-/paper316/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk85q85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper316/reviewers", "ICLR.cc/2017/conference/paper316/areachairs"], "cdate": 1485287625112}}}, {"tddate": null, "tmdate": 1482178087580, "tcdate": 1482178087580, "number": 2, "id": "BkxN0nr4l", "invitation": "ICLR.cc/2017/conference/-/paper316/official/review", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["ICLR.cc/2017/conference/paper316/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper316/AnonReviewer2"], "content": {"title": "Optimization of a ReLU network under new assumptions", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work analyzes the continuous-time dynamics of gradient descent when training two-layer ReLU networks (one input, one output, thus only one layer of ReLU units). The work is interesting in the sense that it does not involve some unrealistic assumptions used by previous works with similar goal. Most importantly, this work does not assume independence between input and activations, and it does not rely on noise injection (which can simplify the analysis). Nonetheless, removing these simplifying assumptions comes at the expense of limiting the analysis to:\n1. Only one layer of nonlinear units\n2. Discarding the bias term in ReLU while keeping the input Gaussian (thus constant input trick cannot be used to simulate the bias term).\n3. Imposing strong assumption on the representation on the input/output via (bias-less) ReLU networks: existence of orthonormal bases to represent this relationships.\n\nHaving that said, as far as I can tell, the paper presents original analysis in this new setting, which is interesting and valuable. For example, by exploiting the symmetry in the problem under the assumption 3 I listed above, the authors are able to reduce the high-dimensional dynamics of the gradient descent to a bivariate dynamics (instead of dealing with original size of the parameters). Such reduction to 2D allows the author to rigorously analyze the behavior of the dynamics (e.g. convergence to a saddle point in symmetric case, or to the optimum in non-symmetric case).\n\nClarification Needed: first paragraph of page 2. Near the end of the paragraph you say \"Initialization can be arbitrarily close to origin\", but at the beginning of the same paragraph you state \"initialized randomly with standard deviation of order 1/sqrt(d)\". Aren't these inconsistent?\n\nSome minor comments about the draft:\n1. In section 1, 2nd paragraph: \"We assume x is Gaussian and thus the network is bias free\". Do you mean \"zero-mean\" Gaussian then?\n2. \"standard deviation\" is spelled \"standard derivation\" multiple times in the paper.\n3. Page 6, last paragraph, first line: Corollary 4.1 should be Corollary 4.2\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512625490, "id": "ICLR.cc/2017/conference/-/paper316/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper316/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper316/AnonReviewer1", "ICLR.cc/2017/conference/paper316/AnonReviewer2", "ICLR.cc/2017/conference/paper316/AnonReviewer3"], "reply": {"forum": "Hk85q85ee", "replyto": "Hk85q85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512625490}}}, {"tddate": null, "tmdate": 1481835878314, "tcdate": 1481835878309, "number": 1, "id": "HkAvHKxNl", "invitation": "ICLR.cc/2017/conference/-/paper316/official/review", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["ICLR.cc/2017/conference/paper316/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper316/AnonReviewer1"], "content": {"title": "Hard to read paper; unclear conclusions.", "rating": "4: Ok but not good enough - rejection", "review": "In this paper, the author analyzes the convergence dynamics of a single layer non-linear network under Gaussian iid input assumptions. The first half of the paper, dealing with a single hidden node, was somewhat clear, although I have some specific questions below. The second half, dealing with multiple hidden nodes, was very difficult for me to understand, and the final \"punchline\" is quite unclear. I think the author should focus on intuition and hide detailed derivations and symbols in an appendix. \n\nIn terms of significance, it is very hard for me to be sure how generalizable these results are: the Gaussian assumption is a very strong one, and so is the assumption of iid inputs. Real-world feature inputs are highly correlated and are probably not Gaussian. Such assumptions are not made (as far as I can tell) in recent papers analyzing the convergence of deep networks e.g. Kawaguchi, NIPS 2016. Although the author says the no assumption is made on the independence of activations, this assumption is shifted to the input instead. I think this means that the activations are combinations of iid random variables, and are probably Gaussian like, right? So I'm not sure where this leaves us.\n\nSpecific comments:\n\n1. Please use D_w instead of D to show that D is a function of w, and not a constant. This gets particularly confusing when switching to D(w) and D(e) in Section 3. In general, notation in the paper is hard to follow and should be clearly introduced.\n\n2. Section 3, statement that says \"when the neuron is cut off at sample l, then (D^(t))_u\" what is the relationship between l and u? Also, this is another example of notational inconsistency that causes problems to the reader.\n\n3. Section 3.1, what is F(e, w) and why is D(e) introduced? This was unclear to me.\n\n4. Theorem 3.3 suggests that (if \\epsilon is > 0), then to have the maximal probability of convergence, \\epsilon should be very close to 0, which means that the ball B_r has radius r -> 0? This seems contradictory from Figure 2. \n\n5. Section 4 was really unclear and I still do not understand what the symmetry group really represents. Is there an intuitive explanation why this is important?\n\n6. Figure 5: what is a_j ?\n\nI encourage the author to rewrite this paper for clarity. In it's present form, it would be very difficult to understand the takeaways from the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512625490, "id": "ICLR.cc/2017/conference/-/paper316/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper316/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper316/AnonReviewer1", "ICLR.cc/2017/conference/paper316/AnonReviewer2", "ICLR.cc/2017/conference/paper316/AnonReviewer3"], "reply": {"forum": "Hk85q85ee", "replyto": "Hk85q85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512625490}}}, {"tddate": null, "tmdate": 1481321409629, "tcdate": 1481321409622, "number": 1, "id": "r1cTjjO7x", "invitation": "ICLR.cc/2017/conference/-/paper316/public/comment", "forum": "Hk85q85ee", "replyto": "SyFvP7Lml", "signatures": ["~Yuandong_Tian1"], "readers": ["everyone"], "writers": ["~Yuandong_Tian1"], "content": {"title": "Answer", "comment": "Thanks the reviewer for the comments!\n\nThe i.i.d Gaussian assumption is extensively used in theoretical analysis in machine learning and statistics, and is in general not a strong assumption at all, in particular when a close-form solution is to be derived. As mentioned in the introduction, previous analysis often uses independence assumptions of ReLU activations for the same input, independence assumption about different path from the input to the target in deep network, or assuming more parameters than the number of parameters, etc, which are potentially stronger than our assumptions in my opinion. We emphasize that considering the difficulty in analyzing nonlinear and non-convex multilayer models, making reasonable assumptions is often the key to sensible conclusions.\n\nIn this paper, the Gaussian input assumption is mainly used for obtaining a close-form solution for weight update in one ReLU node (Eqn. 10-11), which reveals interesting nonlinear structure of ReLU node, used in more complicated case of multiple hidden nodes. Intuitively, this assumption is used to \"smooth out\" the non-differentiable nature of ReLU for easier analysis. For general (multimodal) distributions, since they can be represented as a mixture of Gaussians, similar analysis can also be applied (by rewriting Eqn. 21 as a summation of conditional expectations, and by considering bias and variance terms in the integral computation), although the corresponding close form could be quite complicated. Furthermore, we also show empirically (Sec. 5, Fig. 3c), other zero-mean distributions, e.g., uniform distribution, also follows Eqn. 10 in high-dimensional case.\n\nWe made all our assumptions explicit in the theorems and corollaries. For example, In Corollary 4.2, we explicitly specify that (1) the network is bias-free two-layered with i.i.d Gaussian input and upper weights fixed to be one, (2) the weights of teacher's network is orthonormal and (3) the student network starts with initialization (x, y, y, ...) in the coordinates of teacher's weights, with x > y but could be arbitrarily small. Other theorems follow similar patterns. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287625112, "id": "ICLR.cc/2017/conference/-/paper316/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hk85q85ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper316/reviewers", "ICLR.cc/2017/conference/paper316/areachairs"], "cdate": 1485287625112}}}, {"tddate": null, "tmdate": 1481156449118, "tcdate": 1481156449112, "number": 1, "id": "SyFvP7Lml", "invitation": "ICLR.cc/2017/conference/-/paper316/pre-review/question", "forum": "Hk85q85ee", "replyto": "Hk85q85ee", "signatures": ["ICLR.cc/2017/conference/paper316/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper316/AnonReviewer1"], "content": {"title": "Gaussian input assumption", "question": "The assumption of Gaussian input X seems rather strong. Do you have a sense of how the analysis would change if the input features have non-Gaussian or multi-modal distributions (as is most likely the case with real-world input features)?  Are there any other implicit or explicit assumptions made in the paper?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481156449642, "id": "ICLR.cc/2017/conference/-/paper316/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper316/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper316/AnonReviewer1"], "reply": {"forum": "Hk85q85ee", "replyto": "Hk85q85ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper316/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481156449642}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478378844413, "tcdate": 1478285965922, "number": 316, "id": "Hk85q85ee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hk85q85ee", "signatures": ["~Yuandong_Tian1"], "readers": ["everyone"], "content": {"title": "Symmetry-Breaking Convergence Analysis of Certain Two-layered Neural Networks with ReLU nonlinearity", "abstract": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free networks in the form of $g(x; w) = \\sum_{j=1}^K \\sigma(w_j \\cdot x)$, where $\\sigma(\\cdot)$ is ReLU nonlinearity. We assume that the input $x$ follow Gaussian distribution. The network is trained using gradient descent to mimic the output of a teacher network of the same size with fixed parameters $w*$ using $l_2$ loss. We first show that when $K = 1$, the nonlinear dynamics can be written in close form, and converges to $w*$ with at least $(1-\\epsilon)/2$ probability, if random weight initializations of proper standard derivation ($\\sim 1/\\sqrt{d}$) is used, verifying empirical practice. For networks with many ReLU nodes ($K \\ge 2$), we apply our close form dynamics and prove that when the teacher parameters $\\{w*_j\\}_{j=1}^K$ forms orthonormal bases, (1) a symmetric weight initialization yields a convergence to a saddle point and (2) a certain symmetry-breaking weight initialization yields global convergence to $w*$ without local minima. To our knowledge, this is the first proof that shows global convergence in nonlinear neural network without unrealistic assumptions on the independence of ReLU activations. In addition, we also give a concise gradient update formulation for a multilayer ReLU network when it follows a teacher of the same size with $l_2$ loss. Simulations verify our theoretical analysis.", "pdf": "/pdf/8670fcf3516920611234172645ca701c213c6935.pdf", "TL;DR": "In this paper, we use dynamical system to analyze the nonlinear weight dynamics of two-layered bias-free ReLU networks.", "paperhash": "tian|symmetrybreaking_convergence_analysis_of_certain_twolayered_neural_networks_with_relu_nonlinearity", "conflicts": ["fb.com"], "authors": ["Yuandong Tian"], "authorids": ["yuandong@fb.com"], "keywords": ["Theory", "Deep learning", "Optimization"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": ["r1lVgRNtx"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 10}