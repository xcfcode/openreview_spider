{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730157060, "tcdate": 1509138915146, "number": 1132, "cdate": 1518730157047, "id": "H1Nyf7W0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "H1Nyf7W0Z", "original": "rkOoZXbCZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation", "abstract": "Neural sequence generation is commonly approached by using maximum- likelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using \u03b1-divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to \u03b1 \u2192 0 and RL to \u03b1 \u2192 1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing \u03b1. Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.", "pdf": "/pdf/4122d80b6740caf9641d8bbc9dc1cf00e2259f51.pdf", "TL;DR": "Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.", "paperhash": "koyamada|alphadivergence_bridges_maximum_likelihood_and_reinforcement_learning_in_neural_sequence_generation", "_bibtex": "@misc{\nkoyamada2018alphadivergence,\ntitle={Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation},\nauthor={Sotetsu Koyamada and Yuta Kikuchi and Atsunori Kanemura and Shin-ichi Maeda and Shin Ishii},\nyear={2018},\nurl={https://openreview.net/forum?id=H1Nyf7W0Z},\n}", "authorids": ["sotetsu.koyamada@gmail.com"], "keywords": ["neural network", "reinforcement learning", "natural language processing", "machine translation", "alpha-divergence"], "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260076295, "tcdate": 1517250212869, "number": 862, "cdate": 1517250212845, "id": "HJps8JTBz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers agreed that this paper is not quite ready for publication at ICLR.  One of the reviewers thought the paper was well written and easy to follow while the two others said the opposite.  One of the main criticisms was issues with the composition.  The paper seems to lack a clear formal explanation of the problem and the proposed methodology.  The reviewers in general weren't convinced by the experiments, complaining about the lack of a required baseline and that the proposed method doesn't seem to significantly help in the experiment presented.\n\nPros:\n- The proposed idea is interesting\n- The problem is timely and of interest to the community\n- Addresses multiple important problems at the intersection of ML and RL in sequence generation\n\nCons:\n- Novel but somewhat incremental\n- The experiments are not compelling (i.e. the results are not strong)\n- A necessary baseline is missing\n- Significant issues with the writing - both in terms of clarity and correctness."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation", "abstract": "Neural sequence generation is commonly approached by using maximum- likelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using \u03b1-divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to \u03b1 \u2192 0 and RL to \u03b1 \u2192 1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing \u03b1. Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.", "pdf": "/pdf/4122d80b6740caf9641d8bbc9dc1cf00e2259f51.pdf", "TL;DR": "Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.", "paperhash": "koyamada|alphadivergence_bridges_maximum_likelihood_and_reinforcement_learning_in_neural_sequence_generation", "_bibtex": "@misc{\nkoyamada2018alphadivergence,\ntitle={Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation},\nauthor={Sotetsu Koyamada and Yuta Kikuchi and Atsunori Kanemura and Shin-ichi Maeda and Shin Ishii},\nyear={2018},\nurl={https://openreview.net/forum?id=H1Nyf7W0Z},\n}", "authorids": ["sotetsu.koyamada@gmail.com"], "keywords": ["neural network", "reinforcement learning", "natural language processing", "machine translation", "alpha-divergence"], "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388261, "tcdate": 1511104021018, "number": 1, "cdate": 1511104021018, "id": "HyTf0MygM", "invitation": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "writing issues, missing baseline", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes another training objective for training neural sequence-to-sequence models. The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p. The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively.\n\nThe paper has significant writing issues. In Paragraph \u201cMaximum Likelihood\u201d, page 2, the formalization of the studied problem is unclear. Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only?  In the former case, the statement \u201cx is uniformly sampled from X\u201d does not make sense because X is practically infinite. Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation. If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set. Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used.\n\nThe proposed method is evaluated on just one dataset. Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al. The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison.\n\nTo sum up, I can not recommend the paper to acceptance, because (a) an important baseline is missing (b) there are serious writing issues.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation", "abstract": "Neural sequence generation is commonly approached by using maximum- likelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using \u03b1-divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to \u03b1 \u2192 0 and RL to \u03b1 \u2192 1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing \u03b1. Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.", "pdf": "/pdf/4122d80b6740caf9641d8bbc9dc1cf00e2259f51.pdf", "TL;DR": "Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.", "paperhash": "koyamada|alphadivergence_bridges_maximum_likelihood_and_reinforcement_learning_in_neural_sequence_generation", "_bibtex": "@misc{\nkoyamada2018alphadivergence,\ntitle={Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation},\nauthor={Sotetsu Koyamada and Yuta Kikuchi and Atsunori Kanemura and Shin-ichi Maeda and Shin Ishii},\nyear={2018},\nurl={https://openreview.net/forum?id=H1Nyf7W0Z},\n}", "authorids": ["sotetsu.koyamada@gmail.com"], "keywords": ["neural network", "reinforcement learning", "natural language processing", "machine translation", "alpha-divergence"], "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388140, "id": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1132/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer2"], "reply": {"forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1132/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388140}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388215, "tcdate": 1512145871120, "number": 2, "cdate": 1512145871120, "id": "BkwRmW1Wz", "invitation": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Review of the paper: unfortunately I do not understand main points of this paper and cannot give accurate reviews", "rating": "4: Ok but not good enough - rejection", "review": "This paper considers a dichitomy between ML and RL based methods for sequence generation. It is argued that the ML approach has some \"discrepancy\" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity. An alpha-divergence formulation is considered to combine both methods.\n\nUnfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper. I therefore have no option but to vote for reject of this paper, based on my educated guess. \n\nBelow are the points that I'm particularly confused about:\n\n1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me. For example, \n\n1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators. From the context, I guess the authors mean \"empirical training distribution\"?\n\n1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a \"discrepancy\" to me. The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand. In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator.\n\nIn addition, I don't see at all why this discrepancy is a discrepancy between training and testing data. As long as both of them are identically distributed, then no discrepancy exists.\n\n1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong. First, the model is *not* trained on the true distribution which is unknown. The model is trained on an empirical distribution whose points are sampled from the true distribution. I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution.\n\n2. For the RL approach, I think it is very unclear as a formulation of an estimator. For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a \"reward\" function, but I don't know what it means and the authors should perhaps explain further. I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation", "abstract": "Neural sequence generation is commonly approached by using maximum- likelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using \u03b1-divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to \u03b1 \u2192 0 and RL to \u03b1 \u2192 1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing \u03b1. Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.", "pdf": "/pdf/4122d80b6740caf9641d8bbc9dc1cf00e2259f51.pdf", "TL;DR": "Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.", "paperhash": "koyamada|alphadivergence_bridges_maximum_likelihood_and_reinforcement_learning_in_neural_sequence_generation", "_bibtex": "@misc{\nkoyamada2018alphadivergence,\ntitle={Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation},\nauthor={Sotetsu Koyamada and Yuta Kikuchi and Atsunori Kanemura and Shin-ichi Maeda and Shin Ishii},\nyear={2018},\nurl={https://openreview.net/forum?id=H1Nyf7W0Z},\n}", "authorids": ["sotetsu.koyamada@gmail.com"], "keywords": ["neural network", "reinforcement learning", "natural language processing", "machine translation", "alpha-divergence"], "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388140, "id": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1132/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer2"], "reply": {"forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1132/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388140}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388162, "tcdate": 1512494947475, "number": 3, "cdate": 1512494947475, "id": "HyiwD8N-M", "invitation": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "signatures": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Incremental novelty, lack of enough experimental evidence showing significance of the proposed method", "rating": "4: Ok but not good enough - rejection", "review": "Summary of the paper: \n\nThis paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions), that addresses three important problems simultaneously: \n(a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT). \n(b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution\n(c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function\u2019s optimization \n\nThen the authors present the results for machine translation task and also analysis of their proposed method.\n\nMy comments / feedback: \n\nThe paper is well written and the problem addressed by the paper is an important one. My main concerns about this work are have two aspects: \n(a)\tNovelty\n1.\tThe idea is a good one and is great incremental research building on the top of previous ideas. I do not agree with statements like \u201cWe demonstrate that the proposed objective function generalizes ML and RL objective functions \u2026\u201d that authors have made in the abstract. There is not enough evidence in the paper to validate this statement.\n(b)\tExperimental Results\n2.\tThe performance of the proposed method is not significantly better than other models in MT task. I am also wondering why authors have not tried their method on at least one more task? E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc.  \n\nSome minor comments: \n\n1.\tIn page 2, 6th line after eq (1), \u201c\u2026 these two problems\u201d --> \u201c\u2026 these three problems\u201d \n2.\tIn page 2, the line before the last line, \u201c\u2026 resolbing problem\u201d --> \u201c\u2026 resolving problem\u201d\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation", "abstract": "Neural sequence generation is commonly approached by using maximum- likelihood (ML) estimation or reinforcement learning (RL). However, it is known that they have their own shortcomings; ML presents training/testing discrepancy, whereas RL suffers from sample inefficiency. We point out that it is difficult to resolve all of the shortcomings simultaneously because of a tradeoff between ML and RL. In order to counteract these problems, we propose an objective function for sequence generation using \u03b1-divergence, which leads to an ML-RL integrated method that exploits better parts of ML and RL. We demonstrate that the proposed objective function generalizes ML and RL objective functions because it includes both as its special cases (ML corresponds to \u03b1 \u2192 0 and RL to \u03b1 \u2192 1). We provide a proposition stating that the difference between the RL objective function and the proposed one monotonically decreases with increasing \u03b1. Experimental results on machine translation tasks show that minimizing the proposed objective function achieves better sequence generation performance than ML-based methods.", "pdf": "/pdf/4122d80b6740caf9641d8bbc9dc1cf00e2259f51.pdf", "TL;DR": "Propose new objective function for neural sequence generation which integrates ML-based and RL-based objective functions.", "paperhash": "koyamada|alphadivergence_bridges_maximum_likelihood_and_reinforcement_learning_in_neural_sequence_generation", "_bibtex": "@misc{\nkoyamada2018alphadivergence,\ntitle={Alpha-divergence bridges maximum likelihood and reinforcement learning in neural sequence generation},\nauthor={Sotetsu Koyamada and Yuta Kikuchi and Atsunori Kanemura and Shin-ichi Maeda and Shin Ishii},\nyear={2018},\nurl={https://openreview.net/forum?id=H1Nyf7W0Z},\n}", "authorids": ["sotetsu.koyamada@gmail.com"], "keywords": ["neural network", "reinforcement learning", "natural language processing", "machine translation", "alpha-divergence"], "authors": ["Sotetsu Koyamada", "Yuta Kikuchi", "Atsunori Kanemura", "Shin-ichi Maeda", "Shin Ishii"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388140, "id": "ICLR.cc/2018/Conference/-/Paper1132/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1132/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1132/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer3", "ICLR.cc/2018/Conference/Paper1132/AnonReviewer2"], "reply": {"forum": "H1Nyf7W0Z", "replyto": "H1Nyf7W0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1132/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388140}}}], "count": 5}