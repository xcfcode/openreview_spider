{"notes": [{"id": "ryxgsCVYPr", "original": "BkeNwgFuDS", "number": 1307, "cdate": 1569439384498, "ddate": null, "tcdate": 1569439384498, "tmdate": 1583912042597, "tddate": null, "forum": "ryxgsCVYPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "bmukdcHZsa", "original": null, "number": 1, "cdate": 1576798719989, "ddate": null, "tcdate": 1576798719989, "tmdate": 1576800916570, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. All reviewers agree that this approach is interesting (verification and validation) and experiments are solid. One of the reviewers raised concerns are promptly answered by authors, raising the average score to accept. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707578, "tmdate": 1576800255804, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Decision"}}}, {"id": "HkeaCf55FH", "original": null, "number": 2, "cdate": 1571623636707, "ddate": null, "tcdate": 1571623636707, "tmdate": 1574222318795, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "\nPaper Summary:\n\nThis paper proposes a neural question requirement inspection models called NeurQuRI.  It is different from existing answer verifiers in that NeurQuRI pinpoints where the mismatch occurs between the question and the candidate answer in unanswerable cases. Experiments with SQuAD 2.0 show the effectiveness of NeurQuRI.\n\nStrengths:\n\n\u2014NeurQuRI improved the accuracy of three popular reading comprehension models, which are BERT, DocQA and QANet, on SQuAD 2.0.\n\n\u2014NeurQuRI requires 33M additional model parameters to check answerability of the candidate answer.  It is quite less than those of the previous answer verifier (150M) proposed by Hu et al. (2019).\n\n\u2014NeurQuRI can be easily extended to other question answering models and tasks.\n\nWeaknesses:\n\n\u2014The authors do not experimentally compare their model to the answer verifier proposed by Hu et al. (2019). The base model of Hu et al. (2019), RMR, is different from the base models used in this paper, BERT, DocQA, and QANet.\n\n\u2014The authors use only one dataset (SQuAD 2.0) to evaluate their method.  NewsQA, MS MARCO, CoQA and QuAC also contain unanswerable questions.\n\nComments/Suggestions:\n\n\u2014I think that coverage mechanisms used in NMT (Tu et al., 2016) and summarization (See et al., 2017) should be included in the section of related work.\n\n\u2014The idea similar to NeurQuRI is used in a multi-hop QA model, proposed by Nishida et al. (2019), to find the evidence sentences that cover important information with respect to the question sentence.\n\n\u2014It is worthwhile to present the results of ablation tests with respect to the modified answerability label (described in Section 2.2).\n\nReview Summary:\n\nThe paper is well motivated.  However, I think the authors need to compare NeurQuRI with the answer verifier proposed by Hu et al. (2019).  Also, I think this paper can benefit a lot with a more comprehensive analysis with other datasets such as CoQA.\n\nReferences:\nHu et al.: Read + Verify: Machine Reading Comprehension with Unanswerable Questions. AAAI 2019: 6529-6537\nTu et al.: Modeling Coverage for Neural Machine Translation. ACL (1) 2016\nSee et al.: Get To The Point: Summarization with Pointer-Generator Networks. ACL (1) 2017: 1073-1083\nNishida et al.: Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. ACL (1) 2019: 2335-2345\n\nEDIT Nov. 20, 2019: \nI appreciate the authors' revision.  \nThe revision has satisfied my concerns, and I decided to increase the score of the paper (weak reject -> weak accept).", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575330268852, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Reviewers"], "noninvitees": [], "tcdate": 1570237739286, "tmdate": 1575330268867, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review"}}}, {"id": "H1gdyfH2oB", "original": null, "number": 6, "cdate": 1573831136280, "ddate": null, "tcdate": 1573831136280, "tmdate": 1573831136280, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "S1x9_DP4iB", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Update of submission based on the AnonReviewer2's feedback", "comment": "Thank you for your constructive feedback and efforts.\n\nWe have uploaded a new revision with the following changes based on your feedback -\n\n1) Evaluations of our method on NewsQA (Trischler et al. 2017) and MS MARCO (Bajaj et al. 2016) datasets have been added, achieving improvement in both these datasets as well. (Table 2)\n2) Comparison with Hu et. al. (2019) answer verifier and SG-Net verifier (Zhang et al. 2019) has been added, and our method outperforms these verifiers. (Table 1)\n3) Ablation study with respect to the modified answerability label has been added. (Table 4)\n4) Discussion regarding coverage mechanisms and QFE (Nishida et al. 2019) has been added. (Related Works)\n\nReferences:\nTrischler et al. NewsQA : Newsqa: A machine comprehension dataset. 2017.\nBajaj et al. MS MARCO: Ms marco: A human generated machine reading comprehension dataset. 2016\nHu et al. 2019: Read + Verify: Machine Reading Comprehension with Unanswerable Questions. AAAI 2019\nZhang et al. 2019 : SG-Net: Syntax-Guided Machine Reading Comprehension, https://arxiv.org/abs/1908.05147v1\nNishida et al.: Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. ACL 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "ryxVdWr3sS", "original": null, "number": 5, "cdate": 1573831019607, "ddate": null, "tcdate": 1573831019607, "tmdate": 1573831019607, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "SklFQPvNsr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Update of submission based on the AnonReviewer1's feedback", "comment": "Thank you for your constructive feedback and efforts.\n\nWe have uploaded a new revision with the following changes based on your feedback -\n\n1) Evaluations of our method on NewsQA (Trischler et al. 2017) and MS MARCO (Bajaj et al. 2016) datasets have been added, achieving improvement in both these datasets as well. (Table 2)\n2) Comparison with Hu et. al. (2019) answer verifier and SG-Net verifier (Zhang et al. 2019) has been added, and our method outperforms these verifiers. (Table 1)\n\nReferences:\nTrischler et al. NewsQA : Newsqa: A machine comprehension dataset. 2017.\nBajaj et al. MS MARCO: Ms marco: A human generated machine reading comprehension dataset. 2016\nHu et al. 2019: Read + Verify: Machine Reading Comprehension with Unanswerable Questions. AAAI 2019\nZhang et al. 2019 : SG-Net: Syntax-Guided Machine Reading Comprehension, https://arxiv.org/abs/1908.05147v1"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "Hkg8reH2iB", "original": null, "number": 4, "cdate": 1573830717666, "ddate": null, "tcdate": 1573830717666, "tmdate": 1573830717666, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Update of submission based on the reviewers\u2019 feedback", "comment": "We would like to thank all reviewers for their constructive feedback and effort in reviewing this paper. \n\nWe have uploaded a new revision and would like to summarize the changes we made to the initial submission based on the reviewers\u2019 feedback -\n\nThe main changes are -\n\n1) Evaluations of our method on NewsQA (Trischler et al. 2017) and MS MARCO (Bajaj et al. 2016) datasets have been added, achieving improvement in both these datasets as well. (Table 2)\n2) Comparison with Hu et. al. (2019) answer verifier and SG-Net verifier (Zhang et al. 2019) has been added, and our method outperforms these verifiers. (Table 1)\n3) An ablation study with respect to the modified answerability label has been added. (Table 4)\n4) Discussion regarding coverage mechanisms and QFE (Nishida et al. 2019) has been added. (Related Works)\n\nReferences:\nTrischler et al. NewsQA : Newsqa: A machine comprehension dataset. 2017.\nBajaj et al. MS MARCO: Ms marco: A human-generated machine reading comprehension dataset. 2016\nHu et al. 2019: Read + Verify: Machine Reading Comprehension with Unanswerable Questions. AAAI 2019\nZhang et al. 2019 : SG-Net: Syntax-Guided Machine Reading Comprehension, https://arxiv.org/abs/1908.05147v1\nNishida et al.: Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. ACL 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "SklFQPvNsr", "original": null, "number": 1, "cdate": 1573316385378, "ddate": null, "tcdate": 1573316385378, "tmdate": 1573450349737, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "BJxQM0_CYr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We appreciate your compliments and valuable feedback.\n\n1. We will update the manuscript by evaluating our model on other datasets. We chose SQuAD as our benchmark dataset due to its popularity.\n\n2. Our answer inspector can be added in general to any MRC Reader as we have consistently demonstrated using DocQA, QANet and BERT. Below we list the published models in the current SOTA on SQuAD 2.0 leaderboard and compare them to NeurQuRI as follows - \n\nALBERT - A Lite weight-shared version of BERT. Since we consistently improve BERT\u2019s score with NeurQuRI, we expect performance improvements on applying our answer verifier to ALBERT.\n\nXLNet + SG-Net Verifier - The verifier used in this paper is a linear layer applied to start and end logit weighted token representations concatenated with \u201c[CLS]\u201d token. We reproduce this verifier on our readers, and NeurQuRI outperforms this verifier on SQuAD 2.0 development set as follows (F1 scores) - \nBERT (SG-Net Verifier vs NeurQuRI) - 82.3 vs 83.1\nDocQA(ELMo) (SG-Net Verifier vs NeurQuRI) - 70.7 vs 73.8     [updated. 11/11]\nQANet (SG-Net Verifier vs NeurQuRI) - 67.6 vs 68.9    [updated. 11/11]\n\nRoBERTa - More robustly trained version of BERT. Our verifier can again be directly applied and we expect performance gains.\n\n3. Regarding the weights of the loss terms in Eq 1, our model is robust to different weights of these loss terms. As we mentioned in Section 3, we use all these relative loss weights set to 1.0. However, other combinations of these weights also lead to comparable scores for all three readers, as long as Lambda_i (corresponding to answerability classification loss) is not made zero as shown in Table 3.\n\nReferences:\nALBERT: A Lite BERT for Self-supervised Learning of Language Representations, https://openreview.net/forum?id=H1eA7AEtvS\nXLNET+SG-Net: (Zhang et al. 2019) SG-Net: Syntax-Guided Machine Reading Comprehension, https://arxiv.org/abs/1908.05147v1 [updated. 11/11]"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "S1x9_DP4iB", "original": null, "number": 2, "cdate": 1573316465655, "ddate": null, "tcdate": 1573316465655, "tmdate": 1573316753308, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "HkeaCf55FH", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We appreciate your compliments and valuable feedback.\n\n1. We will update the manuscript with the F1 score comparison on SQuAD 2.0 development set between NeurQuRI and Verifier from Hu et al. based on DocQA(ELMo) reader as follows, \nDocQA(ELMo)+NeurQuRI - 73.8\nDocQA(ELMo)+Verifier - 70.7. (This result is from Hu et al.,2019)\nNeurQuRI outperforms Hu et. al. verifier by 3.1 F1 score.\nAlso note that Hu et. al. unlike our approach does not explain why a question is unanswerable.\n\n2. We will update the manuscript by evaluating our model on other datasets. We chose SQuAD as our benchmark dataset due to its popularity.\n\n3. We will update the manuscript of related work with as follows,\nIn the case of QFE (Nishida et al. 2019), Hotpot dataset has strong supervision regarding which sentences support/explain the answerability, however NeurQuRI can explain answerability without requiring this supervision, as in datasets such as SQuAD 2.0. We also newly designed the attention accumulation recurrent module that is capable of checking multiple conditions simultaneously unlike the softmax approach used in NMT (Tu et al., 2016),  P-Gen (See et al., 2017), Checklist (Kiddon et al. 2016) and QFE.\n\n4. We will update the manuscript with an ablation study of the modified answerability label as follows,\nground-truth label -> Modified label (F1 score)\nBERT+NeurQuRI: 82.6 -> 83.1\nDocQA(ELMo)+NeurQuRI: 72.8 -> 73.8\nQANet+NeurQuRI: 67.5 -> 68.9\nThe modified answerability label is more important for weaker readers which are more likely to give an incorrect answer.\n\nReferences:\nHu et al.: Read + Verify: Machine Reading Comprehension with Unanswerable Questions. AAAI 2019: 6529-6537\nTu et al.: Modeling Coverage for Neural Machine Translation. ACL (1) 2016\nSee et al.: Get To The Point: Summarization with Pointer-Generator Networks. ACL (1) 2017: 1073-1083\nNishida et al.: Answering while Summarizing: Multi-task Learning for Multi-hop QA with Evidence Extraction. ACL (1) 2019: 2335-2345"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "ryeN6wD4sB", "original": null, "number": 3, "cdate": 1573316539764, "ddate": null, "tcdate": 1573316539764, "tmdate": 1573316539764, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "rkxcXY9JFB", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We appreciate your compliments!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryxgsCVYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1307/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1307/Authors|ICLR.cc/2020/Conference/Paper1307/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158007, "tmdate": 1576860554543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Authors", "ICLR.cc/2020/Conference/Paper1307/Reviewers", "ICLR.cc/2020/Conference/Paper1307/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Comment"}}}, {"id": "rkxcXY9JFB", "original": null, "number": 1, "cdate": 1570904354310, "ddate": null, "tcdate": 1570904354310, "tmdate": 1572972485928, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The authors propose a Neural Question Requirement module that extracts a list of condition from the question which should be met by the candidate answer in the question answering problems. Authors claim that they propose a novel, attention-based loss function in order to check whether a condition is met.\n\nEvaluation of the methodology is performed on the SquAD 2.0 dataset. It is organized behind combining existing, state of the art solutions like BERT  or QANet. It is improving these solutions consistently."}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575330268852, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Reviewers"], "noninvitees": [], "tcdate": 1570237739286, "tmdate": 1575330268867, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review"}}}, {"id": "BJxQM0_CYr", "original": null, "number": 3, "cdate": 1571880458614, "ddate": null, "tcdate": 1571880458614, "tmdate": 1572972485858, "tddate": null, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "invitation": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper incorporates an answer inspection encoder verifying whether the answers selected by the Machine Reading Comprehension (MRC) component is valid, into the MRC reader. For training, the model includes two additional losses and trained jointly. Evaluation is on SQuAD V2.0 which is constructed from Wikipedia and contains unanswerable questions generated by crowd sources. The approaches are verified in the settings with/without including BERT to show the generalization of the answer inspector. \n\nThis is an interesting paper which focuses on the answer verification and validation, and shows the effectiveness of the proposed model. It also gives a good ablation study showing the contributions of each component, and provides examples to illustrate why it works. \n\nHowever, there are a few concerns detailed as follows:\n\n1. The model is only evaluated on SQuAD 2.0, which is over explored by many works. I\u2019m wondering if this could be generalized to other MRC tasks, e.g. MSMARCO or DuReader. It would be nice to see some experiments on them or other datasets.\n\n2. It seems that the performance of state-of-the-art SOTA system on SQuAD is much higher than the proposed approaches. I would like to see some discussion on what are the pros and cons between them.\n\n3. How sensitive are the gammas in Eq 1? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1307/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["scv.back@samsung.com", "sai.chetan@samsung.com", "akhil.kedia@samsung.com", "haejun82.lee@samsung.com", "jchoo@korea.ac.kr"], "title": "NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension", "authors": ["Seohyun Back", "Sai Chetan Chinthakindi", "Akhil Kedia", "Haejun Lee", "Jaegul Choo"], "pdf": "/pdf/0473567aa27752ffefa7b6da332a1739b5cf0c06.pdf", "TL;DR": "We propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model.", "abstract": "Real-world question answering systems often retrieve potentially relevant documents to a given question through a keyword search, followed by a machine reading comprehension (MRC) step to find the exact answer from them. In this process, it is essential to properly determine whether an answer to the question exists in a given document. This task often becomes complicated when the question involves multiple different conditions or requirements which are to be met in the answer. For example, in a question \"What was the projection of sea level increases in the fourth assessment report?\", the answer should properly satisfy several conditions, such as \"increases\" (but not decreases) and \"fourth\" (but not third). To address this, we propose a neural question requirement inspection model called NeurQuRI that extracts a list of conditions from the question, each of which should be satisfied by the candidate answer generated by an MRC model. To check whether each condition is met, we propose a novel, attention-based loss function. We evaluate our approach on SQuAD 2.0 dataset by integrating the proposed module with various MRC models, demonstrating the consistent performance improvements across a wide range of state-of-the-art methods.", "keywords": ["Question Answering", "Machine Reading Comprehension", "Answerability Prediction", "Neural Checklist"], "paperhash": "back|neurquri_neural_question_requirement_inspector_for_answerability_prediction_in_machine_reading_comprehension", "_bibtex": "@inproceedings{\nBack2020NeurQuRI:,\ntitle={NeurQuRI: Neural Question Requirement Inspector for Answerability Prediction in Machine Reading Comprehension},\nauthor={Seohyun Back and Sai Chetan Chinthakindi and Akhil Kedia and Haejun Lee and Jaegul Choo},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=ryxgsCVYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/7c99cffa1be79c95ebe5d30df6656d431de84252.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryxgsCVYPr", "replyto": "ryxgsCVYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1307/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575330268852, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1307/Reviewers"], "noninvitees": [], "tcdate": 1570237739286, "tmdate": 1575330268867, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1307/-/Official_Review"}}}], "count": 11}