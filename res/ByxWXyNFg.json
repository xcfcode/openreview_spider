{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028574148, "tcdate": 1490028574148, "number": 1, "id": "rJLXuKaje", "invitation": "ICLR.cc/2017/workshop/-/paper57/acceptance", "forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028574704, "id": "ICLR.cc/2017/workshop/-/paper57/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028574704}}}, {"tddate": null, "tmdate": 1489438833328, "tcdate": 1489438833328, "number": 2, "id": "ByYdOtEsg", "invitation": "ICLR.cc/2017/workshop/-/paper57/public/comment", "forum": "ByxWXyNFg", "replyto": "rk-Nk2gix", "signatures": ["~Oleksii_Kuchaiev1"], "readers": ["everyone"], "writers": ["~Oleksii_Kuchaiev1"], "content": {"title": "response to ICLR 2017 workshop paper57 AnonReviewer1", "comment": "Thank you very much for the review and feedback.\n\nPlease find our responses below:\n\na)  On \u201cvanilla\u201d LSTM comparison. \nWe used LSTM with projection to fit the model in the GPU DRAM. On the One Billion Word Benchmark, the vocabulary size is around 800K. So, if regular LSTM without projection is used,  then the embedding matrix will be 800K times LSTM cell size, and full softmax layer would require another 800K times LSTM cell size parameters. Therefore, for cell size of 8,192 it would result in approximately 6.5 billion non-LSTM parameters. LSTMP with projection size of 1024 will require ~8 times less non-LSTM parameters (0.8B). We found that LSTMP-based BigLSTM by Josefowicz et al. is already close to the single GPU DRAM limit.\n\nb) On related work. \nThank you for providing related references. We found that \u201cPredicting parameters in deep learning\u201d by  Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas is relevant to F-LSTM and provides further theoretical  support for \u201cfactorization\u201d tricks, hence we included it in Related work section. \nWe  argue that \u201cProgressive neural networks\u201d by Rusu et al. is not directly relevant to our work since it explores the problem of multitask and transfer learning and their \u201ccolumns\u201d are introduced to handle additional tasks and not to improve/speedup learning within single task. We\u2019d also argue that \u201cOnline stabilization of block-diagonal recurrent neural networks\u201d is not closely relevant  to our work because they don\u2019t use groups but assume block diagonal structure only on recurrent connection for the purposes of improving BPTT.\n\nc) We fixed several typos and added few notation clarifications to Figure 1. Also, per reviewers request, we added second plot to the Appendix (Figure 2:B) with wall clock time on the x-axis.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487299320705, "tcdate": 1487299320705, "id": "ICLR.cc/2017/workshop/-/paper57/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper57/reviewers"], "reply": {"forum": "ByxWXyNFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487299320705}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489438587904, "tcdate": 1487299320111, "number": 57, "id": "ByxWXyNFg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "ByxWXyNFg", "signatures": ["~Oleksii_Kuchaiev1"], "readers": ["everyone"], "content": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489186601217, "tcdate": 1489186601217, "number": 2, "id": "rk-Nk2gix", "invitation": "ICLR.cc/2017/workshop/-/paper57/official/review", "forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "signatures": ["ICLR.cc/2017/workshop/paper57/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper57/AnonReviewer1"], "content": {"title": "Interesting paper but needs some more work", "rating": "6: Marginally above acceptance threshold", "review": "Factorization Tricks for LSTMs\n\nSummary:\n\nThis paper empirically investigates different ways of learning parameters of LSTMs to optimize the computational and the memory efficiency of the models. They use LSTMP model from Sak et al 2014 as their baseline and implement two different tricks to speed up their model. The first trick is to use factorize the weight matrices of the neural network to introduce a lower-dimensional bottleneck and the second approach is to create block structure in  They report convincing results on 1-billion word language modeling benchmark. The factorization methods investigated in this paper seems to act like a regularizer and the improve the generalization error as well. \n\nGeneral Comment:\nIt seems like this paper is missing some important references. I would cite [1] for the factorization trick, and progressive networks [2] and block-diagonal recurrent neural networks[3](there have been several other similar papers can be found in the literature, this is the oldest one I could find) for the group structure that you are introducing. The results are interesting.\n\nOverall Review:\n\nPros,\nThe empirical investigation of two different ways to reparametrize LSTM models to speed up and lower the memory consumptions.\nExperiments are convincing and the results are good\n\nCons,\nI think you are missing an important baseline, a regular LSTM language model without projection(not the LSTMP by Sak et al).\nThe writing needs more work, the notation used is a bit difficult to parse.\n\nDetailed and some Minor comments:\n\nOn Page 1, \u201ctransformation 1\u201d \u2014> \"Equation 1\u201d\nFigure 1, needs more description and clarification it is not very clear what d1, d, d2 means. \nPlease use a more formal notation, use subscript for the weights. There are variables used in the equations without being properly defined.\nI would like to see a discussion about the computational complexity of those approaches as well.\nI would like to see the Figure 2 with respect to wall-clock time in the x-axis.\nAfter some more work, this paper can be made much easier to read. This version of the paper is not very easy to understand, unfortunately.\nCan you plot Figure 2 in log-scale, the differences between the learning curves are not very clear to me.\n\n\n[1] Misha Denil, Babak Shakibi, Laurent Dinh, Nando de Freitas, Predicting parameters in deep learning, NIPS 2013.\n[2] Rusu, Andrei A., Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).\n[3] Sivakumar, Shyamala C., William Robertson, and William J. Phillips. \"Online stabilization of block-diagonal recurrent neural networks.\" IEEE Transactions on Neural Networks 10.1 (1999): 167-175.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489186601971, "id": "ICLR.cc/2017/workshop/-/paper57/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper57/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper57/AnonReviewer2", "ICLR.cc/2017/workshop/paper57/AnonReviewer1"], "reply": {"forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper57/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper57/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489186601971}}}, {"tddate": null, "tmdate": 1489112750688, "tcdate": 1489112750688, "number": 1, "id": "ByPh0t1il", "invitation": "ICLR.cc/2017/workshop/-/paper57/public/comment", "forum": "ByxWXyNFg", "replyto": "Skv71Dyol", "signatures": ["~Oleksii_Kuchaiev1"], "readers": ["everyone"], "writers": ["~Oleksii_Kuchaiev1"], "content": {"title": "response to ICLR 2017 workshop paper57 AnonReviewer2", "comment": "Thank you very much for the review and feedback.\n\n1)\tLSTM cell with projection, LSTMP, is indeed quite popular, especially for models with large vocabulary, such as OBW. In fact, we use LSTMP as a baseline (BigLSTM by Josefowicz et al). Our cells also have projections. We updated the text to clearly reflect this:  \u201cExperiments\u201d section explicitly mentions projection size now, and also we did several changes LSTM->LSTMP throughout the text.\n2)\tThank you for the pointer to the convolutional LSTM paper. For cases when problem has spatiotemporal correlations, that is indeed conceptually similar to our approach (exploiting possible structure in the input). Hence, we added it to the related work section.\n3)\tThe plot in Figure 2 demonstrates both number of steps and training losses for several model over exactly the same period of time (1 week). Our original plot legend did not make it clear, hence we updated it to avoid confusion.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487299320705, "tcdate": 1487299320705, "id": "ICLR.cc/2017/workshop/-/paper57/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper57/reviewers"], "reply": {"forum": "ByxWXyNFg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487299320705}}}, {"tddate": null, "tmdate": 1489100575545, "tcdate": 1489100575545, "number": 1, "id": "Skv71Dyol", "invitation": "ICLR.cc/2017/workshop/-/paper57/official/review", "forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "signatures": ["ICLR.cc/2017/workshop/paper57/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper57/AnonReviewer2"], "content": {"title": "nice work", "rating": "6: Marginally above acceptance threshold", "review": "Comments:\n- LSTM with projection is quite standard. Some variant (LSTMP) was introduced in Long short-term memory recurrent neural network architectures for large scale acoustic modeling, by Google, in 2014. You don't compare your work to that. I think it is very related and should be compared.\n- I think Convolutional LSTM Network (https://arxiv.org/abs/1506.04214, from 2015) are also related because you can also see that as kind of grouping.\n- The plot in Figure 2 of the training loss is very nice. I think, in addition, it would be nice to see the same plot but with the training computation time on the X-axis, so you can better see, e.g. after 1 week of training, where you are with each model. In TensorBoard, I think there is even an option to do that.\n\nCons:\n- Lacking related work and comparisons.\n- Lacking experiments on other tasks.\n\nPros:\n- State-of-the-art result on OBW.\n- Open Source code.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Factorization tricks for LSTM networks", "abstract": "Large Long Short-Term Memory (LSTM) networks have tens of millions of parameters and they are very expensive to train. We present two simple ways of reducing the number of parameters in LSTM network: the first one is \u201dmatrix factorization by design\u201d of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the state-of the art perplexity. On the One Billion Word Benchmark we improve single model perplexity down to 24.29.", "pdf": "/pdf/691c904114a2350c4965bfd45f782f28044f04b8.pdf", "TL;DR": "Achieving new single model state-of-the-art (24.29) perplexity on the One Billion Word Benchmark, using new cell structures.", "paperhash": "kuchaiev|factorization_tricks_for_lstm_networks", "conflicts": ["none"], "authorids": ["okuchaiev@nvidia.com", "bginsburg@nvidia.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Oleksii Kuchaiev", "Boris Ginsburg"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489186601971, "id": "ICLR.cc/2017/workshop/-/paper57/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper57/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper57/AnonReviewer2", "ICLR.cc/2017/workshop/paper57/AnonReviewer1"], "reply": {"forum": "ByxWXyNFg", "replyto": "ByxWXyNFg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper57/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper57/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489186601971}}}], "count": 6}