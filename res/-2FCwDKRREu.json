{"notes": [{"id": "-2FCwDKRREu", "original": "WIK8sP_EJcd", "number": 1770, "cdate": 1601308195378, "ddate": null, "tcdate": 1601308195378, "tmdate": 1616022709408, "tddate": null, "forum": "-2FCwDKRREu", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AQbKiZ4_GV", "original": null, "number": 1, "cdate": 1610040459713, "ddate": null, "tcdate": 1610040459713, "tmdate": 1610474062593, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "This paper proposed using the state bisimulation metric to learn invariant representations for reinforcement learning.  The method is generic, effective, and is supported by both theoretical and experimental results.  All reviewers and I think this is a strong contribution to the area."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040459699, "tmdate": 1610474062575, "id": "ICLR.cc/2021/Conference/Paper1770/-/Decision"}}}, {"id": "INN1n437foY", "original": null, "number": 10, "cdate": 1605920247668, "ddate": null, "tcdate": 1605920247668, "tmdate": 1605920261817, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "5slSBViBBY4", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "We thank the reviewer for their assessment and constructive suggestions. We address the two suggestions below -- we are not sure what the reviewer had in mind as an experiment for the second suggestion, and are happy to discuss possible tasks.\n\n\n1. \"the paper would definitely benefit if this claim was backed up with results demonstrating DBC combined with another model-free or model-based algorithm since it might be unclear off-the-shelf if an algorithm requires major or minor changes to work in conjunction with DBC.\"  \nRL is brittle and difficult to get working -- so this takes time. We will note that as a follow up to this work we are combining DBC with Dreamer [1] and using the latent model for planning. We have initial results that show this works better than Dreamer with the original reconstruction loss. This result will be part of a new paper that is not yet finished, so we will not provide those results as part of this publication.\n\n\n\n2. \"While empirically validating this claim from walker_walk -> walker_run / walker_stand is a reasonable starting point, demonstrating another instance where inferring the source and target reward functions is not immediately obvious would definitely benefit the paper.\"  \nAs the reviewer notes, we have a theoretical result (Theorem 4) that dictates what family of reward functions our learned DBC representation can generalize to. We are not sure what \"not immediately obvious\" additional transfer setting the reviewer has in mind, perhaps an example would help?  We know what transfer settings would not work -- for example, if in the original task a robot receives a reward for picking up blue blocks, it will not generalize to red blocks because any red blocks  in the environment will have been dropped by the representation as irrelevant to the original task.\n\n\n\n\n\n1. Dream to Control: Learning Behaviors by Latent Imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi. ICLR 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-2FCwDKRREu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1770/Authors|ICLR.cc/2021/Conference/Paper1770/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855901, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment"}}}, {"id": "3JuP65TjmUp", "original": null, "number": 9, "cdate": 1605918515393, "ddate": null, "tcdate": 1605918515393, "tmdate": 1605918515393, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "6PJ0jwCtAZ2", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for the extensive constructive feedback! We have made changes to the paper in line with your suggestions. We also individually address questions and concerns below.\n\n\u201cKey figures for central results are too small\u201d   \nWe\u2019ve used the now-additional 9th page to enlarge the images.\n\n\u201cFigure 6: it seems that the \u201csimple_distractors\u201d environment is different than \u201csetting 2\u201d natural video setting\u201d... \u201cThe \u2018ideal gas\u2019 is the same as simple_distractors?\u201d  \nYes, sorry for the confusion. The \u201cideal gas\u201d is the same as \u201csimple_distractors\u201d, but distinct from the \u201cnatural video setting\u201d. The \u201cideal gas\u201d / \u201csimple_distractors\u201d were only visualized in the appendix, so to clarify, we have now added them into Figure 4 (middle row), and made the terminology between \u201cideal gas\u201d / \u201csimple_distractors\u201d consistent.\n\n\u201cThere seems to be some ambiguity in notation between observations, underlying state...\u201d   \nWe agree this was ambiguous, and have updated the paper to rename \u201cobservations\u201d as \u201cstates\u201d, since we used frame-stacked images to approximate full observability and a Markov world state.\n\n\u201cInteresting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function. This is to enforce the separateness of the optimizations in algorithm 1?\u201d    \nYes. We found the separation stabilized training. And the stop gradients prevented gradients flowing through the regression\u2019s target, which otherwise resulted in NaN gradients when training the encoder. The dynamics model also needs stop gradients on it\u2019s target to avoid adapting the encoder towards trivial solutions (e.g. always outputting a constant, for sake of accurate dynamics predictions).\n\n\u201cDefinition 2, the bisimulation metric contains a max \u2026 so this is worst case discrepancy between the futures between state space actions and empirical actions. Was average discrepancy considered?\u201d   \nYes, first introduced by Castro 2020: as an expectation w.r.t. the online policy, which is what we also use in equation 4.\n\n\u201cEquation 4 specifies an L1 metric for the distance in abstraction space ||z_i \u2013 Z_j ||. Is there a motivation for this choice? Again trying to bound the worst error?\u201d  \nWe investigate L1 vs L2, but found no difference. We opted for L1 where large errors do not dominate as much as in L2. We are not trying to bound the worst case error, only the averaged w.r.t. the online policy in equation 4, for reason of being able to use the actions stored in the replay buffer.\n\n\u201cTheorem 4 references Theorem 5 which is not in the main text.\u201d  \nAdded.\n\n\u201cIn section 5, ideally epsilon would be briefly described before it appears in a bound.\u201d  \nWe have added additional discussion on epsilon, copied here for convenience. It is the amount of \u201callowed\u201d approximation, where large epsilon leads to a more compact bisimulation partition at the expense of a looser bound on the optimal value function. \n\n\u201cThere is a statement \u201ca single loss function would be less stable and require balancing the components\u201d. Is this speculation or based on experience? Separate optimizations implicitly define a balance between these terms wouldn\u2019t they? Namely equal balance?\u201d  \nPossibly yes, but on reflection we\u2019ve now removed the point about \u201cbalance\u201d, since the point about training stability was much more critical.\n\n\u201cFigure 5: What is the first column of figures to the left? It doesn\u2019t seem to be relevant?\u201d   \nAgreed, removed.\n\n\u201cDrawing a white border between the figure pairs would underscore visually that there are two states in each \u201cimage\u201d\u201d   \nAgreed, done.\n\n\u201cFigure 9: I could not make sense of the images on the sides of the figure. Particularly, the images on the left side. They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was.\u201d   \nThese shapes were a stone wall the car had hit (during training) causing it to flip onto it\u2019s left side, causing the sky to appear on the right side of the image. We\u2019ve updated the caption text and main text about the CARLA task.\n\n\u201cFuture work \u2013 another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds.\u201d   \nThanks for the additional examples! We have included them on page 9.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-2FCwDKRREu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1770/Authors|ICLR.cc/2021/Conference/Paper1770/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855901, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment"}}}, {"id": "EbpHlkPvleu", "original": null, "number": 8, "cdate": 1605917990299, "ddate": null, "tcdate": 1605917990299, "tmdate": 1605917990299, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "RbVptpSUFSy", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment", "content": {"title": "Thank you for your review", "comment": "Thank you for your positive review. We have addressed your two main concerns below and in the updated version of the paper.\n\n\u201cI do not agree with the example as weather can directly alter the dynamics and desired behavior of an AV system\u201d  \nYes, we agree (e.g. wet roads can cause sliding), so we updated our example to day/night lighting which affects the observation space but not the physical dynamics\u201d.\n\n\u201cIn Figure 4 the proposed approach is outperformed by contrastive learning in the default setting. I understand that the goal is to learn robust representations for the natural setting, but can the authors comment on why it fails to beat the contrastive approach here and provide insight on how this may be addressed.\u201d  \nIn the default setting there are no equivalent observations to be compressed. Therefore the contrastive approach performs correctly (whereas it does not when there are bisimilar observations -- two bisimilar observations can end up as the negative pair with a contrastive loss, so it does not compress anything). Therefore, this result shows that when nothing needs to be compressed, a contrastive loss is easier to optimize than matching exact distances in a latent space (a regression problem).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-2FCwDKRREu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1770/Authors|ICLR.cc/2021/Conference/Paper1770/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855901, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Comment"}}}, {"id": "6PJ0jwCtAZ2", "original": null, "number": 1, "cdate": 1603931620743, "ddate": null, "tcdate": 1603931620743, "tmdate": 1605024361290, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review", "content": {"title": "Deep twin network loss implementation of bisimulation-based state abstraction is robust to extraneous details in realistic visual environments ", "review": "# Paper Summary \n\nThe paper presents a new method for embedding visual images into a state space suitable for effective control by an actor-critic style RL algorithm. They show how a previously explored idea of using a bisimulation between state abstractions and reward sequences to group states that are similar from a decision theoretic perspective can be extended to a continuous deep embedded representation using twin network style learning through standard gradient descent optimization. They call the approach Deep Bisimulation for Control (DBC).  The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains. The approach contrasts directly with algorithms that use an autoencoder to find a compact representation by reconstructing input frames or predicting future input frames. These approaches necessarily represent enough information to reconstruct both task relevant and incidental details. Experimentally, the paper shows that performance of reconstruction-based approaches degrades by a large and significant amount when extraneous background detail is present in image frames, while the proposed method is immune. The evaluation is done on widely respected benchmark of Deep Mind MuJoCo based articulated figure simulations and a CARLA realistic image simulation of car driving which shows that focusing on task specific detail is important on realistic tasks. \n\n# Pros and Cons \n\nThe work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images.  The ability to find compact, task relevant representations of high-dimensional inputs is central to the ICLR community. \n\nThe paper covers relevant background in state abstraction in RL and it is clear how it relates to the paper's contribution.  \n\nThe paper clearly explains the background of bisimulation and how the idea of grouping states according to their probable reward sequences can be practically realized by grouping states by their immediate reward and future state distributions. It is also clear in principle that a twin network can be used to induce a latent space with these properties.  \n\nThe paper shows that the policy will converge and that the error will be bounded and that the representation will generalize to any problem domain in which causal dependencies are a subset of the problem domain the representation was trained on. Which is nice.  \n\nThere is a nice evaluation on both MuJoCo articulated figure problems from the Deep Mind Control suite as well as the CARLA simulated driving application that uses large realistically rendered images. Comparison against state-of-the-art RL algorthims makes results convincing. I think the CARLA example is nice as it shows that practical problems without artificial augmentations have the property that the input details can be distracting if fully reconstructed. The superiority of DBC in this case really makes this point well. \n\nThe paper does not report computational load associated with their approach. Presumably due to the closed form Wasserstein approximations, we are probably looking at only a fractional increase in time for the encoder network above the policy, dynamics and reward models. So roughly 30% more?? \n\nKey figures for central results are too small to get meaningful interpretations unless enlarged by a factor of 4 or 5. Somehow this seems to go against the spirit of page limits to me. \n\nFigure 6 and the description don't seem to be aligned, but I can imagine that it can be readily fixed.\n \n\n# Recommendation  \n\nI recommend acceptance. The paper shows how to extend bisimulation principle for grouping states to continuous deep actor critic methods and provides convincing evidence on standard benchmarks that it is effective in extracting a task-relevant abstraction that is robust to noise in observations and focuses on task specific detail. \n\n \n\n# Questions  \n\nFigure 6: it seems that the \u201csimple_distractors\u201d environment is different than \u201csetting 2\u201d natural video setting. This caption could use some reworking to get parallelism clear. The \u2018ideal gas\u2019 is the same as simple_distractors?  Also the graphs seems to be about different experimental types. Does the first experiment also use frozen encoder?  The graphs do not seem to relate to the text which talks about walker_stand, walker_run reward functions. I am confused. \n\nCan the paper have any insight into the relative performance of their algorithm versus benchmarks algorithms across different tasks (finger spin, cheetah, walker) given that they are all stick figures? \n\n# Feedback  \n\nI was able to work through Definition 1 and assure myself it made sense. In the end I made a small picture that helped. \n\nThere seems to be some ambiguity in notation between observations, underlying state and latent variable spaces.  For instance, in section 3, script S is defined as a state space. In section 4, the function d is defined on script S x script S which is described as an observation space.  Admittedly, the work seems to be situated in an approximately fully observable world in which states and observations are somewhat equivalent. I suspect that is why the 5-camera 300-degree suround view was necessary in the CARLA experiments. \n\nInteresting that the paper employ stop gradients on the latent representation terms when they appear in the reward and Wasserstein terms in the loss function. This is to enforce the separateness of the optmizations in algorithm 1? \n\nDefinition 2, the bisimulation metric contains a max \u2026 so this is worst case discrepancy between the futures between state space actions and empirical actions. Was average discrepancy considered? This might be relevant later in discussion about intractability in previous approaches (section 4 paragraph 3). \n\nEquation 4 specifies an L1 metric for the distance in abstraction space ||z_i \u2013 Z_j ||. Is there a motivation for this choice? Again trying to bound the worst error? \n\nTheorem 4 references Theorem 5 which is not in the main text. \n\nIn section 5, ideally epsilon would be briefly described before it appears in a bound. \n\nFigure 3 is very small... the labels \u2013 particularly the subscripts are unreadable. It may not be making a point important enough to include it. The causal variables section could be shorted in general. It is a good point but not completely surprising. \n\nFigure 4 is a key figure to support the paper's hypothesis that deep bisimulation is effective for state abstraction in noisy images. This figure really needs to be bigger. In particular, I had to strain to read the legends to understand if the axes were different between the uncluttered video of articulated figures and the cluttered video of articulated figures with a background movie in them.  It was also hard to make out which line was which.  In particular, the \u201ccheetah\u201d column does not show DBC improving on cluttered video scenario --- it tops out at around 250 in both top and bottom graphs, but the scales are very different. It confuses the message a bit.  \n\nThere is a statement \u201ca single loss function would be less stable and require balancing the components\u201d. Is this speculation or based on experience? Separate optimizations implicitly define a balance between these terms wouldn\u2019t they? Namely equal balance? \n\nTo some degree, figure 9 is making the same argument as figure 5. Could leave this out if you were tight for space. \n\nFigure 5: What is the first column of figures to the left? It doesn\u2019t seem to be relevant? Otherwise, I think the figure is very effective in conveying the structured embedding space does a better job of grouping similar states. It is also very small. Drawing a white border between the figure pairs would underscore visually that there are two states in each \u201cimage\u201d  \n\nFigure 9: I could not make sense of the images on the sides of the figure. Particularly, the images on the left side.  They seemed to be abstract geometric shapes and I could not get an intuition about what the driving scenario was. \n\nSection 6.4 \u201creward highway progression an penalizes collisions\u201d  an => and \n\nFuture work \u2013 another likely avenue for future work would be to introduce some sort of memory to handle partially observable worlds. For instance, can the agent drive a car with only a forward view if given memory? Does this break down if it does not have memory? This could either be an explicit neural memory or implicit memory such as an LSTM \u2026  Estimating uncertainty could also be important to produce agents that can work in the real world and assess when they know what they are doing and when they do not. Another future work area is in modeling of transition distributions as something more complex than Gaussians \u2026 what if there are distinct possible futures that are equally valid: it is ok for the robot to turn left or right as long as it avoids the object straight ahead? ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111027, "tmdate": 1606915789336, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1770/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review"}}}, {"id": "5slSBViBBY4", "original": null, "number": 2, "cdate": 1604113921067, "ddate": null, "tcdate": 1604113921067, "tmdate": 1605024361224, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review", "content": {"title": "Promising approach to learn task relevant representations for control", "review": "**Summary**\nThe paper focuses on how learning state-representations that encode information relevant to the task can improve reinforcement learning from pixels. Often, observations in an MDP can contain information that are irrelevant (\u201cdistractors\u201d) to the task at hand and can likely \u201cdistract\u201d the downstream RL algorithm used. Unlike existing reconstruction based approaches (which don\u2019t explicitly incentivize ignoring task-irrelevant information), the authors propose Deep Bisimulation Control (DBC) that relies on bi-simulation metrics (as the task-aware criterion) that encode behavioral similarity b/w states with respect to the reward structure. Instead of explicitly learning a bi-similarity distance function, authors enforce the representations which under L1 distances correspond to bi-simulation metrics. DBC demonstrates learning these representations in conjunction with the control policy, reward and a dynamics model. Furthermore, the authors highlight connections to causal inference which can hopefully further provide insights into which \u201cnew\u201d reward structures can the learned representations generalize to (since bi-similarity metrics themselves are heavily dependent on the reward structure). Results obtained by the authors demonstrate that DBC can learn task specific representations and a control policy in a robust manner in the presence of distractors on the Deepmind Control Suite and the CARLA simulator. Additionally, the authors also demonstrate how DBC can generalize to new reward functions on Mujoco.\n\n**Strengths**\n\n- The paper is generally well-written and easy to follow for the most part. The authors do a good job of walking the reader through the preliminaries, highlighting distinctions with prior usage of bisimilarity metrics and how DBC ties in with slight modifications to the Soft Actor-Critic algorithm.\n- The results obtained on DM control suite demonstrate that while DBC is competitive or slightly worse compared to other approaches in the absence of distractors, it performs better compared the set of baselines when distractors are introduced \u2014 significantly outperforming the reconstruction and the contrastive approaches. Furthermore, compared to a VAE, DBC places observations that are similar in terms of task information closer in the learnt embedding space.\n- The authors also show that compared to an adaptation of prior usage of bisimulation metrics, DBC is more sample efficient. Additionally, within the considered family of reward functions to generalize to \u201cwalker_run\u201d and \u201cwalker_stand\u201d, DBC shows stronger generalization compared to DeepMDP. Furthermore, learned embeddings on CARLA demonstrate qualitatively that DBC learns to group behaviorally similar distractor states (manifesting as obstacles).\n\n**Weaknesses**\nI don\u2019t have any major weaknesses to point out. I will highlight minor comments / weaknesses which, I think if addressed, would definitely make the paper stronger.\n- While the authors state that DBC in practice can be combined with any model-free of model-based algorithm, the paper would definitely benefit if this claim was backed up with results demonstrating DBC combined with another model-free or model-based algorithm since it might be unclear off-the-shelf if an algorithm requires major or minor changes to work in conjunction with DBC.\n- Task generalization (or generalizing over different reward functions) relies on the assumption that the new reward function depends on contributing factors that likely influence the earlier \u201cseen\u201d reward function as well. While empirically validating this claim from walker_walk -> walker_run / walker_stand is a reasonable starting point, demonstrating another instance where inferring the source and target reward functions is not immediately obvious would definitely benefit the paper.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111027, "tmdate": 1606915789336, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1770/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review"}}}, {"id": "RbVptpSUFSy", "original": null, "number": 3, "cdate": 1604120278154, "ddate": null, "tcdate": 1604120278154, "tmdate": 1605024361155, "tddate": null, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "invitation": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review", "content": {"title": "The paper is well written and presents a clear approach to a well motivated problem with strong evaluation results. ", "review": "The authors propose an approach to robust representation learning of observations for reinforcement learning by training a model to align the euclidean distance between two observations with bisimulation metrics that quantify how similar the states that generated the observations are in terms of the control problem.  This reduces the effect of irrelevant features in the observations on the representations.\n\nThe paper is well written, the problem is clearly motivated and the approach and technical contribution is easy to follow.\n\nThe approach to use the state bisimulation metric to supervise observation representation is intuitive and clearly motivated.  Theoretical analysis is provided with generalization guarantees.\n\n\"As an example, in the context of autonomous driving, an intervention can be a change in weather, or a\nchange from day to night which affects the observation space but not the dynamics or reward.\"  I do not agree with the example as weather can directly alter the dynamics and desired behavior of an AV system.  The point in this paragraph is still clear but I would suggest a different example.\n\nThe evaluations are strong and run on a number of different experiment settings against multiple strong SOTA models.\n\nIn Figure 4 the proposed approach is outperformed by contrastive learning in the default setting.  I understand that the goal is to learn robust representations for the natural setting, but can the authors comment on why it fails to beat the contrastive approach here and provide insight on how this may be addressed.  Some problems have only a few distractors and may fall between the natural and the default setting.\n\nrecommendation and reasoning\n\nThe paper is well written and presents a clear approach to a well motivated problem with strong evaluation results.  I recommend acceptance.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1770/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1770/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Invariant Representations for Reinforcement Learning without Reconstruction", "authorids": ["~Amy_Zhang1", "~Rowan_Thomas_McAllister1", "~Roberto_Calandra1", "~Yarin_Gal1", "~Sergey_Levine1"], "authors": ["Amy Zhang", "Rowan Thomas McAllister", "Roberto Calandra", "Yarin Gal", "Sergey Levine"], "keywords": ["rich observations", "bisimulation metrics", "representation learning", "state abstractions"], "abstract": "We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_invariant_representations_for_reinforcement_learning_without_reconstruction", "pdf": "/pdf/603fb2b2d3c728fdb6a0d1f23a60748227ff46c7.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021learning,\ntitle={Learning Invariant Representations for Reinforcement Learning without Reconstruction},\nauthor={Amy Zhang and Rowan Thomas McAllister and Roberto Calandra and Yarin Gal and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-2FCwDKRREu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-2FCwDKRREu", "replyto": "-2FCwDKRREu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1770/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538111027, "tmdate": 1606915789336, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1770/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1770/-/Official_Review"}}}], "count": 8}