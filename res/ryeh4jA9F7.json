{"notes": [{"id": "ryeh4jA9F7", "original": "HkeDx8REKQ", "number": 37, "cdate": 1538087732268, "ddate": null, "tcdate": 1538087732268, "tmdate": 1545355378117, "tddate": null, "forum": "ryeh4jA9F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJgNCp3-eN", "original": null, "number": 1, "cdate": 1544830411863, "ddate": null, "tcdate": 1544830411863, "tmdate": 1545354530876, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Meta_Review", "content": {"metareview": "Reviewers mostly recommended to reject after engaging with the authors, with one reviewer slightly suggesting to accept, but with confidence 1. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper37/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353360845, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353360845}}}, {"id": "S1lKRChbJE", "original": null, "number": 6, "cdate": 1543782097340, "ddate": null, "tcdate": 1543782097340, "tmdate": 1543782097340, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "r1exBMUK0X", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "content": {"title": "Thanks for the response & further questions", "comment": "Thanks for the detailed response for my previous questions. \n\n\"We add that information in the main text in Sec 3.4 to make it clearer.\"\n\nThe last paragraph of Sec. 3.4 is still unclear. \n\nFirst the authors mentioned \"where Ea,b,\u03c7,\u03b8 is the expectation over the parameters of the affine transformation applied to the patch. In practice, we handle this expectation by sampling uniformly and independently the localization over the image, the angle \u03b8, and the re-scaling parameter (\u03c7) over a range defined in Table 4.\" \n\nQuestion 1: How many i.i.d. samples you drew for each parameter, a, b, \\xi, \\theta? I supposed that given finite-number parameter realizations, the expectation will be written in its empirical sum form, right? Do you need sampling at each iteration to update the universal perturbation?\n\nQuestion 2: Table 4 or Table 2?\n\n\n\" Nonetheless, we do think that providing robustness to universal perturbation is still a useful technique as these perturbations can be deployed in the real world (https://arxiv.org/pdf/1712.09665.pdf).\"\n\nThis motivation is not enough, since generating a per-sample perturbation is much easier than the universal perturbation, right? If so, we need a better motivation on the usefulness of this technique.\n \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611219, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper37/Authors|ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611219}}}, {"id": "HJxnhijqAX", "original": null, "number": 5, "cdate": 1543318452237, "ddate": null, "tcdate": 1543318452237, "tmdate": 1543318452237, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "H1xBXQ8FC7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "content": {"title": "thanks for the response", "comment": "I thank the authors for a clear and helpful response.\n\nThanks for clarifying the approximations to fictitious play. Id recommend also clarifying this in the text.\n\nI agree that the authors are addressing a valuable gap in the literature, since universal perturbations present a more realistic and limited attack scenario in production settings, as do adversarial patches as opposed to norm ball attacks.\n\nI understand the authors point that they provided a natural baseline from the literature. However I do still feel that their method contains two important modifications to adversarial training (limiting oneself to universal perturbations and additionally including attacks of previous models). I do think the paper would need an ablation study of these two modifications before I could recommend acceptance.\n\nI'd encourage the authors to provide these for a future submission."}, "signatures": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611219, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper37/Authors|ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611219}}}, {"id": "H1xBXQ8FC7", "original": null, "number": 3, "cdate": 1543230237440, "ddate": null, "tcdate": 1543230237440, "tmdate": 1543231288086, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "BkeuIKaB2X", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "content": {"title": "Thanks for the review.", "comment": "We would like to thank the reviewer for their comments. The reviewer considers the approach novel and well-motivated however has some concerns regarding the baselines used in the experiments. More precisely, the reviewer wants us to clarify the approximation of the fictitious play process, here are the changes done in our revised version to address their comments:\n \n1. Approximation of fictitious play: Our method does 2 approximations of the Fictitious play process:\n    a. First as we do deep learning, we can\u2019t guarantee that we compute a best response.\n    b. The second is that at each iteration, we do not remember all previous classifiers we computed since the beginning of the training.\n\n2. Indeed there is a gap in the literature that we we address by proposing an algorithm specifically designed to deal with universal adversarial perturbation. However, there are no externally available baselines. Thus we choose the most natural baseline which is adversarial training. \n\n3. This idea proposed by the reviewer would indeed interpolate between the per-sample setting and the universal perturbations setting.\nFor small batches, this method would be close to per-sample perturbations and would probably be less robust at large scale (on dataset like Imagenet).\nFor large batches, computing a good enough perturbation could take time. For example, with our implementation we need around 30min to get a perturbation.\n\nMore specifically on the second comment which concerns the choice of the baselines. Indeed most of the methods in the literature consider conventional adversarial perturbations and here we decide to focus on universal adversarial examples. This choice is motivated by the fact that it is concretely possible to build such a universal patch and fool a recognition algorithm working in the wild. This threat can be considered even more alarming than conventional adversarial perturbations because a unique perturbation can be detrimental to an entire recognition architecture. Hence, we believe that finding a robustification algorithm in this more restrictive scenario should be addressed together or even before the conventional adversarial perturbations.  Besides, every algorithm working on conventional adversarial perturbations should adapt to the more restrictive case of universal adversarial examples. Therefore the set of baselines chosen is legitimate.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611219, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper37/Authors|ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611219}}}, {"id": "r1exBMUK0X", "original": null, "number": 2, "cdate": 1543230007721, "ddate": null, "tcdate": 1543230007721, "tmdate": 1543230891367, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "BJe5rqjvnX", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "content": {"title": "Thanks for the review.", "comment": "We would like to thank the reviewer for their comments. The reviewer have very positive comments by highlighting that our algorithm is an interesting generalization of robust adversarial training. However, several concerns are raised concerning the clarity of the algorithms and experiments. More precisely, the first and second points of the comment section concern the algorithms. We address those comments by the following changes in our paper:\n\nConcerning step 3 of algorithm 1. In the appendix, we provided a table of hyperparameters which contains the number of steps used to compute an adversarial perturbation (see Perturbation loop). Now we add the reference of the table and give explicitly the number of SGD steps used in algorithm 1. \n\nConcerning the generation of the universal perturbation. The parameters of the affine transformation are sampled uniformly and independently for each image of the batch. The range of each parameter was given in Table 2 in the appendix. We do not use an MC particle-based approximation for these random parameters. We add that information in the main text in Sec 3.4 to make it clearer.\n\nIn the robust adversarial training algorithm proposed by Madry, the outer minimization is performed only on perturbed samples. Here, we do a mix (50/50) between perturbed and non-perturbed samples. We observe that it gives good results in practice.\n\nThen points 3 and 4 raise some concerns about the experiments. We address those comments by the following changes in our paper:\nConcerning the number of samples for each datasets (We add those numbers in the appendix):\nCifar10: Train set: 40000, Test set:10000, Valid set:10000\nCifar100: Train set:40000, Test set:10000, Valid set:10000\nImageNet: Train set: 1271167, Test set: 50000, Valid set: 10000.\nConcerning the robustness against per sample perturbation: We do not expect our model to be robust to per-sample perturbation and never claimed such a robustness. Providing such a robustness is quite difficult in Cifar10 and is still out of reach in large dataset such as Imagenet. Nonetheless, we do think that providing robustness to universal perturbation is still a useful technique as these perturbations can be deployed in the real world (https://arxiv.org/pdf/1712.09665.pdf).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611219, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper37/Authors|ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611219}}}, {"id": "H1gqeMLK0m", "original": null, "number": 1, "cdate": 1543229937696, "ddate": null, "tcdate": 1543229937696, "tmdate": 1543229937696, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "r1ln77Jc2m", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "content": {"title": "Thanks for the review.", "comment": "Thanks a lot for your comments on the paper. The reviewer acknowledged that the idea is novel and asks clarifications on the scenario of the attack and on the approximation made.\nConcerning the scenario: Many work on the use of universal perturbations to fool classifier have been developed. For instance, the work on universal patches (https://arxiv.org/pdf/1712.09665.pdf) can be used in the real world. In this paper the authors use a printed patch to fool an image recognition application (the Demitasse application). In our paper, we propose a method to address that problem.\nConcerning the approximation made: As we are dealing with deep neural networks, we are making two approximation to the Fictitious play process. The first one is that at each steps we do not compute the exact best response. The second is that we don\u2019t actually maintain all the best responses computed in the past to compute a perturbation against a uniform mixture of these best response but rather learn this uniform mixture.\nWe do not expect this method to give any form of robustness to per-sample perturbations. This is not the claim we are trying to support in this paper. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611219, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeh4jA9F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper37/Authors|ICLR.cc/2019/Conference/Paper37/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611219}}}, {"id": "r1ln77Jc2m", "original": null, "number": 3, "cdate": 1541169956364, "ddate": null, "tcdate": 1541169956364, "tmdate": 1541534340170, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "content": {"title": "nicely presented ideas, lacking discussion around guarantees (or not)", "review": "Being familiar but not an expert in either game theory or adversarial training, my review will focus on the overall soundness of the proposed method\n\nSummary:\n\nThe authors propose to tackle the problem of adversarial training.\nDeep networks are know to be susceptible to adversarial attacks.\nAdversarial training is concerned with the training of networks that both achieve good performance for the original task while being robust to adversarial attacks.\n\nThey propose to focus on universal adversarial perturbations, as opposed to per-sample perturbations. The latter is a subclass of the former. \nIt doesn\u2019t strike as the most natural scenario: I can\u2019t really think of a practical image classification scenario where one would want to perturb a whole dataset of image with a single perturbation. That said, this focus leads to simpler algorithms (complexity and storage wise) which are worth exploring.\n\nThe authors first present the min-max problem of adversarial training at hand where a classifier f mimizes a loss L for a dataset D, while the conman maximizes the loss over perturbation of the dataset \\epsilon.\nThey then introduce an algorithm to solve it inspired by fictitious play:\nA sequence of classifiers and perturbed datasets are created iteratively by the two players (classifier, conman) and each player uses the complete history of its opponent to make its next move.\n\nThe objective solved by each player  is :\nconman: fool all past classifiers with a single new perturbation\nclassifier: be robust to all past perturbations so far.\n\nAlthough it makes intuitive sense, it is unclear from the manuscript whether this formulation provides any convergence guarantees. It would be great to know whether the connection to fictitious play is purely inspirational or if any of the theoretical guarantees from game theory apply here.\n\nThe conman\u2019s objective to fool all past classifiers is the bottleneck (in terms of storage) and an approximation is proposed: the mean loss over past classifiers is replaced by the loss under a single \u2018average\u2019 classifier trained on all past dataset, with the intuition that this average classifier summarizes all past classifiers\n\nA particular algorithm for perturbation learning is described and the proposed algorithm is compared against two baselines: a pre-existing adversarial training algorithm, an non-adversarial algorithm\n\nThe metrics chosen are accuracy and adversarial accuracy.\nOn standard classification tasks, adversarial algorithms perform slightly less well on the original task (accuracy) but are robust to perturbation as expected,\n\nIt would be interesting to know if these good performances extend to per-sample perturbations: Do a network trained on universal perturbations perform well against per sample perturbation? \n\n\nRemarks:\nsgn missing in the adversarial patch update (and who is alpha?)\nintroduce terminology: white box black box\n", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "cdate": 1542234552257, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335634899, "tmdate": 1552335634899, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJe5rqjvnX", "original": null, "number": 2, "cdate": 1541024322147, "ddate": null, "tcdate": 1541024322147, "tmdate": 1541534339963, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "content": {"title": "A good improvement on robust adversarial training against universal perturbations, but with questions", "review": "In this paper, the authors proposed universal perturbation based robust training framework. With the aid of universal perturbation, the conventional robust training framework can be further interpreted as a fictitious play. Interesting algorithm and results are reported in the paper. My detailed comments are listed as follows. \n\n1) Some details of the proposed algorithm 1 are missing. In step 3, is just single SGD step performed? The generation of universal perturbation is not clearly discussed in Sec. 3.4. How to handle the expectation over the parameters of the affine transformation applied to the patch? MC particle-based approximation for these random parameters? If so, how many particles are used? \n\n2) I am confused on Algorithm\\,2 (AT). Is step 5 same as the robust adversarial training algorithm proposed by Madry \n et al.? What I recall is that SGD (for outer minimization) is only performed over perturbed samples, No? Please clarify it.\n\n3) In experiments, the authors mentioned \"The accuracy (dotted line in the plots) is the fraction of examples that have been correctly classified for a batch of 10000 samples randomly chosen in the train, validation and test sets.\" Please clearly define the train/validation/test datasets, e.g., size and how to generate adversarial examples for testing. \n\n4) In Figure 4-6, is only the universal perturbation based attack evaluated? It does not seem a fair comparison, since the proposed min-max problem builds on the generation of universal perturbations. I wonder how robustness of the proposed method against per-sample perturbation, e.g., C\\&W attack. I think it might be important to find a third-party attack method, e.g., C\\&W or physically transformed attacks, to test both fictitious play and robust adversarial training.\n\nIn general, the paper contains interesting ideas and results. However, there exist questions on their implementation details and empirical results. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "cdate": 1542234552257, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335634899, "tmdate": 1552335634899, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkeuIKaB2X", "original": null, "number": 1, "cdate": 1540901200019, "ddate": null, "tcdate": 1540901200019, "tmdate": 1541534339610, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "content": {"title": "Nice idea, insufficient baselines", "review": "The authors focus solely on universal adversarial perturbations, considering both epsilon ball attacks and universal adversarial patches. They propose a modified form of adversarial training inspired by game theory, whereby the training protocol includes adversarial examples from previous updates alongside up to date attacks.\n\nOriginality: I am not familiar with all the literature in this area, but I believe this approach is novel. It seems logical and well motivated.\n\nQuality and significance: The work was of good quality. However I felt the baselines provided in the experiments were insufficient, and I would recommend the authors improve these and resubmit to a future conference.\n\nClarity: The work was mostly clear.\n\nSpecific comments:\n1) At the top of page 5, the authors propose an approximation to fictitious play. I did not follow why this approximation was necessary or how it differed from an stochastic estimate of the full objective. Could the authors clarify?\n\n2) The method proposed by the authors is specifically designed to defend against universal adversarial perturbations, yet all of the baselines provided defend against conventional adversarial perturbations. Thus, I cannot tell whether the gains reported result from the inclusion of \"stale\" attacks in adversarial training, or simply from the restriction to universal perturbations. This is the main weakness of the paper.\n\n3) Note that as a simple baseline, the authors could employ standard adversarial training, for which the pseudo universal pertubations are found across the current SGD minibatch.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper37/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Official_Review", "cdate": 1542234552257, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper37/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335634899, "tmdate": 1552335634899, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper37/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxwTzBWqQ", "original": null, "number": 3, "cdate": 1538507454861, "ddate": null, "tcdate": 1538507454861, "tmdate": 1538507454861, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "content": {"comment": "The introduction says \"In contrast to the vast body of research dedicated to per-sample adversarial perturbations (Szegedy et al., 2014; Moosavi-Dezfooli et al., 2016; Goodfellow et al., 2015), we focus on universal adversarial perturbations (Brown et al., 2017; Moosavi-Dezfooli et al., 2017)\". Actually the Goodfellow et al 2015 paper describes how to make perturbations that are independent of the input, by adding a weight vector of a linear classifier to the input. Though the examples are made using a linear classifier, they also transfer to deep nets. Andrej Karpathy later demonstrated more of these input-independent examples: http://karpathy.github.io/2015/03/30/breaking-convnets/ The catchy name \"universal adversarial perturbations\" wasn't coined until later but the idea already existed.\n\nI just intend this as feedback for revision, I don't intend for this to have any bearing on whether the paper is accepted or rejected.", "title": "Universal adversarial perturbations were known since 2014"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311933843, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeh4jA9F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311933843}}}, {"id": "Skgc1MSWqm", "original": null, "number": 2, "cdate": 1538507234258, "ddate": null, "tcdate": 1538507234258, "tmdate": 1538507234258, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "content": {"comment": "The abstract makes it sound as if it is an original contribution of this paper to view adversarial examples from the perspective of game theory:\n\"While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\"\n\nHere are several prior works that explicitly describe adversarial examples in terms of a minimax game:\nhttps://pdfs.semanticscholar.org/b5ec/486044c6218dd41b17d8bba502b32a12b91a.pdf\nhttps://ieeexplore.ieee.org/document/7966196\nhttps://arxiv.org/pdf/1803.01442.pdf\nhttp://openaccess.thecvf.com/content_ICCV_2017/papers/Oh_Adversarial_Image_Perturbation_ICCV_2017_paper.pdf\nIncluding some works that mention fictitious play:\nhttps://aaai.org/ocs/index.php/FSS/FSS17/paper/download/15994/15311\nhttps://arxiv.org/pdf/1809.06784.pdf\n\nThere are several more such works, easily discovered by a search engine.\n\nI haven't read the paper at all and it's possible that a related work section acknowledges this prior work exists / it's possible that this paper has a new take on games / fictitious play. Even if that is the case, the abstract really should state the novelty differently.", "title": "Game theory perspective is not novel"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311933843, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeh4jA9F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311933843}}}, {"id": "H1eTnRJAt7", "original": null, "number": 1, "cdate": 1538289332613, "ddate": null, "tcdate": 1538289332613, "tmdate": 1538289332613, "tddate": null, "forum": "ryeh4jA9F7", "replyto": "ryeh4jA9F7", "invitation": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "content": {"comment": "But the content is not. The most critical question of generating adversarial samples is how to keep the concept class unchanged. If we are able to do so, the training is straightforward.", "title": "The title attracted me"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper37/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Playing the Game of Universal Adversarial Perturbations", "abstract": "We study the problem of learning classifiers robust to universal adversarial perturbations. While prior work approaches this problem via robust optimization, adversarial training, or input transformation, we instead phrase it as a two-player zero-sum game. In this new formulation, both players simultaneously play the same game, where one player chooses a classifier that minimizes a classification loss whilst the other player creates an adversarial perturbation that increases the same loss when applied to every sample in the training set.\nBy observing that performing a classification (respectively creating adversarial samples) is the best response to the other player, we propose a novel extension of a game-theoretic algorithm, namely fictitious play,  to the domain of training robust classifiers. Finally, we empirically show the robustness and versatility of our approach in two defence scenarios where universal attacks are performed on several image classification datasets -- CIFAR10, CIFAR100 and ImageNet.", "paperhash": "perolet|playing_the_game_of_universal_adversarial_perturbations", "keywords": ["adversarial perturbations", "universal adversarial perturbations", "game theory", "robust machine learning"], "authorids": ["perolat@google.com", "mateuszm@google.com", "piot@google.com", "pietquin@google.com"], "authors": ["Julien Perolet", "Mateusz Malinowski", "Bilal Piot", "Olivier Pietquin"], "TL;DR": "We propose a robustification method under the presence of universal adversarial perturbations, by connecting a game theoretic method (fictitious play) with the problem of robustification, and making it more scalable.", "pdf": "/pdf/4f61f7e7ae37b7c1dbfba6cf4472e05befd60945.pdf", "_bibtex": "@misc{\nperolet2019playing,\ntitle={Playing the Game of Universal Adversarial Perturbations},\nauthor={Julien Perolet and Mateusz Malinowski and Bilal Piot and Olivier Pietquin},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeh4jA9F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper37/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311933843, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeh4jA9F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper37/Authors", "ICLR.cc/2019/Conference/Paper37/Reviewers", "ICLR.cc/2019/Conference/Paper37/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311933843}}}], "count": 13}