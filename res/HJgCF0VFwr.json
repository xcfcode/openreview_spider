{"notes": [{"id": "HJgCF0VFwr", "original": "rJg3Pw__DH", "number": 1267, "cdate": 1569439366323, "ddate": null, "tcdate": 1569439366323, "tmdate": 1583912048870, "tddate": null, "forum": "HJgCF0VFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "xtD-4hfVb1", "original": null, "number": 1, "cdate": 1576798718903, "ddate": null, "tcdate": 1576798718903, "tmdate": 1576800917666, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a novel approach for pruning deep neural networks using non-parametric statistical tests to detect 3-way interactions among two nodes and the output. While the reviewers agree that this is a neat idea, the paper has been limited in terms of experimental validation. The authors provided further experimental results during the discussion period and the reviewers agree that the paper is now acceptable for publication at ICLR-2020. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719138, "tmdate": 1576800269735, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Decision"}}}, {"id": "Byxg4B70KB", "original": null, "number": 1, "cdate": 1571857704283, "ddate": null, "tcdate": 1571857704283, "tmdate": 1574492856432, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "In this paper, the authors propose a new pruning technique that utilizes the statistical dependency between the corresponding nodes and outputs. The dependency is measured by a kernel based dependency measure which is closely related to MMD. The test statistics derived from the dependency measure have an asymptotic distribution which can be written as a weighted sum of chi-square random variables. The proposed method is numerically investigated using some datasets such as MNIST and CIFAR 10.\n\nPruning is one of important problems for practical deep learning operations. This paper gives an interesting idea for the pruning techniques. I think the idea is novel.\n\nOn the other hand, I also have the following concerns:\n- Although applying the kernel type information measure is an interesting idea, its computational complexity would be large. It is not obvious that it works for large datasets such as ImageNet even if the sub-sampling technique is applied.\n- The numerical experiments are conducted in small datasets. How it works in larger datasets such as ImageNet and (more importantly) how it is compared with SOTA pruning methods in more difficult datasets. Actually, performance comparison is not done for the CIRAR10-VGG16 setting.\n- The asymptotic distribution can be seen as a corollary of existing researches for MMD and HSIC.\n\nFor these reasons, I was not completely convinced with the effectiveness of the proposed method.\n\n\nMinor comment:\n- The test statistics is more like HSIC. It would be nice if there were comments on the connection to HSIC.\n\n===\nUpdate: The computational cost for this method seems not so much demanding, and could be applied to large data-set. The  resultant performance also seems useful. I think more convincing comparisons are needed. However, its idea seems interesting and its practicality is ensured to some extent. Thus, I have raised  my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859348536, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Reviewers"], "noninvitees": [], "tcdate": 1570237739868, "tmdate": 1575859348548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review"}}}, {"id": "H1eqVc_oir", "original": null, "number": 11, "cdate": 1573780017577, "ddate": null, "tcdate": 1573780017577, "tmdate": 1573780017577, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HyxFYBuijr", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "Reply to reviewer 4", "comment": "In the above experiment, the random pruning is a naive approach by randomly setting weights to zero and than retrain the network with same initials for nonzero weight. Thus, it has same treatment with our proposed method. The only difference is that we use the proposed importance metric and random pruning does not use any importance metric. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "HyxFYBuijr", "original": null, "number": 10, "cdate": 1573778817158, "ddate": null, "tcdate": 1573778817158, "tmdate": 1573778901551, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HyeKZYvosB", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "Random pruning questsions", "comment": "I was wondering about the Random pruning results you showed above in particular. Was '``Fine-tuning' using random pruning or importance metric. When you found that random pruning couldn't achieve LCR of less than one was that with or without fine tuning? The treatment of the pruned network whether using random pruning or your importance metric should be the same."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "HyeKZYvosB", "original": null, "number": 9, "cdate": 1573775617176, "ddate": null, "tcdate": 1573775617176, "tmdate": 1573775617176, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HkgWHyVijB", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "reply to Reviewer 4", "comment": "Thank you for the clarification. In our retraining step, as shown in step 6 in Algoirthm 1, we set the same initial value for non-zero weights (or filters) and retrain the sparsified DNN. This is different from assigning a new random initial value for non-zero weights. In the updated comparison, we use our proposed importance metric for the fine-tuning. It shows that our retraining set-up has better performance than fine-tuning. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "HJlXboViir", "original": null, "number": 8, "cdate": 1573763835067, "ddate": null, "tcdate": 1573763835067, "tmdate": 1573764299701, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "Byxg4B70KB", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "Reply to Review #3 - 2", "comment": "\n2. \nReviewer: \nThe numerical experiments are conducted in small datasets. How it works in larger datasets such as ImageNet and (more importantly) how it is compared with SOTA pruning methods in more difficult datasets. Actually, performance comparison is not done for the CIRAR10-VGG16 setting. \n\nResponse: \nWe added the comparison results for the CIRAR10-VGG16 setting as follows:\n\n=========================================\n                                            ER% (*)               CR \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nLCR      PCII                         -                          10x\n             Li 17\u2019 [1]                 -                          2.7x\n             Qianghui 18\u2019 [2]   -                          5.8x\n             Babajide 18\u2019 [3]    -                          4.5x\n             Wining Ticket [4]  -                          5x\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMTE     PCII                         6.01 (+1.54)       3x\n             Li 17\u2019 [1]                 6.60 (+0.15)       2.7x\n             Qianghui 18\u2019 [2]   6.67 (-0.60)        5.8x\n             Babajide 18\u2019 [3]     6.33 (-0.13)       4.5x\n             Wining Ticket [4]   6.10 (+0.30)      2x\n             Louizos17\u2019 [5]        8.60 (-1.05)       14x\t\t\n=========================================\n\n* the result in the parenthesis is the relative accuracy change compared to its own baseline, larger is better.\n\n** We reached out to the author of Louizos 2017 [5] about replicating the results. They told us their experiments usedGaussian process HP optimization and took a few weeks to finish. Our approach only took 312.9 mins in this setting.\n\nAgain, due to the limited computational resources, we can obtain now, we are not able to complete the experiments on ImageNet. So far the intermediate results look very promising and the final results will be included in our revision. \n\n\n3.\nReviewer: \nThe asymptotic distribution can be seen as a corollary of existing research for MMD and HSIC.\n\nResponse: \nOur hypothesis testing procedure is novel in the sense that it is based on the probabilistic decomposition of a tensor RKHS. Following this line, our work can be extended to higher-order interactions. The test statistics are derived by the likelihood principle which is linked to the well-established work on minimax optimal nonparametric estimation. It leaves space to extend our work to regularized likelihood functionals by adding roughness penalty to the likelihood function which may improve the power and lead to the minimax optimal test. Section 4.3 reveals that MMD and HSIC are a special case of our likelihood ratio based score test for interactions, and hence the asymptotic distribution of our score test is consistent with the MMD and HSIC. \n\n\n4. \nReviewer: \nThe test statistics are more like HSIC. It would be nice if there were comments on the connection to HSIC. \n\nResponse: \nWe thank the reviewer for pointing out the connection. MMD can be treated as a measure of dependence between a continuous random variable and a discrete random variable. HSIC essentially measures the dependence between two continuous variables.  Neither MMD nor HSIC covers the high-order dependence structures. Our proposed likelihood-based method models high-order dependence through functional decomposition.  Both MMD and HSIC can be deemed as special cases of our proposed framework.  The above discussion will be elaborated in the revised version. \n\n\n[1] Li, Hao, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016).\n[2] Huang, Qiangui, et al. Learning to prune filters in convolutional neural networks. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.\n[3] Ayinde, Babajide O., and Jacek M. Zurada. Building efficient convnets using redundant feature pruning. arXiv preprint arXiv:1802.07653. 2018.\n[4] Liu, Zhuang, et al. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 2018.\n[5] Louizos et al. Bayesian compression for deep Learning. NeurlPS. 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "rygbRLhYoH", "original": null, "number": 3, "cdate": 1573664456971, "ddate": null, "tcdate": 1573664456971, "tmdate": 1573763988788, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "BJlTymjYcH", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "reply to Review #4 - 1", "comment": "We appreciate the reviewer for suggestions and questions. We address the concerns below and updated the manuscript in which we added a mini-batch implementation to efficiently calculate the test statistics for large datasets. \n\n1. \nReviewer: \nThe empirical results could be clearer. It lacks baselines for larger models on Cifar10. Could you compare it with the published results of other algorithms? How does it do on larger networks like Imagenet? Computational efficiency is mentioned but could be examined in greater detail.  How long does it take to run and how is that affected by model size? \n\nResponse: \nWe added the comparison results for the CIRAR10-VGG16 setting as follows:\n\n                                            ER% (*)               CR \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nLCR      PCII                         -                          10x\n             Li 17\u2019 [1]                 -                          2.7x\n             Qianghui 18\u2019 [2]   -                          5.8x\n             Babajide 18\u2019 [3]    -                          4.5x\n             Wining Ticket [4]  -                          5x\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMTE     PCII                         6.01 (+1.54)       3x\n             Li 17\u2019 [1]                 6.60 (+0.15)       2.7x\n             Qianghui 18\u2019 [2]   6.67 (-0.60)        5.8x\n             Babajide 18\u2019 [3]     6.33 (-0.13)       4.5x\n             Wining Ticket [4]   6.10 (+0.30)      2x\n             Louizos17\u2019 [5]        8.60 (-1.05)       14x\t\n\n* the result in the parenthesis is the relative accuracy change compared to its own baseline, larger is better.\n\nThe computational cost is determined by the number of hypotheses and the sample size. The number of hypothesis tests is equal to the number of edges (in fully connected networks) or the number of filters (CNNs). For each test, the computational time is determined by the sample size. Our approach can be implemented in a mini-batch fashion. Basically, we divide data into mini-batches, calculate the test statistics within each mini-batch, and then average the statistics overall mini-batches to produce the test statistics of the whole dataset. The mini-batch size can be smaller than the sub-sampling size and hence reduce the computational cost further. Theoretically, as shown in [6], the type-I error and power are guaranteed when the batch size is greater than the log(n) order for the Gaussian kernel.   A similar technique was used in kernel-based deep learning methods, such as GMMN [7] and MMD-GAN [8]. In practice, the performance of the averaged test statistics based on the mini-batches is comparable with the oracle one calculated by using the full data, when the mini-batch size is carefully chosen (see [6] for details). We report the running time of our method using sub-sampling and mini-batch on a workstation with a 48-core CPU@2.60GHz:\n\n                                        Lenet-300-100     Lenet-5-Caffe     CIFAR10-VGG16\nsub-sampling (mins)            39.6                     26.8                    312.9\nMini-batch  (mins)                29.6                     21.8                    103.9\n\nThe computational time of our mini-batch implementation improves dramatically over the sub-sampling implementation, especially for larger datasets such as CIFAR10. Due to the limitation of computational resources, we are not able to complete the experiments on ImageNet. So far the intermediate results look very promising and the final results will be included in our revision. We claim the computation cost is manageable for larger datasets and is of less concern for the following reasons:\na. Users of model compression techniques (such as ours) only need to compress a model once, which can be conducted on large computer clusters to deal with the computational cost. Then the compressed model can be deployed to many devices with limited memory benefits.\nb.  Our sub-sampling or mini-batch implementations can significantly reduce computational cost.  \nc. The statistical testing procedure can be implemented parallelly based on GPUs to further improve computing efficiency. \n\n\n[1] Li, Hao, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016).\n[2] Huang, Qiangui, et al. Learning to prune filters in convolutional neural networks. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.\n[3] Ayinde, Babajide O., and Jacek M. Zurada. Building efficient convnets using redundant feature pruning. arXiv preprint arXiv:1802.07653. 2018.\n[4] Liu, Zhuang, et al. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 2018.\n[5] Louizos et al. Bayesian compression for deep Learning. NeurlPS. 2017.\n[6] Liu et al. How Many Machines Can We Use in Parallel Computing for Kernel Ridge Regression?, arXiv,  2019.\n[7] Li et al. Generative moment matching networks. NeurlPS. 2015.\n[8] Li et al. MMD GAN: Towards deeper understanding of moment matching network. NeurlPS. 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "rJx68saFjB", "original": null, "number": 4, "cdate": 1573669717281, "ddate": null, "tcdate": 1573669717281, "tmdate": 1573763888025, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "ByefJP6lqH", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "reply to Review #2 - 1", "comment": "We appreciate the reviewer for suggestions and questions. We address the concerns below and updated the manuscript in which we added a mini-batch implementation to efficiently calculate the test statistics for large datasets. \n\n\n1.\nReviewer: \nHowever, the drawback of the approach is the significantly increased computation due to 1) quadratic complexity for the kernel methods; 2) unable to cache computation among different pairs of nodes (for example, gradient-based approach can compute importance for all node connections in one forward-backward pass). This limits the applicability of the approach to more interesting cases of larger models (for example, models that work on ImageNet) where model compression is of more urgent need. As a result, the significance and impact of the approach is also limited\n\nResponse:\nWe thank the reviewer for pointing out the computational concerns in larger datasets. Our approach can be implemented in a mini-batch fashion. Basically, we divide data into mini-batches, calculate the test statistics within each mini-batch, and then average the statistics overall mini-batches to produce the test statistics of the whole dataset. The mini-batch size can be smaller than the sub-sampling size and hence reduce the computational cost further. Theoretically, as shown in [1], the type-I error and power are guaranteed when the batch size is greater than the polynomial order of log(n) for the Gaussian kernel.   A similar technique was used in kernel-based deep learning methods, such as GMMN [2] and MMD-GAN [3]. In practice, the performance of the averaged test statistics based on the mini-batches is comparable with the oracle one calculated by using the full data, when the mini-batch size is carefully chosen (see [1] for details). Details of the implementation can be found in our revised manuscript Section 4.4. We report the running time of our method using sub-sampling and mini-batch on a workstation with a 48-core CPU@2.60GHz:\n\n=============================================================\n                                        Lenet-300-100     Lenet-5-Caffe     CIFAR10-VGG16\nsub-sampling (mins)            39.6                     26.8                    312.9\nMini-batch  (mins)                29.6                     21.8                    103.9\n=============================================================\n\nThe computational time of our mini-batch implementation improves dramatically over the sub-sampling implementation, especially for larger datasets such as CIFAR10. Due to the limitation of computational resources, we are not able to complete the experiments on ImageNet. So far the intermediate results look very promising and the final results will be included in our revision. We claim the computation cost is manageable for larger datasets and is of less concern for the following reasons:\na. Users of model compression techniques (such as ours) only need to compress a model once, which can be conducted on large computer clusters to deal with the computational cost. Then the compressed model can be deployed to many devices with limited memory benefits.\nb.  Our sub-sampling or mini-batch implementations can significantly reduce computational cost.  \nc. The statistical testing procedure can be implemented parallelly based on GPUs to further improve computing efficiency. \n\n\n[1] Liu et al. How Many Machines Can We Use in Parallel Computing for Kernel Ridge Regression?, arXiv,  2019.\n[2] Li et al. Generative moment matching networks. NeurlPS. 2015.\n[3] Li et al. \"MMD GAN: Towards deeper understanding of moment matching network.\" NeurlPS. 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "B1xITaAdir", "original": null, "number": 2, "cdate": 1573608893795, "ddate": null, "tcdate": 1573608893795, "tmdate": 1573763635175, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "Byxg4B70KB", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "Reply to Review #3 - 1", "comment": "We appreciate the reviewer for suggestions and questions. We address the concerns below and add a mini-batch implementation that is very efficient in calculating the test statistics for large datasets. \n\n\n1. \nReviewer: \nAlthough applying the kernel type information measure is an interesting idea, its computational complexity would be large. It is not obvious that it works for large datasets such as ImageNet even if the sub-sampling technique is applied. \n\nResponse: \nWe thank the reviewer for pointing out the computational concerns in larger datasets. Our approach can be implemented in a mini-batch fashion. Basically, we divide data into mini-batches, calculate the test statistics within each mini-batch, and then average the statistics overall mini-batches to produce the test statistics of the whole dataset. The mini-batch size can be smaller than the sub-sampling size and hence reduce the computational cost further. Theoretically, as shown in [1], the type-I error and power are guaranteed when the batch size is greater than the polynomial order of log(n) for the Gaussian kernel.   A similar technique was used in kernel-based deep learning methods, such as GMMN [2] and MMD-GAN [3]. In practice, the performance of the averaged test statistics based on the mini-batches is comparable with the oracle one calculated by using the full data, when the mini-batch size is carefully chosen (see [1] for details). Details of the implementation can be found in our revised manuscript Section 4.4. We report the running time of our method using sub-sampling and mini-batch on a workstation with a 48-core CPU@2.60GHz:\n\n=============================================================\n                                        Lenet-300-100     Lenet-5-Caffe     CIFAR10-VGG16\nsub-sampling (mins)            39.6                     26.8                    312.9\nMini-batch  (mins)                29.6                     21.8                    103.9\n=============================================================\n\nThe computational time of our mini-batch implementation improves dramatically over the sub-sampling implementation, especially for larger datasets such as CIFAR10. Due to the limitation of computational resources, we are not able to complete the experiments on ImageNet. So far the intermediate results look very promising and the final results will be included in our revision. We claim the computation cost is manageable for larger datasets and is of less concern for the following reasons:\na. Users of model compression techniques (such as ours) only need to compress a model once, which can be conducted on large computer clusters to deal with the computational cost. Then the compressed model can be deployed to many devices with limited memory benefits.\nb.  Our sub-sampling or mini-batch implementations can significantly reduce computational cost.  \n\nc. The statistical testing procedure can be implemented parallelly based on GPUs to further improve computing efficiency. \n\n\n[1] Liu et al. How Many Machines Can We Use in Parallel Computing for Kernel Ridge Regression?, arXiv,  2019.\n[2] Li et al. Generative moment matching networks. NeurlPS. 2015.\n[3] Li et al. \"MMD GAN: Towards deeper understanding of moment matching network.\" NeurlPS. 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "BygxUKXssS", "original": null, "number": 5, "cdate": 1573759303816, "ddate": null, "tcdate": 1573759303816, "tmdate": 1573763490645, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "BJlTymjYcH", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "reply to Review #4 - 2", "comment": "\n2.\nReviewer: \nThe experimental setting is somewhat unclear. The baseline Louizos et al. (2017)  was designed to optimize group sparsity/speed, but the experimental results here only examine the compression rate. Was the baseline run to optimize speed or sparsity? \n\nResponse: \nWe reached out to the author of Louizos 2017 [1] about replicating the results, they told us their experiments are achieved by Gaussian process HP optimization running for a few weeks.\n\n\n3.\nReviewer: \nIt would be interesting to examine the correlation of importance score with actual impact on network performance. This might be done with a comparison with random pruning or pruning higher importance connections. It might be useful to examine the performance of the networks after pruning nodes of differing importance without full retraining or just fine-tuning. It is unclear how important full retraining is in this method.\n\nResponse:\nWe thank the reviewer for the suggestions. We compare the performance of pruning with fine-tuning, random pruning and our proposed approach with respect to the LCR as follows:\n\n=====================================================\n                                         Lenet-300-100          Lenet-5-Caffe\t\t\nRandom pruning          1x                                1x\t\nFine-tuning                    11x                              17x\nRetraining-pruning       26x                             38x\n=====================================================\n\nThe results show that random pruning can not achieve lossless compression. Fine-tuning has a lower LCR compared with our proposed retraining-pruning method. An intuitive explanation is that, in our proposed method, the retraining step keeps the same initial weights for the pruned network which gives more freedom for the algorithm to adapt to higher accuracy.\n\n\n4.\nReviewer: \nIt would be interesting to visualize the importance of features at different depths in the deep convolutional networks. \n\nResponse:\nWe have a visualization of the connections between the input and the first hidden layer to interpretation the contribution of input features in our manuscript; however, due to the unidentifiable issue of the hidden layers, the connections between the hidden layers at different depths are hard to interpret. \n\n\n5.\nReviewer: Minor suggestions: In the introduction, you mention l0 and l1 norm methods, but cite Han et al. (2015) which compares l1 and l2 norm and found l2 norm to be better overall. \n \nResponse:\nWe thank the reviewer for pointing out the l2 norm approach. We revised our paper to add reviews of the l2 norm-based approach.\n\n\n[1] Louizos et al. Bayesian compression for deep Learning. NeurlPS. 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "B1xSzo7ojS", "original": null, "number": 6, "cdate": 1573759756532, "ddate": null, "tcdate": 1573759756532, "tmdate": 1573762884088, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "ByefJP6lqH", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "reply to Review #2 - 2", "comment": "\n2.\nReviewer: \nThe experiment results are interesting in that it shows the proposed approach can achieve better compression rates given the same test error tolerance on a few small datasets. However, as mentioned above, these experiments are less convincing than more complicated models on larger datasets, such as ImageNet where model compression has greater impact. In addition, the test errors on smaller datasets are easier to achieve, sometimes tuning the optimization settings such as learning rates can result in significant improvement. Therefore, such results are more like preliminary.\n\nResponse:\nWe added the comparison results for the CIRAR10-VGG16 setting. We include a comparison with \n\n=========================================\n                                            ER% (*)                  CR      \n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nLCR      PCII                         -                             10x\n             Li 17\u2019 [1]                 -                             2.7x\n             Qianghui 18\u2019 [2]   -                             5.8x\n             Babajide 18\u2019 [3]    -                             4.5x\n             Wining Ticket [4]  -                             5x\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMTE     PCII                         6.01 (+1.54)          3x\n             Li 17\u2019 [1]                 6.60 (+0.15)          2.7x\n             Qianghui 18\u2019 [2]   6.67 (-0.60)           5.8x\n             Babajide 18\u2019 [3]     6.33 (-0.13)          4.5x\n             Wining Ticket [4]   6.10 (+0.30)         2x\n             Louizos17\u2019 [5]        8.60 (-1.05)          14x\t\n=========================================\n\n* the result in the parenthesis is the relative accuracy change compared to its own baseline, larger is better.\n\n** We reached out to the author of Louizos 2017 [5] about replicating the results. They told us their experiments usedGaussian process HP optimization and took a few weeks to finish. Our approach only took 312.9 mins in this setting.\n\nAgain, due to the limited computational resources, we can obtain now, we are not able to complete the experiments on ImageNet. So far the intermediate results look very promising and the final results will be included in our revision. \n\n\n3.\nReviewer:\nThe paper is general clear and well written. The experiment section should include more important information such as what kernel bandwidth is used for Gaussian kernel, which greatly affects performance. Also, the main text introducing the proposed statistical test is a bit verbose, and dense in unnecessary notations. I think it can be made more succinct by presenting a high level idea (removing mean, three way correlations) first and then the final results. Some intermediate results can be put into Appendix.\n\nResponse: \nWe revised the section 4 to simplify the notations. Tuning kernel bandwidth optimally still remains an open problem. In this work, we set as e kernel bandwidth is set at the median distance between 10 percent of randomly selected points.\n\n\n[1] Li, Hao, et al. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710 (2016).\n[2] Huang, Qiangui, et al. Learning to prune filters in convolutional neural networks. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2018.\n[3] Ayinde, Babajide O., and Jacek M. Zurada. Building efficient convnets using redundant feature pruning. arXiv preprint arXiv:1802.07653. 2018.\n[4] Liu, Zhuang, et al. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 2018.\n[5] Louizos et al. Bayesian compression for deep Learning. NeurlPS. 2017.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "HkgWHyVijB", "original": null, "number": 7, "cdate": 1573760825077, "ddate": null, "tcdate": 1573760825077, "tmdate": 1573760973551, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "BygxUKXssS", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment", "content": {"title": "Questions about the random pruning experiments", "comment": "I had some clarifying questions about the random pruning experiments. Were the fine-tuning results with the random pruning? For a fair comparison, the treatment of the pruned network whether using random pruning or your importance metric should be the same. Did you try pruning with your method without retraining?\n\nIt seems like the most drastic results should be shown with fine-tuning of an importance pruned and a randomly pruned network"}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJgCF0VFwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1267/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1267/Authors|ICLR.cc/2020/Conference/Paper1267/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158642, "tmdate": 1576860552529, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Authors", "ICLR.cc/2020/Conference/Paper1267/Reviewers", "ICLR.cc/2020/Conference/Paper1267/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Comment"}}}, {"id": "ByefJP6lqH", "original": null, "number": 2, "cdate": 1572030169944, "ddate": null, "tcdate": 1572030169944, "tmdate": 1572972491083, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new way of evaluating the importance of neural connections which can be used for better model compression. The approach uses a non-parametric statistical test to detect the three way interaction among the two nodes and final output. Some small scale experiments show that the approach achieves better compression rate given the same test error.\n\nThe approach seems interesting in the sense that, unlike existing techniques, it explicitly measures the three way interaction among the two nodes and the output. Also, it removes the average effects and only considers (non-linear) correlation after removing the mean. The explicit link to the final output and removing average effects allows the method to remove more weights without decreasing the loss by much.\n\nHowever, the drawback of the approach is the significantly increased computation due to 1) quadratic complexity for the kernel methods; 2) unable to cache computation among different pairs of nodes (for example, gradient-based approach can compute importance for all node connections in one forward-backward pass). This limits the applicability of the approach to more interesting cases of larger models (for example, models that work on ImageNet) where model compression is of more urgent need. As a result, the significance and impact of the approach is also limited.\n\nThe experiment results are interesting in that it shows the proposed approach can achieve better compression rates given the same test error tolerance on a few small datasets. However, as mentioned above, these experiments are less convincing than more complicated models on larger datasets, such as ImageNet where model compression has greater impact. In addition, the test errors on smaller datasets are easier to achieve, sometimes tuning the optimization settings such as learning rates can result in significant improvement. Therefore, such results are more like preliminary.\n\nThe paper is general clear and well written. The experiment section should include more important information such as what kernel bandwidth is used for Gaussian kernel, which greatly affects performance. Also, the main text introducing the proposed statistical test is a bit verbose, and dense in unnecessary notations. I think it can be made more succinct by presenting a high level idea (removing mean, three way correlations) first and then the final results. Some intermediate results can be put into Appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859348536, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Reviewers"], "noninvitees": [], "tcdate": 1570237739868, "tmdate": 1575859348548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review"}}}, {"id": "BJlTymjYcH", "original": null, "number": 3, "cdate": 1572610789248, "ddate": null, "tcdate": 1572610789248, "tmdate": 1572972491039, "tddate": null, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "invitation": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper introduces a nonparametric score test to estimate the importance of network connections on the final output of Deep Neural Networks. They derive this by modeling the log-transformed joint density of each connection and final output in a tensor product reproducing kernel Hilbert space. They finally derive an asymptotic distribution of the proposed test statistics which only depends on the eigenvalues of the kernel. This importance test is applied to ranking the importance of each connection in Multilayer Perceptron Networks, and Convolutional Networks and sparsifying the networks by removing the least important connections. The method is applied post-training to fully trained networks but evaluated by retraining the sparsified network from scratch. The method is demonstrated in experiments on compressing three networks.\u00a0\n\nI believe this paper is a borderline accept. It provides a more statistically principled method to examine the importance of connections in a Neural Network and rank them compared to existing compression methods. Due to this ranking, there is a clear method to easily adjust the target compression rate. They are able to achieve high lossless compression rates. However, the benefits shown in the empirical results could be more convincing. It lacks baselines on more complex networks and could benefit from more empirical analysis of the theoretical benefits and properties of this approach.\n\nPros: \nThe method is able to maintain accuracy while achieving high compression rates on MNIST and CIFAR10. It does better than the baselines compared to the small networks. They show a capability for increasing generalizability by decreasing error rates.\u00a0\n\nIt provides a well derived statistically principled method to examine the importance of connections and rank them.\n\nIndicates an additional use to visualize the importance of features.\n\nCons:\nThe empirical results could be clearer. It lacks baselines for larger models on Cifar10. Could you compare it with the published results of other algorithms? How does it do on larger networks like Imagenet?\nComputational efficiency is mentioned but could be examined in greater detail. \u00a0How long does it take to run and how is that affected by model size?\n\nThe experimental setting is somewhat unclear. The baseline Louizos et al. (2017)\u00a0 was designed to optimize group sparsity/speed, but the experimental results here only examine the compression rate. Was the baseline run to optimize speed or sparsity?\n\nIt would be interesting to examine the correlation of importance score with actual impact on network performance. This might be done with a comparison with random pruning or pruning higher importance connections.\u00a0It might be useful to examine the performance of the networks after pruning nodes of differing importance without full retraining or just fine-tuning. It is unclear how important full retraining is in this method.\n\nIt would be interesting to visualize the importance of features at different depths in the deep convolutional networks.\n\nMinor suggestions: In the introduction, you mention l0 and l1 norm methods, but cite Han et al. (2015) which compares l1 and l2 norm and found l2 norm to be better overall.\n\ntypos:\nIn Abstract: nonparemtric scoring test\u00a0 ->\u00a0nonparametric scoring test\u00a0\u00a0\nIn 4.4:\u00a0sample averarge ->\u00a0sample average\nIn 4.5: ASYMPTOTICALLY DISTRIBUTION\u00a0 ->\u00a0 \u00a0ASYMPTOTIC DISTRIBUTION\u00a0"}, "signatures": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1267/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xin_xing@fas.harvard.edu", "longsha@brandeis.edu", "hongpeng@brandeis.edu", "zuofeng.shang@njit.edu", "jliu@stat.harvard.edu"], "title": "Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks", "authors": ["Xin Xing", "Long Sha", "Pengyu Hong", "Zuofeng Shang", "Jun S. Liu"], "pdf": "/pdf/0b4d4acde853a70c2593060c34e9c3d57e83700f.pdf", "abstract": "Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels.  We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques", "keywords": [], "paperhash": "xing|probabilistic_connection_importance_inference_and_lossless_compression_of_deep_neural_networks", "_bibtex": "@inproceedings{\nXing2020Probabilistic,\ntitle={Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks},\nauthor={Xin Xing and Long Sha and Pengyu Hong and Zuofeng Shang and Jun S. Liu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgCF0VFwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1fa5e83278ec3558d7208ba74708754f8756610b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgCF0VFwr", "replyto": "HJgCF0VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1267/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859348536, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1267/Reviewers"], "noninvitees": [], "tcdate": 1570237739868, "tmdate": 1575859348548, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1267/-/Official_Review"}}}], "count": 15}