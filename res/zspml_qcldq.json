{"notes": [{"id": "zspml_qcldq", "original": "4kniFUtLWR_", "number": 506, "cdate": 1601308063204, "ddate": null, "tcdate": 1601308063204, "tmdate": 1614985708541, "tddate": null, "forum": "zspml_qcldq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "9kYo8wadbeg", "original": null, "number": 1, "cdate": 1610040435052, "ddate": null, "tcdate": 1610040435052, "tmdate": 1610474035576, "tddate": null, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper discusses the problem of how to augment cross-modal retrieval for the task of multi-modal classification -- it uses image caption pairs to improve downstream multimodal learning, and shows improvement in the task of visual question answering. However, the paper has the following weaknesses: (a) lack of novelty, (b) lack of thorough empirical evaluation, (c) the complex model did not give significant gains."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "zspml_qcldq", "replyto": "zspml_qcldq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040435038, "tmdate": 1610474035558, "id": "ICLR.cc/2021/Conference/Paper506/-/Decision"}}}, {"id": "g4BxevFMd0g", "original": null, "number": 5, "cdate": 1605912411257, "ddate": null, "tcdate": 1605912411257, "tmdate": 1605912786893, "tddate": null, "forum": "zspml_qcldq", "replyto": "pHTBGZTbs1t", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "Thank you for the detailed review.\n\nWe kindly point the reviewer to our \u201cgeneral response\u201d for detailed explanation of major concerns.\n\n### Concern 1+2 - DXR performance\n\n1) **Q:** The reviewer is concerned with the performance of our retriever, with respect to other baselines.\n\n   **A:** As we mention in the related work section, there are two types of retrievers. Type (i), such as our DXR, enjoy an $O(log{N})$ time-complexity for a single query, assuming a knowledge source of size $N$. Resulting in applicable methods for the downstream tasks. Type (ii) methods, which perform better on the cross-modal retrieval task, suffer from high - $O(N)$ time-complexity for a single query, which is not applicable for the proposed integration with a downstream task. We will make sure to clearly state the differences.\n\n1) **Q:** The reviewer is concerned with the different inputs in the training pipeline. Where for some methods, detection features are used for the VQA model, and this is similar to retriever models such as TERAN. \u201c What is the significance to design such a much more complex model for image-caption retrieval? What are the advantages of the proposed method comparing prior superior methods?\u201d\n\n   **A:** We choose to use the raw image as the input to our retriever following recent advancements in VQA methods such as Movie+MCAN, where the input to their model is the raw image as well. Therefore, our method does not enforce additional complexity in that sense.\nRegarding methods such as TERAN - these are methods that suffer from high computational-complexity for retrieval, and are not applicable for the downstream task as we stated above.\nRegarding the complexity of our retriever (DXR) with respect to TERAN - we could not find any significant difference in the complexity of the models. We kindly ask the reviewer to elaborate on this subject.\n\n### Concern  - VQA performance\n\n1) **Q:** \u201cTable 3 does not give us a throughout comparison. There are many results missed in the table\u201d\n\n   **A:** For fair comparison, we follow common training procedures, meaning, Visual BERT and ViLBERT use COCO and CC for pre-training, while Movie+MCAN does not perform any pre-training. Our method improves the given baselines, while not changing their training scheme. We believe this point is in our favor, where we try to remain agnostic to the model we try to improve. Indeed, as models perform better, the gap in performance decreases, but as can be seen, our method introduces additional capabilities which we hope to be applied in future work such as interpretability and verification of models."}, "signatures": ["ICLR.cc/2021/Conference/Paper506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "zspml_qcldq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper506/Authors|ICLR.cc/2021/Conference/Paper506/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870231, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment"}}}, {"id": "HWiQK20LXmN", "original": null, "number": 4, "cdate": 1605911945672, "ddate": null, "tcdate": 1605911945672, "tmdate": 1605912724385, "tddate": null, "forum": "zspml_qcldq", "replyto": "5RDxihUqx8W", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Thank you for the detailed review.\n\nWe kindly point the reviewer to our \u201cgeneral response\u201d for detailed explanation of major concerns.\n\n### Concern 1 - DXR performance\n\n1) **Q:** The reviewer points to additional methods that achieve better performance than DXR, and is concerned about the method performance.\n\n   **A:** We state in the related work section, that we distinguish between two different types of retrievers. The major difference is in the computational-complexity, where methods such as pointed by the reviewer require the computation of all pairwise similarities for a novel sample, which render these methods inapplicable to the downstream task. For fair comparison, we compare with similar methods to ours. We will make sure to add missing methods to the relevant tables.\n\n\n2) **Q:** \u201chow different cross-modal pre-trained models perform as the DXR is also an interesting direction for exploration.\u201d\n\n   **A:** We thank the reviewer for mentioning DXR as an interesting direction. As DXR is shown to be significantly better than similar methods, a degraded retriever for the downstream task can be seen as using a knowledge source with domain shift w.r.t. the VQA dataset. Such an example can be seen when using CC and Flickr as knowledge sources, where performance is lower.\n\n### Concern 2+3 - XTRA performance\n\n1) **Q:** The reviewer is concerned with the performance of the proposed XTRA method, and points to recent methods that achieve better performance.\n\n   **A:** We apologize for missing Oscar and UNITER. Truly, they perform better than what we present in the paper. We will retract the claim for SOTA.\nOur method shows an improvement over the method proposed by the winners of the VQA 2.0 2020 challenge. We will make sure to properly address our contribution, where we show improvement given three different baselines."}, "signatures": ["ICLR.cc/2021/Conference/Paper506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "zspml_qcldq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper506/Authors|ICLR.cc/2021/Conference/Paper506/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870231, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment"}}}, {"id": "ZLOdzIfHauO", "original": null, "number": 3, "cdate": 1605911584066, "ddate": null, "tcdate": 1605911584066, "tmdate": 1605912649317, "tddate": null, "forum": "zspml_qcldq", "replyto": "3RkRIx0Vv60", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "Thank you for pointing out your concerns so clearly.\n\nWe kindly point the reviewer to our \u201cgeneral response\u201d for detailed explanation of major concerns.\n\n### Cross-Modal retrieval & Time-Complexity:\n\n1) **Q:** The reviewer is concerned about the paper\u2019s novelty and comparisons to other works. Additional methods are pointed by the reviewer to achieve higher performance. The reviewer is also concerned about the time complexity of the method.\n   \n   **A:** While methods proposed by the reviewer achieve better performance, they are not applicable to the downstream task, due to their high computational-complexity in finding the most similar sample in the knowledge source. These methods need to compute the entire pairwise similarity to an input sample. Our method on the other hand, computes each embedding independently, and performs a fask KNN search to find the most similar sample. Thus, for fair comparison, we explicitly distinguish between the different methods\n\n2) **Time-Complexity analysis** - Assuming we use a knowledge-source of size $N$, and applying a forward-pass takes $O(1)$. The following are the time-complexity for each retrieval type during inference, for a single sample query:\n   * **Type (i)** - We use \u201cHierarchical Navigable Small World\u201d search, which has time-complexity of $O(AD[\\log N]v)$ (Johnson et al., 2017), where $A$ and $v$ are constants, and $D$ is the degree of the graph. Therefore, the total time complexity of retrieving is $O(AD[\\log N]v)$.\n    * **Type (ii)** - Because these methods must compute pairwise similarities, the total time complexity of retrieving is $O(N)$\n\n### VQA:\n\nWe would like to ask the reviewer for more clarifications regarding the noted concerns. To the best of our understanding, these are the concerns pointed by the reviewer:\n1) **Q:** The reviewer is concerned about the performance of pre-training with \u201cconceptual captions\u201d, which they points out as hurting the performance of visual Bert and ViLBERT\n\n   **A:** We assume the reviewer points to Tab.3, where pre-training on CC is lower than our method. Otherwise, we see that pre-training on \u201cconceptual captions\u201d is actually better than Vanilla, and for ViLBERT is also better than pre-training on COCO\n\n2) **Q:** Does Cross-Modal Retrieval pre-training necessary for VQA?\n\n   **A:** Our method does not apply pre-training on cross modal retrieval. It trains a retriever on a knowledge source, and uses it to retrieve additional inputs, as we describe throughout the paper.\nWe also note that using two pre-trained unimodals (such as ResNet and BERT) to extract embeddings does not work, because the embedding space was not trained to align the two modalities. We will clarify this in the text\n\n3) Regarding the gap in performance between COCO Flickr and CC - It has been shown by [a],  that CC dataset differ in both visual and textual domain from the VQA task, while COCO has the best domain match in both. Flickr30K datasets, on the other hand, is very similar to COCO, but suffers from a short number of samples in an order of magnitude compared to COCO. We will clarify this in the text\n\n[a] Singh et. al. \u201cAre we pretraining it right? Digging deeper into visio-linguistic pretraining\u201d, 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "zspml_qcldq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper506/Authors|ICLR.cc/2021/Conference/Paper506/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870231, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment"}}}, {"id": "BzzwWT6MBC", "original": null, "number": 2, "cdate": 1605910269529, "ddate": null, "tcdate": 1605910269529, "tmdate": 1605910833612, "tddate": null, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment", "content": {"title": "Clarifications of the proposed method", "comment": "We thank the reviewers for their constructive comments. In hindsight, we think that some aspects of our proposed method were not explained as well as they should have been, causing some confusion. In this general response we address comments that are shared across the reviewers:\n\n### Cross-Modal retrieval : Comparison to previous work and time-complexity\n\n* The Cross-modal retriever is trained on each knowledge source, and later used for retrieval in the downstream task, and is not learned during the training of the downstream task. Therefore, we do not pre-train on the cross-modal alignment task, as in Oscar[1] for example. The downstream tasks model are trained from scratch using the proposed method.\n* **In the paper we explicitly distinguish between two types of methods** (\u201cRelated Work\u201d - \u201cCross-Modal Retrieval\u201d): (i) methods that use grid-features and/or vector representations of the embedding space, and (ii) methods that use detection features, sequence representations, or share information between the two modalities for computing the similarity metric.\n* The reason we distinguish between the two types of retrieval methods is due to Time-Complexity and applicability to be integrated in the downstream task: \n  * **Methods of type (i)** , such as ours have the following properties:\n    1) Each modality is computed independently of the other modalities\n    2) For each dataset we compute the embedding only once\n    3) We use MISP to perform fask KNN, and retrieve data from the knowledge source. For that, we only compute the embeddings of a new sample, and the run-time is about $O(\\log N)$, where $N$ is the knowledge source size\n  * **Methods of type (ii)**, such as proposed by the reviewers (Oscar[1], UNITER[2]) suffer from:\n    1) They suffer from an entangled computation of modals similarities, e.g. they cannot compute an independent embedding\n    2) In order to retrieve samples, one needs to compute pairwise similarities between a query sample, and all samples in the dataset, which results in an $O(N)$ run-time, where $N$ is the knowledge source size.\n    3) Though such models enjoy high performance of the evaluation benchmarks, they are not applicable to the proposed downstream task, because of their inefficiency\n    4) Methods suggested by reviewers will be properly addressed and add to the paper\n* We will add a section concerning these differences, and discussing the time complexity of each type\n\n### XTRA performance\n\n* We apologize for missing the comparison to Oscar[1] and UNITER[2]. Indeed, their performance on test-std is better than what we report in the paper. We note that these are both recent papers (ECCV happened 3 months ago), but still. We will retract the claim for SOTA, and properly address those works\n* As our method is modular and can be applied within any given model, we chose to experiment with the method proposed by the winners of the VQA 2.0 2020 challenge, and show improvement over their performance when bounding training to COCO (no additional VQA data is used for training)\n* Our contribution is the training scheme, which involves the retriever and an external knowledge source. We will make sure to properly address the contribution and performance with respect to previous work, where we show an improvement over the proposed baselines\n\n[1] Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks, ECCV 2020\n[2] UNITER: UNiversal Image-TExt Representation Learning, ECCV 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper506/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "zspml_qcldq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper506/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper506/Authors|ICLR.cc/2021/Conference/Paper506/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870231, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Comment"}}}, {"id": "pHTBGZTbs1t", "original": null, "number": 1, "cdate": 1603823857972, "ddate": null, "tcdate": 1603823857972, "tmdate": 1605024673376, "tddate": null, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Review", "content": {"title": "The advantages and significance of the proposed method are unclear and confusing.", "review": "In this paper, the authors present a method to use unstructured external knowledge sources to improve visual question answering and image-caption retrieval. The proposed method can achieve somewhat improvement for visual question answering, but drop the performance for image-caption retrieval with a more complex model. Some concerns are as follows:\n\n1. The authors claim that the proposed method achieved state-of-the-art performance on both COCO and Flickr30k  image-caption retrieval. However, their retrieval scores are lower about 10 than the state-of-the-art counterparts, such as TERAN. The statement is not correct.\n2. Although the authors stated the proposed method uses raw images as input, the adopted backbones (i.e., image/text encoders) should be frozen to extract the features for the following components in their pipeline, which is similar to the other feature-based methods (e.g., TERAN) that also can be seen as freezing their backbones (e.g., Faster R-CNN) during their training and inference stages. Thus, the inputs between the proposed method and other methods have no essential difference. What is the significance to design such a much more complex model for image-caption retrieval? What are the advantages of the proposed method comparing prior superior methods? I am confused that if it is worthy to adopt such a complex model with worse performance.\n3. It is interesting to see that the proposed method could improve the performance of VQA. However, Table 3 does not give us a throughout comparison. There are many results missed in the table, such as different training types for Flickr30K, some results for Movie+MCAN, etc. From the results, we also could draw that the improvement of the proposed method is very limited for a good VQA method, i.e., Movie+MCAN with Vanilla. The experiments could not significantly demonstrate the significance and advantages of the proposed method.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper506/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141615, "tmdate": 1606915782372, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper506/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Review"}}}, {"id": "5RDxihUqx8W", "original": null, "number": 2, "cdate": 1603953313470, "ddate": null, "tcdate": 1603953313470, "tmdate": 1605024673302, "tddate": null, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Review", "content": {"title": "Review", "review": "This paper explores a new direction, to utilize the searched results (image caption pair) to improve downstream multimodal learning tasks. They first pre-trained a cross-modal model using the contrastive learning on the image caption dataset. Then they use the pre-trained model to search the relevant terms for image or text input, and augment the searched results as the input for the downstream multi-modal tasks.\n\nConcerns:\n\n1. The authors claim that the trained alignment model, DXR, \"achieves state-of-the-art image-caption retrieval performance\" on COCO and MIRFlickr. However, plenty of cross-modal retrieval methods achieve better performance than DXR. For example, Oscar [a] and Unicoder-VL [b] could achieve significantly better than DXR on both COCO and MIRFlickr. In my mind, this could not be regarded as a novel contribution. Also, how different cross-modal pre-trained models perform as the DXR is also an interesting direction for exploration.\n\n2. The authors claim that XTRA \"achieved state-of-the-art performance\", which is also not convincing. Oscar [a] and UNITER [c] achieve significantly better than XTRA on COCO VQA val set. Please carefully survey the literature and claim the contribution.\n\n3. The visual-linguistic pre-training methods [a,b,c] also aim to distill the external knowledge from the large-scale image caption dataset to the downstream tasks, but the solution is to provide a pre-trained model as the initialization which is pre-trained on the large scale image-caption dataset. These methods should also be surveyed in the literature, and may be compared in the experiment part.\n\n[a] Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks, ECCV 2020\n\n[b] Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training, AAAI 2020\n\n[c] UNITER: UNiversal Image-TExt Representation Learning, ECCV 2020", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper506/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141615, "tmdate": 1606915782372, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper506/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Review"}}}, {"id": "3RkRIx0Vv60", "original": null, "number": 3, "cdate": 1604319047961, "ddate": null, "tcdate": 1604319047961, "tmdate": 1605024673239, "tddate": null, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "invitation": "ICLR.cc/2021/Conference/Paper506/-/Official_Review", "content": {"title": "Interesting paper, but the proposed method is not novel enough and problems in experiment comparison. ", "review": "This paper proposed a cross-modal retrieval augmentation for the multi-modal classification task (VQA). The authors first introduce a transformer-based image caption retrieval architecture that achieves decent performance. Then, the authors proposed to use the retrieval model to retrieve relevant visual and textual information as augmentation. The proposed method experiment on 3 existing method (Visual Bert, ViLBERT, and Movie + MCAN) and show good improvements over the baseline model. \n\nMy major concern about this paper is the lack of novelty and experiment comparison. The proposed image caption retrieval architecture is not novel at all. Most existing method (ViLBERT, UNITER, VLBERT etc.) has a similar transformer objective, while the image fine-tuning are from Pixel Bert. In the experiment section (Table 2), the author even didn't compare these methods. \n\nIn terms of the VQA performance, pre-training on the conceptual caption actually hurt the performance of visual Bert and ViLBERT. Could the authors explain why a larger dataset can not help with the model? Is Cross-Modal Retrieval pre-training necessary for VQA?  \n\nIn terms of the speed, the deep fusion model for image retrieval is super slow, since the model need to calculate the score for each pair. What is the size of pool when computing the retrieved captions? What is the time complexity? \n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper506/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper506/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cross-Modal Retrieval Augmentation for Multi-Modal Classification", "authorids": ["~Shir_Gur1", "~Natalia_Neverova1", "~Chris_Stauffer2", "~Ser-Nam_Lim3", "~Douwe_Kiela1", "~Austin_Reiter3"], "authors": ["Shir Gur", "Natalia Neverova", "Chris Stauffer", "Ser-Nam Lim", "Douwe Kiela", "Austin Reiter"], "keywords": ["Multi-Modal", "VQA", "Retrieval"], "abstract": "Recent advances in using retrieval components over external knowledge sources have shown impressive results for a variety of downstream tasks in natural language processing. Here, we explore the use of unstructured external knowledge sources of images and their corresponding captions for improving visual question answering (VQA). First, we train a novel alignment model for embedding images and captions in the same space, which achieves state-of-the-art image-caption retrieval performance w.r.t. similar methods.\nSecond, we show that retrieval-augmented multi-modal transformers using the trained alignment model\nsignificantly improve results on VQA over strong baselines.\nWe further conduct extensive experiments to establish the promise of this approach, and examine novel applications for inference time such as hot-swapping indices.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gur|crossmodal_retrieval_augmentation_for_multimodal_classification", "pdf": "/pdf/81da21846e7fa5586bc34982e8a31e85cb0ee0d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=P0CmuT0-l7", "_bibtex": "@misc{\ngur2021crossmodal,\ntitle={Cross-Modal Retrieval Augmentation for Multi-Modal Classification},\nauthor={Shir Gur and Natalia Neverova and Chris Stauffer and Ser-Nam Lim and Douwe Kiela and Austin Reiter},\nyear={2021},\nurl={https://openreview.net/forum?id=zspml_qcldq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "zspml_qcldq", "replyto": "zspml_qcldq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper506/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141615, "tmdate": 1606915782372, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper506/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper506/-/Official_Review"}}}], "count": 9}