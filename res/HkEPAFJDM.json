{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124436585, "tcdate": 1518472796350, "number": 349, "cdate": 1518472796350, "id": "HkEPAFJDM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HkEPAFJDM", "signatures": ["~Richard_Chen1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Evolved Policy Gradients", "abstract": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss, parametrized via temporal convolutions over the agent's experience, enables fast task learning and eliminates the need for reward shaping at test time. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "paperhash": "houthooft|evolved_policy_gradients", "keywords": ["meta-learning"], "_bibtex": "@misc{\n  houthooft2018evolved,\n  title={Evolved Policy Gradients},\n  author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkEPAFJDM}\n}", "authorids": ["rein.houthooft@gmail.com", "richardchen100@gmail.com", "phillip@openai.com", "bstadie@berkeley.edu", "filip@openai.com", "hojonathanho@gmail.com", "pabbeel@cs.berkeley.edu"], "authors": ["Rein Houthooft", "Richard Y. Chen", "Phillip Isola", "Bradly C. Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "TL;DR": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "pdf": "/pdf/384c44ce5d4a45366e4dd918b27081270297fb2c.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582951340, "tcdate": 1520198648104, "number": 1, "cdate": 1520198648104, "id": "BJeb4k9_G", "invitation": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "signatures": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer3"], "content": {"title": "Approach lacks sufficient motivation, justification, and explanation", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes to evolve a differentiable loss function in gradient-based reinforcement learning algorithms. Specifically, the authors consider a scenario where the learner is able to choose among a parametrized family of loss functions, and they propose an epoch-based algorithm where the learner optimizes the agent's policy according to a specific loss function within each epoch and updates the loss function between epochs. \n\nThe paper presents an interesting approach for gradient-based reinforcement learning algorithms. The general algorithm seems for the most part natural and broad enough in scope to be of interest most of the reinforcement learning community.\n\nAt the same time, I found many important details of the presentation to be unclear and the major results to be insufficiently motivated. In particular, the authors do not explain why evolving a loss function may be a better approach than using environment rewards. Moreover, the parametric form of the loss function seems like a crucial aspect of this type of framework, and yet it is never discussed, even in the experiments. Moreover, the authors also do not touch upon evolution of the loss function and/or the policy gradients. The final results on their own are not very convincing or informative about how and why the method works. \n\nSpecific comments and questions:\n1) tau not defined on in equation (1) and onwards. It's presumably a trajectory? \n2) \"The final episodic return R\\tau at evaluation cannot be represented as an explicit function of the loss function L_\\phi\". I'm not sure why this is true if L_\\phi takes \\tau as input. \n3)  Equation (4) isn't motivated.\n4) The form of the R_i is presented without motivation.\n5) \"The loss function does not observe the environment rewards directly.\" Why? It seems like they are available, and they are in fact used later on in the forward-backward Hopper environment. \n5) The precise form of L_\\phi isn't presented in the experiments. \n6) There should at least be a sentence or two discussing related work and the novelty of this paper. For instance, [Reward Design via Online Gradient Ascent - Sorg, Lewis, and Singh] seems to present a similar idea with much better justification.  \n\nMinor comments:\n1) Given a loss functions -> Given a loss function\n\nOverall, I found this paper to be unclear in its novelty and significance and lacking in its motivation and justification. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evolved Policy Gradients", "abstract": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss, parametrized via temporal convolutions over the agent's experience, enables fast task learning and eliminates the need for reward shaping at test time. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "paperhash": "houthooft|evolved_policy_gradients", "keywords": ["meta-learning"], "_bibtex": "@misc{\n  houthooft2018evolved,\n  title={Evolved Policy Gradients},\n  author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkEPAFJDM}\n}", "authorids": ["rein.houthooft@gmail.com", "richardchen100@gmail.com", "phillip@openai.com", "bstadie@berkeley.edu", "filip@openai.com", "hojonathanho@gmail.com", "pabbeel@cs.berkeley.edu"], "authors": ["Rein Houthooft", "Richard Y. Chen", "Phillip Isola", "Bradly C. Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "TL;DR": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "pdf": "/pdf/384c44ce5d4a45366e4dd918b27081270297fb2c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582951118, "id": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer2"], "reply": {"forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582951118}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582690437, "tcdate": 1520715023516, "number": 2, "cdate": 1520715023516, "id": "HkvfraZFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "signatures": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer1"], "content": {"title": "An interesting idea with some promising results - lacking some clarity/context but you can forgive this in the workshop setting.", "rating": "7: Good paper, accept", "review": "Evolved Policy Gradients\n=====================\n\nThis paper proposes a meta-learning approach that \"evolves\"=\"Evolution Strategies\" for shaping rewards to be used with a policy gradient algorithm.\nThis algorithm operates over a distribution of environments and attempts to learn shaping rewards that are good across the distribution.\n\nThere are several things to like about the paper:\n- The idea is appealing, with good motivation / intuition.\n- The practicalities of how to implement this algorithm are well explained.\n- There are experimental results that demonstrate a clear benefit versus standard policy gradient baselines.\n\nHowever, there are also several shortcomings:\n- The connection/relation to other evolutionary/metaRL strategies is not well-motivated or explained... how should we think of this versus MAML, versus RL^2, versus... the authors are no doubt aware of this work but we don't see a discussion or comparison.\n- The connection of \"evolutionary strategies\" to policy gradient should be made more clear... at some level ES just performs policy gradient without gradients (finite difference)... could this whole algorithm then be simplified as just \"do policy gradient on a meta-level\"? If not, then why not?\n- Some of the writing is rushed and confusing:\n\"The loss function does not observe the environment rewards directly. However, in cases where the reward cannot be fully inferred from the environment, such as the forward-backward random Hopper in Section 3, we augment the inputs to the loss function with reward.\"\nSimilarly \"In practice to bootstrap the learning process\"... why is \"in practice\" different to what has been described up to this point?\n\nOverall I think the paper will be valuable to the ICLR workshop so I vote accept.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evolved Policy Gradients", "abstract": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss, parametrized via temporal convolutions over the agent's experience, enables fast task learning and eliminates the need for reward shaping at test time. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "paperhash": "houthooft|evolved_policy_gradients", "keywords": ["meta-learning"], "_bibtex": "@misc{\n  houthooft2018evolved,\n  title={Evolved Policy Gradients},\n  author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkEPAFJDM}\n}", "authorids": ["rein.houthooft@gmail.com", "richardchen100@gmail.com", "phillip@openai.com", "bstadie@berkeley.edu", "filip@openai.com", "hojonathanho@gmail.com", "pabbeel@cs.berkeley.edu"], "authors": ["Rein Houthooft", "Richard Y. Chen", "Phillip Isola", "Bradly C. Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "TL;DR": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "pdf": "/pdf/384c44ce5d4a45366e4dd918b27081270297fb2c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582951118, "id": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer2"], "reply": {"forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582951118}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582656902, "tcdate": 1520776972649, "number": 3, "cdate": 1520776972649, "id": "B1Szvnftf", "invitation": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "signatures": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer2"], "content": {"title": "Trying to overlook the actual submission", "rating": "6: Marginally above acceptance threshold", "review": "Before jumping in, if this were a review only of the submitted paper it would lean heavily towards rejection. There are glaring omissions of crucial details, unclear writing, and a complete lack of any sort of discussion of related work or effort to put this work into a context. Even considering page limitations, it is hard to take anything useful away from the actual submission. However, these complaints are addressed by the version on Arxiv, and so instead of wasting time nitpicking on things already fixed, consider this a review for the Arxiv version with an assumption that you could pair-down that version to something of similar quality within page limits.\n\nThe authors use a simple randomized hillclimbing algorithm, evolutionary strategies (ES) previously applied to policy optimization, to optimize the parameters of a convolutional network computing a loss function used to train a reinforcement learning agent by differentiating through the loss into the agent's policy (one of the inputs to the network). After optimizing the loss parameters, an agent minimizing it out-performs an agent using PPO. The method is demonstrated on two randomized control tasks, Hopper and Reacher, in which parameters of the environment are sampled randomly for each trial (not each episode).\n\nIt is important to emphasize that, during evaluation, the agent trained using the evolved loss does not see the true rewards, whereas the PPO agent does. This means that, for all the emphasis on 'loss functions' this is quite possibly better explained by reward optimization, which would also suggest reasons for not widening the domain distribution.\n\nThe comparison with PPO cannot truly serve as an evaluation of the method because PPO is being trained on one task and this larger system is being trained on many (although I am unsure of how 3000 epochs compares with the test-time training). If the argument was that the learned loss function is general across RL domains, then the comparison would be fair, but obviously the domains used would need to be expanded with only a single loss learned for all. Along these lines, does the Hopper loss work for the Reacher or vice versa? If you merge the two, so that each instance could come from either, does learning a loss for 'both' work? These, I think, would be much more interesting. \n\nAlthough a bit incremental there is a definite insight to be drawn from this work, as well as some novelty in the exact form the meta-learning takes in this instance. The chief lesson I would take from this is that we can exchange human thought for parallel computation, and vice versa. The method is extremely simple, not even evolutionary in any meaningful way, and is able to produce a method that learns significantly faster than PPO. It may also hint at the promise of future combinations of evolutionary methods and RL. As mentioned, I think more understanding into how well the loss generalizes across domains would be very useful here. The effective number of possible Hopper and Reacher variations is likely not *that* large, so depending on the amount of training this could have overfit to them pretty heavily.\n\nPros: Simple, makes an interesting point. Combining 'evolutionary' methods and RL together is a promising direction, and this does so effectively without complicating either optimization.\n\nCons: Reading the submitted version was a waste of time, only consider reading the Arxiv version. Various small hacks included to make it work ('final returns' not final returns, annealing in the PPO loss with the optimized loss). Not clear on the amount of training / compute used, or the dimensionality of the parameters optimized, which we know to be a fairly important interaction with success of these types of methods.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evolved Policy Gradients", "abstract": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss, parametrized via temporal convolutions over the agent's experience, enables fast task learning and eliminates the need for reward shaping at test time. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "paperhash": "houthooft|evolved_policy_gradients", "keywords": ["meta-learning"], "_bibtex": "@misc{\n  houthooft2018evolved,\n  title={Evolved Policy Gradients},\n  author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkEPAFJDM}\n}", "authorids": ["rein.houthooft@gmail.com", "richardchen100@gmail.com", "phillip@openai.com", "bstadie@berkeley.edu", "filip@openai.com", "hojonathanho@gmail.com", "pabbeel@cs.berkeley.edu"], "authors": ["Rein Houthooft", "Richard Y. Chen", "Phillip Isola", "Bradly C. Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "TL;DR": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "pdf": "/pdf/384c44ce5d4a45366e4dd918b27081270297fb2c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582951118, "id": "ICLR.cc/2018/Workshop/-/Paper349/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper349/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper349/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper349/AnonReviewer2"], "reply": {"forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper349/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582951118}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573577074, "tcdate": 1521573577074, "number": 144, "cdate": 1521573576743, "id": "HkbA0RAFf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HkEPAFJDM", "replyto": "HkEPAFJDM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper has not been accepted for presentation at the ICLR workshop. While the full version of the paper is interesting, out of fairness we can only consider the 3 page versions, which for this submission was not clear enough. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evolved Policy Gradients", "abstract": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss, parametrized via temporal convolutions over the agent's experience, enables fast task learning and eliminates the need for reward shaping at test time. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "paperhash": "houthooft|evolved_policy_gradients", "keywords": ["meta-learning"], "_bibtex": "@misc{\n  houthooft2018evolved,\n  title={Evolved Policy Gradients},\n  author={Rein Houthooft and Richard Y. Chen and Phillip Isola and Bradly C. Stadie and Filip Wolski and Jonathan Ho and Pieter Abbeel},\n  year={2018},\n  url={https://openreview.net/forum?id=HkEPAFJDM}\n}", "authorids": ["rein.houthooft@gmail.com", "richardchen100@gmail.com", "phillip@openai.com", "bstadie@berkeley.edu", "filip@openai.com", "hojonathanho@gmail.com", "pabbeel@cs.berkeley.edu"], "authors": ["Rein Houthooft", "Richard Y. Chen", "Phillip Isola", "Bradly C. Stadie", "Filip Wolski", "Jonathan Ho", "Pieter Abbeel"], "TL;DR": "We propose a meta-learning approach for learning gradient-based reinforcement learning (RL) algorithms. Empirical results show that our evolved policy gradient algorithm achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method.", "pdf": "/pdf/384c44ce5d4a45366e4dd918b27081270297fb2c.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}