{"notes": [{"id": "q_eWErV46er", "original": "BhuCNivLFG", "number": 15, "cdate": 1615310251067, "ddate": null, "tcdate": 1615310251067, "tmdate": 1615313020558, "tddate": null, "forum": "q_eWErV46er", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Learning One Representation to Optimize All Rewards", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper15/Authors"], "authors": ["Anonymous"], "keywords": ["reward-free MDP", "successor states", "self-supervision"], "TL;DR": "We introduce a learnable \"summary\" of a reward-free MDP, from which near-optimal policies can be obtained for any reward function specified a posteriori, instantaneously without planning. ", "abstract": "We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It  provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning.\nThe unsupervised FB loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function.  With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches.\nThis is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based Ms. Pacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.", "pdf": "/pdf/6d26e07a256ef49834485d33f0b66393f7599b48.pdf", "paperhash": "anonymous|learning_one_representation_to_optimize_all_rewards", "_bibtex": "@inproceedings{\nanonymous2021learning,\ntitle={Learning One Representation to Optimize All Rewards},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=q_eWErV46er},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}