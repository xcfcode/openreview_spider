{"notes": [{"id": "TBIzh9b5eaz", "original": "4g32ikEub_I", "number": 1257, "cdate": 1601308140737, "ddate": null, "tcdate": 1601308140737, "tmdate": 1612950742945, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "JAITdEPnGG", "original": null, "number": 1, "cdate": 1610040411642, "ddate": null, "tcdate": 1610040411642, "tmdate": 1610474009094, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes O-RAAC, an offline RL algorithm that minimizes the Conditional Value-at-Risk (CVaR) of the learned policy's return given a dataset by a behavior policy. The reviews are generally positive with most agreeing that the paper presents interesting empirical results. \n\nThe experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with more clearer safety settings. \nAgreeably, asking for comparisons with unpublished results may be unfair, however, it would be recommended to authors to include additional comparisons with latest methods in Offline/Batch-RL, including the ones which don't guarantee risk, such as CQL, BRAC, CSC.\n\nFurther, The theoretical properties of the proposed algorithm are largely unclear. It would help to analyze the effect of both convergence rates, and fixed points, further what is the effect of addition of risk, does the algorithm converge to a suboptimal solution or get there slower. Finally empirical reporting of cumulative number of failures (discrete count) during training as well as during evaluation would be very useful to practitioners. \n\nOther relevant and concurrent papers to potentially take note of:\nDistributional Reinforcement Learning for Risk-Sensitive Policies (https://openreview.net/forum?id=19drPzGV691)\nConservative Safety Critics for Exploration (https://openreview.net/forum?id=iaO86DUuKi)\n\nI would recommend acceptance of the paper. I would strong encourage release of sufficiently documented and easy to use implementation.  Given the fact that the main argument is empirical utility of the method, it would be limit the impact of this work if readers cannot readily build on O-RAAC. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040411628, "tmdate": 1610474009078, "id": "ICLR.cc/2021/Conference/Paper1257/-/Decision"}}}, {"id": "MmA8TYFYlyg", "original": null, "number": 12, "cdate": 1606242123734, "ddate": null, "tcdate": 1606242123734, "tmdate": 1606242123734, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "Cp_Tdgp-cuy", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your comments. See answers below.", "comment": "We thank the reviewer for their valuable feedback and comments.\n\nRegarding the Figure 3 in appendix, the D4PG plot is actually overlapped by the WCPG trajectory, since both trajectories are exactly the same. We do agree that it can lead to confusion and we will clarify that in the camera ready version of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "Cp_Tdgp-cuy", "original": null, "number": 11, "cdate": 1606238374999, "ddate": null, "tcdate": 1606238374999, "tmdate": 1606238374999, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "DXSjOGLfS9V", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Comment", "comment": "Thank you for your feedback, some more comments:\n- In Figure 3 in the appendix D4PG plot is missing\n- thank you for adding the results optimizing the other quantile and CPW"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "DXSjOGLfS9V", "original": null, "number": 7, "cdate": 1606206812837, "ddate": null, "tcdate": 1606206812837, "tmdate": 1606206812837, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "ypnmcKP69aq", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your review. See answers below. ", "comment": "We thank the reviewer for their valuable feedback and comments. We answer the points below:\n\n- Performance differences: thanks for catching this, it was a bug in our plotting code. We fixed the training curves in the rebuttal version. \n\n- Different quantiles for CVaR: Yes, we included experiments with $\\alpha=0.25$ in Table 2. \n \n- Different risk-measures: Yes, we included experiments with a different risk measure (cumulative prospect weights) in Table 2. \n\n- Qualitative evaluation: Figure 3 in the appendix is a qualitative demonstration of the risk-averse behavior in a simple 1-d task. Figure 2 in the main paper is also a way of demonstrating this. We added a paragraph in each experiment with a \"Qualitative Evaluation\" title, followed by an appropriate discussion. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "my4HwsJAwdE", "original": null, "number": 6, "cdate": 1606206592754, "ddate": null, "tcdate": 1606206592754, "tmdate": 1606206592754, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "wH1nQMtdrQ3", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your review. See answers below. ", "comment": "We thank the reviewer for their valuable feedback and comments. We answer the points below:\n\n- Risk-neutral experiment: We trained O-RAAC with alpha=0.1, but evaluate using the average return. We clarify this in the rebuttal version (Section 4.3.1). We also added other risk-measures as suggested by other reviewers (Table 2). \n\n- Actor loss nomenclature: We thank the reviewer for this helpful comment. We oversaw this when submitting the paper. The actor loss is the negative of such quantity. We changed this in the rebuttal version (equation 5 and last sentence in first paragraph, as well as in the algorithm)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "A53tkMgj7R-", "original": null, "number": 4, "cdate": 1606206245289, "ddate": null, "tcdate": 1606206245289, "tmdate": 1606206467370, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "a4hTtmA_K4", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your review. See answers below.", "comment": "We thank the reviewer for their valuable feedback and comments. We answer the points below:\n\n- Comparison to SOTA baselines: CQL was accepted to NeurIPS after the ICLR deadline. We include a comparison to BEAR in Table 2. \nNonetheless, BCQ (O-D4PG with a distributional critic) is also a risk-neutral baseline as it is a comparison to a risk-neutral agent. \n\n- Experimental Design: We improved Section 4 and the appendix based on the suggestions of the reviewer.  We also added an ablation on the effect of $\\lambda$ in Appendix A.3.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "rSK_SNCtP5h", "original": null, "number": 5, "cdate": 1606206454153, "ddate": null, "tcdate": 1606206454153, "tmdate": 1606206454153, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "FRIPFWbdRH9", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your review. See answers below.", "comment": "We thank the reviewer for their valuable feedback and comments. We answer the points below:\n\n- Incremental contribution: we disagree with this claim:  we study a novel and highly relevant setting, and provide a novel method, which builds on and adapts state-of-the-art algorithms for this setting. \n\n- The optimal CVaR policy for Eq.(1) can be non-stationary: Although there are non-stationary policies that are optimal, also there exists a stationary policy that is optimal for many risk measures (see \"Andrzej Ruszczy\u0301n .Risk-averse dynamic programming for Markov decision processes. Mathematical programming, 2010\", Theorem 4). The CVaR is one of such risk measures. We agree that in the finite horizon setting, the optimal policy is non-stationary. We clarify this in the rebuttal version. \n\n- Convergence: As in most deep RL algorithms, the theory lacks behind. We did not claim any theoretical guarantees but believe that the empirical results are promising. \n\n- Hyperparameter selection: Indeed this is an interesting open problem for offline RL. We selected the default hyperparameters for BCQ. Whether these were originally chosen based on the true environment is unknown to us. \nWe also provide an ablation experiment on the effect of $\\lambda$ in Appendix A.3. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "wJYz4B7igmP", "original": null, "number": 3, "cdate": 1606206122662, "ddate": null, "tcdate": 1606206122662, "tmdate": 1606206152421, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "h1NTfrC4XAB", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment", "content": {"title": "Thank you for your review. See answers below. ", "comment": "We thank the reviewer for their valuable feedback and comments. We answer the points below:\n\n1- $d^\\beta$ is the distribution induced by the behavior policy (and $\\beta$ quantities). We access it by sampling from the batch. We clarified this in the rebuttal version (first paragraph from section 2). \n2- $\\mathcal{U}(\\cdot)) is the uniform distribution. We clarified this in the rebuttal version (above equation 3). \n3- deterministic vs stochastic policies: Thanks for this comment. We prefer to avoid stochastic policies because they are not optimal, they are an extra source of stochasticity, and in the offline setting, they bring no exploration benefits. We nevertheless agree that they help to avoid overfitting and that is what most works use. We clarified this in the rebuttal version (first two paragraphs from section 3.3). \n4- We indicated the performance of the behavioral policy (Mean and CVaR) in the appendix (Table 3). We now include it in the main paper in the rebuttal version, together with uncertainty estimation via bootstrap (Table 2). \nWe also add the performance of the VAE as an imitation learning benchmark. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "TBIzh9b5eaz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1257/Authors|ICLR.cc/2021/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861793, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Comment"}}}, {"id": "ypnmcKP69aq", "original": null, "number": 1, "cdate": 1603897145587, "ddate": null, "tcdate": 1603897145587, "tmdate": 1605024489557, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review", "content": {"title": "An interesting approach that put together state-of-the-art techniques in order to obtain a strong risk-averse offline learner", "review": "## Brief Summary\nThe authors developed an risk-averse RL algorithm with operates in offline contexts. The main idea consists in putting together the benefits from 3 previous works in the distributional, risk-averse and offline RL fields:\n- the DDPG distributional extension (Barth-Maron et al., 2018), called D4PG\n- the Implicit Quantile Network (Dabney et al., 2018), called IQN\n- actor using an imitation learning component with perturbation to control the bootstrappinge error (Fujimoto et al. 2019)\nThe online version of the resulting algorithm, called RAAC, is then tested on a toy problem. \nLater, the full algorithm (O-RAAC) is tested on 3 MuJoCo tasks, using offline data from the the D4RL dataset (Fu et al., 2020).\nThe performance is analysed on both a risk-averse and risk-neutral point of view.\n\n## Strong Points\n- The state-of-the-art is correctly reported.\n- The paper is clear and easy to follow.\n- The experimental analysis is correct, complete and interesting:\n    + baselines seem to be competitive: they are also augmented with the VAE layer\n    + they  the analysis is sound and provides interesting insights.\n- The proposed approach seems to outperform the baselines on the optimized measure: moreover, as noticed in also in previous work, introducing risk-aversion seems to be useful also to increase the expected return. \n\n## Weak Points\n\n- The algorithm is obtained by composing existing approach: it is not clear to me whether there is a novel methodological contribution or not.\n- The authors claim that their approach is general for any risk-measure, however, the experiments are conducted only for the CVaR case and with a specific percentile.\n- The results in the appendix does not seem to match the ones in the main paper.\n\n## Recommendation\nMy reccomendation is to accept the paper since it provides an interesting approach which puts together, in an original way, techniques from different areas of RL, even if it is not clear whether there is a novel methodological contribution. The shown results seems to be relevant, however, it is still not clear whether they are \n\n## Questions for the authors\n- Performances seem to be worse in the supplementary material, can you explay why?\n- Could you provide experiments with different quantiles for CVaR?\n- Could you provide experiments with different risk-measures?\n- Is it possible to evaluate in a qualitative way the behavior obtained by the agents in the learned task when using the risk-aversion or not?\n\n## Additional Feedback\n- It would be useful to state the dependecies of your approach in a clearer way.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122841, "tmdate": 1606915775848, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1257/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review"}}}, {"id": "wH1nQMtdrQ3", "original": null, "number": 2, "cdate": 1603921191004, "ddate": null, "tcdate": 1603921191004, "tmdate": 1605024489479, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review", "content": {"title": "Recommendation to Accept", "review": "##########################################################################\n\nSummary:\n\nThis paper proposes O-RAAC, an offline RL algorithm that minimizes the Conditional Value-at-Risk (CVaR) of the learned policy's return given  a dataset by a behavior policy. It learns a distributional critic, a VAE for imitation learning, and an actor that perturbs the VAE output to minimize the risk given by the distortion operator D (Here it uses CVaR).\n\n\nSeveral experiments were performed to show the effectiveness of O-RAAC, both with a simple 1-d driving environment and with a modified version of D4RL dataset.\n\n\n \n##########################################################################\nPros: \n\nThe paper is well written, with comparisons with competing methods throughout. This makes the connection with those methods clear and easy to understand. The discussion around the design choices and the tradeoffs are especially insightful. (e.g. Huber loss over l1 or l2; VAE vs vanilla BC; etc.)\n\nThe contribution to risk averse offline reinforcement learning is novel, with the use of CVaR and the idea of a perturbed version of a imitation learning actor. With the assumption that all the risks is captured within the reward. \n\n\n##########################################################################\n\nSuggestions:\n\nThe risk-neutral experiment in 4.3 was a bit confusing. I was not sure whether you measured the O-RAAC\u2019s risk-neutral performance by setting alpha = 0, or you measured its performance with alpha=0.1, but using a risk-neutral metric. I think you did the latter after a few more reading. More clarification either in Table 2 or in 4.3 would be great. \n\nin 3.3, you said \"...is a perturbation model that is optimized maximizing the actor loss (5),\" This part was confusing as well, since one would normally minimize a loss. In this paper, actor loss = risk aversion, so maximizing risk aversion makes sense, but I wouldn't call it a loss. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122841, "tmdate": 1606915775848, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1257/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review"}}}, {"id": "FRIPFWbdRH9", "original": null, "number": 3, "cdate": 1603986751127, "ddate": null, "tcdate": 1603986751127, "tmdate": 1605024489412, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review", "content": {"title": "Encouraging empirical results. Unclear theoretical properties, hard to implement in new tasks.", "review": "The authors propose an RL algorithm for learning risk-averse policies from offline data. Empirically, it is shown that it can outperform some existing risk-neutral approaches on a number of challenging robotic control tasks under risk-sensitive performance measures. Although the empirical results are encouraging, the theoretical properties of the proposed algorithm are unclear and therefore it is not clear how easy it can be implemented in other tasks.\n\nOverall, the paper is easy to read and the presentation is clear.\n\nThe authors address a very important issue that is faced by RL practitioners. Learning risk-averse policies in a fully offline setting is inherently ill-posed and the burden is in incorporating sufficiently strong prior and bias in regularizing the learning system. Unfortunately the current paper falls short in this regard, where the only idea here is to use imitation learning in the form of autoencoders. So the main contribution in terms of new idea is rather incremental.\n\nFurthermore, the theoretical properties of the proposed algorithm are largely unclear. For example, with respect to a fixed distribution for the start state $S_1$, the optimal CVaR policy for Eq.(1) can be non-stationary. The authors restrict the policy search to the space of stationary deterministic policies. Now suppose that the neural network is over-parameterized, and assuming arbitrarily large training set, which policy does the algorithm converge to? Does the algorithm converge?\n\nConsidering the 3 gradient updates in Algorithm 1, one wonders how to choose all those parameters to make it work, and whether a single step-size parameter is enough. In particular, $\\lambda$ seems hard to choose. How sensitive are the empirical results with respect to the choice of these parameters? In the truly offline case, one presumably can only perform fine-tuning using a separate validation set, how should this be done here?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122841, "tmdate": 1606915775848, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1257/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review"}}}, {"id": "a4hTtmA_K4", "original": null, "number": 4, "cdate": 1604372538874, "ddate": null, "tcdate": 1604372538874, "tmdate": 1605024489348, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review", "content": {"title": "Study the problem of safety in offline RL, and propose a novel algorithm for learning risk-averse policy", "review": "Summary\nThe paper studies the problem of safe reinforcement learning, where we want to learn risk-averse policies in the offline setting. It proposes \u201cOffline Risk Averse Actor Critic\u201d (ORAAC) which performs competitively as risk-neural agent, and outperforms D4PG based baseline as a risk-averse agent on D4RL benchmark. The algorithm involves modifying the losses to learn risk-averse actor, distributional critic and a VAE-based imitative policy.\n\n\nReasons for the score:\nI vote in favour of accepting the paper. The paper studies safety aspects of learning algorithms which are competitive as a risk-averse agent in offlineRL. I would strongly encourage the authors to include numbers for baselines like CQL/BEAR. Though not risk-averse, these algorithms are conservative by design and are strong baselines in offlineRL.\n\n\nStrengths:\n+ The problem is well motivated and the ideas are presented clearly. The authors perform extensive experiments, ablations and provide empirical evidence where ORAAC outperform baselines like OD4PG on D4RL benchmark tasks.\n+ Though risk-averse by design, the learned policy is competitive as a risk-neutral agent.\n\nWeaknesses:\n- Comparison to SOTA baselines in the risk-neutral scenario are missing. Though CQL/BEAR are not risk-averse my design, these algorithms are competitive in the risk-neutral setting. It would be critical to know how ORAAC performs in comparison to such baselines.\n- Discussion on the choice of hyperparameters and experiment design are under-explored. Elaborating on this in section 4 would significantly improve the readability of the paper.\n\n[1] CQL : https://arxiv.org/abs/2006.04779\n[2] BEAR : https://arxiv.org/abs/1906.00949\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122841, "tmdate": 1606915775848, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1257/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review"}}}, {"id": "h1NTfrC4XAB", "original": null, "number": 5, "cdate": 1604695327946, "ddate": null, "tcdate": 1604695327946, "tmdate": 1605024489282, "tddate": null, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "invitation": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review", "content": {"title": "Official Blind Review", "review": "The submission investigates the risk averse objective in offline RL. Usually, the parametric uncertainty is the main source of worries in offline RL, and dealing with the stochastic uncertainty on top of it, in order to account for risk aversion, is very challenging. The authors expose clearly their method and algorithm, even though we sometimes would have liked a bit more argumentation on why this and why not that. Since no theoretical analysis is provided, the only validation is empirical. It is rather complete regarding both the settings and the domains. The results are quite impressive, in particular in the offline setting. For all these reasons, I recommend to accept the submission.\n\nPlease answer/address these comments in the rebuttal/final version:\n* 1- It is unclear whether d^\\beta is defined as the empirical distribution in the batch, or the true distribution. I believe that it would have been helpful in general to formalize more the distributions definitions.\n* 2- U(.) is not introduced.\n* 3- First sentence of 3.2: It is frequent in offline RL to use stochastic policies in order to leverage the risk taken in the face of parametric uncertainty. It is therefore rather odd to state here that only deterministic policies are considered. Even more when we notice in 3.3 that the actual policy is stochastic, since b is sampled from the stochastic behavioral policy.\n* 4- Please indicate the performance of the behavioural policy (average and cvar) in the experiments.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1257/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1257/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Risk-Averse Offline Reinforcement Learning", "authorids": ["~N\u00faria_Armengol_Urp\u00ed1", "~Sebastian_Curi1", "~Andreas_Krause1"], "authors": ["N\u00faria Armengol Urp\u00ed", "Sebastian Curi", "Andreas Krause"], "keywords": ["offline", "reinforcement learning", "risk-averse", "risk sensitive", "robust", "safety", "safe"], "abstract": "Training Reinforcement Learning (RL) agents in high-stakes applications might be too prohibitive due to the risk associated to exploration. Thus, the agent can only use data previously collected by safe policies. While previous work considers optimizing the average performance using offline data, we focus on optimizing a risk-averse criteria, namely the CVaR. In particular, we present the Offline Risk-Averse Actor-Critic (O-RAAC), a model-free RL algorithm that is able to learn risk-averse policies in a fully offline setting. We show that O-RAAC learns policies with higher CVaR than risk-neutral approaches in different robot control tasks. Furthermore, considering risk-averse criteria guarantees distributional robustness of the average performance with respect to particular distribution shifts. We demonstrate empirically that in the presence of natural distribution-shifts, O-RAAC learns policies with good average performance. \n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "urp\u00ed|riskaverse_offline_reinforcement_learning", "one-sentence_summary": "We propose the first risk-averse reinforcement learning algorithm in the fully offline setting. ", "supplementary_material": "/attachment/b7bf79c087b9f9edfb39a001e48f90236729c498.zip", "pdf": "/pdf/be66503efdccf10359c7112b8c3732b28564db16.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nurp{\\'\\i}2021riskaverse,\ntitle={Risk-Averse Offline Reinforcement Learning},\nauthor={N{\\'u}ria Armengol Urp{\\'\\i} and Sebastian Curi and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=TBIzh9b5eaz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "TBIzh9b5eaz", "replyto": "TBIzh9b5eaz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1257/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538122841, "tmdate": 1606915775848, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1257/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1257/-/Official_Review"}}}], "count": 14}