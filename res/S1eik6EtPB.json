{"notes": [{"id": "S1eik6EtPB", "original": "BJlzB6qHvH", "number": 315, "cdate": 1569438947467, "ddate": null, "tcdate": 1569438947467, "tmdate": 1577168287556, "tddate": null, "forum": "S1eik6EtPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QwpN3Lfjxm", "original": null, "number": 1, "cdate": 1576798693097, "ddate": null, "tcdate": 1576798693097, "tmdate": 1576800942308, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Decision", "content": {"decision": "Reject", "comment": "This submission studies an interesting problem. However, as some of the reviewers point out, the novelty of the proposed contributions is fairly limited.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716361, "tmdate": 1576800266486, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper315/-/Decision"}}}, {"id": "BkexHiNTqH", "original": null, "number": 3, "cdate": 1572846392469, "ddate": null, "tcdate": 1572846392469, "tmdate": 1574207496723, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper studies how a min-max framework can incorporate different tasks related to adversarial robustness. Specifically, the authors study adversarial attacks against model ensembles, universal perturbations, and attacks constrained by the union of Lp norms. They propose optimizing a probability distribution over \"domains\" (models in an ensemble, inputs, Lp balls; respectively per task) and regularizing it to be close to uniform. They perform experiments to evaluate their method.\n\nFrom a conceptual point of view, I did not find the contribution of the paper significant. All of the tasks discussed are direct application of the min-max framework and have been studied to a certain extent in prior work (https://arxiv.org/abs/1811.11304, https://arxiv.org/abs/1706.04701, https://arxiv.org/abs/1904.13000). The novel tools introduced are the regularizer on simplex probability and the attack diversity regularizer. However, the theoretical justification for these tools is rather weak and their utility would need to be demonstrated empirically.\n\nFrom an experimental point of view, the baselines considered are very weak. At a high level, the authors compare their version of min-max optimization to a very simple average-case optimization. In order to demonstrate the utility of the tools introduced the authors would need to at least compare to a reasonable min-max baseline. For instance, a very simple heuristic capping the loss of each domain in the finite-sum formulation (https://arxiv.org/abs/1811.11304) would be the bare minimum. In their current state, the experiments only demonstrate that a min-max approach outperforms an average case approach, which is fully expected. At the same time, the diversity regularizer does seem to offer some empirical gains and I would encourage the authors to investigate further.\n\nOverall, the conceptual and experimental contributions of the paper are rather weak and I thus recommend rejection.\n\n=========================\nUPDATE: I appreciate the authors' response and additional experimental results. \n\nI am still quite concerned about the universal perturbation baseline. I suspect that the clipping factor used might be too large since clipping barely has any impact (the attack is still focusing too much on B ignoring C). Conceptually, clipping should be quite similar to a min-max formulation. \n\nI do see that the proposed method outperforms the one proposed in Shafahi et al in terms of universal adversarial training. I feel like this is a more reasonable baseline and I am increasing my score to a 3.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779526210, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper315/Reviewers"], "noninvitees": [], "tcdate": 1570237753910, "tmdate": 1575779526226, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Review"}}}, {"id": "r1eAJd-qiH", "original": null, "number": 10, "cdate": 1573685221553, "ddate": null, "tcdate": 1573685221553, "tmdate": 1573699235278, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Revised manuscript has been uploaded", "comment": "We would like to thank the reviewers again for their thoughtful reviews and valuable comments. We have made our best efforts to improve the manuscript according to the comments. The key changes are listed as follows: \n\n- clarifying the novelty and contributions\n- adding heuristic weighting schemes baselines (Appendix D2, Table A4)\n- comparing proposed generalized AT with another min-max baseline - universal adversarial training (Shafahi et al., 2018) (Appendix E2, Table A7)\n- conducting a sensitivity analysis of quadratic regularizer on probability simplex (Appendix D4, Figure A1)\n- improving the presentation of the paper, adding a table of contents in Appendix for ease of associating our main sections with supplementary details\n\nAny further discussions are highly welcome and appreciated!\n\nReference:\n[1] Universal Adversarial Training. Shafahi et al., 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "S1xhnPzvsB", "original": null, "number": 6, "cdate": 1573492660454, "ddate": null, "tcdate": 1573492660454, "tmdate": 1573496801947, "tddate": null, "forum": "S1eik6EtPB", "replyto": "r1xkrwMPoB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 2/3)", "comment": "Q4: Questions on  universal perturbations.\nA4:  First, the universal perturbation is generated by solving the problem (6) using APGD, where we set $\\gamma = 4$. It is worth noting that as $\\gamma$ approaches 0 and $\\infty$, problem (6) reduces the maximum and the averaging strategy for generating universal perturbation.\n\nSecond, our goal is not to propose a new threat model for an adversary being interested in group-level success. Instead, we consider group-level success since it is a complementary metric to average-evasion success:  A high average-evasion success rate does not imply a high group-level success and vice versa. We would like to show that as K increases, attacking all of the images in an entire group becomes much more difficult. As a result, the proposed min-max attack can achieve better group-level success but possibly at the cost of degraded average-evasion success. As K is small, the min-max strategy outperforms the averaging strategy under both group-level success and average-evasion success (e.g., K = 2 for MNIST, and K = 2 or 4 for CIFAR). In the revision, we will make our motivation on group-level success and our comparison with average-evasion success clearer.\n\nThird, in the example of universal perturbation, we compare the min-max strategy and the averaging strategy by choosing K=2, 4, 5, 10, which are factors that can be divided by the total number of images 10,000. In the example of data transformation, we choose K = 6 data transformations following previous works (Athalye et al., 2018; Athalye et al., 2019;  Brown et al., 2017), i.e., flipping horizontally (flh) or vertically (flv), adjusting brightness (bri), performing gamma correction (gam) and cropping (crop) (see Table 3).\n\nReference:\nSynthesizing robust adversarial examples. Athalye et al., ICLR 2018.\nObfuscated gradients give a false sense of security. Athalye et al., ICLR 2019.\nAdversarial patch. Brown et al., 2017.\n\nQ5: not clear whether the improvements observed for defense are statistically significant (were multiple runs averaged in Table 4, without DPAR, the improvement is negligible)\nA5: The improvements are statistically significant - the experiment is repeated ten times under different random seeds (see Figure A3b for the learning curve). \n\nIn this setting ($\\epsilon_{\\ell_\\infty} = 0.2$, $\\epsilon_{\\ell_2} = 2.0$), the improvement of min-max over average strategy is small because the large overlapping between $\\ell_\\infty$ and $\\ell_2$ balls (see Sec 4.2 and Figure A2). That is also the reason why promoting diversity helps improve robustness further. Figure 3a also demonstrates that the learned weights for the two types of perturbations are very close. Moreover, Figure A3a shows that our proposed min-max scheme results in faster convergence than the averaging scheme due to the benefit of self-adjusted domain weights.\n\nAlso, we do not think that 1% improvement is negligible since it is achieved over multiple runs and different scenarios. Moreover, the robustness improvement of AT under a single perturbation type by the ICML\u201919 work [Table 1, Wang et al. 2019] is also around 1%, although a different network architecture and an AT method were considered in our work. \n\nReference: \nOn the Convergence and Robustness of Adversarial Training. Wang et al., ICML 2019.\n\nQ6: should emphasize the contribution of DPAR (whether a beneficial or required supplement to adversarial training)\nA6: Thanks for the suggestion. DPAR is a beneficial supplement to AT under multiple types of $\\ell_p$ perturbations. In Table 4, we observe that DPAR yields consistent robustness improvement under both Acc$_{\\mathrm{adv}}^{\\mathrm{avg}}$ and  Acc$_{\\mathrm{adv}}^{\\mathrm{max}}$ at different scenarios. We will add more discussions in the revised manuscript.\n\nQ7: Some typos in the paper\nA7: Thanks for the meticulous proofreading. We will correct these typos and further revise our manuscript."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "Sye9PwXvoS", "original": null, "number": 9, "cdate": 1573496674144, "ddate": null, "tcdate": 1573496674144, "tmdate": 1573496674144, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1xhnPzvsB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 3/3) ", "comment": "Detailed comments & questions:\n1 - Adversarial attack is a tautology (an attack is always adversarial)\n\u201cAdversarial attack\u201d is a terminology that is widely used in the community (Huang et al., 2017; Madry et al., 2018; Samangouei et al., 2018). It usually represents well-crafted prediction-evasion attacks during the inference. This is in contrast with the training-phase data poisoning attack (Steinhardt et al., 2017; Shafahi et al., 2018).\n\nReference:\nAdversarial Attacks on Neural Network Policies. Huang et al., ICLR 2017.\nTowards Deep Learning Models Resistant to Adversarial Attacks. Madry et al., ICLR 2018.\nDefense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models. Samangouei et al., ICLR 2018.\nCertified Defenses for Data Poisoning Attacks. Steinhardt et al., NeurIPS 2017.\nPoison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks. Shafahi et al., NeurIPS 2018.\n\n7 - What does \u201crobust\u201d adversarial attack mean?\nThe \u201crobust\u201d refers to the improvement on \u201cworst-case\u201d attack performance over multiple domains. We will try to make this point clearer and more accurate. \n\n7 - What is CAAD-18?\nAs shown in Sec 4.1, CAAD-18 denotes \u201cCompetition on Adversarial Attacks and Defenses 2018\u201d.\n\n7 - Define ASR$_{all}$: what does evade mean here? Is the attack targeted or untargeted?\nASR$_{all}$ refers to the attack success rate (ASR) of fooling model ensembles simultaneously. Our framework is applicable to both untargeted and targeted attacks. In the experiments, we focus on the former setting. Although this metric was defined in the paper, the reviewer\u2019s comment reminds us to have a clearer organization of our experiment details. Thanks!\n\n7 - What is an \u201cadvanced\u201d DNN?\nHere, \u201cadvanced\u201d DNN means more complicated DNNs such as VGG, ResNet, Wide-ResNet, GoogLeNet. We will clarify it in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "r1xkrwMPoB", "original": null, "number": 5, "cdate": 1573492534551, "ddate": null, "tcdate": 1573492534551, "tmdate": 1573496616375, "tddate": null, "forum": "S1eik6EtPB", "replyto": "Hyg4PfI4qB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (Part 1/3) ", "comment": "Thanks for your valuable comments and suggestions.\n\nQ1: The paper exceeds the recommended page limit (so apply higher standard). It is not self-contained in the main body and leaves some important details in the appendix.\nA1: Thank you. The reviewer\u2019s suggestion reminds us to better organize the paper and to make our presentation clearer. For ease of understanding, we will add a table of contents in the Appendix for ease of associating our main sections with supplementary details in Appendix. Moreover, in the main paper (especially in the experiment section), we will make a better organization when the figures/tables/sections in Appendix are cited. \n\nQ2: Sec 4.1 presents results on MNIST, which is known to be a poor dataset to study adversarial examples (https://arxiv.org/abs/1902.06705)\nA2: First, we conducted the same experiments on CIFAR-10 and tested more models (e.g., VGG16, Wide-ResNet, GoogLeNet) in the appendix (Table 3, 4). To make it more clearly, we will move the CIFAR-10 experiment to the main body in the updated paper. Moreover, we obtained consistent attack results on both MNIST and CIFAR-10.\n\nSecond, MNIST is a dataset, which provides images of easy visualization. For example, we use MNIST to visualize the effect of self-adjusted weights $\\mathbf w$ on the attack performance (Table A8 and A9). It is clear to see that the larger domain weights correspond to the MNIST letters with clearer appearance, implying that the learnable weights $\\mathbf w$ could offer visual interpretability of \u201cimage robustness\u201d. \n\nThird, we hesitate to call MNIST a poor dataset for studying adversarial examples. To the best of our knowledge, many works (Madry et al, 2017; Athalye et al., 2019; Tram\u00e8r et al., 2019) considered MNIST as a standard dataset. Even for the seminal work (Carlini et al., 2019), we did not see a clear objection on MNIST to study adversarial examples. A possible relevant point in the provided paper is that one should consider different perturbation radiuses for different datasets; for example, $\\epsilon=0.2$ ($\\ell_\\infty$ attack) used for MNIST becomes too large for CIFAR-10. In our work, we follow the commonly-used setting of the perturbation radius, 0.2 for MNIST, 0.05 or 0.03 for CIFAR-10.\n\nReference:\nTowards Deep Learning Models Resistant to Adversarial Attacks. Madry et al., ICLR 2017.\nObfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. Athalye et al., ICLR 2019.\nAdversarial Training and Robustness for Multiple Perturbations. Tram\u00e8r et al., NeurIPS 2019.\nOn Evaluating Adversarial Robustness. Carlini et al., 2019.\n\nQ3: Questions about ensemble models (better baselines, insights of attacking model ensembles). \nA3: Thanks for the suggestion. \n\nFirst, we would like to highlight that the weights $\\mathbf w$ to combine ensemble models are learnable, which avoids supervised manual adjustment. Thus, our approach does not need prior knowledge on the robustness level of different models from both attacker and defender\u2019s perspectives. \n\nSecond, following the reviewer's suggestion, we conduct additional experiments (Table 1 in https://tinyurl.com/t6hax2m) to show that it is nontrivial to set the heuristic weights beforehand. For example, when an adversary evades models C+D only, the generated adversarial examples have a poor transferability to even less robust models A and B. This implies that having the ensemble attack to learn the adjusted weights by itself not only avoids the issue of heuristic weight selection but also boosts the transferability of attacks to different models. Note that it is actually a common practice to attack ensemble models to promote the transferability to unknown black-box models. For instance, in NIPS 2017, CAAD-2018 competitions, the winner solutions usually integrate multiple adversarially trained models (usually > 5) to enhance the transferability with equal or manually fine-tuned weights. However, we have shown in Table1 that our min-max solution outperforms this averaging strategy. \n\nThird, our approach does not rely on specific choices of the model ensemble. For available public models, the importance weights $\\mathbf w$ are jointly learned, and the resulting results of $\\mathbf w$ could be used as metrics to reveal the robustness of individual models in the ensemble (Figure 1c). "}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "S1lloBMPoB", "original": null, "number": 3, "cdate": 1573492120353, "ddate": null, "tcdate": 1573492120353, "tmdate": 1573495004871, "tddate": null, "forum": "S1eik6EtPB", "replyto": "BkexHiNTqH", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": " Response to AnonReviewer4 (Part 1/2)", "comment": "Thanks for your valuable comments and suggestions.\n\nQ1: Conceptual novelty?\n\nA1: Thanks for your comments. In the revision, we will clarify our conceptual contributions, and elaborate on our differences with the existing works (Shafahi et al., 2018; Tram\u00e8r et al., 2019; He et al., 2017).\n\nWe are sorry to hear that \u201cAll of the tasks discussed are direct applications of the min-max framework and have been studied to a certain extent in prior work\u201d. We strongly believe that our work contains substantial differences to the existing works, and is not a direct application of the min-max framework. \n\nDifferences to  (Shafahi et al., 2018; Tram\u00e8r et al., 2019; He et al., 2017). \nThe prior work (Shafahi et al., 2018)  proposed a min-max based AT by leveraging universal perturbations (rather than per-image perturbation). We highlight two main differences even just at the defense side. First, different from Shafahi et al., we generalize AT subject to mixed types of $\\ell_p$ adversarial perturbations. Figure A2 motivates on why the consideration of multiple perturbation types could matter in AT. Second, our min-max formulation (11) stems from the min-max-max formulation (18), where the last max step is conducted on the importance weights $\\mathbf w$. In Lemma 1, it is our contribution to show the equivalence between (18) and (11). \n\nAlthough the prior work (Tram\u00e8r et al., 2019) considered a modified AT method subject to multiple types of adversarial perturbations, our work is still quite different from it. Note that some differences had been highlighted in Sec. 2 of the original manuscript. We make further clarification as below. First, the incorporation of domain weights $\\mathbf w$, the corresponding regularization on $\\mathbf w$ and the diversity-promoting regularization are new to AT under multiple types of adversarial perturbations. Second, our proposed framework is general, which applies to both attack generation and AT. Even at the defense side, our approach covers the formulations in Tram\u00e8r et al., as $\\gamma$ approaches $0$ and $\\infty$. Third, please note that ours and Tram\u00e8r et al., 2019 are actually independent works.\n\nThe prior work (He et al., 2019) considered an ensemble-based defensive method. We feel that this is less relevant to the min-max AT framework. Our differences to the previous work (Shafahi et al., 2018) hold for He et al., 2019.\n\nA summary of our conceptual contributions. \n\n(Attack) Even if the concept of min-max optimization was used in other works, our formulation on the min-max attack and its specification to ensemble attack, universal perturbation, and attack under physical transformations are new to the field. In Proposition 1, we derived the analytical solution to the projection operator subject to the intersection of $\\ell_p$ norm ($p = 0,1,2,\\infty$) inequality constraint and the box constraint. This is different from the conventional attack design, where the  $\\ell_p$ norm was regularized in the objective function. The derived solution to the projection operator subject to the hard constraints also facilitates the implementation of AT under multiple types of adversarial perturbations (see Sec. 3.2). \n(Defense) The proposed min-max formulation (11) on AT under mixed types of $\\ell_p$ perturbations is not trivial. It actually stems from the min-max-max formulation (18), where the last max step is conducted on the importance weights $\\mathbf w$. In Lemma 1, we show that problem (18) can equivalently be transformed into the proposed min-max problem (11). Moreover, the diversity regularization on multiple perturbation directions is new to AT.\nIn both a) and b), the introduction of self-adjusted weights $\\mathbf w$ and the strongly concave regularization are not trivial. First, the learnable $\\mathbf w$ can adjust the model robustness or attack power automatically during the training. Second, the introduction of strongly concave regularization is useful, which ensures $O(1/T)$ convergence rate for APGD (Theorem 1) and helps the training process in AT under multiple types of $\\ell_p$ perturbations (Figure A3). \n\nReference:\nUniversal Adversarial Training. Shafahi et al., 2018.\nAdversarial Training and Robustness for Multiple Perturbations. Tram\u00e8r et al., NeurIPS 2019.\nAdversarial Example Defenses: Ensembles of Weak Defenses are not Strong. He et al., WOOT 2017"}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "HJgBn_zwsB", "original": null, "number": 8, "cdate": 1573492908962, "ddate": null, "tcdate": 1573492908962, "tmdate": 1573492908962, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "General Response to Reviewers", "comment": "We thank the reviewers for their valuable comments and suggestions. We have made our best efforts to address all reviewers' concerns and questions. Some highlights are provided below: \n\n1) We clarified the novelty of our work and carefully made a distinction between ours and others suggested by reviewers. We would like to respectfully mention that the seemingly 'direct' application of min-max optimization does not mean that our contributions are minor, instead it demonstrates the generalizability and flexibility of the proposed approach in both attack design and robust training. And we have made extensive experiments (the additional experiments suggested from reviewers are quite valuable) to support our conceptual contributions. The detailed response can be found when answering each specific question.\n\n2) We conducted new experiments as suggested by reviewers and the results are available in this anonymous link: https://tinyurl.com/t6hax2m. Specifically, we added 1) stronger heuristic baselines with the prior knowledge of model robustness (Table 1); 2) comparison with universal adversarial training (Shafahi et al., 2018); 3) sensitivity analysis of proposed quadratic regularizer on probability simplex.\n\nWe will update the manuscript soon based on reviewers' feedback. Any further discussions and suggestions are highly welcome and appreciated.\n\nReference:\nUniversal Adversarial Training. Shafahi et al., 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "rJgAWufPjr", "original": null, "number": 7, "cdate": 1573492741912, "ddate": null, "tcdate": 1573492741912, "tmdate": 1573492741912, "tddate": null, "forum": "S1eik6EtPB", "replyto": "rkxIOeMg9B", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "We thank Reviewer #2 very much to recognize our contributions and have positive comments on our paper! We will further update our paper by adding new experiment results (suggested by other reviewers) as well as improving our presentation."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "SJl0HLfPiB", "original": null, "number": 4, "cdate": 1573492293997, "ddate": null, "tcdate": 1573492293997, "tmdate": 1573492293997, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1lloBMPoB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (Part 2/2)", "comment": "Q2: Experimental contributions and baselines?\n\nA2:  We would like to thank the reviewer\u2019s suggestion on baselines and additional empirical studies. \n\nFirst, we summarize the newly conducted experiments (https://tinyurl.com/t6hax2m). \n\nTo demonstrate the usefulness of the self-adjusted domain weights $\\mathbf w$, we compare the performance of the proposed ensemble attack with that of the min-max approach using a heuristic weighting structure as well as a clipping loss strategy used in Shafahi et al., 2018. We show that the use of learnable $\\mathbf w$ avoids supervised manual adjustment on the importance of attack losses, and our approach outperforms the heuristic weighting structure  (Table 1 in https://tinyurl.com/t6hax2m). We also conduct a sensitivity analysis of $\\gamma$ and show how the proposed regularization affects the eventual performance (Figure 2 in https://tinyurl.com/t6hax2m). \n\nWe add Universal Adversarial Training (UAT) (Shafahi et al., 2018.) as one of our defense baselines in Table 2 (https://tinyurl.com/t6hax2m). We find that a) our proposed approach outperforms UAT under per-image $\\ell_p$ attacks. Taking Table 2a as an example, our avg and min-max generalized AT (with DPAR) result in average 17.85% and 17.97% improvement in adversarial test accuracy (ATA), b) our approach has just 3.7% degradation in ATA when encountering universal attacks, and c) both methods yield very similar normal test accuracy. \n\nSecond, we do not think that our previous experimental contributions are weak. We would like to highlight the following contributions. \n\nWe conducted extensive experiments on showing the power of our proposed min-max formulation and method on the design of adversarial attacks (Figures 1, Table 1-3, Table A3-A6). We also provided empirical insights on why the self-adjusted weights $\\mathbf w$ matter (see Figures 1c, 3a, Table A8). For example, the min-max universal perturbation offers interpretability of \u201cimage robustness\u201d by associating domain weights with image visualization:  Larger domain weights correspond to the MNIST letters with clearer appearance; see Table A8 and A9 (Appendix D.3). \nBesides the defensive results that we showed in Sec. 4.2, we conducted additional experiments in the Appendix. For example, in Figure A2, we provide insights on why AT is trained over multiple perturbation types could matter: It leads to a defense consistently strong through different $\\ell_p$ attacks (see Table 4, Figure 3b-3c, Figure A7). In Figure A3, we show that our proposed defensive method leads to a faster convergence during training compared to average-based AT, especially when the strengths of two attacks diverge greatly. \n\nReference:\nUniversal Adversarial Training. Shafahi et al., 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper315/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1eik6EtPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper315/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper315/Authors|ICLR.cc/2020/Conference/Paper315/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173240, "tmdate": 1576860532346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper315/Authors", "ICLR.cc/2020/Conference/Paper315/Reviewers", "ICLR.cc/2020/Conference/Paper315/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Comment"}}}, {"id": "rkxIOeMg9B", "original": null, "number": 1, "cdate": 1571983469598, "ddate": null, "tcdate": 1571983469598, "tmdate": 1572972610860, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a unified framework for adversarial training & robustness. The problem is important and interesting. The proposed framework has solid theory and is well conceived. A generic method is proposed with O(1/T) convergence rate, which is also empirically demonstrated with good performance on often-used MNIST and CIFAR-10 benchmarks. An alternating multi-step PGD is also proposed. Empirical experiments are thorough and well organized. Overall I feel it is a well written paper with sufficient contributions and is of interest to a range of ICLR audience.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779526210, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper315/Reviewers"], "noninvitees": [], "tcdate": 1570237753910, "tmdate": 1575779526226, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Review"}}}, {"id": "Hyg4PfI4qB", "original": null, "number": 2, "cdate": 1572262491994, "ddate": null, "tcdate": 1572262491994, "tmdate": 1572972610818, "tddate": null, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "invitation": "ICLR.cc/2020/Conference/Paper315/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Note: I applied a higher standard to this paper given that it significantly exceeds the recommended page limit. Furthermore, important details are left out in the appendices, which make it difficult to read the main body of the paper in a self-contained fashion. Given that the main body was already over the recommended page limit, I did not read the appendices.\n\nThis paper generalizes the min max formulation of adversarial training, and proposes a formulation that encompasses adversarial training of an ensemble, robustness to universal adversarial examples, and robustness to non-adversarial transformations. This formulation is used to derive an adversarial training procedure that trains against the worst-case adversarial example among adversarial examples generated by a set of attacks. Experiments seek to demonstrate applicability of this framework to both attacks and defenses.\n\nAs far as experiments are concerned, Section 4.1 presents results on MNIST, which is known to be a poor dataset to study adversarial examples on [https://arxiv.org/abs/1902.06705]. If models C and D are more difficult to attack, could better baselines be employed than attacking the ensemble A+B+C+D? For instance, would an adversary evading models C+D only perform better? It is difficult to draw insights that are generally applicable from a single ensemble. How was the ensemble chosen? Why would a defender add models which are known to be significantly less robust to the ensemble?\n\nWhen discussing universal perturbations, how are they generated? Given that the performance of the proposed approach significantly degrades average evasion across all images from all groups, what is the threat model for an adversary being interested in group-level success rather than average evasion across all images from all groups? How were the values of K chosen? This comment also \tapplies to experiments over data transformations. For these experiments, what was the value of K?\n\nAs far as the defensive perspective is concerned, it is not clear whether the improvements observed are statistically significant. Were multiple runs averaged to produce Table 4? Given that without DPAR, the improvement is negligible, this is important to interpret results. It appears that most of the robustness gains in both the average and max settings stem from DPAR. This should be clearly surfaced in the introduction and presentation of contributions if DPAR is required for the proposed generalized min max formulation to improve robustness. In particular, it is not clear whether DPAR is \u201ca beneficial supplement to adversarial training\u201d or a required supplement to adversarial training - per the formulation in this paper.\n\n\nThere are issues with grammar throughout the document, which make it difficult to read. Some specific issues:\n\n1 - Adversarial attack is a tautology (an attack is always adversarial)\n\n7 - What does \u201crobust\u201d adversarial attack mean?\n\n7 - What is CAAD-18?\n\n7 - Define ASR_all: what does evade mean here? Is the attack targeted or untargeted?\n\n7 - What is an \u201cadvanced\u201d DNN?"}, "signatures": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper315/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wangjksjtu@gmail.com", "tzhan120@syr.edu", "sijia.liu@ibm.com", "pin-yu.chen@ibm.com", "coldstudy@sjtu.edu.cn", "makan@syr.edu", "lxbosky@gmail.com"], "title": "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness", "authors": ["Jingkang Wang", "Tianyun Zhang", "Sijia Liu", "Pin-Yu Chen", "Jiacen Xu", "Makan Fardad", "Bo Li"], "pdf": "/pdf/b17d35e4a2a9a4fa2b017871f52cdd45675778c3.pdf", "TL;DR": "A unified min-max optimization framework for adversarial attack and defense", "abstract": "The worst-case training principle that minimizes the maximal adversarial loss, also known as adversarial training (AT), has shown to be a state-of-the-art approach for enhancing adversarial robustness against norm-ball bounded input perturbations. Nonetheless, min-max optimization beyond the purpose of AT has not been rigorously explored in the research of adversarial attack and defense. In particular, given a set of risk sources (domains), minimizing the maximal loss induced from the domain set can be reformulated as a general min-max problem that is different from AT. Examples of this general formulation include attacking model ensembles, devising universal perturbation under multiple inputs or data transformations, and generalized AT over different types of attack models. We show that these problems can be solved under a unified and theoretically principled min-max optimization framework.  We also show that the self-adjusted domain weights learned from our method provides a means to explain the difficulty level of attack and defense over multiple domains. Extensive experiments show that our approach leads to substantial performance improvement over the conventional averaging strategy.", "keywords": ["Ensemble attack", "adversarial training", "diversity promotion"], "paperhash": "wang|towards_a_unified_minmax_framework_for_adversarial_exploration_and_robustness", "original_pdf": "/attachment/62b06ac167de80d8697e124c503998fcb8f7de0d.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness},\nauthor={Jingkang Wang and Tianyun Zhang and Sijia Liu and Pin-Yu Chen and Jiacen Xu and Makan Fardad and Bo Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1eik6EtPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1eik6EtPB", "replyto": "S1eik6EtPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper315/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779526210, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper315/Reviewers"], "noninvitees": [], "tcdate": 1570237753910, "tmdate": 1575779526226, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper315/-/Official_Review"}}}], "count": 13}