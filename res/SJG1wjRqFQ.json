{"notes": [{"id": "SJG1wjRqFQ", "original": "r1eClnNqKQ", "number": 228, "cdate": 1538087767194, "ddate": null, "tcdate": 1538087767194, "tmdate": 1545355391787, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeo6e3gxE", "original": null, "number": 1, "cdate": 1544761539028, "ddate": null, "tcdate": 1544761539028, "tmdate": 1545354519222, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Meta_Review", "content": {"metareview": "This paper introduces a planning phase for NMT.  It first generates a discrete set of tags at decoding time, and then the actual words are generated conditioned on those tags.  The idea in the paper is interesting.\n\nHowever, the paper's experimental settings could improve by comparing on larger datasets and also using stronger baselines.  The writing could also improve -- why were only the few coarse POS tags used?  Have the authors tried a larger set?  I think without such controlled comparisons, it would be hard to understand why only those coarse tags are used.\n\nThe reviewers express concern about some of the above issues and there is consensus that the paper should be improved for acceptance at a venue like ICLR.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Meta Review"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper228/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353291454, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353291454}}}, {"id": "ryg9daF53m", "original": null, "number": 2, "cdate": 1541213553732, "ddate": null, "tcdate": 1541213553732, "tmdate": 1542823738414, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "content": {"title": "Attacking an interesting but mostly solved problem with weak baselines and questionable ML techniques", "review": "The authors consider the problem of generating diverse translations from a neural machine translation model. This is a very interesting problem and indeed, even the best models lack meaningful diversity when generating with beam-search. The method proposed by the authors relies on prefixing the generation with discrete latent codes. While a good general approach, it is not new (exactly the same general approach that was used in the \"Discrete Autoencoders for Sequence Models\" [1] paper, https://arxiv.org/abs/1801.09797, for generating diverse translations, which is not cited directly but a follow-up work is cited, though without mentioning that a previous work has tackled the same problem). Also, the authors rely on additional supervised data (namely POS tags) which has no clear motivation and seems to cause a number of problems -- why not use a purely unsupervised approach when it has already been demonstrated on the same problem? Additionally, the authors compare to a weak translation baseline on small data-sets, making it impossible to judge whether the results would hold on a larger data-set. So the following ablations and comparison to baselines are missing:\n* comparing with a stronger NMT architecture and larger data-set\n* does the chosen discretization method matter? Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order.\n* comparison to fully unsupervised latents and some other system, e.g., the system from [1] above\n\nIn the absence of these comparisons and with little novelty, the paper is a clear reject.\n\n[Revision]\n\nGreatly appreciate the answers provided by the authors. The Ja-En dataset is indeed much larger than I thought, so I increased my score. When the other points are addressed (as the authors say they will do) it may be a good paper -- but the review must stick to the submitted version, not a future one.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "cdate": 1542234509857, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677166, "tmdate": 1552335677166, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HyxXDrx70m", "original": null, "number": 9, "cdate": 1542813018910, "ddate": null, "tcdate": 1542813018910, "tmdate": 1542813018910, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "BylTbuUGAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Needs more work", "comment": "In summary tag planning gives higher BLEU scores, but discrete latents allow controlling diversity?\n\nWhile these results are interesting, it seems to me that there are too many moving parts that needs to be rigorously investigated for the paper to be publishable. I therefore keep my score."}, "signatures": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "BylTbuUGAQ", "original": null, "number": 7, "cdate": 1542772740986, "ddate": null, "tcdate": 1542772740986, "tmdate": 1542772808393, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "BkgXLmTIaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Additional experiment results without teaching forcing on the tags", "comment": "Hi, we conducted additional experiments with a model does not apply teacher forcing on the tags. This means we first train the model to predict tags. Then conditioning on the predicted tags, we  train the model to generate translations.\n\nIn the additional experiments, we use standard transformer (6 encoders, 6 decoders) architecture on the large ASPEC Ja-En dataset (3M sentence pairs). Here are the quantitive results:\n\nModel \t\tBS=3\t\tBS=5\t\tBS=10\nBaseline\t\t25.92\t\t26.02\t\t26.02\ncode_plan\t26.97\t\t27.4\t\t27.39\ntag_plan\t\t27.32\t\t27.6\t\t27.75\n\nThe first observation is that both planning approach improves the BLEU scores even more significantly with transformer. They are all very high scores.\n\nFor the tag planning approach , we found that the accuracy of predicting the tag sequences is only 10.6%. Around 1/3 of the sentences are predicted to have the structure \u201cN V N V N V N .\u201d, which is wrong. As a result, the translation sentence does not follow the tag sequence it predicted. \n\nExample translation: \nN V N V N V N . <eoc> except for 3PCIS and three-phase reference signal generator , it is composed of the same parts as the conventional optical stripe range finder .\n\nWe can see that the resultant translation is not related to the predicted structural tags. It implies that we are unable to control the structure of translation with the tags. In contrast, the planning approach with discrete codes works well with teacher forcing on the codes. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "BkexY2vOTX", "original": null, "number": 6, "cdate": 1542122615686, "ddate": null, "tcdate": 1542122615686, "tmdate": 1542122615686, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "BkgXLmTIaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Re: Fundamental issues remain", "comment": "I understand your concern. You mean that translation quality may be improved if we remove the exposure bias on tags. I'm working to get the results in this scenario. I will first train a NMT model to predict the tag sequence from the source sentence, then concatenate the predicted tags and the target sentences to train a full NMT model.\n\nMy concern is that if we do not use the reference tags when training the NMT, then the tags can not be used to condition the structure of translations. "}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "BkgXLmTIaQ", "original": null, "number": 5, "cdate": 1542013770920, "ddate": null, "tcdate": 1542013770920, "tmdate": 1542013813930, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "SyxRv1PUaX", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Fundamental issues remain", "comment": "Thank you for your response.\n\nThere seems to be a fundamental misunderstanding re: teacher forcing. It is not teacher forcing that is the issue, the issue is that when training with gold part-of-speech tag sequences the model can look into the future as the tag sequence is derived directly form the target side to be predicted. This issue is not as severe for the autoencoded discrete codes since these are predicted from the source side only. If the part-of-speech tag sequence was predicted in the same way, there would be no time travel effect.\n\nRe: using a fixed number codes. Sure this makes the model slightly simpler, but it is a fundamental limitation since it cannot account for the structure of longer sentences."}, "signatures": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "ryx7VpEw3Q", "original": null, "number": 1, "cdate": 1540996395024, "ddate": null, "tcdate": 1540996395024, "tmdate": 1542013182701, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "content": {"title": "Interesting approach to translation diversity, but experiments somewhat lacking and details missing", "review": "The authors propose modeling structural diversity of translations by conditioning the generation on both the source sentence and a latent encoding of the overall structure (captured by simplified part-of-speech tags). Specifically, they first train a conditional autoencoder to learn a latent code optimized towards reconstructing the tag sequence. They then prefix the inferred latent code to the target sentence before generation. A diversity metric which measures pairwise BLEU scores between beam items is also proposed. Experiments show that the latent codes lead to greater structural diversity as well as marginally improved translation results when combined with beam search.\n\nContributions\n-----------------\nA simple method for improving structural diversity.\n\nThe use of conditional autoencoding to capture structural ambiguity, while not in itself novel, could be interesting for other problems as well.\n\nExperiments suggest that the method is rather effective (albeit only improving translation quality marginally)\n\nI like the proposed discrepancy score based on pairwise BLEU scores.\n\nIssues\n---------\nIt is not clear if teacher forcing was used in the \"tag planning\" setting. If gold tag sequences were used during training there is a major train/test mismatch which would explain the dramatic drop in BLEU scores. If so, this is a major issue, since the authors claim that as the motivation for the use of discrete latent codes. To make the \"tag planning\" setting comparable to the latent code setting, you would need to train the tag prediction model first and then condition on predicted tags when training the translation model (potentially you would need to do jack-knifing to prevent overfitting as well).\n\nIt is unfortunate that there is no empirical comparison with the most closely related prior work, in particular Li et al. (2016) and Xu et al. (2018), which are both appropriately cited. As it stands it is not possible to tell which of these approaches is most useful in practice.\n\nNo details are provided on the tagset used and what system is used to predict it, or to what degree of accuracy.\n\nHaving a fixed number of codes regardless of sentence length seems like a major shortcoming. I would urge the authors to consider a variable coding length scheme, e.g., by generating codes autoregressively instead of with a fixed number of softmaxes. It would also be interesting to break down the numbers in table 1 with respect to sentence length.\n\nMinor issues\n-----------------\nCitation for the Xavier method is missing.\n\nNotation is somewhat hard to follow. Please add a few sentences describing it and make sure it is consistent.\n\nThere are many grammatical errors. Please make sure to proofread!\n\n\"Please note that the planning component can also be a continuous latent vector, which requires a discriminator to train the model in order that the latent cap.\" What does this mean?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "cdate": 1542234509857, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677166, "tmdate": 1552335677166, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxRv1PUaX", "original": null, "number": 4, "cdate": 1541988198055, "ddate": null, "tcdate": 1541988198055, "tmdate": 1541988198055, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "ryx7VpEw3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Q1 It is not clear if teacher forcing was used in the \"tag planning\" setting\n\nYes, as the decoder part of the coding model is a language model, it is teacher forcing for the \u201ctag planning\u201d approach. Please note that it is the same situation for the \u201cdiscrete planning\u201c approach. \n\nThe core problem is that the tags contain  critical information about the translation not only in structural level. For example, if the NMT model fails to generate a NN tag for sentence \u201cA dog is playing\u201d, then the final translation result will be very wrong. Therefore, the tag predicting part has to be fairly accurate in order to eliminate the influence to the translation performance.\n\nBy learning the codes to capture things that are not obvious given the source sentence, this problem is solved. The codes will not contain information about whether there is a NN tag (if it is an obvious thing) but the order of the tags.\n\nQ2 no empirical comparison with the most closely related prior work\n\nIndeed, the decoding approach of Li et al. (2016)  is easy to implement, we are going to include the results in the paper. However, as those methods are trying to increase the diversity on the choice of words,  it is unfair to evaluate them with the structural diversity metric.\n\nAs the examples in the qualitative analysis show, our approach only produce diversity in structural level. Therefore, our approach is suitable for translation systems to generate multiple translation candidates, where the users are not expecting the system to use \u201csurprisingly creative\u201d words.\n\nQ3 No details are provided on the tagset used and what system is used to predict it\n\nFor the POS tagging part, we are using nltk.pos_tag .\n\nQ4 Having a fixed number of codes regardless of sentence length seems like a major shortcoming.\n\nWe actually consider this as the strength of the discrete coding approach, because :\n\n1) using a fixed number of codes can potentially reduce the chance of error when predicting the codes.\n\n2)  we can enumerate all candidate codes when they have a fixed number of sub codes."}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "Hyx4SKX-6m", "original": null, "number": 3, "cdate": 1541646652131, "ddate": null, "tcdate": 1541646652131, "tmdate": 1541646652131, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "ryg9daF53m", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for spending time for reviewing my paper.\n\nQ1 comparing with a stronger NMT architecture and larger data-set\n\nAs the ASPEC Ja-En dataset contains 3M bilingual sentence pairs, it shall not be considered as a small dataset. \n\nAlso our baseline model  achieves a strong BLEU score with various techniques.  On IWSLT14,  our baseline model achieves a  BLEU score of 29.34 (beam size = 10). A recent facebook paper (Gehring et al.,  2017: A Convolutional Encoder Model for Neural Machine Translation) also reported scores on the exactly same training and test data. Their convolutional encoder model achieves a BLEU score of 29.9, and a deep convolutional setting achieves 30.4 BLEU. So we do not think  our baseline is a weak model.\n\nQ2 Other methods have been shown to strongly out-perform Gumbel-Softmax in this context, so a comparison would be in order\n\nWe will also report the results with improved semantic hashing technique (Kaiser and Bengio, 2018: Discrete Autoencoders for Sequence Models)\n\nQ3 comparison to fully unsupervised latents and some other system\n\nThe main point of this paper is to generate translations with drastically different structures. Previous work of diverse language generation are focusing on letting the model generate sentences with creative vocabularies but not structures.\n\n(Kaiser and Bengio, 2018)  shows some examples of diverse translations resulted by using the unsupervised latents, which are trained with improved semantic hashing. However, they did not evaluate the quality of the sampled diverse translations. \n\nWe can indeed generate very diverse translation results if we are allowed to significantly degrade the translation quality.  However, such a performance degradation is not desirable in real products.\n\nOur approach preserve the translation quality by:\n\n1) Using syntactic tags so that the utterances of the results will not be constrained.\n2) Letting the codes contain only the target-side structural information that can not be predicted given the source sentence.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "BJgt-vklam", "original": null, "number": 1, "cdate": 1541564161292, "ddate": null, "tcdate": 1541564161292, "tmdate": 1541564717143, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "BkxQsCrAn7", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "content": {"title": "Response of AnonReviewer1", "comment": "Thank you for spending time reviewing my paper.\n\nThe main points of this paper are misinterpreted in the comments, here are the responses for them.\n\nQ1  What is this structure an example ?\n\nGrammar structure.\n\nQ2  Why is this approach naive?\n\nNo modification to the model is required.\n\nQ3  Fig.1 This example is also confusing because the \u201cstructural tags\u201d are non-sensical\n\nAs the caption tells, it is the illustration of the proposed approach. \u201cPRP V N\u201d is the extracted structural tags for the example sentence. Please see section 2.1 for details.\n\nQ4  What is the motivation behind the heuristics for the \u201ctwo-step process that simplifies the POS tags\u201d?\n\nExtracting global sentence structures (title of section 2.1)\n\nQ5  The description of the model is confusing. A simple seq2seq model is the obvious first thing to try.\n\nA simple seq2seq model can not learn discrete codes. AnonReviewer1 failed to understand why a discretization method is required here. The Gumbel-softmax bottleneck allows the coding model to encode structural information into discrete codes. \n\nQ6  The related work is a laundry list of papers, explained without relation to the current paper.\n\nThis comment is a false statement.  In the end of every parts of the related work, we are discussing the relation with this work.\n\nQ7 It\u2019s also unclear to me what is means to \u201creconstruct\u201d\n\nThe sequence auto-encoder encodes the \u201ctag sequence\u201d into discrete \u201ccodes\u201d and \u201creconstruct\u201d the \u201coriginal tag sequence\u201d.  \n\nThe code accuracy shows how accurate can the codes be predicted given the source sentence. \n\nNote that the auto encoder produces the codes using the target-side structural tags. So after training the coding model, we have no idea whether the NMT model can correctly predict the codes only based on the source-side information.\n\nTo reveal the predictability of the codes, we train an independent neural model  simultaneously with the coding model to predict the codes based on the source sentence, and report the accuracy.\n\nQ8 Given the minor differences in these numbers ...\n\n> Evaluation Results: Table 2 shows the resultant BLEU scores of different models, which indicates that our proposed planning approach does not degrade the translation performance in both translation tasks.\n\nAs described in the paper, the numbers show that our approach of generating diverse translations does not significantly hurts the translation quality. This evaluation is important  as there is a trade-off between diversity and translation quality.\n\nQ9  it seems like a possibly natural consequence of adding a noisy sequence\n\nIf it is the consequence of adding noise, how to explain the results in Table 5?\n\nQ10  \u201cInstead of letting the beam search decide the best \u2026 we use beam search to obtain three code sequences with highest scores.\u201d\n\nUsing beam search, we can either adopt  only the best result, or retrieve a N-best list. In section 5.1, we use beam search to obtain top-3 code sequences, and generate sentences following each code sequence."}, "signatures": ["ICLR.cc/2019/Conference/Paper228/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622549, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG1wjRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper228/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper228/Authors|ICLR.cc/2019/Conference/Paper228/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers", "ICLR.cc/2019/Conference/Paper228/Authors", "ICLR.cc/2019/Conference/Paper228/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622549}}}, {"id": "BkxQsCrAn7", "original": null, "number": 3, "cdate": 1541459611251, "ddate": null, "tcdate": 1541459611251, "tmdate": 1541534176258, "tddate": null, "forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "content": {"title": "Poorly motivated and confusing", "review": "\nThis paper is not ready for publication in ICLR or most other venues. The model is poorly motivated, many modeling choices are confusing, and the experiments are not convincing.  I found much of the paper confusing. A (far from complete) sample:\n\n\n\u00a71 \u00b61  What is this structure an example of? What sentence structures do you mean, concretely? Syntax? The introduction is very vague\u2014I\u2019m not convinced this is meaningful.\n\n\u00a71 \u00b62-3 These paragraphs also vague.\n\n\u00a71 \u00b65 Why is this approach naive? Is this a well-known method? There are no citations.\n\nFig.1 Very confusing: it looks like the target sentence, \u201cstructural tags\u201d and \u201ccoding model\u201d form a loop! This example is also confusing because the \u201cstructural tags\u201d are non-sensical\u2026 they have no relation to this example sentence! I can\u2019t tell if this is because they were made up without relation to the input sentence, or worse, that they\u2019re an actual example from the data, in which case there is something very wrong with the tagger used in the \u201cnaive\u201d experiments.\n\nSec. 2.1 What is the motivation behind the heuristics for the \u201ctwo-step process that simplifies the POS tags\u201d?\n\nSec 2.2. The description of the model is confusing. If I understand correctly, wehave training data for these \u201ccodes\" (in the form of \u201csimplified\u201d POS tags), and a simple seq2seq model is the obvious first thing to try. Most of the choices that deviate from this (e.g. use of Gumbel-softmax, also confusingly called \u201csoftplus\u201d in Eq. 2) are never explained.\n\nSec. 3 The related work is a laundry list of papers, explained without relation to the current paper. It simply gets in the way of the rest of the paper and isn\u2019t needed.\n\nTable 1. I\u2019m not sure what the code accuracy tells us. It\u2019s also unclear to me what is means to \u201creconstruct\u201d the \u201coriginal tag sequence\u201d from the codes, esp. given the description in Sec 2.1.\n\nTable 2. Given the minor differences in these numbers and the confusing description of the model and training process, I am skeptical of these numbers, which look quite a bit like noise. Note that the use of four columns corresponding to different beam sizes is misleading\u2026 this makes it look as if there are four separate experiments for each condition, but this is not really true, we expect these scores to correlate across different beam sizes, so seeing the bold numbers at the bottom of each column does not add substantial information.\n\nTable 4. These are interesting, but it seems like a possibly natural consequence of adding a noisy sequence of characters to the beginning of the decoded sequence; I\u2019m not convinced that the sequences mean anything per se, but it\u2019s a bit like adding some random noise to the decoder state before generating the word sequence.\n\n5.1 \u201cInstead of letting the beam search decide the best \u2026 we use beam search to obtain three code sequences with highest scores.\u201d I\u2019m confused: what is the difference?\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper228/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discrete Structural Planning for Generating Diverse Translations", "abstract": "Planning is important for humans when producing complex languages, which is a missing part in current language generation models. In this work, we add a planning phase in neural machine translation to control the global sentence structure ahead of translation. Our approach learns discrete structural representations to encode syntactic information of target sentences. During translation, we can either let beam search to choose the structural codes automatically or specify the codes manually. The word generation is then conditioned on the selected discrete codes. Experiments show that the translation performance remains intact by learning the codes to capture pure structural variations. Through structural planning, we are able to control the global sentence structure by manipulating the codes. By evaluating with a proposed structural diversity metric, we found that the sentences sampled using different codes have much higher diversity scores. In qualitative analysis, we demonstrate that the sampled paraphrase translations have drastically different structures. ", "keywords": ["machine translation", "syntax", "diversity", "code learning"], "authorids": ["shu@nlab.ci.i.u-tokyo.ac.jp", "nakayama@ci.i.u-tokyo.ac.jp"], "authors": ["Raphael Shu", "Hideki Nakayama"], "TL;DR": "Learning discrete structural representation to control sentence generation and obtain diverse outputs", "pdf": "/pdf/a644831b836e754acab8f4f0216ef6b0830153cd.pdf", "paperhash": "shu|discrete_structural_planning_for_generating_diverse_translations", "_bibtex": "@misc{\nshu2019discrete,\ntitle={Discrete Structural Planning for Generating Diverse Translations},\nauthor={Raphael Shu and Hideki Nakayama},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG1wjRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper228/Official_Review", "cdate": 1542234509857, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG1wjRqFQ", "replyto": "SJG1wjRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper228/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335677166, "tmdate": 1552335677166, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper228/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}