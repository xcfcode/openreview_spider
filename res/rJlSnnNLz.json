{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582880682, "tcdate": 1520534443112, "number": 1, "cdate": 1520534443112, "id": "r1mnQZkYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "signatures": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer2"], "content": {"title": "Promising direction but not clear", "rating": "4: Ok but not good enough - rejection", "review": "Summary: \nThis paper proposes some ways to mixing deep representation from two modalities: audio and visual inputs. They evaluated on AVLetters and CUAVE datasets and seems to beat the baseline architecture without the proposed connections.\n\nStrength: \n- Cross-modal architecture design is a very important topic. We need more insights on how to mix representation from two difference sources. This paper is a great piece of effort towards this direction. Experimental results are also convincing.\n\nWeakness: \n- The paper is very unclear. I don\u2019t know the exact formulas for X-conn and Res-conn. The only description of the model is to read the figure, which is not very intuitive. The connections in Figure 1 a) is hard to see. It seems to me that X-conn is just a reshape operator by reading Figure 1 b) and I still don\u2019t know what is the exact form of ResConn.\n\n- The experiments lack proper ablation studies. It seems that the baseline without any additional connections is obviously worse, but there are so many connections that are added. Which one is the most useful? What kind of effect does ResConn and X-Conn bring, separately?\n\n- The section 3.3 is very confusing. I don\u2019t understand the purpose of the plots, especially Figure 2 b) and c). It\u2019s not clear what is \u201cgood\u201d in those plots and what we are expected to see or learn about.\n\nConclusion:\nAlthough the core theme of the paper is a promising direction, I feel that the current presentation of the paper does not convey clear methodology and experimentation to the readers. Therefore I recommend reject.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XFlow: Cross-modal Dataflow Neural Networks for Audiovisual Classification", "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between several feature extractors, deriving more interpretable features and obtaining a better representation than through unimodal learning. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal research in two essential ways: (1) it presents a novel method for performing cross-modality (which could easily be generalised to other kinds of data) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. We also illustrate some of the representations learned by the connections and present Digits, a new dataset consisting of three audiovisual data types. Both architectures outperformed their baselines and achieved state-of-the-art results on AVletters and CUAVE.", "pdf": "/pdf/e03b6869738c6d64c58a632a65cd067162b39c90.pdf", "TL;DR": "Two novel multimodal deep learning architectures with cross-modal dataflow in the feature extraction phase. State-of-the-art results on three audiovisual classification benchmarks.", "paperhash": "cangea|xflow_crossmodal_dataflow_neural_networks_for_audiovisual_classification", "keywords": ["machine learning", "deep learning", "multimodal", "audiovisual", "classification", "cross-modal", "cross-connections"], "authors": ["C\u0103t\u0103lina Cangea", "Petar Veli\u010dkovi\u0107", "Pietro Li\u00f2"], "authorids": ["catalina.cangea@cst.cam.ac.uk", "petar.velickovic@cst.cam.ac.uk", "pietro.lio@cst.cam.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582880398, "id": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper20/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer3"], "reply": {"forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582880398}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582786339, "tcdate": 1520631093144, "number": 2, "cdate": 1520631093144, "id": "r1pEa_gFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "signatures": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer1"], "content": {"title": "Limited empirical evaluation", "rating": "4: Ok but not good enough - rejection", "review": "This paper investigates multimodal learning (image/video + sound) and investigate the use of cross-modality connection. Authors compare model with cross-connection versus baseline without cross connection on 3 different datasets (one of the dataset is a contribution of the paper as well). Empirical results show that cross-model achieve better results than previous works and baseline without the cross-correlation. \n\nProposed approach has somewhat limited novelty and the empirical evaluation is carried out on relatively small scale dataset.  I would recommend to evaluate the approach on larger dataset to better assess the benefit of  the approach. In addition it would be nice to add an ablation  to see the impact of the different individual cross connections.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XFlow: Cross-modal Dataflow Neural Networks for Audiovisual Classification", "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between several feature extractors, deriving more interpretable features and obtaining a better representation than through unimodal learning. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal research in two essential ways: (1) it presents a novel method for performing cross-modality (which could easily be generalised to other kinds of data) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. We also illustrate some of the representations learned by the connections and present Digits, a new dataset consisting of three audiovisual data types. Both architectures outperformed their baselines and achieved state-of-the-art results on AVletters and CUAVE.", "pdf": "/pdf/e03b6869738c6d64c58a632a65cd067162b39c90.pdf", "TL;DR": "Two novel multimodal deep learning architectures with cross-modal dataflow in the feature extraction phase. State-of-the-art results on three audiovisual classification benchmarks.", "paperhash": "cangea|xflow_crossmodal_dataflow_neural_networks_for_audiovisual_classification", "keywords": ["machine learning", "deep learning", "multimodal", "audiovisual", "classification", "cross-modal", "cross-connections"], "authors": ["C\u0103t\u0103lina Cangea", "Petar Veli\u010dkovi\u0107", "Pietro Li\u00f2"], "authorids": ["catalina.cangea@cst.cam.ac.uk", "petar.velickovic@cst.cam.ac.uk", "pietro.lio@cst.cam.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582880398, "id": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper20/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer3"], "reply": {"forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582880398}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582663312, "tcdate": 1520753962473, "number": 3, "cdate": 1520753962473, "id": "HyGNTUMKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "signatures": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer3"], "content": {"title": "Good results but more analysis needed", "rating": "6: Marginally above acceptance threshold", "review": "Typical neural nets applied to classification tasks for multi-modal data consist of separate uni-modal pathways which are fused at the higher levels. The proposed model adds some extra connections from the lower layers of the unimodal pathways to the higher layers of the other modality's pathway. This architectural change is shown to lead to better classification performance.\n\nThis paper re-affirms the idea that skip connections help, which is somewhat widely known now. Unless there is some specific insight to be had, this particular instantiation of the idea would only be of moderate interest. The authors provide some analysis (section 3.3) but it is not immediately clear from that what is the point being made. However, the quantitative results show significantly better performance.\n\nMinor corrections and suggestions-\n- The terms \"residual connection\" and \"cross connection\" should be explained more clearly. This nomenclature seems confusing because both connections cut cross modalities and can be seen to be \"residual\". Based on Figure 1(b) the only difference between them seems to be that the first operation is conv vs max pool.\n\n- In the abstract :\"for performing cross-modality\" : something missing here?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XFlow: Cross-modal Dataflow Neural Networks for Audiovisual Classification", "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between several feature extractors, deriving more interpretable features and obtaining a better representation than through unimodal learning. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal research in two essential ways: (1) it presents a novel method for performing cross-modality (which could easily be generalised to other kinds of data) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. We also illustrate some of the representations learned by the connections and present Digits, a new dataset consisting of three audiovisual data types. Both architectures outperformed their baselines and achieved state-of-the-art results on AVletters and CUAVE.", "pdf": "/pdf/e03b6869738c6d64c58a632a65cd067162b39c90.pdf", "TL;DR": "Two novel multimodal deep learning architectures with cross-modal dataflow in the feature extraction phase. State-of-the-art results on three audiovisual classification benchmarks.", "paperhash": "cangea|xflow_crossmodal_dataflow_neural_networks_for_audiovisual_classification", "keywords": ["machine learning", "deep learning", "multimodal", "audiovisual", "classification", "cross-modal", "cross-connections"], "authors": ["C\u0103t\u0103lina Cangea", "Petar Veli\u010dkovi\u0107", "Pietro Li\u00f2"], "authorids": ["catalina.cangea@cst.cam.ac.uk", "petar.velickovic@cst.cam.ac.uk", "pietro.lio@cst.cam.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582880398, "id": "ICLR.cc/2018/Workshop/-/Paper20/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper20/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper20/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper20/AnonReviewer3"], "reply": {"forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper20/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582880398}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573595853, "tcdate": 1521573595853, "number": 226, "cdate": 1521573595513, "id": "SJEk11kcz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJlSnnNLz", "replyto": "rJlSnnNLz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XFlow: Cross-modal Dataflow Neural Networks for Audiovisual Classification", "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between several feature extractors, deriving more interpretable features and obtaining a better representation than through unimodal learning. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal research in two essential ways: (1) it presents a novel method for performing cross-modality (which could easily be generalised to other kinds of data) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. We also illustrate some of the representations learned by the connections and present Digits, a new dataset consisting of three audiovisual data types. Both architectures outperformed their baselines and achieved state-of-the-art results on AVletters and CUAVE.", "pdf": "/pdf/e03b6869738c6d64c58a632a65cd067162b39c90.pdf", "TL;DR": "Two novel multimodal deep learning architectures with cross-modal dataflow in the feature extraction phase. State-of-the-art results on three audiovisual classification benchmarks.", "paperhash": "cangea|xflow_crossmodal_dataflow_neural_networks_for_audiovisual_classification", "keywords": ["machine learning", "deep learning", "multimodal", "audiovisual", "classification", "cross-modal", "cross-connections"], "authors": ["C\u0103t\u0103lina Cangea", "Petar Veli\u010dkovi\u0107", "Pietro Li\u00f2"], "authorids": ["catalina.cangea@cst.cam.ac.uk", "petar.velickovic@cst.cam.ac.uk", "pietro.lio@cst.cam.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1517763640049, "tcdate": 1517763640049, "number": 20, "cdate": 1517763640049, "id": "rJlSnnNLz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJlSnnNLz", "signatures": ["~Petar_Veli\u010dkovi\u01071"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "XFlow: Cross-modal Dataflow Neural Networks for Audiovisual Classification", "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between several feature extractors, deriving more interpretable features and obtaining a better representation than through unimodal learning. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are nontrivially exchangeable. Our work improves on existing multimodal research in two essential ways: (1) it presents a novel method for performing cross-modality (which could easily be generalised to other kinds of data) and (2) extends the previously proposed cross-connections which only transfer information between streams that process compatible data. We also illustrate some of the representations learned by the connections and present Digits, a new dataset consisting of three audiovisual data types. Both architectures outperformed their baselines and achieved state-of-the-art results on AVletters and CUAVE.", "pdf": "/pdf/e03b6869738c6d64c58a632a65cd067162b39c90.pdf", "TL;DR": "Two novel multimodal deep learning architectures with cross-modal dataflow in the feature extraction phase. State-of-the-art results on three audiovisual classification benchmarks.", "paperhash": "cangea|xflow_crossmodal_dataflow_neural_networks_for_audiovisual_classification", "keywords": ["machine learning", "deep learning", "multimodal", "audiovisual", "classification", "cross-modal", "cross-connections"], "authors": ["C\u0103t\u0103lina Cangea", "Petar Veli\u010dkovi\u0107", "Pietro Li\u00f2"], "authorids": ["catalina.cangea@cst.cam.ac.uk", "petar.velickovic@cst.cam.ac.uk", "pietro.lio@cst.cam.ac.uk"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}