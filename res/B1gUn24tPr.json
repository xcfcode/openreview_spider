{"notes": [{"id": "B1gUn24tPr", "original": "Bkl2MP4MPH", "number": 191, "cdate": 1569438894416, "ddate": null, "tcdate": 1569438894416, "tmdate": 1577168228779, "tddate": null, "forum": "B1gUn24tPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OVEpOfB95M", "original": null, "number": 1, "cdate": 1576798689817, "ddate": null, "tcdate": 1576798689817, "tmdate": 1576800945337, "tddate": null, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Decision", "content": {"decision": "Reject", "comment": "The paper is interested in Chinese Name Entity Recognition, building on a BERT pre-trained model. All reviewers agree that the contribution has limited novelty. Motivation leading to the chosen architecture is also missing. In addition, the writing of the paper should be improved.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730210, "tmdate": 1576800282958, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper191/-/Decision"}}}, {"id": "BJlwlG5MiB", "original": null, "number": 3, "cdate": 1573196270796, "ddate": null, "tcdate": 1573196270796, "tmdate": 1573196270796, "tddate": null, "forum": "B1gUn24tPr", "replyto": "ryejz8JhFr", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment", "content": {"title": "Some Explanation about your confusion.", "comment": "thank you so much for your kind review. Here gives answsers to all of your questions\n\n1\u3001\tResult compare with ERNIE+CRF\n        Baidu did not publish the pre-training model for the Ernine 2.0, but from the paper, the results of MSRA are as follows:\n       Ernie 1.0 Base    f1:93.8\n       Ernie  2.0 Base   f1:93.8\n       Ernie  2.0 large  f1:95.0\n       Also,we are trying to conduct experiment of Ernine 1.0 +CRF\n\n2\u3001\tSimilarities with published papers \u300aJoint Embedding of Words and Labels for Text Classification\u300b\n       Thanks to the reviewer's recommendation, but I had not read any papers about label embeddings in NLP including the paper\u300aJoint Embedding of Words and Labels for Text Classification\u300bbefore. \n       The idea that giving greater weight to characters in the statement that are similar to a particular entity type was gradually formed in working with NER task. The original source of inspiration came from the Lattice paper https://arxiv.org/abs/1805.02023. I found it pretty useful to combine word embeddings to the character-level model. However, the effect of the whole model has a lot to do with the completeness of predefined lexicon (word level) and pretrained word embedding. So, I tried to eliminate the step of obtaining predefined lexicon, and directly replace the external resources with the easy-to-obtain entity class embeddings.\n     After reading this article many times in the past two days, I found that although the results of both methods are weighted representations of certain words in the article, the process of achieving this result is two completely different approaches.\n     In the paper\u300aJoint Embedding of Words and Labels for Text Classification\u300b, the label embeddings are trained together with the model, hoping to obtain a class template that could highlight related words with the particular text Topic .For instance, in Figure4(a),it highlights \u201ccoaches\u201d\u3001\u201dsports\u201d for STORT news and \u201crock\u201d, \u201cdrummer\u201d for Entertainment news.\n      However, our goal is to highlight word closed to predefined names-entity classes only. The class embeddings are fixed at the initial stage of training\n      For instance, suppose there is a series of Olympic news and entertainment news that needs to be classified. According to the model proposed by\u300aJoint Embedding of Words and Labels for Text Classification\u300b\uff0cwords like \u201cgymnasium\u201d\u3001\u201dswimming pool\u201d\u3001\u201dgold medal\u201d may also be highlighted along with PERSON- related words \u201cathlete\u201d, \u201ccoaches\u201d. However, if the predefined entity class in our method is only PERSON, entities in the remaining categories will not be highlighted under ideal conditions\n      In my opinion, our approach has a more dominant effect on the NER field, or in areas with a defined reading goal, because the class embedding for attention is preset to the entity class that you want to extract. But label embedding also gives potential entity information from the opposite perspective. In the future, we can even try to combine the two methods and discover more potential entities, especially for Internet data! \n     As can be seen from the experimental department, compared to the regular datasets, our method works especially well for user generated data like Weibo, which also shows the validity of class attention in searching potential entities.\n    Finally, I would like to thank the reviewers for recommending the paper\u300aJoint Embedding of Words and Labels for Text Classification\u300b. We will continue to follow other articles in this direction.\n\n4,Sorry for the problems of Table index and notations. They will be fixed in the next version.\n\n5,We did not freeze the pretrained BERT model, but we freeze all the class embeddings.\n\n6,- in Table 2 and Table 3, why the 13-layer BERT + CRF performs significantly worse on Recall (Table 2) and significantly better on Recall (Table 3)?\nThis is the result of a real experiment. My own guess is the irregularity of Weibo data. \n"}, "signatures": ["~Ge_Yuchen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ge_Yuchen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504212673, "tmdate": 1576860588892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper191/Authors", "ICLR.cc/2020/Conference/Paper191/Reviewers", "ICLR.cc/2020/Conference/Paper191/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment"}}}, {"id": "SylCtptboB", "original": null, "number": 2, "cdate": 1573129606318, "ddate": null, "tcdate": 1573129606318, "tmdate": 1573129606318, "tddate": null, "forum": "B1gUn24tPr", "replyto": "ByeUmO-R5H", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment", "content": {"title": "Thank you for your comment ", "comment": "Thank you for your kind suggestion, \nwe realized that we should clearly state our potential value in the next version\nI think the Classification Attention structure may have other uses in the future, although this article is merely focusing on the contribution to the NER task only.\n\nFor example, in the field of textual understanding (Text classification, QA and so on), explicit entities displayed in the sentence can bring reading focus to the text, and we will try to apply this structure in other fields in future work.\n\nFor innovation\nOur approach proposes two innovations\n1.\tIt proposed a simple but effective way to obtain entity class embeddings from the pre-trained BERT model. This method avoids the need for additional lexicon (avoiding OOV problem) and has strong practicability.\n2.\tIt proposed a structure to use word level information (entity class embeddings) to locate protentional characters in the sentence that may compose an entity \n\uf06c\tIt can be applied to other language with similar language structure with Chinese. \uff08Chinese word can be composed of one or a couple of characters and that word boundaries cannot detected graphically\uff09\n\uf06c\tAnd this structure is different from the scale dot product in terms of both purpose and structure. It can display the position of the potential entity characters in the highlighted statement, which is useful for subsequent tasks. \n\uf06c\tfrom the experiment section, especially the user-generated network data (i.e Weibo dataset), we have made a big improvement to the traditional method, which proves the effectiveness of using class embedding to find potential entities in the statement.\n"}, "signatures": ["~Ge_Yuchen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ge_Yuchen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504212673, "tmdate": 1576860588892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper191/Authors", "ICLR.cc/2020/Conference/Paper191/Reviewers", "ICLR.cc/2020/Conference/Paper191/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment"}}}, {"id": "S1gg_BFZiS", "original": null, "number": 1, "cdate": 1573127527650, "ddate": null, "tcdate": 1573127527650, "tmdate": 1573127527650, "tddate": null, "forum": "B1gUn24tPr", "replyto": "rylcHyzA9r", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment", "content": {"title": "Some Explanation about your confusion.", "comment": "First of all, thank you for your kind review. Before answering your questions, I think some description in my article is indeed misleading for non-Chinese readers. The next version will be carefully modified. Here is a common explanation for you:\n\nA Chinese word is usually composed of multiple Chinese characters, and there is no interval between Chinese word or Chinese characters in a sentence, so it is very difficult to detect the boundary for the NER problem.\n\nA simple Example can be given like: \n\nEnglish\uff1aI   heart   Beijing\nChinese\uff1a\u6211\u7231\u5317\u4eac \nChinese CWS: \u6211\uff08I\uff09/\u7231(heart) /\u5317\u4eac(Beijing)   \n\nJust like Beijing refers to \u5317\u4eac in Chinese\uff0cthe word \u5317\u4eac is composed of 2 Chinese characters \u5317 and \u4eac. In English, There are obvious boundaries between words , but in Chinese, all the Chinese characters are written in a whole. \n\nThe following is an explanation of some confusing words in the article.\n\n1\u3001\u201dcharacter\u201d in this paper should refer to \u201cChinese character\u201d,  \uff08eg. \u5317\uff09\n2\u3001\u201cword\u201d in this paper should refer to \u201cChinese word \u201c.\uff08eg. \u5317\u4eac\uff09\n2\u3001\t\u201cword entity\u201d in this paper should refer to \u201cnamed-entity \u201cin the paper\n3\u3001\u201cEntity class embedding\u201c in this paper refer to a single representative embedding of all the named-entities within a certain class \uff08eg. embedding for class People\uff09\n4\u3001\u201clexicon\u201c refer to \u201cuser defined dictionary\uff08in Chinese word level\uff09\u201d\n\n#Summery\nentity is referring to named-entities\n\n#1 Introduction\n\"Due to the differences in language structure\" : this paragraph is not clear. It should say explicitly that in Chinese word can be composed of one or a couple of characters and that word boundaries can not detected graphically.\nYes, I should say \"in Chinese word can be composed of one or a couple of characters and that word boundaries cannot detected graphically\" directivity\n\n\n\"A common mitigation is to add external lexicon as a reference\" \nThe next sentence behind it \u201cWhether directly integrated into character-based models by means of word embeddings, or directly repairing the boundaries of entities recognized by character-based models, the performance of lexicon-assisted model is strongly related to the quality of the lexicon.\u201d gives the idea of how to include Chinese word embedding into Chinese character model\n\n\" an entity classification-assisted model\u201d \nPerhaps I should say the assistance from entity classes.\n\n\" The first step is to form word embeddings of entities appearing in the training sentences through character embeddings and the second step is to aggregate entity embeddings by category and generate classification embeddings\" \nAs illustrated in 3.1 OVERALL ARCHITECTURE,\" the embedding extraction step is performed firstly in a pipeline structure and the results will be aggregated to the attention model.\"\n\n\"After that, we designed a novel Attention mechanism to integrate entity \"\nWe have already explained that we have proposed two proposals, the last paragraph is explaining the first proposal(which consists of several steps), and this paragraph is the second, and the two proposals are formed by the pipeline structure.\n\nSection2\nmore works Yang et al. (2016) , Ruder12 et al. (2017) refers to two paper\nSorry for the missing comma.\n\n\"What\u2019s more, the attention mechanism...\"\nEach paragraph in this section gives a way to promote the traditional model, and this paragraph gives the attention mechanism used in Chinese NER\n\n3.2 EMBEDDING EXTRACTION FOR ENTITY CLASS\n\" the smooth inverse frequency is abandoned\" \nThe reason is in the next sentence\u201d due to the dynamic semantic information encoded in H. \u201c\n\n\"the weighted projection of the word embeddings on their first singular vector is removed.\" : explain. If it is a common practice, give a citation otherwise justify this choice. \nThe reference to this method has already been mentioned in the previous article. \u201cInspired from Arora et al. (2016), in this paper, we illustrate a simple way to extract word embeddings of entities in the hidden outputs of BERT.\u201d\n\n3.3\nHere again, the proposed attention system is described but not justified : why would a class specific attention system be better ? What are the expected advantages ?\n\nBecause this attention mechanism can use the extracted class embedding to focus on the potential entities of the same type in the text. It is explained in the paper: It aims to get a series of weighted representation of the input sentences so that characters similar to a particular entity class could have greater feature\n\n4.ablation study.\nWe designed two baseline experiments to do the ablation study, one is pure bert+crf to eliminate the superiority of the bert , and the other is a 13-layer bert (the original bert is 12) Layer) +crf, used to eliminate the extra layer of scaled dot product attention itself, showing that the structure of our model : the original bert of 12 layer + 1 layer of our proposed class attention + crf is working\n"}, "signatures": ["~Ge_Yuchen1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Ge_Yuchen1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504212673, "tmdate": 1576860588892, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper191/Authors", "ICLR.cc/2020/Conference/Paper191/Reviewers", "ICLR.cc/2020/Conference/Paper191/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper191/-/Public_Comment"}}}, {"id": "ryejz8JhFr", "original": null, "number": 1, "cdate": 1571710483133, "ddate": null, "tcdate": 1571710483133, "tmdate": 1572972627207, "tddate": null, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n      This paper discussed an approach to do named entity resolution (NER, the paper focuses only on Chinese NER but I think it could generalize to other languages as well). The idea is based on smart integration and extension of multiple existing building blocks: 1) BERT pre-trained model 2) a previous work to get document embedding by doing weighted average of word embedding (https://openreview.net/pdf?id=SyK00v5xx) and 3) Scaled dot-product attention mechanism applied directly to multi-label classification. The \"Introduction\", \"Related work\", and \"Experiment Settings\" sections are well written and covers many details and decent references. Especially, the \"experiments\" section is described in a great amount of details, which should be very helpful for reproducibility. \n      \nContributions:\n      * The author found an interesting application of the original algorithm (https://openreview.net/pdf?id=SyK00v5xx) to represent the entity class embedding based on averaging \"BERT\" embeddings of all the component words. This could be implemented as a pre-processing step against any training dataset to derive \"pre-learned\" entity class embedding.\n      * Instead of the common approach of connecting the BERT sequence outputs directly to CRF layer, the author added an intermediate layer to calculate the classification attention between a sentence (sequence of token embedding) and any entity class (based on the above pre-learned entity embedding). This result plus the original sentence embedding are concatenated.  The concatenation is further fed into a few additional layers to produce the final inputs into CRF layer.\n\nWeakness:\n     * The paper lacks novelty. As pointed above, I did not see that the contribution from the paper is sufficiently original. It is a good application of various existing methods though.\n\nI also have a few suggestions/questions below:\n\n* The ERNIE paper (https://arxiv.org/abs/1907.12412v1) is mentioned in the related work. Since ERNIE can potentially learn a good vocab for Chinese, did you ever compare your approach vs ERNIE+CRF? \n* There is one paper that I know which is pretty relevant to what you are doing here, which is probably worth a reference. https://arxiv.org/abs/1805.04174.  In that paper, the idea is to co-learn a class embedding and perform text classification. Their class attention is performed through dot-production attention though.\n* The Table index seems wrong in your paper. (I think Table 2 is not mentioned in your paper, but all tables (3-6) is offset by 1). \n* There are some minor typos or places that need some clarifications.\n   - in the abstract: \"character-based\" model. This is a little confusing. Because BERT is a word-piece based model. word-piece could across multiple characters for English. IIUC, You probably want to say \"Chinese-character\" instead of character.\n   - in \"Introduction\", \"providing greater weight to characters identical to each entity class\", you might want to revise this sentence to clarify its meaning further.\n   - In section 3.2, you might want to give some explanation to some notations (the first time you refer to it). For example, what is $L$, what is $m$ and $n$.  What is $S$?  Also why the denominator of Emb(Word) is not $n-m+1$? \n\n   - The last paragraph in section 3.3 needs more clarification as well. How do you merge the three tensors after attention stage? (a concatenation ?) . The last sentence mentioned \"residential\", I guess instead you want to say \"residual\".  You might also want to clarify where the \"3 layers\" of residual appear in your network.\n\n  - In your experiment, (if I did not miss), did you freeze the BERT parameters and entity embeddings when finetuning your NER model?\n\n  - in Table 2 and Table 3, why the 13-layer BERT + CRF performs significantly worse on Recall (Table 2) and significantly better on Recall (Table 3)? \n\n\n  "}, "signatures": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575672202630, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper191/Reviewers"], "noninvitees": [], "tcdate": 1570237755710, "tmdate": 1575672202645, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper191/-/Official_Review"}}}, {"id": "ByeUmO-R5H", "original": null, "number": 2, "cdate": 1572898846390, "ddate": null, "tcdate": 1572898846390, "tmdate": 1572972627165, "tddate": null, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper tries to improve the performance of Chinese NER by developing a novel attention mechanism that leverages BERT pre-trained model which considers bi-directional context. Experiments on a number of tasks show that the proposed approach is effective.\n\nComments:\n[1] A bunch of experiments are conducted\n[2] Chinese NER is a hard problem, but it would be great to see the proposed approach generalizable to other tasks. So, the contribution of this paper is limited\n[3] The proposed algorithm is simple and effective, but the novelty is a bit low\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575672202630, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper191/Reviewers"], "noninvitees": [], "tcdate": 1570237755710, "tmdate": 1575672202645, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper191/-/Official_Review"}}}, {"id": "rylcHyzA9r", "original": null, "number": 3, "cdate": 1572900674426, "ddate": null, "tcdate": 1572900674426, "tmdate": 1572972627123, "tddate": null, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "invitation": "ICLR.cc/2020/Conference/Paper191/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Comments by sections : \n\nSummary : \n\nThe use of entity is not clear. Are you referring to named-entities ?\n\n1 INTRODUCTION\n\n\"Due to the differences in language structure\" : this paragraph is not clear. It should say explicitly that in Chinese word can be composed of one or a couple of characters and that word boundaries can not detected graphically. \n\n \"A common mitigation is to add external lexicon as a reference\"   references are needed on how to includes words in embedding (except just training word embeddings)\n \n \" an entity classification-assisted model\" : not very clear : the classification is assisted ?\n \n \" The first step is to form word embeddings of entities appearing in the trainning sentences through character embeddings and the second step is to aggregate entity embeddings by category and generate classification embeddings\" : is it a multi-task training ?\n \n \"After that, we designed a novel Attention mechanism to integrate entity \" : is it the same model or two different propositions of the paper ? \"After that\" is not very clear as a transition.\n \n Section 2 :\n \n \"more works Yang et al. (2016) Ruder12 et al. (2017) \" : strange formulation\n \n \"What\u2019s more, the attention mechanism...\" : odd expression.\n \n \"In this paper, we revise the Scaled Dot-Product Attention to Classification Attention which would give a weighted representation of the input sentences through a series of entity classes.\" : maybe it should be moved to the introduction as a novelty proposed by the paper.\n \n \n 3.2  EMBEDDING EXTRACTION FOR ENTITY CLASS\n \n \" the smooth inverse frequency is abandoned\" : why ? please explain this choice.\n \"the weighted projection of the word embeddings on their first singular vector is removed.\" : explain. If it is a common practice, give a citation otherwise justify this choice.\n \n 3.3 CLASSIFICATION ATTENTION \n \n Here again, the proposed attention system is described but not justified : why would a class specific attention system be better ? What are the expected advantages ?\n \n 4.1.3 EXPERIMENTAL RESULTS\n Experiments are conducted on 4 dataset and the proposed model is compared to a \"standard\" BERT-based model and several results form the litterature. The proposed model outperforms sometimes the other models, often by a small margin as it is usually the case in NER experiments. \n But more insight on the strengths of the models should be given by conducting an ablation study. \n  \n  \n In conclusion ,this paper present an incremental improvement over BERT-based NER for Chinese. The proposed approach is not sufficiently justified and experiments, even if showing improvements over state-of-the-art models or published results, does not sufficiently explore the benefits of the proposed model (with ablation study for example).  "}, "signatures": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper191/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Classification Attention for Chinese NER", "authors": ["Yuchen Ge", "FanYang", "PeiYang"], "authorids": ["geyc2@lenovo.com", "yangfan24@lenovo.com", "yangpei4@lenovo.com"], "keywords": ["Chinese NER", "NER", "tagging", "deeplearning", "nlp"], "TL;DR": "Classification Attention for Chinese NER", "abstract": "The character-based model, such as BERT, has achieved remarkable success in Chinese named entity recognition (NER). However, such model would likely miss the overall information of the entity words. In this paper, we propose to combine priori entity information with BERT. Instead of relying on additional lexicons or pre-trained word embeddings, our model has generated entity classification embeddings directly on the pre-trained BERT, having the merit of increasing model practicability and avoiding OOV problem. Experiments show that our model has achieved state-of-the-art results on 3 Chinese NER datasets.", "pdf": "/pdf/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "paperhash": "ge|classification_attention_for_chinese_ner", "original_pdf": "/attachment/c4df8bbf5593032c160b6e31ed084b4a73f3daba.pdf", "_bibtex": "@misc{\nge2020classification,\ntitle={Classification Attention for Chinese {\\{}NER{\\}}},\nauthor={Yuchen Ge and FanYang and PeiYang},\nyear={2020},\nurl={https://openreview.net/forum?id=B1gUn24tPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1gUn24tPr", "replyto": "B1gUn24tPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper191/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575672202630, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper191/Reviewers"], "noninvitees": [], "tcdate": 1570237755710, "tmdate": 1575672202645, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper191/-/Official_Review"}}}], "count": 8}