{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124471815, "tcdate": 1518456347800, "number": 165, "cdate": 1518456347800, "id": "BkVmRByPG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BkVmRByPG", "signatures": ["~Yingzhen_Li1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "abstract": "There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with deep generative models, and verifies its robustness against a number of existing attacks. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers.", "paperhash": "li|are_generative_classifiers_more_robust_to_adversarial_attacks", "keywords": ["generative models", "adversarial attacks", "defences"], "_bibtex": "@misc{\n  li2018are,\n  title={Are Generative Classifiers More Robust to Adversarial Attacks?},\n  author={Yingzhen Li},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVmRByPG}\n}", "authorids": ["yl494@cam.ac.uk"], "authors": ["Yingzhen Li"], "TL;DR": "We show initial evidence that generative classifiers (using conditional DGMs) might be more robust to recent attacks than DNN-based discriminative classifiers.", "pdf": "/pdf/0774b205fd32d00d1fa708e7415a74b8cbc6e950.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582856401, "tcdate": 1520568481111, "number": 1, "cdate": 1520568481111, "id": "ryYo_YJtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper165/Official_Review", "forum": "BkVmRByPG", "replyto": "BkVmRByPG", "signatures": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer2"], "content": {"title": "Anecdotal evidence on generative classifiers and adversarial examples", "rating": "4: Ok but not good enough - rejection", "review": "This paper finds that one class of deep generative neural networks is less vulnerable to the FGSM attack than a standard CNN on MNIST. Of course, this may be due to gradient masking (which the paper, to its credit, does acknowledge), and the advantage may or may not hold on other datasets or when the CNNs are trained robustly (e.g., with the methods of Madry et al. (2018)). There could be something interesting here -- it may indeed be the case that generative models tend to be more robust. However, the evidence is inconclusive. Before this paper is ready for publication (even as a workshop paper), there needs to be clearer evidence that the result is real. This is especially important for work on adversarial examples, since many papers have failed to live up to their claims when evaluated against stronger attacks.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "abstract": "There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with deep generative models, and verifies its robustness against a number of existing attacks. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers.", "paperhash": "li|are_generative_classifiers_more_robust_to_adversarial_attacks", "keywords": ["generative models", "adversarial attacks", "defences"], "_bibtex": "@misc{\n  li2018are,\n  title={Are Generative Classifiers More Robust to Adversarial Attacks?},\n  author={Yingzhen Li},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVmRByPG}\n}", "authorids": ["yl494@cam.ac.uk"], "authors": ["Yingzhen Li"], "TL;DR": "We show initial evidence that generative classifiers (using conditional DGMs) might be more robust to recent attacks than DNN-based discriminative classifiers.", "pdf": "/pdf/0774b205fd32d00d1fa708e7415a74b8cbc6e950.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582856212, "id": "ICLR.cc/2018/Workshop/-/Paper165/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper165/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper165/AnonReviewer3"], "reply": {"forum": "BkVmRByPG", "replyto": "BkVmRByPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582856212}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582744717, "tcdate": 1520659070501, "number": 2, "cdate": 1520659070501, "id": "SJLYqk-YG", "invitation": "ICLR.cc/2018/Workshop/-/Paper165/Official_Review", "forum": "BkVmRByPG", "replyto": "BkVmRByPG", "signatures": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer3"], "content": {"title": "An empirical study of the effect of adversarial attacks on generative classifiers", "rating": "6: Marginally above acceptance threshold", "review": "The paper discusses some empirical results that seem to suggest that generative classifiers may be more robust to adversarial attacks than discriminative classifiers. I\u2019m not an expert on the topic, but from my very limited knowledge the paper seems to be quite clear in the distinction with the related work. \n\nThe paper is well-written and the work seems novel enough, but not yet very well developed. If I\u2019m not missing something, my impression is that it is relying mostly on limited empirical results on MNIST. \n\nPros:\n- The topic is quite interesting and has significant applications.\n- The paper is generally well-written.\n\nCons:\n- This seems to be mostly a relatively limited empirical evaluation, so I'm not sure one could draw any major conclusions yet.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "abstract": "There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with deep generative models, and verifies its robustness against a number of existing attacks. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers.", "paperhash": "li|are_generative_classifiers_more_robust_to_adversarial_attacks", "keywords": ["generative models", "adversarial attacks", "defences"], "_bibtex": "@misc{\n  li2018are,\n  title={Are Generative Classifiers More Robust to Adversarial Attacks?},\n  author={Yingzhen Li},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVmRByPG}\n}", "authorids": ["yl494@cam.ac.uk"], "authors": ["Yingzhen Li"], "TL;DR": "We show initial evidence that generative classifiers (using conditional DGMs) might be more robust to recent attacks than DNN-based discriminative classifiers.", "pdf": "/pdf/0774b205fd32d00d1fa708e7415a74b8cbc6e950.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582856212, "id": "ICLR.cc/2018/Workshop/-/Paper165/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper165/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper165/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper165/AnonReviewer3"], "reply": {"forum": "BkVmRByPG", "replyto": "BkVmRByPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper165/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582856212}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573585615, "tcdate": 1521573585615, "number": 182, "cdate": 1521573585275, "id": "H1c0ARRFM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BkVmRByPG", "replyto": "BkVmRByPG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Are Generative Classifiers More Robust to Adversarial Attacks?", "abstract": "There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with deep generative models, and verifies its robustness against a number of existing attacks. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers.", "paperhash": "li|are_generative_classifiers_more_robust_to_adversarial_attacks", "keywords": ["generative models", "adversarial attacks", "defences"], "_bibtex": "@misc{\n  li2018are,\n  title={Are Generative Classifiers More Robust to Adversarial Attacks?},\n  author={Yingzhen Li},\n  year={2018},\n  url={https://openreview.net/forum?id=BkVmRByPG}\n}", "authorids": ["yl494@cam.ac.uk"], "authors": ["Yingzhen Li"], "TL;DR": "We show initial evidence that generative classifiers (using conditional DGMs) might be more robust to recent attacks than DNN-based discriminative classifiers.", "pdf": "/pdf/0774b205fd32d00d1fa708e7415a74b8cbc6e950.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}