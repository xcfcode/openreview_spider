{"notes": [{"id": "MDX3F0qAfm3", "original": "OhdZq0vlVh1", "number": 1112, "cdate": 1601308125022, "ddate": null, "tcdate": 1601308125022, "tmdate": 1614985629248, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HuuG-XJWZPH", "original": null, "number": 1, "cdate": 1610040531058, "ddate": null, "tcdate": 1610040531058, "tmdate": 1610474140620, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Dear authors,\n\nThank you for your submission. The reviewers all appreciated the direction of research and the message that GN can be a bad measure of generalization. That said, they all shared concerns regarding the strength of the conclusions that can be drawn from your work.\n\nI encourage you to address their comments and submit a revised version to a later conference.\n\n---------------------------------\nReviewer 1 wanted to update their review but couldn't so here is the update:\n\n\nSome more details on my original concerns\n\nThank you for your detailed responses. I wanted to add more details to the ones not discussed by other reviewers.\n\n- Regarding the speed of computing the gradient norm, I still don't agree that the computation cost is high. Figure 6 in the Backpack paper shows the cost of computing individual gradients at most 4x the cost of a single backprop not 100-1000x. In reference [2] that I gave, there is also a cheaper approximation discussed with computational costs detailed in Appendix B. As long as the computation of gradient norm is comparable with the cost of a single back-prop it should be cheap enough to run all your experiments.\n\n- Regarding the conclusions in the paper. Thank you for giving more details. Adding those explanations to the paper would help. I personally missed some of those takeaway messages.\n\nOverall, I strongly recommend either strengthening the link between GN and AGN or using better approximations. As well as better discussing the conclusions. Of course in addition to the suggestions by other reviewers."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040531045, "tmdate": 1610474140604, "id": "ICLR.cc/2021/Conference/Paper1112/-/Decision"}}}, {"id": "h5pb0oyBc6", "original": null, "number": 4, "cdate": 1603858685368, "ddate": null, "tcdate": 1603858685368, "tmdate": 1606799888028, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review", "content": {"title": "Gradient norm is interesting to be studied, but the specific approximation proposed does not seem to be a good.", "review": "Summary:\nThis paper studies the gradient norm as a measure of generalization in deep learning. The authors first an approximation to the gradient norm (GN) that is the norm of the gradients for only fully connected layers (AGN). Then they empirically evaluate the correlation between AGN and GN as well as GN and the generalization error. In Section 2.1, the authors conclude that AGN is highly correlated with GN and both are correlated with generalization error. In Section 3, the authors conclude that the correlation between AGN and generalization error is not consistent in a wider family of models. In Section 4, authors propose to use AGN for model selection and conclude that AGN is not good for model selection unless the hyperparameter for mixing AGN with another metric is optimal.\n\nPros:\n- Recently there has been huge interest in measures of generalization error. Studies of this sort even with mixed results can be helpful in better evaluation of individual proposed measures.\n- Gradient norm is a measure proposed by prior published work (Li et al. (2020)) with theoretical justifications without extensive empirical evaluations which is done in this work.\n\nCons:\n- The main limitation of this work is that the authors propose an approximate measure to gradient norm (AGN), then the results are mostly negative for AGN in Section 3 and 4. But this doesn\u2019t necessarily mean gradient norm is a poor measure of generalization. The link in Section 2 is only empirically shown for a MNIST, Fashion-MNIST, and CIFAR-100 datasets and there is no theoretical reason to justify this. So I don\u2019t find the results to be conclusive about gradient norm.\n- Gradient norm can be computed efficiently in much less time than the claimed time in the paper. See [1] for a framework that computes gradient norm efficiently and [2] for general dot products between gradients.\n- Empirical results in Fig 1 are the main support of the paper for using the approximate gradient norm in the next sections. It is not clear if the conclusions from MNIST, Fashion-MNIST, and CIFAR-100 datasets would hold for other datasets. Especially that the conclusion in future sections seems to be negative for the application of AGN.\n\nAdditional notes:\n- In abstract given that only an approximation to gradient norm is computed the following statement cannot be made: \u201c...gradient norm also fails to predict the generalization performance\u201d.\n- Fig 1, a-d: the norm of the gradient on MNIST should tend to zero by the end of the training as the loss tends to 0. Despite that, the range of values for AGN and GN in this figure is way above 100. In contrast, GN in Fig 1.e is below 1 which matches the expectation. Have models in a-d converged?\n- The authors could also look into recent work on \u201crobust\u201d generalization measures [3]. AGN seems not to be a robust measure based on Figure 3. The authors could study better approximations to gradient norm and their robustness.\n\nTypos:\n- focuses -> focus\n- some advanced measure -> measures\n-  Thomas et al. (2019) did they derive TIC? Or only try it out?\n- correlates to -> with\n-  Thomas et al. (2019) is a posterior measure -> TIC is a\n- our work make -> makes\n- technical contributions -> contribution\n\n[1] Dangel, F., Kunstner, F., & Hennig, P. (2019). BackPACK: Packing more into backprop. arXiv preprint arXiv:1912.10985.\n[2] Faghri, F., Duvenaud, D., Fleet, D. J., & Ba, J. (2020). A Study of Gradient Variance in Deep Learning. arXiv preprint arXiv:2007.04532.\n[3] Dziugaite, G. K., Drouin, A., Neal, B., Rajkumar, N., Caballero, E., Wang, L., ... & Roy, D. M. (2020). In Search of Robust Measures of Generalization. arXiv preprint arXiv:2010.11924.\n\n============\nAfter rebuttal:\nI thank authors for their response. I share concerns with others reviewers and I highly encourage authors to consider answering questions suggested by Reviewer 3 at the end of their discussion. I believe a systematic study of gradient norm is interesting but this work does not provide a solid set of answers. As such, I'm reducing my rating to 4.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126636, "tmdate": 1606915808736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1112/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review"}}}, {"id": "zd9Sh8CeT2", "original": null, "number": 2, "cdate": 1603780445321, "ddate": null, "tcdate": 1603780445321, "tmdate": 1606776279097, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review", "content": {"title": "Sum of Gradient Norms as Measure for Generalization", "review": "The paper empirically investigates the sum of gradient norms as a measure to determine the generalization abilities of a neural network. The approach is inspired by the theoretical work of Li, et al. 2020 which showed that the generalization gap can be upper bounded by a function of the sum of the full gradient norms of the training path. \n\nThe paper empirically investigates the gradient norm as a measure to predict the generalization gap of neural networks. For that, the paper proposes an approximation of the sum of the full gradient norms that is more computationally efficient. The paper shows that this approximation is, albeit being in a different scale, strongly correlated with the full gradient measure. The paper then continues to openly investigate the properties of the measure and finds that it is only partially correlated with the generalization gap. When used for hyperparameter tuning, it can slightly improve the results. Lastly, the paper shows that the measure is not effective in predicting the generalization ability of DNNs with different architectures. \nThe empirical evaluation is sound (although limited to only fairly simple image classification problems), the results are interesting, and I particularly regard the honesty about the negative results. I think that this is a valuable study. \n\nWhat is missing though is a discussion of these findings and a placement of these results into the wider context of generalization in deep learning. Do these results imply that the gradient norm is not a sufficient measurement? Or can the gradient norm be an effective measure in certain situations? The paper hints at that, saying that for well-trained networks the measure correlates more strongly with the generalization gap. \n\nA particularly interesting discussion here would be the relation to flatness measures which have been found to be strongly correlated with the generalization gap [1,2,3,4,5]. It seems, the gradient path norm does not necessarily outperform those measures but might be related. I.e., if a model is in a very flat minimum, at least for a part of its training path (the last part) it will have been in that flat region and thus gradient norms would be small. \n\nIn summary, the paper presents an interesting empirical analysis and makes some technical contributions in the form of the proposed AGN measure. However, the missing in-depth discussion limits the contribution of this paper. Since such a discussion would go beyond the minor edits for a CRC, I am not convinced this paper is ready for publication, yet.\n\nMore out of interest than as a critique of this paper, I would like the authors to clarify how the gradient norm could be a meaningful measure to determine generalization in general. Li, et al. 2020 have shown a modified PAC-Bayesian bound that uses the gradient norm over the path, but it seems that most of the heavy lifting there is done by assuming a distribution dependent (i.e., perfect) prior - please correct me if I am wrong here. It seems to me then that the gradient norm can at best be a part of the explanation for good generalization (i.e., the one that relates to flatness and thus robustness). To illustrate my point, assume your data is drawn with x uniform in R and y = cos(v*x) + eps, where eps is some Gaussian noise. That is, the data is a noisy cosine function. Our model class consists of functions f_w (x) = cos(w*x) with a single parameter w. The optimal model has parameter w^*=v. Now the error surface of that model space if we use, for example, the squared loss, has lots of local minima of various depths and one global minimum at w^*=v (both in terms of empirical risk and true risk). If we now initialize our model randomly, it can start at a steep part of a local minimum and jump into the next and from there on to the next with fairly large gradients until it ends up (hopefully) in the global minium, where it will not escape (all also depending on the learning rate, of course). In this hypothetical (and arguably quite artificial) example, the path from most initializations to the global minimum would have large gradient norms. If we instead initialize close to a local minimum, the gradients will be small and if we start close enough to the minimum, the model will not escape. It will remain in that locally flat area with very small gradients. Thus, the gradient norm of the path is very small, yet the generalization error is large (the gap then depends on the actual sample and can be either small or quite large). My question now is: am I misunderstanding Li et al. or is my example too simple? Could the authors give some intuition on how the gradient norm could explain generalization?\n\n\n[1] Jiang, et al. Fantastic generalization measures and where to find them. ICLR 2020.\n[2] Keskar, et al. On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017.\n[3] Petzka, et al. Relative Flatness and Generalization in the Interpolation Regime. arxiv prerpint, 2020.\n[4] Neyshabur, et al. Exploring generalization in deep learning. NIPS, 2017.\n[5] Tsuzuku, et al. Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using PAC-Bayesian analysis. arxive preprint, 2019.\n\n\n------ After Discussion -------\nI had an interesting discussion with the authors that clarified some important points. I came to the conclusion that the topic is interesting and the study is valuable, however, not yet conclusive. Consequently, the paper is not ready for publication, yet. Thus I have lowered my score by one. I hope the authors continue this work, since I'm convinced a complete empirical study on the impact of the gradient norm along the training path is insightful and a valuable contribution to the community.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126636, "tmdate": 1606915808736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1112/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review"}}}, {"id": "oNkFQj_Ycvx", "original": null, "number": 9, "cdate": 1605860256175, "ddate": null, "tcdate": 1605860256175, "tmdate": 1605861790502, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "dRGwjlLK88-", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Our contributions and their significance", "comment": "Thank you very much for your follow-up question. Our contribution is re-iterated as follows. There are previously theoretical works that demonstrate gradient norm as a measure of generalization [4, 5]. Although they have extensive and convincing theoretical proofs that show GN is a sound generalization metric, there is a lack of empirical study to verify (1) whether the metric behaves as expected in practice and (2) whether it could be employed as a generalization metric efficiently in practice. Our work fills this gap by \n1.\tProposing AGN, a fast and effective proxy that makes it possible to use GN in practice (main results are shown in Fig. 1 and 2 in the manuscript).\n2.\tConducting extensive experiments to validate in what cases the metric performs well and when it does not behave as expected (shown in Fig. 3 in the manuscript).\n3.\tEvaluating the strengths and shortcomings of using GN/AGN in practice in the context of hyper-parameter search and cross-architectural model selection (shown in Table 1, 2, and 3 in the manuscript).\n\nWe believe the above contributions are significant because\n1.\tThe proposed approximation for GN reveals an interesting correlation between the FC layer gradient norm and the full network gradient norm. This observation might spark further interests to explore a theoretical understanding that explains the relations of gradients across different layers of a neural network model.\n2.\tWe provide empirical verification that is not offered in the previous theoretical papers [4, 5] about gradient norm. We confirm that GN is a good metric which correlates to generalization gap in a wide range of experiments when models are well-fitted. We also observe that, when models are under-fitted, the metric is not applicable. This is a surprising observation to some people because of the opposing trend observed. As a result, it might motivate researchers to generate novel insights into gradient norm as a generalization metric.\n3.\tWe demonstrate that when runtime is not a concern and without considering other state-of-the-art generalization measures, AGN/GN could be used in two different scenarios of model selection (i.e. hyper-parameter search for a fixed architecture and cross-architectural model selection). However, the computational overhead is heavy, causing it to be an inefficient metric in practice. Our empirical study helps practitioners avoid repeating such resource-consuming experiments. Furthermore, these limitations would motivate researchers to improve the current gradient metric with the hope to discover a more efficient one.\n\nIn essence, our work reveals the fact that a sound theory might not be as appealing as it is imagined to be in practice. We strongly believe realizing the insufficiency of a theoretical result when putting into practical use is also a step forward in our community. Moreover, our findings would stir new research directions. We hope you agree with us regarding these significances. We will address your concern and emphasize the significance of our contributions in the revised manuscript. \n\nIf you have more comment, please do not hesitate to post here. We will be happy to have further discussion with you. Thank you for your time.\n\n[4] Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In International Conference on Learning Representations, 2020.\n[5] Mou W, Wang L, Zhai X, et al. Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints. Conference on Learning Theory. 2018: 605-638."}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "dRGwjlLK88-", "original": null, "number": 8, "cdate": 1605831366550, "ddate": null, "tcdate": 1605831366550, "tmdate": 1605831366550, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "Pt2yViZ1E3n", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Thanks for your reply", "comment": "Thank you for your reply. If I understand it correctly, then AGN is not useful in practice: it improves parameter evaluation at the cost of runtime, but is not competitive with other metrics. At the same time, it remains unclear whether AGN or GN could even be a sound metric. Thus, I am unsure what the contribution of this paper then is. Maybe I am misunderstanding you, though. Thus, may I ask you to elaborate on your contribution and why it is significant?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "TWspKlLaIo4", "original": null, "number": 7, "cdate": 1605598813696, "ddate": null, "tcdate": 1605598813696, "tmdate": 1605619595367, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "YuUvrJcsl_C", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "Many thanks for your constructive comments. We believe all your comments could greatly help us improve the manuscript. We are now working hard in revising the manuscript to address your concerns. The updated version will be available shortly. Here goes a quick response to your comments. We hope our response clarifies your questions. We hope the manuscript still can receive your full consideration for acceptance.\n\n1.\tWe realize that the plots using MLP on MNIST are not the most representative of the correlation between GN/AGN and generalization gap because MNIST could be easily overfitted after training 40 epochs. We still refer to Fig. 3 in the manuscript as our main empirical result for the correlation between AGN and generalization in practice. We will update the manuscript to address your concern. The updated version will be made available soon.\n2.\tIn Fig. 1(e) and (f), the differences in Pearson coefficients are mainly due to the varying scales of the plots. The Pearson coefficients are the lowest in the first 80 epochs because there are points in the early optimization path that have significantly larger values for both GN and AGN. The existence of several such extreme values causes the scale to be wide in the leftmost plot, which in turn causes Pearson coefficients to be lower. In contrast, the Pearson coefficients are both nearly 1 for epochs 80-120 and 121-160 because both GN and AGN tend to stabilize around a small value as the model gets closer to convergence.\n\nPlease feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "Pt2yViZ1E3n", "original": null, "number": 6, "cdate": 1605598701146, "ddate": null, "tcdate": 1605598701146, "tmdate": 1605619556290, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "zd9Sh8CeT2", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "Many thanks for your constructive comments. We believe all your comments could greatly help us improve the manuscript. We are now working hard in revising the manuscript to address your concerns. The updated version will be available shortly. Here goes a quick response to your comments. We hope our response clarifies your questions. We hope the manuscript still can receive your full consideration for acceptance.\n\nWe would like to summarize our conclusion about the experimental results again. GN is inherently not a worthwhile indicator of generalization due to its heavy computational cost. We thus proposed AGN as a fast and effective proxy for GN. Without considering (1) any other prior-based generalization metrics or (2) computation budget, we found that the use of AGN can help search the hyper-parameters with close performance to the ones searched by validation set. Even with validation set included in the search, AGN+Val could be slightly better than the one using validation set only. All the above experiments are based on blackbox optimization of hyper-parameters (shown in Table 1). In terms of bandit-based search which reduces computational cost, AGN did not demonstrate any advantages, no matter whether a validation set is used. After all, we conclude that when computational budget is not a concern, AGN has the potential to complement the validation set but is still not comparable to other prior-based metrics. \n\nTo put GN/AGN in a wider context, we agree that GN/AGN does not necessarily outperform other state-of-the-art metrics such as the flatness of local minimum where the model converges to. In fact, we have compared the effectiveness of AGN with a few other generalization metrics in section 4.2. The comparison results demonstrate the incompetence of AGN compared to flatness metric.\n\nWe note your final question in the comment. Unfortunately, it is out of our ability to answer the question though it is very interesting. This question is more relevant to [4] rather than ours. \n\nPlease feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript.\n\n[4] Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In International Conference on Learning Representations, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "10DuQsYl0Je", "original": null, "number": 5, "cdate": 1605598586654, "ddate": null, "tcdate": 1605598586654, "tmdate": 1605619515548, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "6nfM2UmeeF7", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Thank you for your question and comments", "comment": "Many thanks for your question and comments. We believe all your comments could greatly help us improve the manuscript. We are now working hard in revising the manuscript to address your concerns. The updated version will be available shortly. Here goes a quick response to your comments. We hope our response clarifies your questions. We hope the manuscript still can receive your full consideration for acceptance.\n\nWe know that [4] established generalization upper bound for models trained by stochastic gradient Langevin dynamics (SGLD). We believe SGLD is a good proxy model to analyze stochastic gradient descent and a lot of assumptions. There are other works that derive theoretical bounds for non-convex learning by using SGLD as a convenient starting point for analysis [5, 6, 7]. SGLD is helpful for researchers to develop an operable theoretical bound. In practice, the applications in deep learning today tend to use SGD rather than SGLD. Our experiments have shown that when models are well-fitted, the bound remains applicable even if we use SGD.\n\nThe title of our paper is \u201cCan We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?\u201d We thus adopt a pathway of systematic empirical studies, where we first analyze the correlation between generalization gap and gradient norms in an ad hoc manner. Then we present comprehensive experimental studies to investigate the feasibility of using gradient norm for hyper-parameter search in a black-box manner (through embedding the gradient norm metrics in the search objectives of commonly used hyper-parameter search frameworks). The experimental results contribute to the community as a test of theories and reveal the gap between a theoretic metric and its helpfulness in practice. GN is a good metric that correlates to generalization gap in a wide range of experiments. Further, it would be a good search objective when computational budget is not an issue. This finding would potentially motivate the community to improve gradient norm as a better and more efficient generalization metric.\n\nPlease feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript.\n\n[4] Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods for non-convex learning. In International Conference on Learning Representations, 2020.\n[5] Mou W, Wang L, Zhai X, et al. Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints. Conference on Learning Theory. 2018: 605-638.\n[6] Negrea J, Haghifam M, Dziugaite G K, et al. Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates. Advances in Neural Information Processing Systems. 2019: 11015-11025.\n[7] Pensia A, Jog V, Loh P L. Generalization error bounds for noisy, iterative algorithms[C]//2018 IEEE International Symposium on Information Theory (ISIT). IEEE, 2018: 546-550.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "zJ73WJcLLym", "original": null, "number": 4, "cdate": 1605598441881, "ddate": null, "tcdate": 1605598441881, "tmdate": 1605619491696, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "h5pb0oyBc6", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment", "content": {"title": "Thank you for your comments", "comment": "Many thanks for your constructive comments on the manuscript. We believe all your comments could greatly help us improve the manuscript. We are now working hard in revising the manuscript to address your concerns. The updated version will be available shortly. Here goes a quick response to your comments. We hope our response clarifies your questions. We hope the manuscript still can receive your full consideration for acceptance.\n\nGN is inherently not a worthwhile indicator of generalization due to its heavy computational cost. It is not even feasible to conduct experiments on large-scale datasets such as CIFAR-10/100 using GN (even with Backpack). Motivated by this limitation, we propose AGN as an efficient approximation to GN. This approximation is shown to be very efficient in our experiments. For example, our proposed AGN (based on [3]) could be 200\u223c20,000 times faster than the na\u00efve implementation of the gradient norm estimator, as illustrated in Fig. 2. In comparison, other implementations like Backpack only improves the na\u00efve approach by a factor of 10. Actually, both BackPack[1] and AGN are derived from [3]. We believe the time consumption of BackPack is still unacceptable for estimating the sum of gradient norms over a complete optimization path. Our empirical experiments have demonstrated the viability of using AGN to approximate GN in practice. The theoretical understanding of why AGN approximates GN well is out of the scope of this paper for an empirical study. Nonetheless, this problem remains an important open direction for future research.\n\nSorry for misleading in the manuscript. We are not arguing that GN completely fails to predict generalization performance. We meant to say the effectiveness of GN in predicting generalization across architectures is largely limited in comparison with other state-of-the-art metrics. Other generalization indicators have greater potential to be employed in practice than GN. Without considering (1) any other prior-based generalization metrics or (2) computation budget, we found that the use of AGN can help search the hyper-parameters with close performance to the ones searched by validation set. Even with validation set included in the search, AGN+Val could be slightly better than the one using validation set only. All the above experiments are based on blackbox optimization of hyper-parameters (shown in Table 1). In terms of bandit-based search which reduces computational cost, AGN did not demonstrate any advantages, no matter whether a validation set is used. After all, we conclude that when computational budget is NOT an issue at all, AGN has the potential to complement the validation set but is still not comparable to other prior-based metrics. \n\nFig.1 (a) \u2013 (d) shows the total GN and AGN along the optimization path respectively (i.e. summation of GN or AGN in 40 epochs) whereas Fig. 1(e) and (f) show GN and AGN in every single epoch. Fig.1 (a) \u2013 (d) demonstrates the viability of using AGN as a proxy of GN by directly following equation (4) in our paper. Fig. 1(e) and (f) prove the same thing from a different point of view: if AGN well approximates GN in each epoch, the summation AGN is also a good approximation of GN along the optimization path. Despite the different representations, the plots show the same conclusion that the correlation between GN and AGN is strong, consistent, and statistically significant.\n\nThank you very much for catching our typos. We will correct them in the updated manuscript, which will be made available soon.\n\nPlease feel free to comment on the thread of discussion and timely shepherd us for improving the manuscript.\n\n\n[1] Dangel, F., Kunstner, F., & Hennig, P. (2019). BackPACK: Packing more into backprop. arXiv preprint arXiv:1912.10985. \n[2] Faghri, F., Duvenaud, D., Fleet, D. J., & Ba, J. (2020). A Study of Gradient Variance in Deep Learning. arXiv preprint arXiv:2007.04532.\n[3] Ian Goodfellow. Efficient per-example gradient computations. arXiv preprint arXiv:1510.01799, 2015.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MDX3F0qAfm3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1112/Authors|ICLR.cc/2021/Conference/Paper1112/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Comment"}}}, {"id": "YuUvrJcsl_C", "original": null, "number": 1, "cdate": 1603694979995, "ddate": null, "tcdate": 1603694979995, "tmdate": 1605024527741, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review", "content": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice", "review": "In this paper, they provide the empirical studies  to understand the effectiveness and efficiency of the use of the gradient norm (induced by [the Li et al., 2020]) as the model selection criterion. To speed up the calculation process the of the gradient norm, they first propose an approximate gradient norm (AGN) based on the depth-wise, sample-wise and epoch-wise accelerations.  Their empirical studies find that the use of AGN can select the models with lower generalization error, but fails for bandit-based or population-based algorithms, and fails to predict the generalization performance of models based on different architectures.  In conclusion, they do not recommend using (approxiamte) gradient norm for model selection in practice.\n\nPros:\n\n-1:  In this paper, they propose an approximate gradient norm (AGN) based on an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully-Connected Layers, which can significantly reduce the computation cost (200\u223c20,000 times faster) than the original one. They also find in empirical evaluations that AGN and GN behave identically with respect to empirical generalization gap.\n\n-2: They carry out extensive experiments to validate the correlations between generalization performance and AGN, and find that, when the models are well-fitted, AGN well corresponds with the empirical generalization gap, but for the  under-fitted models, the correlations  between empirical generalization gap and AGN are not consistent.\n\n-3: They use AGN  as an objective for  hyper-parameter selection under both black\u0002box optimization and bandit-based search, and find that,  AGN may help black-box optimization algorithms with an additional hyper-parameter, but fails to bandit-based or population-based algorithms, and fails to predict the generalization performance of models based on different architectures.\n\nCons:\n\n-1: In Figure 1(a) and (c), we can find that for  Multi-Layer Preceptors (MLP), the trends of AGN behave identically with respect to the generalization gap, but, in Figure 3, they show that for ResNet-20, ResNet-56, ResNet-110 and DenseNet 100*24, the correlations between AGN and generalization gap are not consistent. These results confuse me, can we draw a new conclusion that, when the model is simple,  the  correlations  between AGN and generalization gap are consistent, but for complex models, the correlations are not consistent.  I want to see the additional experiments on the analysis of the MLP in Figure 3, I am surprised why ignoring the MLP in Section 3 and Section 4.\n\n-2:  In Figure 1(e) and (f), one can see that the Pearson coeff  for epoch 81-120 are higher than that of the Epoch 1-80 and Epoch 121-160. This result seems a bit counterintuitive. Why the middle epoch can obtain the highest Pearson coeff?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126636, "tmdate": 1606915808736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1112/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review"}}}, {"id": "6nfM2UmeeF7", "original": null, "number": 3, "cdate": 1603805278412, "ddate": null, "tcdate": 1603805278412, "tmdate": 1605024527535, "tddate": null, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "invitation": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review", "content": {"title": "empty", "review": "Authors of the paper conducted many numerical experiments on the relationship between generalization and their proposal on an approximated form of gradient norms. \n\nThe motivation for their study is the article Li et al. (2020) which discovers deep relation between generalization and gradient norms. However, as far as I can understand, what the article Li et al. (2020) precisely studies is a noisy version of gradient descent (stochastic gradient Langevin dynamics), and without the noisy assumption their theoretical analysis is not valid. Based on these, I am not convinced that studying the relation between generalization and a *variant* of gradient norms with respect to the *true* SGD is a proper topic that the ML community should consider. Many experiments in the gap should be carried on.\n\nMoreover, in 'contribution' (3) the authors make a digression discussing generalization and hyperparameter searching, and mark as one of the major contribution. The conclusion is that their 'approximated gradient norm' is not well-behaved and has many constrains in application, so I would mark these as merely observations rather than 'contributions'. I think this part is rather incomplete.\n\nTo me the article seems to be an experimental report on what has been observed during the numerical trials. It might be a good submission to workshop, but not qualify for ICLR main conference.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1112/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1112/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?", "authorids": ["~Haozhe_An1", "~Haoyi_Xiong1", "~Xuhong_Li3", "~Xingjian_Li1", "~Dejing_Dou1", "~Zhanxing_Zhu1"], "authors": ["Haozhe An", "Haoyi Xiong", "Xuhong Li", "Xingjian Li", "Dejing Dou", "Zhanxing Zhu"], "keywords": [], "abstract": "The recent theoretical investigation (Li et al., 2020) on the upper bound of generalization error of deep neural networks (DNNs) demonstrates the potential of using the gradient norm as a measure that complements validation accuracy for model selection in practice.  In this work, we carry out empirical studies using several commonly-used neural network architectures and benchmark datasets to understand the effectiveness and efficiency of using gradient norm as the model selection criterion, especially in the settings of hyper-parameter optimization. While strong correlations between the generalization error and the gradient norm measures have been observed, we find the computation of gradient norm is time consuming due to the high gradient complexity. To balance the trade-off between efficiency and effectiveness, we propose to use an accelerated approximation (Goodfellow, 2015)  of gradient norm that only computes the loss gradient in the Fully-Connected Layer (FC Layer) of DNNs with significantly reduced computation cost (200~20,000 times faster). Our empirical studies clearly find that the use of approximated gradient norm, as one of the hyper-parameter search objectives, can select the models with lower generalization error, but the efficiency is still low (marginal accuracy improvement but with high computation overhead). Our results also show that the bandit-based or population-based algorithms, such as BOHB, perform poorer with gradient norm objectives, since the correlation between gradient norm and generalization error is not always consistent across phases of the training process. Finally, gradient norm also fails to predict the generalization performance of models based on different architectures, in comparison with state of the art algorithms and metrics.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "an|can_we_use_gradient_norm_as_a_measure_of_generalization_error_for_model_selection_in_practice", "supplementary_material": "/attachment/c63313cc5119d5d976378b740cb30a86ce4f3c89.zip", "pdf": "/pdf/75683e4c0c6dedaf3e0936a0df3cf8817488130f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1JNvHKaa7O", "_bibtex": "@misc{\nan2021can,\ntitle={Can We Use Gradient Norm as a Measure of Generalization Error for Model Selection in Practice?},\nauthor={Haozhe An and Haoyi Xiong and Xuhong Li and Xingjian Li and Dejing Dou and Zhanxing Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=MDX3F0qAfm3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MDX3F0qAfm3", "replyto": "MDX3F0qAfm3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1112/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126636, "tmdate": 1606915808736, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1112/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1112/-/Official_Review"}}}], "count": 12}