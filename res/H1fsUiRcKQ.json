{"notes": [{"id": "H1fsUiRcKQ", "original": "SJxkQ4M9tX", "number": 206, "cdate": 1538087763298, "ddate": null, "tcdate": 1538087763298, "tmdate": 1545355376430, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgMoTgMxV", "original": null, "number": 1, "cdate": 1544846746081, "ddate": null, "tcdate": 1544846746081, "tmdate": 1545354532309, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Meta_Review", "content": {"metareview": "The paper combines the ideas of VAT and Bad GAN, replacing the fake samples in Bad GAN objective with VAT generated samples. The motivation behind using the K+1 SSL framework with VAT examples remains unclear, particularly in the light of Prop. 2 which shows smoothness of classifier around the unlabeled examples is enough (which VAT already encourages). R2 and R3 have raised the point of limited insight and lack of motivation behind combining VAT and Bad GAN objectives in this way. R2 and R3 are also concerned about the empirical results which show only marginal improvements over VAT/BadGAN in most settings. \n\nAC feels that the idea of the paper is interesting but agrees with R2/R3 that the proposed objective is not motivated well enough (what is the precise advantage of using K+1 SSL formulation with VAT examples?). The paper really falls on the borderline and could be improved if this point is addressed convincingly. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Potentially interesting idea but motivation remains somewhat unclear; marginal improvements in SSL performance"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper206/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353299171, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353299171}}}, {"id": "S1g-q0VK0Q", "original": null, "number": 7, "cdate": 1543224969203, "ddate": null, "tcdate": 1543224969203, "tmdate": 1543224969203, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "HyldOAVF0m", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "Response to reviewer #3 (cont.)", "comment": "- The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak. \n\nA. We agreed. Bad samples are not necessarily close to the decision boundary. However, in the conditions of Dai et al. (2017), they assumed that bad samples should locate in between true data classes where no true data exist. Hence, there is no problem when low density regions and the decision boundary coincide. A possible problem of FAT is that the estimated classifier may not locate at the low density region. In this case, generating bad samples using the estimated classifier may not work well. We know this problem and we agree that FAT only works when the initial classifier is reasonable. However, note that VAT term in FAT plays a key role to find a reasonable initial classifier as explained in Section 5 of the revised manuscript. \n\n- There is not ablation study of different terms in the objective function. \n\nA. We did ablation study about the various terms in the objective function of FAT in Section 6.3. We found that L^VAT is indispensable but the role of L^true is somehow mixture.\n\n- In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison.\n\nA. Thank you for your suggestion. As you recommended, we conducted Bad GAN for SVHN and CIFAR10 datasets by running the codes given in the github and added the results in Figure 4. The results again confirm that FAT achieves competitive results with much fewer training epoch and training time than other existing researches. In addition, the test accuracies of Bad GAN without PixcelCNN++ are significantly inferior to FAT (the accuracies of Bad GAN without PixcelCNN++ are similar to those of FM-GAN) even though we did not report them in the revised paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}, {"id": "HyldOAVF0m", "original": null, "number": 6, "cdate": 1543224944170, "ddate": null, "tcdate": 1543224944170, "tmdate": 1543224944170, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "rJeyUf5whm", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "Response to reviewer 3.\n\nWe thank you for your thoughtful review and comments. We give a response to each of your comments in cons and questions:\n\n- I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here? \n\nA. As you addressed, both Bad GAN and VAT encourage a decision boundary in the low-density region. Bad samples are assumed to locate at low density regions and they pull the decision boundary to the low density regions where they locate. In contrast, the VAT encourages the class probabilities of the true data not changing much. This is achieved if the decision boundary does not locate at the areas where the true data live. In this sense, we say that the VAT push the decision boundary from the high density regions.\n\n- Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy. \n\nA. FAT dominates vanilla VAT for all datasets we considered even if the margins are not super large. But, the margins of Bad GAN, which is known to be the state-of-art algorithm, are also not that super large. Considering the margins of Bad GAN with other competitors, we believe that the margins of FAT compared to vanilla VAT are significant for FAT to be a useful method. Also, we investigated the performances with fewer labeled samples and we found that the performance differences between FAT and vanilla VAT become larger, which is stated in Section 6.1. Regarding Bad GAN, FAT does not dominate Bad GAN but note that Bad GAN requires additional very computationally heavy algorithms such as PixcelCNN++ which is not even publicly unavailable. There is a Bad GAN algorithm in github which does not use PixcelCNN++. Bad GAN without PixcelCNN++ is almost the same as FM-GAN which is significantly inferior to FAT for SVHN and CIFAR10. \n\n- If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method? \n\nA. The main idea of FAT is to simplify Bad GAN by generating bad samples only by use of the current estimated classifier without generator. But, for this idea to work well, the current classifier should be reasonably well and VAT does something for this as explained by 5.1. On the other hand, we may think FAT as an improved version of VAT by adding Bad GAN idea. Theoretically, VAT can find a good classifier but it frequently happens that theoretical results are not realized in practice. Our contribution is that combining the idea of Bad GAN with VAT, we can realize the advantage of VAT more easily in real data.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}, {"id": "B1lxrAVYCX", "original": null, "number": 5, "cdate": 1543224888484, "ddate": null, "tcdate": 1543224888484, "tmdate": 1543224888484, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "SJxwMRNtRX", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "Response to reviewer #2 (cont.)", "comment": "- I think the biggest weakness of this paper is the experiments. Taking Table 1 at face value, the conclusion that FAT is simply competitive with existing approaches suggests that the additional machinery isn\u2019t particularly useful, providing little more than a vanilla VAT. \n\nA. FAT dominates vanilla VAT for all datasets we considered even if the margins are not super large. But, the margins of Bad GAN, which is known to be the state-of-the-art algorithm, are also not that super large. Considering the margins of Bad GAN with other competitors, we believe that the margins of FAT compared to vanilla VAT are significant for FAT to be a useful method. Also, we investigated the performances with fewer labeled samples and we found that the performance differences between FAT and vanilla VAT become larger.\n\n- I also think MNIST/SVHN has run its course as good semi-supervised learning benchmarks and would prefer to see such algorithms being scaled to more complex data. \n\nA. Thank you for your recommendation. We used the three well-known datasets first because VAT and bad GAN used these datasets. For FAT combines VAT and bad GAN, we thought that the comparisons of FAT with VAT and bad GAN over these datasets should be preceded. However, as you recommended, we applied FAT and modified VAT to CIFAR 100 and we found that FAT outperformed modified VAT, which is summarized in Section 6.1. Note that CIFAR 100 is very difficult to be classified and hence bad samples generated by the estimated classifier may not be \u2018good\u2019 bad samples. Based on this argument, we can conclude that FAT is not affected much from \u2018not good\u2019 bad samples. To sum up, FAT improves VAT and FAT is robust to \u2018not good\u2019 bad samples. \n\n- The main argument for why FAT should be prefered over VAT comes from Section 6.2. Figure 4 is more interesting, but is complicated by the fact that FAT checks both possible eigenvectors (+/- u) during training, which requires two forward passes in the classifier; did the authors give a similar treatment to VAT? Please show wall-clock time too. \n\nA. In the revised manuscript, we added the results of the modified VAT. We found that the modified VAT improves the vanilla VAT but still is dominated by FAT in the three benchmark datasets. In addition, we added results of computing times in Section 6.4 of the revised manuscript.\n\n- Unfortunately the computational efficiency gain seems to only hold true for MNIST/SVHN, but not for CIFAR. I worry that the observed gains will not sustain once we move to more complicated datasets.\n\nA. We redid comparison of computational complexity (based on the numbers of epochs to arrive at the prespecified test accuracies) and found that FAT is more efficient than the other competitors for not only MNIST and SVHN but also CIFAR10. See Section 6.4 for the results. Apparently, there are two reasons for these differences. First, in the initial experiment, we did not know some randomness of PyTorch besides the manual seed to control GPU and thus the results in the original manuscript would not be a fair comparison. In the revised manuscript, we fixed the randomness in Pytorch and did several analyses and averaged out the numbers of epochs. The second reason is that we found better hyperparameters for CIFAR10. Note that the accuracy of CIFAR10 in the revised manuscript is little bit better than that in the original manuscript (85.31 vs. 85.22). \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}, {"id": "SJxwMRNtRX", "original": null, "number": 4, "cdate": 1543224847424, "ddate": null, "tcdate": 1543224847424, "tmdate": 1543224847424, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "Bkxkf55Kh7", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "Response to reviewer #2", "comment": "Response to reviewer 2.\n\nThank you for your thoughtful review and comments. We give a response to each of your comments in cons and questions:\n\n- In particular more could be done to describe the terms in Equation 5. I\u2019m also curious about the behavior of L^true, which is equivalently the fourth term in Eq 1. Even when reading Bad GAN paper, I did not quite understand their claim that this can be correctly interpreted as a conditional entropy term (if they really wanted conditional entropy, they should probably have either done H(p(k|x)) or H(p(k|x, k <= K))). \n\nA. Yes, L^true is not a conditional entropy. But utilizing L^true is a more reasonable choice than H(p(k|x)) or H(p(k|x,k<=K)). \nDai et al. (2017) stated the following three sufficient conditions for Bad GAN to find a perfect classifier:\ni) A classifier classifies the labeled data perfectly, that is, argmax_k g_k(x;\\theta)=y for all (x,y) in labeled data.\nii) Unlabeled data are classified to real label, not bad label, that is, max_k g_k(x;\\theta)>0 for all x in unlabeled data. \niii) Bad data are classified to bad label, that is, max_k g_k(x;\\theta)<0 for all x in bad data.\n\nL^true is introduced to encourage the classifier to satisfy the condition ii). Our understanding about L^true is as follows: Maximizing H(p(k|x)) or H(p(k|x,k<=K)) does not guarantee the condition ii). Firstly suppose that we use H(p(k|x)) and there exists a real datum x such that the current classifier classifies x to the bad label, i.e. max_k g_k(x;\\theta)<0. Then minimizing H(p(k|x)) encourages max_k g_k(x;\\theta) to become negatively smaller which is undesirable for semi-supervised learning. On the other hand, using H(p(k|x,k<=K)) is also problematic since minimizing H(p(k|x,k<=K)) is nothing to do with max_k g_k(x;\\theta)>0 because H(p(k|x,k<=K)) just maximizes the relative differences between g_k(x;\\theta)s and hence there may exist a real datum x with large H(p(k|x,k<=K)) but negative max_k g_k(x;\\theta). Thus, L^true is more helpful than H(p(k|x)) or H(p(k|x,k<=K)) for Bad GAN.\n\n- I encourage the authors to do an ablation test to convince the reader that \u201cthis modification helps to improve convergence speed of the test accuracy.\u201d \n\nA. We investigated the effects of the modification and added these results in Table 1 and Figure 4. We can clearly see that the modified VAT does improve the test accuracy and the convergence speed but is inferior to FAT.\n\n- If the authors could extend this to more general non-linear classifiers (perhaps subject to some assumptions), that would be more interesting. \n\nA. We added a discussion about the relation of adversarial direction and bad samples for the DNN with ReLU activation function in Appendix.\n\n- I don\u2019t think Proposition 2 has any real value and recommend its relegation to the appendix. \n\nA. One of the main message of FAT is that the role of VAT is different from the role of bad samples. We believe that explanation of the role of VAT is a key ingredient of the proposed method and hence we decided to keep it in the main body. But we condensed it as much as possible.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}, {"id": "Hye43a4F0X", "original": null, "number": 3, "cdate": 1543224747535, "ddate": null, "tcdate": 1543224747535, "tmdate": 1543224747535, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "SyeJXc193X", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Response to reviewer 1.\n\nThank you for your valuable review and for highlighting the paper is well written. We give a response to each of your comments in cons and questions:\n\n- Requires additional hyperparmeter tuning in tau and rho. Tuning these with large validation sets can lead to an overoptimistic estimate of the generalization. How sensitive is the performance to these parameters? \n\nA. We did sensitivity studies with MNIST data in Section 6.2 in the revised manuscript. We found that the results are quite robust to the choice of \\tau unless \\tau is too small or too large. For \\delta, the performances degrade much when \\delta is too small, i.e. smaller than \\epsilon, but we found that the performance is similar when \\delta is close to \\epsilon. \n\n- As the authors point out, the method has limitations when the number of labeled samples is much smaller. It will be nice to see some results in this aspect. \n\nA. We did additional experiment with very few labeled data. Surprisingly, FAT performances better compared to the other competitors for MNIST and SVHN datasets. We added the results of the experiment in Section 6.1.\n\n- Please include more details to clarify what is meant by \u2018the role of the second term of (1) and (2) are overlapped\u2019.\n\nA. We changed the statement by \u201cWe delete the second term of (1) because it can be easily shown that a perfect classifier of labeled data can be obtained from the minimizer of the objective function (1) without the second term under the conditions in Dai et al. (2017).\u201d \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}, {"id": "SyeJXc193X", "original": null, "number": 3, "cdate": 1541171735019, "ddate": null, "tcdate": 1541171735019, "tmdate": 1541534195655, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "content": {"title": "A good paper on combining BadGAN and VAT for SSL", "review": "The authors propose to combine BadGAN framework and VAT to accelerate learning in the semi-supervised setting. The paper shows that the VAT approach is actively pushing the decision boundary away from the high-density regions. While the BadGAN approach pulls the decision boundary to low-density regions. This simultaneous push and pull lead to after convergence in testing accuracy. The authors also report competitive results on standard datasets used for SSL such as SVHN and CIFAR10.\n\nPositives:\nThe approach overcomes some of the difficulties with BadGAN which arise from training a GAN and density estimation network for generating \u201cbad samples\u201d useful for SSL. Instead of using a GAN, the proposed approach uses adversarial samples using VAT that are sufficiently confusing to the current estimate of the classifier. \nThe theoretical justifications for the VAT interpretation are interesting and convincing. The visualizations of the bad samples show qualitatively that the bad samples from the BadGAN and proposed approach differ. Several other visualization aids in understanding the behavior of the algorithm.\n\nNegatives:\nRequires additional hyperparmeter tuning in tau and rho. Tuning these with large validation sets can lead to an overoptimistic estimate of the generalization. How sensitive is the performance to these parameters? \nAs the authors point out, the method has limitations when the number of labeled samples is much smaller. It will be nice to see some results in this aspect.\nPlease include more details to clarify what is meant by \u2018the role of the second term of (1) and (2) are overlapped\u2019.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "cdate": 1542234514965, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335672171, "tmdate": 1552335672171, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkxkf55Kh7", "original": null, "number": 2, "cdate": 1541151238842, "ddate": null, "tcdate": 1541151238842, "tmdate": 1541534195375, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "content": {"title": "Some interesting observations, but not quite convincing just yet", "review": "This paper makes the interesting observation that the generative procedure proposed by Bad GAN paper can be replaced by a slightly modified VAT procedure. The reasoning is sound and leverages the intuition that adversarial examples (subject to a sufficiently small perturbation radius) are likely to be closer to a decision boundary than the original sample. \n\nThe paper is generally easy to follow but the presentation could be improved. In particular more could be done to describe the terms in Equation 5. I\u2019m also curious about the behavior of L^true, which is equivalently the fourth term in Eq 1. Even when reading Bad GAN paper, I did not quite understand their claim that this can be correctly interpreted as a conditional entropy term (if they really wanted conditional entropy, they should probably have either done H(p(k|x)) or H(p(k|x, k <= K))). I agree with the authors that the roles of the second and fourth terms overlapped, and I think this is sufficiently interesting to warrant some further elaboration in the paper. I also liked the reminder that power iteration selects a non-unique sign for the first eigenvector (subject to the random vector initialization); I encourage the authors to do an ablation test to convince the reader that \u201cthis modification helps to improve convergence speed of the test accuracy.\u201d\n\nThe propositions in this paper were, in my opinion, not particularly insightful. While I think it is nice that the authors went through the effort of providing some formalism to the intuition that VAT has a \u201cpush decision boundary away from high-density regions\u201d, I\u2019m less sure if propositions 1 and 2 really provides any additional insight the behavior of VAT. Proposition 1 is pretty weak in that it only covers a 2-class logistic regression; it seems obvious that the adversarial perturbation points in the direction toward the decision hyperplane. If the authors could extend this to more general non-linear classifiers (perhaps subject to some assumptions), that would be more interesting. I don\u2019t think Proposition 2 has any real value and recommend its relegation to the appendix.\n\nI think the biggest weakness of this paper is the experiments. Taking Table 1 at face value, the conclusion that FAT is simply competitive with existing approaches suggests that the additional machinery isn\u2019t particularly useful, providing little more than a vanilla VAT. I also think MNIST/SVHN has run its course as good semi-supervised learning benchmarks and would prefer to see such algorithms being scaled to more complex data. The main argument for why FAT should be prefered over VAT comes from Section 6.2. Figure 4 is more interesting, but is complicated by the fact that FAT checks both possible eigenvectors (+/- u) during training, which requires two forward passes in the classifier; did the authors give a similar treatment to VAT? Please show wall-clock time too. Unfortunately the computational efficiency gain seems to only hold true for MNIST/SVHN, but not for CIFAR. I worry that the observed gains will not sustain once we move to more complicated datasets.\n\n\nPros:\n+ Simple and clean proposal\n+ Easy to read\nCons:\n- Limited insight\n- Weak experiments", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "cdate": 1542234514965, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335672171, "tmdate": 1552335672171, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeyUf5whm", "original": null, "number": 1, "cdate": 1541018182976, "ddate": null, "tcdate": 1541018182976, "tmdate": 1541534195032, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "content": {"title": "review", "review": "The paper proposes to use the technique in VAT to generate adversarial complementary examples in the (K+1)-class semi-supervised learning framework described by the Bad GAN paper. This leads to a formulation that combines the VAT loss and the (K+1)-class classification loss. The paper also provides analysis regarding why VAT is useful for semi-supervised learning.\n\nPros\n1. It is interesting to bridge two state-of-the-art semi-supervise learning methods in a meaningful.\n2. Some positive results have been presented in Table 1 and Figure 4.\n\nCons and questions\n1. I don't understand the authors' claim that FAT uses both pushing and pulling operations. It might be true that both Bad GAN and VAT encourage a decision boundary in the low-density region, but how are they different? Are pushing and pulling really different things here?\n2. Unfortunately the proposed method does not give substantial improvement over Bad GAN or VAT in terms of accuracy.\n3. If using VAT to generate bad samples is a reasonable approach, then based on the theory in Dai et al., the Bad GAN formulation would not need the additional VAT regularization term to guarantee generalization. On the other hand, based on the theory of Proposition 2, VAT itself should be sufficient. Why do we still need the (K+1)-class formulation. It seems that combination of Bad GAN and VAT objectives has not been well motivated or fully justified. Does this explain the fact that not much empirical gain was obtained by this method?\n4. The authors try to use Proposition 1 to motivate the use of VAT for generating complementary examples. However, it seems that the authors misinterprets the concept of bad examples proposed in Dai et al. The original definition (which led to the theoretical guarantees in Dai et al) of bad examples is low-density data samples. In the current paper, the authors assume that data samples close to decision boundaries are bad examples. This is not sound because low-density samples are not equivalent to samples close to decision boundaries, especially when the classifier is less perfect. As a result, the theoretical justification of using VAT to sample complementary examples is a bit weak.\n5. There is not ablation study of different terms in the objective function.\n6. In Figure 4, you can compare your method with Bad GAN without a PixelCNN. Bad GAN does not need a PixelCNN to achieve the reported results in their paper, and their results are reproducible by running the commands given in the github repo. It would be good to add this comparison.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper206/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Review", "cdate": 1542234514965, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1fsUiRcKQ", "replyto": "H1fsUiRcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335672171, "tmdate": 1552335672171, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxf9k46nm", "original": null, "number": 2, "cdate": 1541386122293, "ddate": null, "tcdate": 1541386122293, "tmdate": 1541386122293, "tddate": null, "forum": "H1fsUiRcKQ", "replyto": "H1lIlywM2Q", "invitation": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "content": {"title": "No, it is not relevant", "comment": "Please convince me otherwise."}, "signatures": ["ICLR.cc/2019/Conference/Paper206/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper206/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast adversarial training for semi-supervised learning", "abstract": "In semi-supervised learning, Bad GAN approach is one of the most attractive method due to the intuitional simplicity and powerful performances. Bad GAN learns a classifier with bad samples distributed on complement of the support of the input data. But Bad GAN needs additional architectures, a generator and a density estimation model, which involves huge computation and memory consumption cost. VAT is another good semi-supervised learning algorithm, which\nutilizes unlabeled data to improve the invariance of the classifier with respect to perturbation of inputs.  In this study, we propose a new method by combining the ideas of Bad GAN and VAT. The proposed method generates bad samples of high-quality by use of the adversarial training used in VAT. We give theoretical explanations why the adversarial training is good at both generating bad samples and semi-supervised learning. An advantage of the proposed method is to achieve the competitive performances with much fewer computations. We demonstrate advantages our method by various experiments with well known benchmark image datasets.", "keywords": ["Deep learning", "Semi-supervised learning", "Adversarial training"], "authorids": ["dongha0718@hanmail.net", "pminer32@gmail.com", "jae-joon.han@samsung.com", "changkyu_choi@samsung.com", "ydkim0903@gmail.com"], "authors": ["Dongha Kim", "Yongchan Choi", "Jae-Joon Han", "Changkyu Choi", "Yongdai Kim"], "TL;DR": "We propose a fast and efficient semi-supervised learning method using adversarial training.", "pdf": "/pdf/ebec2b63b06ea2db76cce42a1ddff4cb0a16b658.pdf", "paperhash": "kim|fast_adversarial_training_for_semisupervised_learning", "_bibtex": "@misc{\nkim2019fast,\ntitle={Fast adversarial training for semi-supervised learning},\nauthor={Dongha Kim and Yongchan Choi and Jae-Joon Han and Changkyu Choi and Yongdai Kim},\nyear={2019},\nurl={https://openreview.net/forum?id=H1fsUiRcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper206/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611386, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1fsUiRcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper206/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper206/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper206/Authors|ICLR.cc/2019/Conference/Paper206/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper206/Reviewers", "ICLR.cc/2019/Conference/Paper206/Authors", "ICLR.cc/2019/Conference/Paper206/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611386}}}], "count": 11}