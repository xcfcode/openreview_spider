{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396414758, "tcdate": 1486396414758, "number": 1, "id": "S1PZ2f8de", "invitation": "ICLR.cc/2017/conference/-/paper177/acceptance", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "While this is interesting work, one major concern comes from Reviewer 2 regarding the attempt of characterizing the tuning properties, which has proven useless both in neuroscience and in machine learning. Currently, no attempt so far has lived up to the promises this line of research is aiming for. In summary, this work is explorative and incremental but worthwhile. We encourage the authors to further refine their research effort and resubmit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396415301, "id": "ICLR.cc/2017/conference/-/paper177/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396415301}}}, {"tddate": null, "tmdate": 1486108650303, "tcdate": 1486107817193, "number": 7, "id": "HybnVh-ug", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["~Anh_Nguyen1"], "readers": ["everyone"], "writers": ["~Anh_Nguyen1"], "content": {"title": "Some connections/cites", "comment": "Hi authors,\n\nCongrats for an interesting submission! I wonder what other selectivity indices you could use beside color and class.\n\nI just noticed that your definition of Neuron Feature (weighted average image) is very close to what we use in this paper (an average image):\n\nhttp://www.evolvingai.org/mfv\n\nAlso you might want to know about or discuss the connection between your work and our class visualization here:\nhttp://www.evolvingai.org/synthesizing\n\nSo we show that you can synthesize an image that activates two neurons at the same time (Fig. S11, S12). I think it would be interesting if you could compare them with your techniques. Do we get a real image that contains both objects or mostly just one?\n\nCheers,\n\nAnh"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1484875107995, "tcdate": 1484875107995, "number": 2, "id": "Sy2DBk1ve", "invitation": "ICLR.cc/2017/conference/-/paper177/official/comment", "forum": "SJBr9Mcxl", "replyto": "HJfMrHIIg", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "content": {"title": "Keep pushing", "comment": "It's a thorough investigation of the red-selective vs. mushroom-selective issue. I'll leave my review the same at \"7: accept\" as I think it already reflects the largely positive view of the paper.\n\nMany of the above observations seem like a work in progress (much data, little conclusion). As is they're of limited use; however, they're exactly the types of subtle (and difficult to communicate) observations that lead to great discoveries. I'd recommend that the authors keep pushing (whether or not this paper is accepted)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698494, "id": "ICLR.cc/2017/conference/-/paper177/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698494}}}, {"tddate": null, "tmdate": 1484309841818, "tcdate": 1484309769844, "number": 6, "id": "HJfMrHIIg", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "H1cc-_xSl", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "Thanks for the inspiring review and sorry for our delay in the reply.", "comment": "\nWe will answer in three separate sections to the final 3 comments: \n\nAbout the 1st comment of the reviewer. \n                 Yes, you are right the reference is wrong:\n\n                 \u201cThe cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\u201d.\n\n                 The authors of the reference are incorrect. We are going to correct it in  an updated version of the paper and we will probably \n                 change it to the posterior PAMI paper on a similar work.\n\n                 Alexey Dosovitskiy, Jost Springenberg, Maxim Tatarchenko, Thomas Brox\n                 Learning to Generate Chairs, Tables and Cars with Convolutional Networks\n                 IEEE Trans. On PAMI, 2016\n\n\n\nAbout the 2nd comment of the reviewer (it is separated in 3 questions/comments)\n\n    ** Reviewer\u2019s first question: \u201c What exactly is the Color Selectivity Index computing?\u201d \n\n                  Color selectivity index is defined to represent with a single number, the level of activation of a neuron when a specific color is\n                  present in the input image.\n\n\n    ** Reviewer\u2019s second question: \u201cThe Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me.\u201d\n\n                 The color-opponent space is a 3D space that has one axis representing intensity information (grey-level or black-white axis) and two\n                 axes to represent chromaticity information, which is decomposed in the basis of red-green and blue-yellow axes. In color science,\n                 black and white is considered as the lack of color (achromatic). An easy way to understand the opponent space is relating the\n                 intensity axis with the diagonal line of the RGB cube that goes from black (0,0,0) to white (255,255,255), crossing all the greys\n                 (128,128,128) is the mean grey that can be seen as the (0,0,0) of the opponent space; the two chromaticity axes are on a plane that\n                 is orthogonal to this diagonal (intensity axis). In this sense, the opponent space transform can be seen as a translation plus two\n                 rotations from the usual RGB space. We did not put the definition since we assumed it was enough known in computer vision (In van de\n                 Sande, et-al PAMI-2010, they proved Opponent-SIFT as the best color descriptor for object recognition). However, we can add the\n                 definition in the text or in a footnote if you think it is necessary, or maybe we can add a citation to this PAMI about opponent\n                 sift where the opponent space is defined. \n\n    ** Reviewer\u2019s third comment: \u201cIntuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity\n                 NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge\n                 pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should \n                 probably be more clearly described.\u201d\n\n                 Considering the structure of the opponent color space, we have proposed to measure color selectivity index using the angle between \n                 the 1st principal component of the color distribution of the neuron feature and the intensity (black-white axis), the smaller is \n                 this angle the more achromatic is the neuron feature information, in this way we capture the bias of the neuron feature towards a \n                 chromatic component. A neuron feature computed from a set of images with a large diversity of color presents a grey average, similar \n                 to a neuron feature that is the average of grey-level images. \n                 Our color selectivity index is not specific to a single color, it measures the capacity of a neuron to give a high response on a \n                 specific color (or colors) and a low response when the colors of the images that maximally spike a neuron present just intensity \n                 variations; e.g, our color selective index determines if a neuron is achromatic or chromatic, and the degree of chromaticity if the\n                 second case.\n\n                 What we found in the analysed CNNs is that in lower layers, the neurons with high color selectivity index present a predominant 1st\n                 PCA component, e.g. the color distribution of the neuron feature is aligned with a single color axis, therefore if there are two\n                 colors, they are aligned. However, as we go deeper in the hierarchy, the alignment of the color distribution seems to disappear to \n                 give a more hue-based representation. As you can see in Fig. 5, there is a neuron in conv4 with a high color-selectivity index which \n                 is very selective to green and blue which are two non-aligned colors. This could be related with other findings in the human brain,\n                 but more work is needed to be able to do this statement. \n\n\n\n\nAbout the 3rd comment of the reviewer:\n\n\n                 \u201c... For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not \n                 for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or \n                 low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only \n                 to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper) \u2026\u201d\n\n                 This is a very inspiring observation. In our methodology, the specific case that you mention will determine that the neuron is \n                 highly color-selective, since the neuron feature is going to be the average of red-mushrooms only. We assume that the neuron feature \n                 is computed on the dataset that the CNN has been trained (not tested) on. However, it seems quite unlikely that such a neuron that \n                 activates for an *any color*-mushroom image can exist in the trained CNN if the training dataset only had red mushrooms (we assume \n                 that a CNN only learns things that have been seen during the training and therefore, it seems difficult that this CNN, that gives a \n                 high representative power to color, can generalize to mushrooms of any color if it has only seen that this type of object is always \n                 red).\n\n                 To support the previous ideas, in figure 6 (of the paper), we can see the cropped images for the red-mushroom neuron. Among the \n                 crops we only find red mushrooms and one table under a red-curtain image. This make us to think that the activation of this neuron \n                 is highly dependent on the activation of a red horizontal edge which would not be highly activated  for *any color*-mushroom. If the \n                 training dataset only had red-mushrooms this color-dependency could even be reinforced. The discriminative power of color for \n                 specific classes seems to be used by the network, this is what also seems to be exploited by this CNN in classes like cardoon or \n                 digital-clock for which we find class-selective neurons in shallow layers. \n\n                 To prove this in more depth, we have studied the activation of the red-mushroom neuron in conv 4 (neuron 34) along the dataset. \n\n                 (Annexed document:  We have compiled in this document all the images we have extracted from some further experiments. It has been \n                 stored in the public link:\n                 \n                 ****  https://docs.google.com/document/d/1KMu09fO3Y4UuFJCUKAMPGC_5L2OMmBYs48eeyn_Xm9E/edit?usp=sharing ****)\n\n                 Therefore, we can see in Fig. A-1 that, although this neuron can activate for any-color-mushrooms, its red selectivity is highly \n                 preserved up to the first 500 activation (ranked by the activation value). In the figure we have sampled some cropped images that \n                 activates this neuron along the first 1000 highest activations, we qualitatively observe that up to a 0,6 activation value, red \n                 selectivity is highly preserved.   \n\n                 To explore further on the question suggested by the reviewer and considering the current dataset, which contains a variety of \n                 colored mushrooms, we can observe that the CNN has a kind of *any color*-mushroom neuron in the deepest layer.  We have analysed two                  \n                 similar final classes of mushrooms (here we are using the generic meaning of mushroom that determines anything resembling a                  \n                 mushroom) that exist in the Imagenet version that was used to train the CNN we are studying; these are: AGARIC and MUSHROOM. You can \n                 see two random subsets of these two different classes in Fig. A-2 of the annexed document. \n\n                 We can observe the composition of these two classes:\n                                     **AGARIC: is a class essentially presenting red mushrooms \n                                     **MUSHROOM:  is a class presenting a higher diversity of colored mushrooms, including red ones. \n                 So we have two classes with similar properties, mushrooms is generic, but one with just red-mushrooms and the second are any-color-\n                 mushrooms.\n\n                 To understand and compare how the CNN is encoding these two classes, we have selected a subset of essential neurons per each class \n                 (see Fig.A-4). We have used our proposed class-selectivity index. Therefore, we select the neurons presenting a not-null class-\n                 selectivity index for layers conv1, conv2, conv3, conv4 and conv5, for the mentioned classes, mushroom and/or agaric.  This will \n                 allow us to study the population of neurons that encode these two classes such that it is possible to observe how shared properties \n                 and non-shared properties are represented. \n\n                 **1st observation: \n                                  The more diversity the class has, the larger the number of neurons that seems to be required for encoding that \n                                  class.\n\n                                  \t                              Conv1     Conv2     Conv 3      Conv 4    Conv 5       Total\n                                   \t   Agaric               2             13          34             23           23       =     95\n                                  \tMushroom           1             19          48             41           24       =   133\n\n                                  The value of class-selectivity indexes of neurons usually increases with depth, however it is dependent on the \n                                  number of neurons that are used to encode each class. In this case, Agaric class is encoded with a smaller number \n                                  of neurons (95) than Mushroom class (133). They seem to present a different population code scheme. \n\n                 **2nd observation: \n                                  Neurons selective to both classes represent properties shared by both classes, e.g. red caps of different sizes and \n                                  shapes, white stems, amongst others basic shared structures and background contexts. Neurons selective to a single \n                                  class reinforce the specific features of the class. \n\n\n                 To prove these facts, we have separated the neuron population of the two classes in 3 groups: \n\n                                  Group 1: Neurons selective to BOTH classes (39 neurons)\n                                  Group 2: Neurons selective just to AGARIC (56 neurons) \n                                  Group 3: Neurons selective just to MUSHROOMS (94 neurons)\n\n                 The properties of these three groups are plotted in Fig. A-3 of the annexed document. We show the relevant neuron features for each \n                 group by visualizing their Neuron Features and the first cropped image of the classes activating that neuron. As we already \n                 mentioned above, we can observe that shared Neuron Features represent generic properties of mushrooms like red caps of different \n                 sizes, white stems, or common context properties. The other two groups present more specific features of the classes, like the color \n                 diversity of caps for mushroom class, or a blue-specific case for agaric class.\n\n\n                 In addition to visualizing the NF of these relevant neurons, in Fig. A-4 of the annexed document, we have studied the selectivity \n                 indexes for each group. We have used different colors to represent different indexes. First of all we observe that color selectivity \n                 indexes (in green) are equally distributed in all the groups. However, class-selectivity indexes present different distributions.  \n                 Agaric class presents higher indexes (in pink) than mushroom class (in blue). The same behaviour is observed by the neurons that are \n                 shared by these two classes. It means that the CNN has very specialized neurons for red mushrooms and they are shared by both \n                 classes. \n\n                 Finally, we also analyze which are the colors (hues) to which the relevant features are selective to. We build the distribution of \n                 color-selectivity covered by the Neuron Features for all the groups in Fig. A-5.  In all the groups color selectivity is mainly \n                 represented in the hue range that goes from red to orange/brown to yellow neurons, as expected, since the majority of the mushrooms \n                 caps in the nature have colors in this range. However, we can see some differences. In Group 1 color-selectivity is concentrated on \n                 the red-orange region representing the commonalities between the two classes. Group 2 is also concentrated on the red-orange region \n                 (Mean: 58.76), while Group 3 is shifted to the orange-yellow region (Mean: 67.18), representing the color differences between the\n                 two classes.  \n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------\nIf the reviewer thinks it is worth to introduce this discussion in the paper, we suggest to add the following paragraph in an updated version of the \npaper. It could be inserted in Section 4.3 (Experiments on class-selectivity index) after the paragraph \u201cSecondly, we have visualized \u2026\u201d\n\n                 \u201cThirdly, we have explored how to use the class-selectivity index to visualize commonalities and differences between the neurons \n                 encoding two similar classes in the studied CNN. We focused on Agaric and Mushroom (Imagenet classes) both are composed by mushroom-\n                 shaped fungus, but agaric are specifically red, while mushrooms present a higher diversity of colors. Therefore, there exist some \n                 intersection and differences between the two classes (from their visual features point of view). Two main conclusion have arisen. \n                 Firstly we have analysed the number of relevant neurons per each class. We have defined as relevant neurons for X class as those \n                 presenting a non-null X-class-selectivity index. The number of relevant neurons for Agaric class (95) is lower than for Mushroom \n                 class (133), additionally agaric-class index values are higher than mushroom-class indexes,  that which makes us to conclude that \n                 they present different population code schemes. Secondly, we have separated the relevant neurons in three two groups: shared neurons                  \n                 and class-specific neurons. We have observed that neuron features represent shared properties by both classes, e.g. red caps of \n                 different sizes and shapes, white stems, amongst other basic shared structures and background contexts. Neurons selective to a \n                 single class reinforce the specific features of the class and their own diversity. Even color features differs between the groups.\u201d\n\n\nHowever we want to note that the length of our paper is currently 13 pages (including bibliography). We are not sure if the increase in the length can be a problem for the aim of this conference.  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1482956802188, "tcdate": 1482956802188, "number": 5, "id": "S1qbgsZrg", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "HytLDIuNg", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "response to reviewer2", "comment": "Thanks a lot  for this very Interesting discussion.\n\n1) We are not neuroscientists, we are AI researchers, however, l think we can not agree on your comments about the progress in neuroscience research, you say:\n\n\u201c... In my view, much of what was done in neurophysiology of the type you seem to be inspired by did not advance the understanding of processing in the visual system, beyond the initial work of people like Hubel and Wiesel, and some others in higher ventral areas,...\u201d\n\nDo you really think that works like, Doris Tsao et-al. on face-selectivity, Bewil Conway et-al on specific-hue selectivity or  R.Q. Quiroga et-al on grand-mother cells (amongst others) did not advance in the understanding of processing in the visual system?\n\nWe can agree that studying individual neuron selectivity must be a hard work, and to make it really useful you need to formulate very good hypothesis, and this must be really hard. However, we\u2019ve been speaking with neuroscientists working in the color field and they have agreed that works like ours can help a lot in giving new intuitions about how to formulate good new hypothesis. Our results on color selectivity in shallow layers confirms what is already known in the brain, but color in deep layers of the brain is still far from being understood, and our results can be a source of inspiration.  \n\n2) About the works correlating indexed group of neurons with brain evidences, that you mentioned as further research lines, we think they should be published in other kind of journals apart from ICLR,which has a more engineering audience. We plan to publish our results on this topic in other vision journals like JOV, VR, PNAS or JN \u2026 \n\n3)   About this comment on our work or similar works published previously in ICLR, NIPS or CVPR \u2026 \n\n\u201cYou're right, my comments would apply to any other work along these lines as well.   Basically, in my view, the approach of visualizing the intrinsic features of CNN has been really disappointing, precisely because neither of the two main directions I mentioned in my review have been that useful.\u201d\n\nWe would like to mention that the computer vision community has spent the last decades hand-crafting image descriptors to achieve good classification rates in object recognition. Concepts like SIFT, bag-of-words, spatial-pyramids, part-based models, Fisher-vectors (amongst others), have been the focus attention. And the study of their properties, like scale or color invariance, how to combine them, and a lot of  different representation problems have been the aim of a lot of published papers.  These days, the use of CNNs has shifted the focus of attention, and now the design of good representations is not the aim anymore, but to understand the representations we get after training, it is still important. Trying to disentangle how color or space invariance is achieved, or how similarities between different objects are represented, or how the network hierarchy is entangling the visual representation is still important to tackle a lot of different problems open in the industry.  We are in academia, but we live surrounded by students dealing with their projects and companies posing new problems that require a better understanding of what the CNNs are learning. \n\nWe really believe that a catalogue of neuron selectivities across the layers of a trained network additionally with a visualization of the average feature of each neuron can be a useful tool for a better understanding of the visual properties encoded across the network hierarchy. This can help in the design of new architectures to solve multiple new problems that are constantly emerging these days."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1482879378167, "tcdate": 1482879378167, "number": 3, "id": "H1cc-_xSl", "invitation": "ICLR.cc/2017/conference/-/paper177/official/review", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "content": {"title": "Solid work leading to a few interesting conclusions", "rating": "7: Good paper, accept", "review": "This paper makes three main methodological contributions:\n - definition of Neural Feature (NF) as the pixel average of the top N images that highly activation a neuron\n - ranking of neurons based on color selectivity\n - ranking of neurons based on class selectivity\n\nThe main weaknesses of the paper are that none of the methodological contributions are very significant, and no singularly significant result arises from the application of the methods.\n\nHowever, the main strengths of the paper are its assortment of moderately-sized interesting conclusions about the basic behavior of neural nets. For example, a few are:\n - \u201cIndexing on class selectivity neurons we found highly class selective neurons like digital-clock at conv2, cardoon at conv3 and ladybug at conv5, much before the fully connected layers.\u201d As far as I know, this had not been previously reported.\n - Color selective neurons are found even in higher layers. (25% color selectivity in conv5)\n - \u201cour main color axis emerge (black-white, blue-yellow, orange-cyan and cyan- magenta). Curiously, these two observations correlate with evidences in the human visual system (Shapley & Hawken (2011)).\u201d Great observation!\n\nOverall, I\u2019d recommend the paper be accepted, because although it\u2019s difficult to predict at this time, there\u2019s a fair chance that one of the \u201csmaller conclusions\u201d would turn out to be important in hindsight a few years hence.\n\n\nOther small comments:\n - The cite for \u201cLearning to generate chairs\u2026\u201d is wrong (first two authors combined resulting in a confusing cite)\n\n - What exactly is the Color Selectivity Index computing? The Opponent Color Space isn\u2019t well defined and it wasn\u2019t previously familiar to me. Intuitively it seems to be selecting for units that respond to a constant color, but the highest color selectivity NF in Fig 5 i for a unit with two colors, not one. Finally, the very last unit (lowest color selectivity) is almost the same edge pattern, but with white -> black instead of blue -> orange. Why are these considered to be so drastically different? This should probably be more clearly described.\n\n - For the sake of argument, imagine a mushroom sensitive neuron in conv5 that fires highly for mushrooms of *any* color but not for anything else. If the dataset contains only red-capped mushrooms, would the color selectivity index for this neuron be high or low? If it is high, it\u2019s somewhat misleading because the unit itself actually isn\u2019t color selective; the dataset just happens only to have red mushrooms in it. (It\u2019s a subtle point but worth considering and probably discussing in the paper)\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482879378666, "id": "ICLR.cc/2017/conference/-/paper177/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper177/AnonReviewer1", "ICLR.cc/2017/conference/paper177/AnonReviewer2", "ICLR.cc/2017/conference/paper177/AnonReviewer3"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482879378666}}}, {"tddate": null, "tmdate": 1482348369138, "tcdate": 1482348369138, "number": 1, "id": "HytLDIuNg", "invitation": "ICLR.cc/2017/conference/-/paper177/official/comment", "forum": "SJBr9Mcxl", "replyto": "SJ0daSOEe", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer2"], "content": {"title": "reviewer's response", "comment": "\n\"We would love to know why this methodology is considered as a \u201cplague\u201d by the reviewer in the following comment:\"\n\n--> Because I think it's close to being pointless, unless much more work is done to make it useful (along the lines that I suggested).  In   \n       my view, much of what was done in neurophysiology of the type you seem to be inspired by did not advance the understanding of \n       processing in the visual system, beyond the initial work of people like Hubel and Wiesel, and some others in higher ventral areas,   \n       identifying large-scale coarse properties of the system.    So much of the \"catalog of selectivities\" work seems to me to have  \n       basically not lead to any significant advances.   what has happened in the past few years is that real models have come along from \n       appropriate deep nets for use in neuroscience -- but none of the insights needed to make deep networks work, aside from the basic \n       ideas that were discovered by the late 1970s or early 80s, really came out of the neurophysiology.   In particular, the whole concept   \n       of \"tuning curve\" has been really  hard to make that useful for thinking about higher visual cortex -- just looking at distributions of     \n       selectivities has really not, even within pure neuroscience, provided much in the way of qualitative insight into the structure and  \n       behavior of the visual system.  Obviously this is my personal view as a computational neuroscientist, but I think it is well-reasoned.\n\n\"If the reviewer specifies the concrete drawbacks of the selectivity-based methodology, perhaps it could help us to redirect our research,  or at least to fully understand all  the reviewer comments.\"\n\n    --> That's exactly what I tried to do in the two suggestions of where I think the research can go.  I know it sounds very arrogant to \n          dismiss the goal of the research this way, and to make such sweeping suggestions of what I think you should be doing instead.   I \n          apologize for the way it comes across.   \n\n\"Our proposal is fully aligned with this challenge [of bridging AI and neuroscience] since it is providing with a generic framework for inspiring about new architectures in AI or validating findings in brain research.\"\n\n     --> I really disagree.  Im pretty skeptical that the selectivity catalogues produced by your framework will inspire new architectures.   \n           You also have not shown that this is true in any significant way.    However, one of the two suggestions I made was to try to do \n           that - e.g. to show that you could get better AI results by somehow using the results of the selectivity catalogues you propose \n           collecting.   I am very skeptical it will work, but if it did, I would be super-excited about the results.  However, to make your\n           submission worthy of ICLR, here now for 2017, in my view, I think you'd have to at least show some evidence that this happens.\n\n\n\"(3) Understanding and visualizing the intrinsic features that activates a neuron in a CNN have been focus of interest in the computer vision community in the last years.\"\n\n    --> You're right, my comments would apply to any other work along these lines as well.   Basically, in my view, the approach of \n          visualizing the intrinsic features of CNN has been really disappointing, precisely because neither of the two main directions I \n          mentioned in my review have been that useful.   Specifically:\n\n               a. Connecting to neural data.   Lots of work has been done to connect CNNs to neural data. It has been shown that the \n                   Relational Dissimilarity Matrices (RDMs) of a neural network are similar to those of neural data (like in van Gerven or \n                   Kriegeskorte's work) and that neural network features explain high levels of variance in stimulus-level variation of individual \n                   visual neurons (like Yamins and DiCarlo's work).  So what additional \"juice\" comes from making a comparison between \n                   selectivity catalogs?   Will that help validate some specific models (e.g. GoogleNet) as opposed to others (e.g. AlexNet)?  \n                   Does it really push the envelop of what we already understand?   It might.  But that *needs to be demonstrated* for the \n                   current work to make a viable ICLR submission, in my mind.   The current state of the field is already too advanced for the \n                   \"just might be possibility\" to be that useful. \n\n               b. Improving self- or un-supervised learning algorithms by showing that the specific statistics being collectd actually put a \n                   constraint on the neural system.   This has really been a bust.  Basically the community has had to learn to move on beyond  \n                   \"basic statistical summaries\" (e.g. selectivity profiles) as providing strong constraints. Instead, perhaps the kind of \n                   constraints emerging from variational autoencoders or other more rich interactive task-driven techniques (like those being \n                   pushed by Yann LeCun) really seem to be necessary to get less supervised versions that are powerful the way the \n                   supervised learning techniques are. \n\n\"(4) We think it is curious that the reviewer shows such a disapproval with this work at the same time he/she is proposing further research lines that can be derived from it and criticizing the fact we did not add the corresponding results.\"\n\n     --> Well, except that the further research to obtain the \"corresponding results\" will be where 80% of the hard work is.   My point is \n           that to make this a contribution that moves the ICLR community forward, I think the actual advances on these these further lines \n           need to be concretely accomplished.    Otherwise we're not much further than where we were in 2013.   \n         \n\"Actually, we are already working on trying to correlate the derived indexed groups of neurons with already published results about color coding in the brain; and we are also working on an image dataset to train new architectures using these indexes to constraint the training stage in a classical computer vision problem, but results are still preliminary.\"\n\n      --> Great, I think those plans sound very promising.   When either of those results are ready, *then* you guys will have a good ICLR  \n             submission.  At some level, it's not really your guys' fault.   I would have the same comments for basically any exploratory  \n             visualization-of-intrinsic-features analysis that does not make the leap to these other more concrete results.   \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698494, "id": "ICLR.cc/2017/conference/-/paper177/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698494}}}, {"tddate": null, "tmdate": 1482345845897, "tcdate": 1482345845897, "number": 4, "id": "SJ0daSOEe", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "rkn9pmD4e", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "Review Comments ", "comment": "Thanks for this long review. We are really grateful to the reviewer for having read this paper considering he is \u201c... biased against the whole enterprise of this paper...\u201d.\n\nAbout specific comments of the reviewer we want to point out on some aspects:\n\n(1) Firstly, we want to admit that this paper is influenced by the ideas coming from neurophysiology literature where selectivity indexes of single neurons are used to understand internal representation; i.e., finding neurons which are selective to faces, to eyes or to specific color-hues can inform about visual codes in the brain. Therefore, adopting a multidisciplinary point of view, in this paper  we propose to translate this methodology to artificial intelligence at a moment where understanding CNN internal representations is a focus of attention in the area.\n\nWe would love to know why this methodology is considered as a \u201cplague\u201d by the reviewer in the following comment:\n\n\u201c... In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop ... \u201d\n\nIf the reviewer specifies the concrete drawbacks of the selectivity-based methodology, perhaps it could help us to redirect our research,  or at least to fully understand all  the reviewer comments.\n\n(2) Recently, Prof. Bengio in a talk at NIPS pointed out that one of the big challenges in the area is: \u201cBridging the gap between deep learning and neuroscience is an opportunity to get inspiration and constraints\u201d. Our proposal is fully aligned with this challenge, since it is providing with a generic framework for inspiring about new architectures in AI or validating findings in brain research.\n\n(3) Understanding and visualizing the intrinsic features that activates a neuron in a CNN have been focus of interest in the computer vision community in the last years. Several papers have been published with this aim  (Zeiler&Fergus, Simonyan-et al, Yosinksi-et al, amongst others). Here we propose a new work, that is in the same line but trying to solve the main drawbacks of these previous works. This was already justified in a former comment of this review and was added to the paper in the updated version. Apart from giving a new approach to understand and visualize we also add the concept of selectivity index that gives a generic framework to go beyond the single neuron and group them considering their behaviours.\n\n(4) We think it is curious that the reviewer shows such a disapproval with this work at the same time he/she is proposing further research lines that can be derived from it and criticizing the fact we did not add the corresponding results.  We agree with the reviewer about the further lines that can be derived from this work. We thought it was more adequate first to publish this basic framework and subsequently use it to correlate neurophysiological evidences or to use it to constrain the loss function of an untrained network. Actually, we are already working on trying to correlate the derived indexed groups of neurons with already published results about color coding in the brain; and we are also working on an image dataset to train new architectures using these indexes to constraint the training stage in a classical computer vision problem, but results are still preliminary. We thought all this was out of the scope of this paper where we focused on a paper that deals with internal representations from a generic point of view, that we think matches the aim of this ICLR conference.   "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1482272148273, "tcdate": 1482272148273, "number": 2, "id": "rkn9pmD4e", "invitation": "ICLR.cc/2017/conference/-/paper177/official/review", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer2"], "content": {"title": "review of \"UNDERSTANDING TRAINED CNNS BY INDEXING NEURON SELECTIVITY\"", "rating": "3: Clear rejection", "review": "The authors analyze trained neural networks by quantifying the selectivity of individual neurons in the network for a variety of specific features, including color and category.   \n\nPros:\n   * The paper is clearly written and has good figures. \n   * I think they executed their specific stated goal reasonably well technically.   E.g. the various indexes they use seem well-chosen for their purposes. \n\nCons:\n   * I must admit that I am biased against the whole enterprise of this paper.   I do not think it is well-motivated or provides any useful insight whatever.   What I view their having done is produced, and then summarized anecdotally, a catalog of piecemeal facts about a neural network without any larger reason to think these particular facts are important.  In a way, I feel like this paper suffers from the same problem that plagues a typical line of research in neurophysiology, in which a catalog of selectivity distributions of various neurons for various properties is produced -- full stop.  As if that were in and of itself important or useful information.   I do not feel that either the original neural version of that project, or this current model-based virtual electrophysiology, is that useful.   Why should we care about the distribution of color selectivities?   Why does knowing distribution as such constitute \"understanding\"?    To my mind it doesn't, at least not directly.   \n\nHere's what they could have done to make a more useful investigation:\n  \n     (a) From a neuroscience point of view, they could have compared the properties that they measure in models to the same properties as measured in neurons the real brain.   If they could show that some models are better matches on these properties to the actual neural data than others, that would be a really interesting result.   That is is to say, the two isolated catalogs of selectivities (from model neurons and real neurons)  alone seem pretty pointless.  But if the correspondence between the two catalogs was made -- both in terms of where the model neurons and the real neurons were similar, and (especially importantly) where they were different --- that would be the beginning of nontrivial understanding.   Such results would also complement a growing body of literature that attempts to link CNNs to visual brain areas.  Finding good neural data is challenging, but whatever the result, the comparison would be interesting. \n\nand/or \n\n    (b) From an artificial intelligence point of view, they could have shown that their metrics are *prescriptive* constraints.   That is, suppose they had shown that the specific color and class selectivity indices that they compute, when imposed as a loss-function criterion on an untrained neural network, cause the network to develop useful filters and achieve significantly above-chance performance on the original task the networks were trained on.     This would be a really great result, because it would not only give us a priori reason to care about the specific property metrics they chose, but it would also help contribute to efforts to find unsupervised (or semi-supervised) learning procedures, since the metrics they compute can be estimated from comparatively small numbers of stimuli and/or high-level semantic labels.    To put this in perspective, imagine that they had actually tested the above hypothesis and found it to be false:  that is, that their metrics, when used as loss function constraints, do not improve performance noticeably above chance performance.  What would we then make of this whole investigation?  It would then be reasonable to think that the measured properties were essentially epiphenomenal and didn't contribute at all to the power of neural networks in solving perceptual tasks.  (The same could be said about neurophysiology experiments doing the same thing.)  \n     [--> NB: I've actually tried things just like this myself over the years, and have found exactly this disappointing result.  Specifically,  I've found a number of high-level generic statistical property of DNNs that seem like they might potentially \"interesting\", e.g. because they apparently correlate with complexity or appear to illustrate difference between low, intermediate and high layers of DNNs.  Every single one of these, when imposed as optimization constraints, has basically lead nowhere on the challenging tasks (like ImageNet) that cause the DNNs to be interesting in the first place.  Basically, there is to my mind no evidence at this point that highly-summarized generic statistical distributions of selectivities, like those illustrated here, place any interesting constraints on filter weights at all.   Of course, I haven't tried the specific properties the authors highlight in these papers, so maybe there's something important there.]\n\nI know that both of these asks are pretty hard, but I just don't know what else to say -- this work otherwise seems like a step backwards for what the community ought to be spending its time on. \n ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482879378666, "id": "ICLR.cc/2017/conference/-/paper177/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper177/AnonReviewer1", "ICLR.cc/2017/conference/paper177/AnonReviewer2", "ICLR.cc/2017/conference/paper177/AnonReviewer3"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482879378666}}}, {"tddate": null, "tmdate": 1482015149727, "tcdate": 1482015149727, "number": 1, "id": "HJLhbHQ4x", "invitation": "ICLR.cc/2017/conference/-/paper177/official/review", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer1"], "content": {"title": "Review of \"indexing neuron selectivity\"", "rating": "7: Good paper, accept", "review": "This paper attempts to understand and visualize what deep nets are representing as one ascends from low levels to high levels of the network.  As has been shown previously, lower levels are more local image feature based, whereas higher levels correspond to abstract properties such as object identity.  In semantic space, we find higher level nodes to be more semantically selective, whereas low level nodes are more diffuse.\n\nThis seems like a good attempt to tease apart deep net representations.  Perhaps the most important finding is that color figures prominently into all levels of the network, and that performance on gray scale images is significantly diminished.  The new NF measure proposed here is sensible, but still based on the images shown to the network.  What one really wants to know is what function these nodes are computing - i.e., out of the space of *all* possible images, which most activate a unit?  Of course this is a difficult problem, but it would be nice to see us getting closer to understanding the answer.  The color analysis here I think brings us a bit closer.  The semantic analysis is nice but I'm not sure what new insight we gain from this.  \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482879378666, "id": "ICLR.cc/2017/conference/-/paper177/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper177/AnonReviewer1", "ICLR.cc/2017/conference/paper177/AnonReviewer2", "ICLR.cc/2017/conference/paper177/AnonReviewer3"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482879378666}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481929417291, "tcdate": 1478269500741, "number": 177, "id": "SJBr9Mcxl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJBr9Mcxl", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481922600259, "tcdate": 1481922600259, "number": 3, "id": "rJeVOC-Nl", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "Updated version", "comment": "We have updated the pdf of the paper after the reviewer comments, in this way:\n  - Giving more details about similarities and differences between the proposed approach and previous.\n  - Adding the clarifications required by the reviewers on Figure1\n  - Improving the global understanding by joining section 1 and 2 into a single one. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1481898989925, "tcdate": 1481898959427, "number": 2, "id": "r1v0j_b4l", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "rkBv_8lVe", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "Clarification of Figure 1", "comment": "Sorry for not having explained it adequately in the first version.\n \nThe y-axis represents the normalized activation value of a single neuron to an image of the dataset. Images are ranked on the x-axis according with their activation value, from highest to lowest activation (we just plot the first 400 images for each neuron).  \n\nBy normalized activation (y-axis) we mean the value of the maximum activation of a neuron for a specific input image, which is normalized by the maximum of these values achieved by the same neuron over all the images in the dataset. Therefore, the first relative activation value is always 1 for all neurons and then the normalized activation values decrease monotonically.  This normalization allows to compare different neuron behaviors, from neurons which are activated by most of the images (flatter behavior), to neurons that highly activates only for a subset of images and have very little activation for the rest (steeper behavior).\n\nIn figure 1 we show the behavior for a subset of  neurons at each layer. Neurons have been selected according to their area under the curve (AUC) value, which is represented by a percentage of area.  The percentage of area is computed over the area of the neuron that presents the maximum AUC in the entire architecture.  For each layer we have picked some neurons sampling different AUC values, equally distributed in the layer. We can observe different behaviors in all layers. In general, we can state that in deeper layers the behavior of the neurons is steeper (lower AUC), i.e. neurons highly spike for a small number of images. However, in shallower layers the behavior is flatter, i.e. neurons highly spike for a lot of images. This is an expected behavior, since the image features spiking neurons in first layers (e.g. oriented edges) are shared by almost all the images, while the features spiking shallow neurons are more selective features (e.g. faces) that only spike for specific images.\n\nWe are going to change this figure explanation in a paper update."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1481824374370, "tcdate": 1481824374360, "number": 2, "id": "rkBv_8lVe", "invitation": "ICLR.cc/2017/conference/-/paper177/pre-review/question", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer3"], "content": {"title": "Details of NF", "question": "In Figure 1, what is the x-axis label? What is the y-axis label (i.e. what do the percents represent)? When computing the NF, Section 3 specifies \u201cThey [the activations] need to be accordingly ranked with the rest of the activations of the layer.\u201d Does this mean that activations are compared from one neuron against other neurons in that same layer?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481824374923, "id": "ICLR.cc/2017/conference/-/paper177/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper177/AnonReviewer1", "ICLR.cc/2017/conference/paper177/AnonReviewer3"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481824374923}}}, {"tddate": null, "tmdate": 1481674382542, "tcdate": 1481674311918, "number": 1, "id": "rkgU0bR7g", "invitation": "ICLR.cc/2017/conference/-/paper177/public/comment", "forum": "SJBr9Mcxl", "replyto": "Hyfgtls7e", "signatures": ["~Ivet_Rafegas_Fonoll1"], "readers": ["everyone"], "writers": ["~Ivet_Rafegas_Fonoll1"], "content": {"title": "Clarification of Neuron Feature (NF)", "comment": "Likewise in Zeiler&Fergus, we pursuit visualizing the intrinsic feature that activates a neuron. As them, we perform it by analyzing the images that maximally activates a specific neuron. This aim is also shared in some other works, such as Simonyan-et al and Yosinksi-et al (amongst others), but in this case, they generate the image that maximizes the activation of the neuron.\n\n\nAlthough our approach is based on the images that maximally activates a neuron, like Zeiler&Fergus, our Neuron Feature is different from what they do in the sense that our estimation of the intrinsic property is much more generic. Zeiler&Fergus just visualize the projected feature of a single image activation, and they show up the first 9, but separately, i.e. without considering the commonalities in between them. However, our estimation is a weighted average that combines the N-first maximum activations (N=100 in the paper) and makes it not to be image-specific. We can see in Figure 3 of our paper that the variability in between the images that maximally activates a neuron, can present a high variability between them. This fact questions Zeiler&Fergus individual visualisation. In our Neuron Feature, strong spatial variability is visualized by the level of blurring of specific Neuron Feature regions. \n\n\nIn relation with the second group of works, Simonyan-et al and Yosinksi-et al, they also seeked to overcome the image-specific drawback by generating an image that captures a generic feature that highly activates that neuron. However, the gradient-based techniques, used to generate these non image-specific visualizations, makes to emerge a new drawback, which is their non realistic appearance. Their visualizations present important artifacts that complicate the understanding of the intrinsic property. Therefore, they explore different regularizations that  achieve more realistic intrinsic feature representations. Our Neuron Feature overcomes this problem by directly averaging on the image space, this has two main advantages: (a) keeping the properties of the natural images, and (b) providing a more straightforward approach to compute them. \n\n\nConsidering all the above considerations we end up with a new visualization that is not image-specific, is unique for each individual neuron, presents an understandable representation, and is obtained by a straightforward approach that can be applied to any neuron at any layer, convolutional or not. All these properties allow to achieve a higher level of abstraction in the understanding of a single neuron, making easier to define neuron descriptors that can derive selectivity indexes. These indexes allow to get comprehensible maps of the neuron functions at any level of any CNN.\n\n\nThe advantages of our approach with respect to Zeiler&Fergus and the rest of methods that visualize single neuron activity (Yosinksi-et al, Simonyan-et al) can be summed up as:\n   - More generic (non image-specific) and unique version of the intrinsic feature of a neuron\n   - More natural representation directly computed in the image space\n   - More understandable neuron function thanks to a higher level of abstraction in the neuron description\n  \nNow we see, we should have introduced this comparison in the paper. We will do it in an updated version of it. \n\n\nReferences:\n\n\n[ Simonyan-et al ] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In ICLR Workshop 2014.\n\n\n[ Yosinksi-et al ] Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural networks through deep visualization. In ICML Workshop 2015.\n\n\n[ Zeiler&Fergus ] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287698622, "id": "ICLR.cc/2017/conference/-/paper177/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJBr9Mcxl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper177/reviewers", "ICLR.cc/2017/conference/paper177/areachairs"], "cdate": 1485287698622}}}, {"tddate": null, "tmdate": 1481472233826, "tcdate": 1481472233817, "number": 1, "id": "Hyfgtls7e", "invitation": "ICLR.cc/2017/conference/-/paper177/pre-review/question", "forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "signatures": ["ICLR.cc/2017/conference/paper177/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper177/AnonReviewer1"], "content": {"title": "Clarification of neural feature", "question": "Could you clarify how your proposed \"Neural Feature\" is different from the Zeiler & Fergus method of finding images that maximally activate a neuron?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Understanding trained CNNs by indexing neuron selectivity", "abstract": "The impressive performance and plasticity of convolutional neural networks to solve different vision problems are shadowed by their black-box nature and its consequent lack of full understanding. To reduce this gap we propose to describe the activity of individual neurons by quantifiyng their inherent selectivity to specific properties. Our approach is based on the definition of feature selectivity indexes that allow the ranking of neurons according to specific properties. Here we report the results of exploring selectivity indexes for: (a) an image feature (color); and (b) an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer conv4 or class selective neurons such as dog-face neurons in layer conv5, and establishes a methodology to derive other selectivity properties.  Indexing on neuron selectivity can statistically draw how features and classes are represented through layers at a moment when the size of trained nets is growing and automatic tools to index can be helpful. ", "pdf": "/pdf/28736c858f715b58f4e74cf4e438b4070c328a8a.pdf", "paperhash": "rafegas|understanding_trained_cnns_by_indexing_neuron_selectivity", "conflicts": ["uab.cat", "cvc.uab.cat", "uab.es", "cvc.uab.es", "ubi.pt"], "keywords": ["Computer vision", "Deep learning"], "authors": ["Ivet Rafegas", "Maria Vanrell", "Lu\u00eds A. Alexandre"], "authorids": ["ivet.rafegas@uab.cat", "maria.vanrell@uab.cat", "lfbaa@ubi.pt"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481824374923, "id": "ICLR.cc/2017/conference/-/paper177/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper177/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper177/AnonReviewer1", "ICLR.cc/2017/conference/paper177/AnonReviewer3"], "reply": {"forum": "SJBr9Mcxl", "replyto": "SJBr9Mcxl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper177/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481824374923}}}], "count": 16}