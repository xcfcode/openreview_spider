{"notes": [{"id": "BJgxzlSFvr", "original": "HylKcbxtDr", "number": 2161, "cdate": 1569439752362, "ddate": null, "tcdate": 1569439752362, "tmdate": 1577168284418, "tddate": null, "forum": "BJgxzlSFvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK", "authors": ["Diego Klabjan", "Baiyang Wang"], "authorids": ["d-klabjan@northwestern.edu", "baiyang@u.northwestern.edu"], "keywords": ["learning to rank", "deep learning"], "TL;DR": "learning to rank with several embeddings and attentions", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "pdf": "/pdf/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "paperhash": "klabjan|an_attentionbased_deep_net_for_learning_to_rank", "original_pdf": "/attachment/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "_bibtex": "@misc{\nklabjan2020an,\ntitle={{\\{}AN{\\}} {\\{}ATTENTION{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}NET{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}TO{\\}} {\\{}RANK{\\}}},\nauthor={Diego Klabjan and Baiyang Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgxzlSFvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "r53gJhyrWv", "original": null, "number": 1, "cdate": 1576798742095, "ddate": null, "tcdate": 1576798742095, "tmdate": 1576800894120, "tddate": null, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2161/-/Decision", "content": {"decision": "Reject", "comment": "All three reviewers felt the paper should be rejected and no rebuttal was offered. So the paper is rejected.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK", "authors": ["Diego Klabjan", "Baiyang Wang"], "authorids": ["d-klabjan@northwestern.edu", "baiyang@u.northwestern.edu"], "keywords": ["learning to rank", "deep learning"], "TL;DR": "learning to rank with several embeddings and attentions", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "pdf": "/pdf/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "paperhash": "klabjan|an_attentionbased_deep_net_for_learning_to_rank", "original_pdf": "/attachment/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "_bibtex": "@misc{\nklabjan2020an,\ntitle={{\\{}AN{\\}} {\\{}ATTENTION{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}NET{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}TO{\\}} {\\{}RANK{\\}}},\nauthor={Diego Klabjan and Baiyang Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgxzlSFvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725439, "tmdate": 1576800277331, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2161/-/Decision"}}}, {"id": "BJxrToa2YB", "original": null, "number": 1, "cdate": 1571769276968, "ddate": null, "tcdate": 1571769276968, "tmdate": 1572972375260, "tddate": null, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this paper, the authors propose to use attention to combine multiple input representations for both query and search results in the learning to rank task. When these representations are embeddings from differentiable functions, they can be jointly learned with the neural network which predicts rankings. A limited set of experiments suggest the proposed approach very mildly outperforms benchmark approaches.\n\nMajor comments\n\nTo the best of my knowledge, this is the first paper to apply attention to the learning to rank problem. However, the main methodological innovation seems to be the use of attention to create and train an ensemble of models; this has been previously explored in the literature (e.g., [Kim et al., ECCV 2018]).\n\nThe paper is also missing important context in that it omits developments in using deep learning for the learning to rank problem (e.g., [Pang et al., CIKM 2017; Ai et al., WWW 2018]). The experimental evaluation does not include any other deep methods; thus, it is not clear if the (very minor) improvement in performance are due to the deep models or the proposed attention approach.\n\nThe datasets used in the experiments are not appropriate for evaluating learning to rank algorithms. A variety of learning to rank datasets are available, and these should be used rather than (or in addition to) the toy datasets considered here. Examples: http://arogozhnikov.github.io/2015/06/26/learning-to-rank-software-datasets.html, http://quickrank.isti.cnr.it/istella-dataset/, https://www.cl.uni-heidelberg.de/statnlpgroup/nfcorpus/, \n\nMinor comments\n\nConcerning Section 3.3, in what sense is SGD used to \u201ccalibrate\u201d the model? It seems as though the authors just mean it is used to \u201ctrain\u201d the model. However, is there some other meaning of calibration (e.g., in the sense of a Brier score) here?\n\nIn Table 1, what is the meaning of a dropout p value of 1? In most deep learning frameworks (e.g., Keras and PyTorch), this would mean all nodes are dropped out.\n\nIn what sense are the \u201c5 randomized runs\u201d for the experiments randomized? Are different train, test splits used? or just different random seeds? or something else?\n\nHow is it that the error rates are higher when using superclasses for evaluation?\n\nTypos, etc.\n\nThe paper has several significant problems with the \u201c\\cite\u201ds and \u201c\\ref\u201ds in the paper. First, the \u201c\\cite\u201ds should presumably be \u201c\\citep\u201ds or something since the references are not set off from the rest of the text. Second, the paper includes references to equation numbers which are not present in the paper, such as \u201cequation (12)\u201d. It seems as the equations are in the paper, but are included in some unnumbered environment (\u201c\\begin{align*}\u201d or some such). This makes it very difficult to track down to which equations the authors intend to refer. Third, the reference numbers to figures and tables in the text is wrong. For example, the text refers to \u201cTables 8 and 9\u201d for 20 newsgroups (at the end of Section 4). Clearly, this is supposed to be Tables 6 and 7. It seems like the authors moved the CIFAR-10 discussion to the appendix but did not update the references in the text.\n\nTables 2 and 4 are exactly the same.\n\nFigure 4 is not referenced in the text.\n\nIt would be helpful to put Figure 2 a bit closer to where it is discussed in the text.\n\nThe references are not consistently formatted.\n\n\u201ccomponents of for each\u201d -> \u201ccomponents for each\u201d\n\nPlease define acronyms like MAP at least once.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK", "authors": ["Diego Klabjan", "Baiyang Wang"], "authorids": ["d-klabjan@northwestern.edu", "baiyang@u.northwestern.edu"], "keywords": ["learning to rank", "deep learning"], "TL;DR": "learning to rank with several embeddings and attentions", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "pdf": "/pdf/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "paperhash": "klabjan|an_attentionbased_deep_net_for_learning_to_rank", "original_pdf": "/attachment/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "_bibtex": "@misc{\nklabjan2020an,\ntitle={{\\{}AN{\\}} {\\{}ATTENTION{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}NET{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}TO{\\}} {\\{}RANK{\\}}},\nauthor={Diego Klabjan and Baiyang Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgxzlSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399150171, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2161/Reviewers"], "noninvitees": [], "tcdate": 1570237726824, "tmdate": 1575399150183, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review"}}}, {"id": "rJedKG9y5B", "original": null, "number": 2, "cdate": 1571951231932, "ddate": null, "tcdate": 1571951231932, "tmdate": 1572972375208, "tddate": null, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use attention mechanism for combining different embeddings of the queries and search results. Besides, a decoder mechanism is used to do listwise ranking for the results. The experiments show that the proposed approach outperforms some classic learning-to-rank baselines.\n\nThis paper is below the bar of acceptance for the following reasons:\n\n1.\tLimited technical contribution: some previous papers have explored the idea of learning attention weights for combining different embeddings, and simply applying this idea to learning-to-rank application does not seem to be a big contribution.\n\n2.\tChoice of datasets: the datasets used in this paper are typically used for tesing classification models rather than ranking models. In these datasets, for each query image/doc, there are many images/docs of the same class that could be considered relevant, which makes the ranking task less challenging. Since the paper focuses on learning-to-rank problem, probably the authors should consider include more datasets dedicated to learning-to-rank problems.\n\n3.\tInsufficient baselines: the baseline methods used in the paper are not very recent (e.g., OASIS, RankSVM and LambdaMart have been proposed for more than 10 years). There have been many neural-network based retrieval/ranking methods proposed in the past 5 years. Hence, the experimental results could be more convincing if the paper include more \n\n4.\tLack of justification for the model architecture: some design choices of the model are not well-motivated/justified. For example, how does the decoder mechanism using multiple states in the model (listwise) help improve the ranking results compared to pairwise ranking? Ablation study could help whether such decoder mechanism help show the usefulness of this module.\n\n5.\tParameter sensitivity study: study on how hyper-parameter values affects the model performance could also help.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK", "authors": ["Diego Klabjan", "Baiyang Wang"], "authorids": ["d-klabjan@northwestern.edu", "baiyang@u.northwestern.edu"], "keywords": ["learning to rank", "deep learning"], "TL;DR": "learning to rank with several embeddings and attentions", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "pdf": "/pdf/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "paperhash": "klabjan|an_attentionbased_deep_net_for_learning_to_rank", "original_pdf": "/attachment/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "_bibtex": "@misc{\nklabjan2020an,\ntitle={{\\{}AN{\\}} {\\{}ATTENTION{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}NET{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}TO{\\}} {\\{}RANK{\\}}},\nauthor={Diego Klabjan and Baiyang Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgxzlSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399150171, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2161/Reviewers"], "noninvitees": [], "tcdate": 1570237726824, "tmdate": 1575399150183, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review"}}}, {"id": "BygEsdrNqr", "original": null, "number": 3, "cdate": 1572259996186, "ddate": null, "tcdate": 1572259996186, "tmdate": 1572972375155, "tddate": null, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "invitation": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed an attention-based deep neural network for implementing 'learning to rank' algorithm. Particularly, the proposed method implements a listwise approach which outputs the ranks for all search results given a query. The search results are claimed to be sorted by their degree of relevance or importance to the query. However, it is not clear to me how the ranking was decided in equation 6 by the softmax function. For example, as per section 4, the documents of the same topic are considered related, then how the proposed model was trained with one document having higher relevance than others in the same topic category.  \n\nThere are other confusions that need to be addressed for better understanding. For example, how softmax probabilities can be used as an embedding as described in the line: \u201cFrom training this model, we may take the softmax probabilities as the embedding, and create different embeddings with different neural network structures. \u201d Also, what does the line means: \u201cthe number of documents of the same topic is uniformly distributed from 3 to 7, the number of documents of the same superclass but different topics is also uniformly distributed from 3 to 7, and the remaining documents are of different super classes.\u201d  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2161/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AN ATTENTION-BASED DEEP NET FOR LEARNING TO RANK", "authors": ["Diego Klabjan", "Baiyang Wang"], "authorids": ["d-klabjan@northwestern.edu", "baiyang@u.northwestern.edu"], "keywords": ["learning to rank", "deep learning"], "TL;DR": "learning to rank with several embeddings and attentions", "abstract": "In information retrieval, learning to rank constructs a machine-based ranking model which given a query, sorts the search results by their degree of relevance or importance to the query. Neural networks have been successfully applied to this problem, and in this paper, we propose an attention-based deep neural network which better incorporates different embeddings of the queries and search results with an attention-based mechanism. This model also applies a decoder mechanism to learn the ranks of the search results in a listwise fashion. The embeddings are trained with convolutional neural networks or the word2vec model. We demonstrate the performance of this model with image retrieval and text querying data sets.", "pdf": "/pdf/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "paperhash": "klabjan|an_attentionbased_deep_net_for_learning_to_rank", "original_pdf": "/attachment/0ae37e54268c4939018762b656784a5852c1d43f.pdf", "_bibtex": "@misc{\nklabjan2020an,\ntitle={{\\{}AN{\\}} {\\{}ATTENTION{\\}}-{\\{}BASED{\\}} {\\{}DEEP{\\}} {\\{}NET{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}TO{\\}} {\\{}RANK{\\}}},\nauthor={Diego Klabjan and Baiyang Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgxzlSFvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgxzlSFvr", "replyto": "BJgxzlSFvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2161/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399150171, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2161/Reviewers"], "noninvitees": [], "tcdate": 1570237726824, "tmdate": 1575399150183, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2161/-/Official_Review"}}}], "count": 5}