{"notes": [{"id": "SJlPOCEKvH", "original": "Skl_opwdDr", "number": 1214, "cdate": 1569439342812, "ddate": null, "tcdate": 1569439342812, "tmdate": 1577168263888, "tddate": null, "forum": "SJlPOCEKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1hPAy5wXO", "original": null, "number": 1, "cdate": 1576798717638, "ddate": null, "tcdate": 1576798717638, "tmdate": 1576800918925, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Decision", "content": {"decision": "Reject", "comment": "This work explores weight pruning for BERT in three broad regimes of transfer learning: low, medium and high.\n\nOverall, the paper is well written and explained and the goal of efficient training and inference is meaningful. Reviewers have major concerns about this work is its technical innovation and value to the community: a reuse of pruning to BERT is not new in technical perspective, the marginal improvement in pruning ratio compared to other compression method for BERT, and the introduced sparsity that hinders efficient computation for modern hardware such as GPU. The rebuttal failed to answer a majority of these important concerns.\n\nHence I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727428, "tmdate": 1576800279657, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Decision"}}}, {"id": "S1lvPpVDsr", "original": null, "number": 3, "cdate": 1573502303066, "ddate": null, "tcdate": 1573502303066, "tmdate": 1573502303066, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "HJeqpvgaYr", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Thank you for taking the time to review our paper! We will try to answer your questions in order:\n\n1a. How did we pick 3 epochs for fine-tuning? Would a different amount of fine-tuning help?\n\n3 epochs was the amount of fine-tuning used in the original BERT paper. However, we also tried fine-tuning between 1 and 12 epochs before pruning 60%. The results in Figure 3 (right) show that the development accuracy does not improve whether you train for 1 epoch or 12 before pruning.\n\nMore importantly, however, is that the weights pruned do not change much whether we fine-tune for 1 epoch or 12 epochs. We observed in Figure 4 (right) that weights quickly settle into a new sorting order within the first epoch of fine-tuning. This implies that no matter how much longer we fine-tune, the weights selected for pruning will not change much.\n\n1b. Why is pruning after fine-tuning worse than pruning during pre-training?\n\nThis is really only true around 60% pruning and above. We believe the explanation for this is that we continue training after pruning to recover accuracy. If we prune during pre-training, this allows the model to recover some of the deleted pre-training information in the remaining weights. If we prune during fine-tuning, however, that information is no longer accessible to the model, so it cannot recover as well.\n\n2. Would examining the pruning thresholds tell us whether some pruning ratio is reasonable?\n\nWe completely agree that this is an interesting signal. It is difficult, however, to interpret the threshold without knowing the distribution of weights in each matrix. For example, a threshold of 0.01 might be reasonable if the standard deviation of weights is 1, but not if the standard deviation is 0.001.\n\nExamining the percentage of total magnitudes pruned might be a better choice here, since it is also easy to compute given the weights. This can be found in Figure 5 in the Appendix.\n\n3a. Why were models pruned after fine-tuning trained until training losses were comparable?\n\nWe wanted to make sure that the pruned models fit the downstream data just as well as models pruned during pre-training. This would imply that the models pruned after fine-tuning \u201clearned\u201d the downstream data just as well as the others.\n\n3b. Doesn\u2019t it look like the models are over-fitting?\n\nWe don\u2019t think so. The un-pruned models in each experiment have both the lowest training loss out of all the models and also the highest development accuracy. Pruning, which acts like a regularizer, decreases model complexity and increases training loss. In this case, it also happened to decrease the development accuracy.\n\nIt is known that neural networks can sometimes fit the data perfectly and still generalize well. This has been called the \u201cdouble-descent risk curve,\u201d[1] and we conjecture that it may explain the role of pre-training in generalization as well as some of our results.\n\n[1] https://arxiv.org/abs/1812.11118"}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlPOCEKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1214/Authors|ICLR.cc/2020/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159471, "tmdate": 1576860541606, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment"}}}, {"id": "HkxT-iVDor", "original": null, "number": 1, "cdate": 1573501700567, "ddate": null, "tcdate": 1573501700567, "tmdate": 1573502096874, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "BJeAGC3pYS", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment", "content": {"title": "Response to Review #2 and Review #3", "comment": "Thank you for taking the time to review our paper! We understand your concerns\nabout the practicality/novelty of the pruning method we used. To summarize:\n\n1. Pruning does not seem practically useful.\n  a. Random sparsity is difficult to accelerate on GPUs; structured sparsity might be better.\n  b. Some methods under submission achieve higher compression rates.\n  c. We cannot prune much (30-40%) without losing accuracy.\n\n2. We do not propose any new technical methods.\n\nWe would like to emphasize, however, that these practical questions are\northogonal to the thesis of our paper. Our main contribution is a scientific\ninvestigation of a general question: how does compressing a universal feature\nextractor affect task-specific problems? We believe this paper merits being\nshared with the community on a scientific basis alone. We will, however, try to\nclarify some of the practical aspects as well.\n\n1a. Random sparsity is difficult to accelerate on GPUs. Structured sparsity might be better.\n\nAccelerating unstructured sparse matrix multiplication is an active area of\nresearch in which recent progress has been made. Bank-balanced sparsity (which\nis closely related to unstructured sparsity) achieves near-ideal speed-ups while\nrequiring a minimal deviation from unstructured sparsity.[1] We believe our\nresults transfer to this technique, since bank-balanced sparsity preserves\nalmost 95% of weight magnitudes when compared with unstructured pruning. On the\nsystems side, adaptive sparse matrix multiplication has shown promising results\non GPUs.[2]\n\nStructured pruning, on the other hand, is not currently practical. Block sparse\npruning imposes optimization / model constraints that quickly degrade\naccuracy.[1][3] It is also not clear whether other types of structured pruning\n(attention head, etc.) are orthogonal to weight pruning (explored in Section 6),\nso it may make sense to do both on a single model.\n\n1b. Some methods currently under submission achieve higher compression rates.\n\nThese papers [4][5][6] utilize some combination of knowledge distillation, word\nembedding factorization, and parameter sharing. However, these methods are not\nwell understood, which makes it difficult to use them to answer scientific\nquestions. Weight magnitude pruning, on the other hand, is simple and\nwell-motivated. It\u2019s known that when an over-parameterized neural network\nachieves a global minimum, many subnetworks have zero weights [8]. We should\nalso note that many of these other techniques also do not show a practical\ninference speed improvement.\n\n1c. We cannot prune much (30-40%) without losing accuracy.\n\nAs Reviewer #1 points out, 30-40% pruning is not practically useful. However,\nthe specific numbers are not important to our work. Before we started, we did\nnot know BERT was prunable and why performance would degrade, if it did. Our\nwork has shown the existence of three distinct regimes of pruning: most of\nBERT\u2019s capacity encodes the pre-training inductive bias, and only a small\nfraction is needed to fit downstream data. This implies that the size of the\npre-training dataset is the limiting factor in model compression, which should\ndrive future work towards understanding the nature of that inductive bias.\n\nAlso, several application domains demand very memory constrained models.\nPractitioners in these domain will accept \u201clossy\u201d compression (60-70%), as long\nas they can quantify the memory / accuracy trade-off.\n\n2. We do not propose any new technical methods.\n\nWe are interested in exploring the previously unexplored question of how\ncompression affects transfer learning. For this purpose, we choose to use a\nwell-understood technique. Weight magnitude pruning is old, but it has recently\nbeen validated as one of the most effective and fine-grained pruning\ntechniques.[7] While other compression methods may achieve smaller model sizes,\nthis is orthogonal to our main contributions.\n\nAlso, some of our conclusions are independent of magnitude weight pruning:\n\n- Fine-tuning does not change the weight distribution much, giving further\n  evidence for focusing on compressing during pre-training rather than for\n  specific tasks.\n\n- Ablating BERT's inductive bias affects different tasks at different rates.\n  This provides an additional lens into why language model pre-training helps\n  other tasks, which is particularly interesting to the natural language\n  processing community, since we lack a philosophical justification for LM\n  pre-training.\n\nAgain, we thank you for your reviews and hope you will consider allowing us to\npresent this work at ICLR 2020.\n\n[1] https://arxiv.org/abs/1811.00206 / https://dl.acm.org/citation.cfm?doid=3289602.3293898\n[2] https://dl.acm.org/citation.cfm?doid=3293883.3295701\n[3] https://openreview.net/forum?id=HJaDJZ-0W\n[4] https://openreview.net/forum?id=H1eA7AEtvS\n[5] https://openreview.net/forum?id=rJx0Q6EFPB\n[6] https://openreview.net/forum?id=SJxjVaNKwB\n[7] https://arxiv.org/abs/1902.09574\n[8] https://tinyurl.com/yjj33x45 "}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlPOCEKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1214/Authors|ICLR.cc/2020/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159471, "tmdate": 1576860541606, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment"}}}, {"id": "rklUVnVDiH", "original": null, "number": 2, "cdate": 1573501997640, "ddate": null, "tcdate": 1573501997640, "tmdate": 1573501997640, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "H1xSiSF6tr", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Thank you so much for reviewing our paper!\n\nOur response to this review has been merged with the response to Review #2, since your concerns have a large overlap. Please view it here: https://openreview.net/forum?id=SJlPOCEKvH&noteId=HkxT-iVDor"}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlPOCEKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1214/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1214/Authors|ICLR.cc/2020/Conference/Paper1214/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159471, "tmdate": 1576860541606, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Comment"}}}, {"id": "ByeKAjOzir", "original": null, "number": 2, "cdate": 1573190609443, "ddate": null, "tcdate": 1573190609443, "tmdate": 1573191734114, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Public_Comment", "content": {"title": "Questions on the main claim of this paper", "comment": "Previous reviewers have mentioned that the experiment result from this paper is too limited in the perspective of weight pruning.\nHowever, my concern is the main claim of this paper: the three regimes about transfer learning: low, medium and high. This claim maybe is right, maybe not. But the experiments in this paper do not prove their claim to me:\n\n1, the accuracy score declines on downstream tasks do not determinedly correlates to the \"prevent useful pre-training information from being transferred to downstream tasks\", there are many other explanations, like this: the pruning in this paper destroyed the representation of language model. How do the authors assure the useful pre-training information is prevented to be transferred, rather than destroyed before transferred?\n\n2, This paper discusses the transfer learning, but only adopts almost half of the tasks on GLUE, and not mentioning SQUAD. Although the authors mention their reason using one/two sentences. But this really confused me and shocked me, how the conclusion could be true on transfer leaning when you just choose half of the transfer learning tasks manually?\n\n3, The experiment figure is coarse, in figure 1, the \"average GLUE loss\"? Interesting, when you choose half of the GLUE tasks, you still need to average them?\n"}, "signatures": ["~Anonymous_Review1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anonymous_Review1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504197964, "tmdate": 1576860575069, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Public_Comment"}}}, {"id": "BJeQOh2kjr", "original": null, "number": 1, "cdate": 1573010538718, "ddate": null, "tcdate": 1573010538718, "tmdate": 1573010538718, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Public_Comment", "content": {"title": "Some Concerns about the Experiments", "comment": "1. The advantage of BERT is the BERT-LARGE, instead of BERT-BASE. This paper only shows the results on BERT-BASE, so the conclusion does not make sense to me, based only on the experiments about BERT-BASE. At least it is half-baked, maybe less than half, as the BERT-LARGE is more difficult than BERT-BASE for experiments, I think.\n\n2. The setting about the experiments is not clear, on what kind of device do the authors conduct their experiments?\n\n3. The pruning method in this paper is not new and is very old. \n\n4. The final pruning ratio is just 30~40%, and the pruning process is progressive(iteratively prune 10%). I think this is drop-out, instead of pruning. Because the dropout ratio is often 20%~30%, bigger than the iteratively pruning step in this paper, and close to the final pruning result in this paper. \n"}, "signatures": ["~Anonymous_Review1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anonymous_Review1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504197964, "tmdate": 1576860575069, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1214/Authors", "ICLR.cc/2020/Conference/Paper1214/Reviewers", "ICLR.cc/2020/Conference/Paper1214/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Public_Comment"}}}, {"id": "HJeqpvgaYr", "original": null, "number": 1, "cdate": 1571780545901, "ddate": null, "tcdate": 1571780545901, "tmdate": 1572972497941, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper conducts a series of interesting experiments on compressing BERT and makes several conclusions. The compression technique is magnitude weight pruning based on an existing work. The paper mainly tested different compression rates and the stages when the compression can be applied. Compared to the existing work, one main contribution of the paper is to show that the BERT model can be pruned prior to fine-tuning any specific downstream tasks by 30%-40% without affecting all tested downstream tasks much. The paper is well motivated and presents interesting experimental results and conclusions. I have some concerns on their experiment details, which needs some clarification.\n\n1. the observation in 3.4 is a little counter-intuitive to me. The model has all pre-trained weights and should be able to determine, during fine-tuning, which weights to decrease to nearly zero or to abandon. However, the experimental results show that the pruning at that point produces a worse dev accuracy. For the experiments, 3 epochs is used for fine-tuning and then the pruning is applied. I was wondering what happen if you first fine-tune the model to get the best dev accuracy and prune the weights at that point. How did you choose the number 3? I am guessing that the pruning in the middle of fine-tuning process may throw away useful information too early. \u00a0 \n2. It will be helpful to show the thresholds of pruning and how these thresholds relate to the training loss and accuracy. I think the value of the thresholds can tell whether some pruning ratios are reasonable.\n3. when the authors continue training the model, for example in 3.4, the training stops when the training losses are comparable. Why did the training loss is used as the metric instead of the dev accuracy? Figure 1 right seems to show that those models are overfitting.\u00a0"}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576220827376, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Reviewers"], "noninvitees": [], "tcdate": 1570237740647, "tmdate": 1576220827388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review"}}}, {"id": "H1xSiSF6tr", "original": null, "number": 2, "cdate": 1571816861294, "ddate": null, "tcdate": 1571816861294, "tmdate": 1572972497898, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work is an empirical study of testing how pruning at the pre-training stage affects subsequent transfer learning (through fine-tuning) stage. The main idea is to carefully control the amount of sparsity injected into BERT through weight magnitude pruning and study the impact on accuracy. The experimental setup is mostly well done, especially the part that disentangles the complexity restriction and information deletion. During the exploration, the authors made several interesting observations, such as 30-40% model weights do not encode any useful inductive bias, which could help shed some light for future work on both training and compressing BERT-like models.\n\nOverall, the paper is well written and explained. The goal is meaningful, and this is a sensible contribution to the ongoing interests of compressing BERT-like large models for efficient training and inference. \n\nMy major concern is on its novelty and how directly it can provide benefit to computation.  First, although the findings are interesting, the methods used in this paper are not new.  Various pruning techniques have been explored in prior work, which makes the novelty contribution of this paper somewhat limited. \n\nFurthermore, the study has mostly focused on the impact of random sparsity to accuracy. However, as it is known that it is really difficult for modern hardware to benefit from random sparsity because it leads to irregular memory accesses, which negatively impact the performance. It has been observed that speedups are very limited or can be negative even the random sparsity is >95% [1]. Therefore, it is hard to judge how inference or training can benefit from 30-40% weight sparsity. Going forward, the authors are encouraged to choose pruning methods that lead to regular memory access to avoid adversely impacting practical acceleration in modern hardware platforms.\n\n[1] Learning Structured Sparsity in Deep Neural Networks. Wen et al. NeurIPS 2016"}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576220827376, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Reviewers"], "noninvitees": [], "tcdate": 1570237740647, "tmdate": 1576220827388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review"}}}, {"id": "BJeAGC3pYS", "original": null, "number": 4, "cdate": 1571831317985, "ddate": null, "tcdate": 1571831317985, "tmdate": 1572972497858, "tddate": null, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "invitation": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nThis work explores weight pruning for BERT. It finds that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of\npruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation.\n\nMy major concern about this work is its technical innovation and value to the community.\n1. This is simply a study of model pruning for BERT. There is nothing new technically.\n\n2. It shows BERT can be pruned for 30-40% parameters. Actually, this is not surprising; instead I'm even disappointed about this result. 30-40% weight reduction does not really speed up inference much or save model size much. Besides, to handle sparse weight matrixes, one may need additional operations to use the pruned models on a modern GPU.\n\n3. Several other submissions show that BERT models can be compressed for 5-10x without accuracy loss. Comparing with this work, this paper seems to tell me that pruning is not suitable for BERT.   "}, "signatures": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1214/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning", "authors": ["Mitchell A Gordon", "Kevin Duh", "Nicholas Andrews"], "authorids": ["mgordo37@jhu.edu", "kevinduh@cs.jhu.edu", "noa@jhu.edu"], "keywords": ["compression", "pruning", "pre-training", "BERT", "language modeling", "transfer learning", "ML", "NLP"], "abstract": "Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.", "pdf": "/pdf/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "paperhash": "gordon|compressing_bert_studying_the_effects_of_weight_pruning_on_transfer_learning", "original_pdf": "/attachment/dd50bc024ec86e5254325e826e5552c8893fc1f1.pdf", "_bibtex": "@misc{\ngordon2020compressing,\ntitle={Compressing {\\{}BERT{\\}}: Studying the Effects of Weight Pruning on Transfer Learning},\nauthor={Mitchell A Gordon and Kevin Duh and Nicholas Andrews},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlPOCEKvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlPOCEKvH", "replyto": "SJlPOCEKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1214/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576220827376, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1214/Reviewers"], "noninvitees": [], "tcdate": 1570237740647, "tmdate": 1576220827388, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1214/-/Official_Review"}}}], "count": 10}