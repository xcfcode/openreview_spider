{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396496528, "tcdate": 1486396496528, "number": 1, "id": "ryKUnfUue", "invitation": "ICLR.cc/2017/conference/-/paper309/acceptance", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper proposes some novel architectural elements, and the results are not far from published DNC results. However, the main issues of this paper are the complexity of the model, lack of justification for certain architectural choices, gaps with reported DNC numbers on BABI, and also a somewhat toy-ish task."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396497012, "id": "ICLR.cc/2017/conference/-/paper309/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396497012}}}, {"tddate": null, "tmdate": 1485213721224, "tcdate": 1485213721224, "number": 4, "id": "ryZXgfNPx", "invitation": "ICLR.cc/2017/conference/-/paper309/official/comment", "forum": "BkSmc8qll", "replyto": "H11PCHUHx", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "content": {"title": "Incremental paper", "comment": "The reported results of this paper are indeed close to DNC and have demonstrated the effectiveness of the proposed improvements. It is a solid incremental paper and the experimental result is encouraging.  I improved my previous rating of the paper.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627728, "id": "ICLR.cc/2017/conference/-/paper309/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627728}}}, {"tddate": null, "tmdate": 1485212958173, "tcdate": 1482243543782, "number": 3, "id": "Hkg1A2IVx", "invitation": "ICLR.cc/2017/conference/-/paper309/official/review", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The authors proposed a dynamic neural Turing machine (D-NTM) model that overcomes the rigid location-based memory access used in the original NTM model. The paper has two main contributions: 1) introducing a learnable addressing to NTM. 2) curriculum learning using hybrid discrete and continuous attention. The proposed model was empirically evaluated on Facebook bAbI task and has shown improvement over the original NTM.\n\nPros:\n+ Comprehensive comparisons of feed-forward controllers v.s. recurrent controllers\n+ Encouraging results on the curriculum learning on hybrid discrete and continuous attentions\n\nCons:\n- Very weak NTM baseline (due to some hyper-parameter engineering?) in Table 1, 31% err. comparing to the NTM 20% err. reported in Table 1 in(Graves et al, 2016, Hybrid computing using a neural network with dynamic external memory). In fact, the NTM baseline in (Graves et al 2016) is better than the proposed D-NTM with GRU controller. Maybe it is worthwhile to reproduce their results using the hyper-parameter setting in their Table2 which could potentially lead to better D-NTM performance?\n- Section 3 of the paper is hard to follow. The overall clarity of the paper needs improvement.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627068, "id": "ICLR.cc/2017/conference/-/paper309/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper309/AnonReviewer3", "ICLR.cc/2017/conference/paper309/AnonReviewer1", "ICLR.cc/2017/conference/paper309/AnonReviewer4"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627068}}}, {"tddate": null, "tmdate": 1483572038849, "tcdate": 1482941226273, "number": 6, "id": "BkzNmP-rg", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "H1Rs4CRme", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Code Available, Answers +  Further Clarifications", "comment": "We apologize for the delay and thank you for your comments. We have uploaded a newer version of the paper, fixing the issues you have pointed out in your earlier comment.\n\n> Are you planning to release the code and what is your estimate release date?\nYes, we were planning to release our codes and have released them on github:\nhttps://github.com/caglar/dntm\n\n> summarize the terms in the cost \u2026\n\nIn our paper, we have defined our cost with the REINFORCE gradients. We provide the cost which the model is minimizing in Section 4. In that equation, seven different terms appear. One of them is for the cross-entropy cost for predicting the answer to the question. The rest of the six terms are basically defined for the REINFORCE with the read, write and erase head\u2019s terms along with the entropy regularizations. 3 of the 6 terms are basically the entropy regularization terms for the REINFORCE. In a nutshell the cost function, we have defined in section 4, $C^n(\\TT)$ is just minimizing the cross-entropy for predicting the answer to the question with REINFORCE and entropy regularization for the read, write and erase heads. \n\n> Some variables seems to have a different definition. In particular w_t and b, would it be possible to clarify this?\nOK thanks, we will clarify those definitions.\n\n> \u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\ngamma_t is an output of a single-layer MLP conditioned on the hidden state of the controller. As we mention in the same quoted sentence, $\\gamma_t$ is a scalar (since u_t^{\\gamma} is a vector and h_t^{\\gamma} is also a vector). We will clarify this in the text further.\n\n> \"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn Appendix E, we have provided additional experimental results on training with soft attention during the training and rounding attention at the test-time (namely using discrete attention). However, this approach didn\u2019t work very well.\n\n>  In the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an \u2026\n\nIn Memory network (Weston et al. 2015b) paper, authors defined a matching function, s(.) which assigns a weight to each memory location in the memory. Although they didn\u2019t train their model in an end-to-end manner and s(.) does not use a softmax. It is still possible to think of s(.) as a discrete attention mechanism. However, indeed the soft-attention mechanism in a more traditional sense (as is done in NMT) is introduced to memory networks in Sukhbaatar et al., 2015.\n\n> ... the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. \n\nThanks for your comment, we will further clarify those paragraphs. In that paragraph, by referring to \u201cmemory networks\u201d, we do not discuss the particular memory model proposed by Weston et al, 2015. Rather, we are referring to models using memory without learning to write as opposed to NTM and gave two successful applications of those models on different NLP tasks. \n\n> In your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, ...\n\nZaremba & Sutskever used an LSTM controller and for each timestep, their controller emits the current input tape, the value of the current memory cell, and a representation of all the actions that have been taken in the previous timestep. The addressing mechanism of RL-NTM is very different from D-NTM too. In that sense, it is difficult to compare the both models in an absolute manner. But our model with FF-controller and the discrete attention is simpler than the RL-NTM\u2019s LSTM controller and D-NTM\u2019s GRU controller, both in terms of \"number of parameters\" and in terms of \"difficulty of implementation\". Let us note that our FF-controller is just a simple MLP which receives the current input(representation of the fact) and the memory cell. On the other hand, FF-controller does not have a memory itself. Thus it can not keep a history of its previous actions and its only way to access the previously seen facts is the external memory. However, LSTM/GRU controller has their own memory through the recurrent connections across the cell and the hidden-states. Thus an LSTM controller with REINFORCE over the external memory can still answer an answer a question correctly, even if the controller fails to read the correct memory location by falling back to the memory of the LSTM controller. However, feedforward controller can not do that, since the only memory that it relies on to answer the question is the external memory. However, with soft attention and the feedforward controller, the controller almost never puts 0 weight on the correct memory cell in the external memory which helps the gradients flow through the write memory location and learn the addressing more easily. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1483280191608, "tcdate": 1483280191608, "number": 9, "id": "SkdHk9LSl", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "HJpJEBZNl", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "On the gap between D-NTM and Memory Networks", "comment": "\nThanks for your valuable feedback and review.\n\n> Big gap to MemN2N and DMN+ in performance.\nBoth MemN2N and DMN+ computes the attention over the whole input context, such that its memory grows linearly as the input sequence grows. For the bAbI task, very likely that the optimal strategy is to perform attention over all facts in the story. However, D-NTM keeps a limited-size memory and it learns to write into the memory as well which is much more difficult. In particular, on tasks which involve ambiguities, it is very easy for the controller to learn a deficient explicit memory usage that does not generalize well for the other tasks. Because the part of the memory to read from or write into becomes ambiguous as well. Furthermore, reading depends on the writing as well, if the controller fails to write into the correct location in the memory, reading from that part of the memory becomes futile. The advantage of using limited external memory becomes more evident when the input sequence gets very long, such that computation of attention over the whole input sequence would be infeasible. \n\n> Code not available.\nWe made our code freely available at:\n https://github.com/caglar/dntm\n\n> There could be more exp\u2026\nThanks for noticing this we will add more results on more realistic tasks. We plan to add one more experiment on SNLI task to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1483274116885, "tcdate": 1483274069672, "number": 8, "id": "Sk0LD_LBe", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "rJWPUiWNl", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Further clarifications", "comment": " Thanks for your feedback.\n>  Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model.\n\nPlease note that 6 of the terms of our cost is for the REINFORCE with entropy regularization. REINFORCE with entropy regularization is a very standard way to train neural networks with discrete stochastic decision variables and has been used in many other papers [1,2 ...]. One of the term is the original cross-entropy cost function for predicting the answer to the question about the story. The other two terms are for the regularizations, which are justified in the paper. Thus if we include the original cost, REINFORCE with entropy regularization and the other two regularizations which we have proposed in this paper (next fact prediction and the read/write consistency) we would have 9 terms. However, only TWO OF THE TERMS of the cost are introduced in our paper. The rest of the terms are very typical for neural networks trained with REINFORCE. We have justified all the design choices we have taken in our paper and achieved significant improvements over LSTMs despite not using tricks such as linear restart, multi-runs with different seeds (or hyperparameters), joint training, etc \u2026 which have been used in most of the recent memory augmented neural networks related papers.\n\n>  slightly above those of a vanilla LSTM\nOn bAbI task, LSTM got 36.41% and this result is reported in Sukhbaatar 2015 for non-joint training (with joint training it is possible to improve this result, but that would not be a fair comparison). Our model with GRU controller got 21.8 and using feedforward controller we got 12.8 percent error over all tasks. We find the improvement from 36.41% to 12.8% error to a significant improvement. \n\n> There is no code available nor plan to release it (afaik).\nWe have released our code on github: https://github.com/caglar/dntm . We are improving our codebase and adding more documentation into our repository at the moment.\n\n> The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d) \u2026\n\nWe find \u201csoftplus\u201d to be a widely adopted term for log(exp(x)+1) and many papers in machine learning literature has used it [3]. However, we agree that some people still may not be very familiar with it. \nWe have already improved the readability of our equations in the direction that you have suggested. Please check the new revised version of the paper. We will make further clarifications(mostly in Section 2 and 3) and upload another version of the paper soon as well.\n\n[1] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., ... & Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2(3), 5.\n[2] Vezhnevets A, Mnih V, Osindero S, Graves A, Vinyals O, Agapiou J. Strategic attentive writer for learning macro-actions. In Advances in Neural Information Processing Systems 2016 (pp. 3486-3494).\n[3] https://scholar.google.ca/scholar?q=softplus+machine+learning&btnG=&hl=en&as_sdt=0%2C5"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1483263575391, "tcdate": 1483263575391, "number": 7, "id": "H11PCHUHx", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "Hkg1A2IVx", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Our NTM results are not that far from DNC paper. 20% result which Graves et al 2016 has reported is for the BEST NETWORK AFTER MULTIPLE RUNS.", "comment": "Thanks for your valuable feedback and comments about our paper.\n\n> Very weak NTM baseline ...\nThe experimental setup followed on the bAbI dataset in DNC paper (Graves et al 2016) and our paper is very different. Firstly, in Graves et al 2016, they report results only on joint-training. However, in our paper, we have only trained the models on each task separately. Potentially, the transfer between different tasks in joint training can have a huge impact on the results and it can improve generalization. \n\nMost importantly, 20% error across over all the tasks which \"Graves et al 2016\" reports is the result obtained with the BEST NETWORK on the validation set after 20 RUNS (please see the caption of Table 1), whereas our results are obtained only from a single-run.  However, in DNC paper (see Table 1), they also report mean results of their networks along with the best network after the multiple runs, their mean result is very close to ours 28.5% with std of +/- 2.9. Nevertheless, please note that the 31% result which we report in our paper is very close to the mean result of the NTM in the DNC paper (within a single standard deviation of their mean result). \n\nLastly, we use the representation of GRU over the tokens in each fact of every story in our paper. However, in Graves et al 2016, their models emit the word embeddings for all the facts in the story. \n\n> Section 3 of the paper is hard to follow\u2026\nThanks for the comment, we have improved the readability of both the Section 2 and Section 3  to fix the issue that you have pointed out and improved the clarity of our paper. We will further improve those two sections and upload a newer version of the paper to openreview again.\n\nIf these two concerns are the main reasons for you to give a low score to our paper, we would kindly ask you to re-evaluate your decision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1482675697080, "tcdate": 1482675697080, "number": 5, "id": "r1YeLUT4e", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "BJmCs11Ql", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Uploaded a new version with Fixed Equations", "comment": "Thanks again for your detailed comment and feedbacks for our paper. We have fixed the typos pointed out by you in this comment and added a new revision of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1482586136558, "tcdate": 1478285853114, "number": 309, "id": "BkSmc8qll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkSmc8qll", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "content": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482160749278, "tcdate": 1482160679961, "number": 4, "id": "Byx4c_HVx", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "About the codes for our models", "comment": "Dear Reviewers and Readers,\n\nFor the codes of the models and the tasks which we have explored/experimented in our paper, please see our repo:\nhttps://github.com/caglar/dntm/\n\nWe are still in the process of refactoring and adding more documentation for our code. We also hope that the framework we used in our paper, will be providing an easy framework to implement Memory Augmented Neural Networks(MANNs) in Theano."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1481910215980, "tcdate": 1481909848959, "number": 2, "id": "rJWPUiWNl", "invitation": "ICLR.cc/2017/conference/-/paper309/official/review", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper introduces a variant of the neural Turing machine (NTM, Graves et al. 2014) where key and values are stored. They try both continuous and discrete mechanisms to control the memory.\n\nThe model is quite complicated and seem to require a lot of tricks to work. Overall it seems that more than 10 different terms appear in the cost function and many different hacks are required to learn the model. It is hard to understand the justification for all of these tricks and sophisticated choices. There is no code available nor plan to release it (afaik).\n\nThe model is evaluated on a set of toy problems (the \u201cbabi task\u201d) and achieves performance that are only slightly above those of a vanilla LSTM but are much worse than the different memory augmented models proposed in the last few years.  \n\nIn terms of writing, the description of the model is quite hard to follow, describing different blocks independently, optimization tricks and regularization. The equations are hard to read, using non standard notation (e.g., \u201csoftplus\u201d), overloading notations (w_t, b\u2026), or write similar equations in different ways (for example, eq (8-9) compared to (10-11). Why are two equations in scalar and the other in vectors? Why is there an arrow instead of an equal?\u2026).\n\nOverall it is very hard to put together all the pieces of this model(s), there is no code available and I\u2019m afraid there is not enough details to be able to reproduce their numbers. Finally, the performance on the bAbI tasks are quite poor compared to other memory augmented models.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627068, "id": "ICLR.cc/2017/conference/-/paper309/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper309/AnonReviewer3", "ICLR.cc/2017/conference/paper309/AnonReviewer1", "ICLR.cc/2017/conference/paper309/AnonReviewer4"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627068}}}, {"tddate": null, "tmdate": 1481884644581, "tcdate": 1481884644581, "number": 1, "id": "HJpJEBZNl", "invitation": "ICLR.cc/2017/conference/-/paper309/official/review", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "content": {"title": "interesting extension to NTM", "rating": "7: Good paper, accept", "review": "The paper extends the NTM by a trainable memory addressing scheme.\nThe paper also investigates both continuous/differentiable as well as discrete/non-differentiable addressing mechanisms.\n\nPros:\n* Extension to NTM with trainable addressing.\n* Experiments with discrete addressing.\n* Experiments on bAbI QA tasks.\n\nCons:\n* Big gap to MemN2N and DMN+ in performance.\n* Code not available.\n* There could be more experiments on other real-world tasks.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627068, "id": "ICLR.cc/2017/conference/-/paper309/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper309/AnonReviewer3", "ICLR.cc/2017/conference/paper309/AnonReviewer1", "ICLR.cc/2017/conference/paper309/AnonReviewer4"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627068}}}, {"tddate": null, "tmdate": 1481786434944, "tcdate": 1481725093669, "number": 3, "id": "H1Rs4CRme", "invitation": "ICLR.cc/2017/conference/-/paper309/official/comment", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer1"], "content": {"title": "Code available? + other questions", "comment": "The model described in his paper is quite complicated and reproducing the method from its description may be challenging: Are you planing to release the code and what is your estimate release date?\n\nDifferent cost functions and regularization are introduce in the paper. Would it be possible to summarize the overall cost function minimized by this model? \n\nSome variables seems to have different definition. In particular w_t and b, would it be possible to clarify this?\n\n\u201cgamma_t is a shallow MLP\u201d: What does it mean? Is gamma_t a function or a variable? It seems from eq. (10) that it is a vector (or a scalar?).\n\n\"curriculum learning for the discrete attention\": Can you compare this to simpler schemes? Like rounding the continuous attention?\n\nIn your introduction, you state that \"it is possible to use the discrete non-differentiable attention mechanism\", referencing Zaremba & Sutskever, 2016 that use REINFORCE and a simple controller. However, in your sec. 4, you are stating that  \"Training discrete attention with feed-forward controller and REINFORCE is challenging\". It seems that these two statements contradict each other, could you comment on it? \n\nIn the introduction, you state that \"Memory network (Weston et al. 2015b) [..] uses an attention-based mechanism to index them\". To the best of my knowledge, it is actually Sukhbaatar et al., 2015 that has introduce the attention mechanism to memory networks. \n\nIn the introduction, the authors state that \u201cmemory networks [\u2026] [are] used in real tasks (Bordes et al. 2015, Dodge et al. 2015)\u201d. However in both papers, they use memory network systems different from Weston et al. The current statement is quite misleading, would it be possible to clarify it?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627728, "id": "ICLR.cc/2017/conference/-/paper309/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627728}}}, {"tddate": null, "tmdate": 1481234063909, "tcdate": 1481234063899, "number": 3, "id": "BkdqIIw7x", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "BJmCs11Ql", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "RE: formulas, notations, address matrix usage, etc ...", "comment": "Thanks for your valuable comments and questions.\n\n> You introduce the address matrix, so I expected that you only use the address vectors $a$ for the addressing mechanism but in chapter 3 where you describe the addressing mechanism, you use the whole memory $m$ and not just $a$. Is that right? \n\nAddressing for D-NTM involves using both the address vectors $a$ and the content vectors $c$. $m$ in the paper corresponds to the concatenation of $a$ and the $c$ vectors. By using $m$ instead of $a$ in the addressing mechanism our model can combine both the content-based and the location-based addressing together.\t\n\n> If so, I don't see where you use the address matrix at all and how this is different to the normal NTM. Can you explain? If you meant $a$ instead of $m$ in chapter 3, the update $\\overline{m}$ is still meant to update both address vector and content vector, right?\n\n$m$ is being used only when we compute the cosine similarity between the the key generated by the controller and the cells $m$ in the memory. However, when we update the memory, we do not update/write the address part of the memory. However, since the $a$ is still being used to compute the addressing weights, it can still be learned with the backpropagation. In the Equation 6, the $m_t$ should have been $c_t$. Thanks for your remark and the correction.\n\n> I think for better understandability, you should state the vector space of all the variables, e.g. $\\overline{m}^t \\in \\R^{d_h}$, etc. Also, $m_i$ is maybe confusing, why not call it $M_i$ or $M[i]$?\n\nThanks for pointing this. We are going to fix them in the next update of the paper.\n\n> In equation (3), the $h^t$ and $x^t$ are not bold, are that different variables?\n\nThanks for pointing out this typo. We are going to fix it.\n\n>For erasing and writing, you use $e^t$ and $u^t_j$. So it means that the erase is global over all memory cells?\n\nThe writing mechanism is very similar to the one used in [1]. $e^t$ is a vector in $R^{d_c}$ where $d_c$ is the number of features in the memory cell. It influences the columns of the memory content and $u^t_j$ is a scalar that influences each memory cell/row.\n\n> However, in chapter 3, you say that the erase ($e$) vector is just the same as the read ($w$) and write ($u$) vector. So should it be $e^t_j$ here?\n\nComputation of the address vector described in Section 3.1 is only true for \"read\" and \"write\" operations. As described above erase vector is shared by all the memory cells and it is just an sigmoid MLP conditioned on the hidden state of the controller as also described in [1]. Thanks for pointing out our mistake in the equations. We are going to fix this.\n\n> And is it $e^t \\in \\R^{d_h}$ or is it a scalar?\n\nAs described earlier $e^t \\in \\R^{d_c}$ where $d_c$ corresponds to the number of features in $c^t$. We will fix this mistake.\n\n> And is it $u^t_j \\in \\R^{d_h}$ or is it a scalar?\n\nYes it is scalar.\n\n> In chapter 3, you write $w^t_i$ which is a scalar. In equation (5), it is $w^t$ in bold, so that is over all $i$, i.e. $w^t \\in \\R^N$?\n\nYes that is true.\n\n> In chapter 7.1.2, what is CBA? In table 1, what is LBA?\n\nCBA corresponds to the \"Content Based Addressing\". LBA  corresponds to the \"Location based addressing\". As mentioned in Table 1, LBA^$\\ast$ combines both location and the content based addressing as described in [1].\n\n> In Table 1 & 2, you call it \"Soft D-NTM\" and in Table 3 & 4, you call it \"D-NTM cont.\". Is that the same?\n\nYes they are same, we will make it consistent. \n\n> Have you tried to use a NTM with multiple steps? I only see 1-step NTMs.\n\nIn Table 3, we have shown multi-step results (3-step results) for both NTM and D-NTM. \n\n> Why do you think the discrete D-NTM is worse than the cont. D-NTM for pMNIST?\n\nThe length of the sequences on p-mnist is much longer than the bAbI tasks. The variance of the reinforce increases with the length of the sequences as well, which makes it more difficult to train over very long sequences.\n\n> As well as with FF controller (Table 2).\n\nWe found training with REINFORCE by just using FF-controller to be very challenging. The main reason is that the model has to learn to read the correct location since the only memory that the controller relies on is the external memory that it uses. However, in GRU-controller the model can also use the GRU units of the controller as a memory.\n\n> The curriculum strategy of chapter 4 was only applied for bAbI trained with FF controler (Table 2) and not for the other experiments?\n\nThe reason why we use curriculum learning for FF-controller using discrete attention was due to the difficulty of training which arises due to the difficulty of learning a proper addressing mechanism as we discussed earlier. However, for the other experiments we used GRU-controller which we did not experience any difficulty in terms of optimization.\n\n[1] Graves, A., Wayne, G., & Danihelka, I. (2014). Neural turing machines. arXiv preprint arXiv:1410.5401."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1481158631169, "tcdate": 1481158631164, "number": 2, "id": "SJyeg4UXx", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "Sk4xfi1mx", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Re:Question about experiments", "comment": "> Is sole FF being compared in table 1? \nWe have put the results of FF-controller in Table 2. On the other hand, our overall best performing model was D-NTM with FF-controller and soft attention. The rest of the results for the same data configurations, but for the GRU controller are presented in Table 1.\n\n> sec 7.1.2 mentioned that FF controller are harder to train then why is table 2 number are better than table 1?\n\nIndeed, FF controller with discrete attention is more difficult to train than the GRU controller using soft/discrete addressing. This phenomenon can be observed from the low performance compared to the results of FF-controller with soft attention and GRU-controller with discrete attention. However, this does not apply to FF-controller with soft attention. The final generalization performance of the model using FF-controller was better. The main reason of why GRU controller underperforms (in terms of generalization) compared to the FF-controller and sometimes GRU controller tends to learn to ignore the memory and despite that still manages to achieve good training performance just by relying on the memory of the controller.\n\n> how comparable are the D-NTM vs MemN2N in table 1 in terms of the number of parameters and representation power?\nD-NTM with FF-controller has almost the same number of parameters with the MemN2N. However, GRU controller has more parameters (but still comparable to the MemN2N)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1481109334553, "tcdate": 1481109334546, "number": 1, "id": "ByC8k_rQx", "invitation": "ICLR.cc/2017/conference/-/paper309/public/comment", "forum": "BkSmc8qll", "replyto": "HkwrZRAMg", "signatures": ["~Caglar_Gulcehre1"], "readers": ["everyone"], "writers": ["~Caglar_Gulcehre1"], "content": {"title": "Fixing the footnote", "comment": "Thanks for pointing out our mistake. That footnote was unnecessary, we are going to remove in the next updated version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627869, "id": "ICLR.cc/2017/conference/-/paper309/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627869}}}, {"tddate": null, "tmdate": 1480729068493, "tcdate": 1480729068490, "number": 2, "id": "Sk4xfi1mx", "invitation": "ICLR.cc/2017/conference/-/paper309/pre-review/question", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer4"], "content": {"title": "Question about the experiments", "question": "Is sole FF being compared in table 1? \n\nsec 7.1.2 mentioned that FF controller are harder to train then why is table 2 number are better than table 1?\n\nhow comparable are the D-NTM vs MemN2N in table 1 in terms of the number of parameters and representation power?\n\n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959347522, "id": "ICLR.cc/2017/conference/-/paper309/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper309/AnonReviewer3", "ICLR.cc/2017/conference/paper309/AnonReviewer4"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959347522}}}, {"tddate": null, "tmdate": 1480682442966, "tcdate": 1480682442962, "number": 1, "id": "BJmCs11Ql", "invitation": "ICLR.cc/2017/conference/-/paper309/pre-review/question", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "content": {"title": "formulas, notations, address matrix usage, etc", "question": "You introduce the address matrix, so I expected that you only use the address vectors $a$ for the addressing mechanism but in chapter 3 where you describe the addressing mechanism, you use the whole memory $m$ and not just $a$. Is that right? If so, I don't see where you use the address matrix at all and how this is different to the normal NTM. Can you explain? If you meant $a$ instead of $m$ in chapter 3, the update $\\overline{m}$ is still meant to update both address vector and content vector, right?\n\nI think for better understandability, you should state the vector space of all the variables, e.g. $\\overline{m}^t \\in \\R^{d_h}$, etc. Also, $m_i$ is maybe confusing, why not call it $M_i$ or $M[i]$?\nIn equation (3), the $h^t$ and $x^t$ are not bold, are that different variables?\n\nFor erasing and writing, you use $e^t$ and $u^t_j$.\nSo it means that the erase is global over all memory cells?\nHowever, in chapter 3, you say that the erase ($e$) vector is just the same as the read ($w$) and write ($u$) vector. So should it be $e^t_j$ here?\nAnd is it $e^t \\in \\R^{d_h}$ or is it a scalar?\nAnd is it $u^t_j \\in \\R^{d_h}$ or is it a scalar?\n\nIn chapter 3, you write $w^t_i$ which is a scalar. In equation (5), it is $w^t$ in bold, so that is over all $i$, i.e. $w^t \\in \\R^N$?\n\nIn chapter 7.1.2, what is CBA?\nIn table 1, what is LBA?\nIn Table 1 & 2, you call it \"Soft D-NTM\" and in Table 3 & 4, you call it \"D-NTM cont.\". Is that the same?\n\nHave you tried to use a NTM with multiple steps? I only see 1-step NTMs.\n\nWhy do you think the discrete D-NTM is worse than the cont. D-NTM for pMNIST?\nAs well as with FF controller (Table 2).\n\nThe curriculum strategy of chapter 4 was only applied for bAbI trained with FF controler (Table 2) and not for the other experiments?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959347522, "id": "ICLR.cc/2017/conference/-/paper309/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper309/AnonReviewer3", "ICLR.cc/2017/conference/paper309/AnonReviewer4"], "reply": {"forum": "BkSmc8qll", "replyto": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959347522}}}, {"tddate": null, "tmdate": 1480675646841, "tcdate": 1480675646838, "number": 2, "id": "HkwrZRAMg", "invitation": "ICLR.cc/2017/conference/-/paper309/official/comment", "forum": "BkSmc8qll", "replyto": "BkSmc8qll", "signatures": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper309/AnonReviewer3"], "content": {"title": "footnote page 2", "comment": "I think the footnote is missing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This addressing scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both continuous, differentiable and discrete, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and  write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of Facebook bAbI tasks and shown to outperform NTM and LSTM baselines. We also provide further experimental results on sequential MNIST, associative recall and copy tasks.", "pdf": "/pdf/016146d3fbbf4996346b1820211c5b307573a049.pdf", "TL;DR": "We propose a new type of Neural Turing Machine, which is simpler than the original model and achieves better results than the baselines on non-trivial tasks. ", "paperhash": "gulcehre|dynamic_neural_turing_machine_with_continuous_and_discrete_addressing_schemes", "keywords": ["Deep learning", "Natural language processing", "Reinforcement Learning"], "conflicts": ["umontreal.ca", "nyu.edu", "twitter.com"], "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "authorids": ["gulcehrc@iro.umontreal.ca", "apsarathchandar@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287627728, "id": "ICLR.cc/2017/conference/-/paper309/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkSmc8qll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper309/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper309/reviewers", "ICLR.cc/2017/conference/paper309/areachairs"], "cdate": 1485287627728}}}], "count": 19}