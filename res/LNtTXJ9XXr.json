{"notes": [{"id": "LNtTXJ9XXr", "original": "u5Wn2MkkRrS", "number": 2196, "cdate": 1601308241796, "ddate": null, "tcdate": 1601308241796, "tmdate": 1614985690776, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WZ83vD8W82", "original": null, "number": 1, "cdate": 1610040459082, "ddate": null, "tcdate": 1610040459082, "tmdate": 1610474061966, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviews were a bit mixed, with some concerns on the incremental nature of this work, which the AC concurs (after independently going through both the submission and Xie et al 2020). In a nutshell, the main contribution on the authors' side appears to be a simple linear interpolation of two masks so that it is possible to leverage attacks with varying strengths. Other claimed contributions are not substantiated. In particular: \n\n(a) Fig 1 and its conclusion are a bit disturbing. It is suggested that the authors back up their claim with more empirical and theoretical evidence. For example, why can one conclude from the same mean and variance that there is no distribution mismatch between clean and adversarial examples (as claimed in Xie et al)? If the two sources have similar distribution, why is there a sharp difference in gamma for the two? When one claims different results from previous work, due diligence is required. For instance, did the authors reproduce the mean and variance on the same architecture and dataset of Xie et al? How about other BN layers (in addition to the first one)? How to explain the difference in gamma? The fact that you are using different masks for different epsilon is an indication that their distributions are probably different. The authors mentioned the joint effect between gamma and relu activation, which could be potentially insightful. However, this is a bit speculative in its current presentation. How about an ablation study with leaky relu or tanh/sigmoid? Without these careful comparisons, these claimed contributions are not appropriate to publish in their current form.\n\n(b) As the reviewers pointed out, why BN, after all for models that do not use BN they still suffer from adversarial examples? Even when we restrict to models that use BN, why replicating BN for different sources helps generalization? Here, an excellent experiment point is to compare to fine-tuning or replicating other layers in the network. During rebuttal, the authors only tried to fine-tune ONE convolution layer and quickly concluded its ineffectiveness. Note that in contrast the authors fine-tuned ALL BN layers. How about all convolution layers, some pooling layers, the last softmax layer? These experiments could help us understand if there is some magic in BN. Or maybe it is just more convenient to fine-tune BN because of its small number of parameters? In any case these experiments would largely strengthen the findings of this work.\n\n(c) As pointed out by the reviewers, Xie et al hinted at the advantage of using multiple BN masks and the authors proposed to linearly interpolate the masks. In the ablation study, what if we increase the number of BN masks in Xie et al, say we discretize p into 11 values p = {0, 0.1, ..., 0.9, 1} and have 1 mask for each value? Here an interesting experiment is to compare K = 11 (basically AdaProp) with smaller K (such as 2 or 5). The authors seemed to suggest that a larger K does not seem to help, which would be clarified through the preceding experiment (and perhaps more). Note that using a uniformly random p is equivalent as adversarial training with a weaker (and varied) attack, and the better tradeoffs shown in the experimental section (e.g. Table 3) are perhaps expected.\n\n(d) As pointed out by the reviewers, a head to head comparison against AdaProp (preferably with more masks) is desirable. The authors mentioned some difficulty in conducting this experiment fully. If it is only the software side, maybe check the sources here: \nhttps://paperswithcode.com/paper/adversarial-examples-improve-image\n\n(e) Finally, a minor point: Algorithm 1 with k=2 and p=1 does not reduce to AdaProp as one will only train on adversarial examples and ignore all clean samples? \n\nIn the end this submission appears to be a bit incremental. However, the authors are strongly suggested to follow the reviewers' comments to further polish their work and address the concerns above. With proper revision this work can eventually become a solid contribution on top of AdaProp."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040459069, "tmdate": 1610474061949, "id": "ICLR.cc/2021/Conference/Paper2196/-/Decision"}}}, {"id": "BWhOnU82YNU", "original": null, "number": 2, "cdate": 1603911132698, "ddate": null, "tcdate": 1603911132698, "tmdate": 1606691601980, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review", "content": {"title": "An interesting extension of AdvProp with limited evidence of practicality", "review": "## Overview \n\nThe paper focuses on the generalization issue with adversarial training that various work has recently demonstrated. The paper studies the role of batch normalization (BN) in adversarial robustness and generalizability. The authors single out the rescaling operator in BN to significantly impact the clean and robustness trade-off in CNNs.  They then introduce Robust Masking (Rob-Mask), which is shares similarities to the CVPR2020 paper by Xie et al. (2020). Xie et al. use an auxiliary BN in adversarial training, which uses different batch normalization parameters for adversarial samples, to improve the generalizability of CNNs. Still, the authors clearly state the differences between Rob-Mask and AdvProp. \n\n## Contributions\n\nThe contributions of the paper are as follows:\n\n1. Showing the effect of BN (and, more specifically, the scale parameter of BN together with ReLU) as adversarial masking.\n\n   a. Authors show that *adversarial fine-tuning* of only the BN parameters of a *vanilla-trained* network provides some adversarial robustness, although at the trade-off losing test accuracy. \n\n   b. Authors show that *standard fine-tuning* of only the BN parameters of an *adversarially-trained* network increases the network's generalizability, although at the trade-off losing robustness. \n2. Showing that interpolating between the BN parameters in Contribution 1 provides a smooth trade-off between generalizability and robustness.\n\n3. Devising an approach for utilizing different perturbation strengths for model training. The authors build on their Contribution 2 and propose $k$ basic (or better to say principle) rescaling parameters, the linear combination of which leads to a rescaling parameter. \n\n4. Providing a short yet informative, ablation study to show the effectiveness of Contribution \n5. Showing experimental benefits over AdvProp on CIFAR10 and CIFAR100 datasets.\n\nContribution 3 turns AdvProp into a particular case of RobMask.  In fact, Xie et al. (2020) mention in their paper that \"a more general usage of multiple BNs will be further explored in future works,\" which seems to be the inspiration behind this paper.  \n\n## Weaknesses\n\n1. The main limiting factor for the impact of this paper is the experiments. The paper only reports performance on CIFAR10 and CIFAR100. Given that the paper can be considered an extension/improvement over AdvProp, it is desirable to have similar largescale experiments in Xie et al. (2020) on ImageNet and its variations. A head-to-head comparison with the experiments in Xie et al. (2020) would provide a clearer picture to show the proposed method's power.\n2. Regarding the practicality of the approach, I am missing a computational analysis of the approach to compare it against BN and AdvProp, e.g., it would be great if the authors provided a head-to-head comparison of training curves. Does your method take much longer to train?\n3. How many times did you run each experiment? What are the standard deviations in Table 3 (and other tables)? Providing this information, at least in the supplementary materials, could clarify your results' statistical significance.\n\n## Questions and comments for the authors\n\n1. The notation $\\gamma_i$ is used both for BN's scaling parameter and for the learning rate, which turns the equations hard to follow.\n2. On the bottom of page 7, you wrote: \"It is because both AdvProp and Adversarial training models are trained with adversarial examples generated with \u000f$\\epsilon= 8/255$, while our methods use a random perturbation where \u000f$\\epsilon_{max}=8/255$.\"  The term \"random perturbation\" is misleading here, as I believe you are also using PGD attack, but the adversarial perturbation's strength is randomized. Is that correct?\n3. Please refer to Weaknesses 2.\n4. I don't find Figure 2 informative at all. I suggest that the authors remove the figure and use the space to address the raised concerns.\n\n## Evaluation logic\n\nI find the paper an interesting extension of the CVPR2020 paper by Xie et al. However, the paper's experimental section does not provide enough information to the reader to see the concrete benefit of the proposed method in training a large scale CNNs. I think the paper could significantly benefit from a more extensive experimental setting. Given the limited novelty and lack of concrete evidence of practicality, I score the paper as a 5. \n\n## Post rebuttal evaluation\n\nI thank the authors for providing answers to the raised questions and providing further experiments. Regarding Figure 3, I suggest that the authors provide accuracy as a function of wallclock instead of epochs currently reported in the paper. As a result of the authors' responses, I increase my score to 6. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101892, "tmdate": 1606915789180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2196/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review"}}}, {"id": "iFHcrJDk6C", "original": null, "number": 9, "cdate": 1606260582637, "ddate": null, "tcdate": 1606260582637, "tmdate": 1606260582637, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "cm3qgaVHmpX", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": "Thanks for the update. However, we want to address the improvement.", "comment": "Thanks for the great suggestion on the adversarial training details and we keep updating it in further revision. However, we still want to address our improvement on the generalization, especially for the clean data. As mentioned in the previous response, the training time of AdvProp[2] and RobMask is almost the same. While kept the running time equally, RobMask has achieved a much more significant improvement (+1 to 2 percent) over normal training and (+0.5 to 1.5 percent) over AdvProp. To our best knowledge, the improvement over generalization around 1% to 2% should be considered as significant. Some works such as Advprop and Network Deconvolution [1] achieving performance improvements around 1% are considered as significant. \n\nPlease let us know if you have any other concerns or disagreements with our responses and claims. Thanks!\n\n\n\n\n[1] Ye, Chengxi, et al. \"Network deconvolution.\" ICLR 2020.\n[2] Xie, C., Tan, M., Gong, B., Wang, J., Yuille, A. L., & Le, Q. V. (2020). Adversarial examples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 819-828)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "cm3qgaVHmpX", "original": null, "number": 1, "cdate": 1603896107649, "ddate": null, "tcdate": 1603896107649, "tmdate": 1606221979188, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review", "content": {"title": "Interesting approach, but needs further analysis ", "review": "Summary:\nThe paper investigates the role of Batch Normalization (BN) in the generalization of deep networks, and its impact in the trade-off between clean and robust accuracy in adversarially trained networks. The authors demonstrate that the rescaling operations in BN when considered in conjunction with the ReLU activation, serve as a feature masking operation. Based on these observations, the authors propose RobMask, which uses a linear combination of such rescaling operations to achieve improved generalization in deep networks. \n\n\nPros: \n1) Interesting observation, and highlights the needs to re-examine techniques from the standard training paradigm that are directly used in adversarial training of deep networks \n2) Easy to integrate with any network architecture that already uses Batch Normalization\n3) Demonstrates enhanced standard performance over different network architectures and datasets\n4) Achieves improved trade-off between clean and robust accuracy for adversarially trained networks on smaller constraint sets (L-infinity eps = 2/255 to 6/255)\n\nCons: \n1) The paper lacks novelty from the standpoint that very similar observations were made by Xie et al. [1]. \n2) Further, the primary results are demonstrated for the case of k=2, using a linear combination of Batch Normalization parameters obtained for normal and adversarial training, which represents only a minor change from the algorithm proposed in [1]. \n3) While the method itself is not difficult to understand, it is still unclear why it helps strike a better balance for the accuracy-robustness trade-off in adversarially trained networks. Could the authors provide an intuitive or theoretical explanation for the same?\n4) The adversarial training of larger networks such as DenseNet-121 can be highly computationally intensive, requiring almost an additional order of magnitude in training time. Thus the performance comparison made in Table-3 at the 20th epoch for models trained using normal training and RobMask is unfair, since RobMask uses 7-step adversarial training. Perhaps a better metric to consider would be the standard performance obtained after a fixed training time.\n5) The clean accuracy shown in Table-1 and Table-3 for the adversarially trained ResNet-18 model on CIFAR-10 is quite low (78%), compared to standard values reported in [2,3], which is often around 82%. \n6) To quote from the last para of Section4: \u201cAlso, the proposed RobMask method is more general than Advprop, and AdvProp can be considered as one special case of RobMask when we set the linear layer rank k to 2 and freeze p = 1 in the whole training process.\u201d Thus, could the authors provide additional results for k=3 or k=4? This is quite important to set apart the proposed method from AdvProp, which is highly similar, as final results are only reported with k=2 in Table-3.\n7) In test time, it is not immediately clear what choice of $u_i$ should be used, since it determines the effective batch normalisation parameters that are used. Could the authors clarify the exact BN parameters used in final evaluation on clean and adversarial samples?\n8) When k is set to a value larger than 2, it is not immediately clear what the different $w_k$\u2019s would represent, since the 2-dimensional $u_i$\u2019s already encode information about different $\\ell_\\infty$ constraints. Could the authors clarify this? Further, it would be beneficial to the reader to include an example with k=3, along similar lines to what is presented in Section 4. \n9) Evaluation on PGD-100 step attack alone is not sufficient, as with stronger attacks such as MultiTargeted attack [4] and AutoAttack [5], the difference in adversarial accuracies might be much lower for eps=2/255 to 6/255, as reported in Table-3.\n10) Since the authors say that the proposed methods does not achieve better adversarial accuracy for eps=8/255 in Table-4 due to the sampling of p, could the authors show results where the maximum constraint is set to 10/255 to show an improvement for the standard evaluation setting of 8/255? This would greatly help in the comparison of different methods, particularly the clean accuracy of different models.\n\n[1] Xie, C., Tan, M., Gong, B., Wang, J., Yuille, A. L., & Le, Q. V. (2020). Adversarial examples improve image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 819-828).\n[2] Wong et al. Fast is Better than Free: Revisiting Adversarial Training, ICLR 2020\n[3] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020, https://arxiv.org/abs/2002.11569 \n[4] Gowal et al., An Alternative Surrogate Loss for PGD-based Adversarial Testing, https://arxiv.org/pdf/1910.09338.pdf\n[5] Croce et al., Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks, ICML 2020\n\nIn summary, although the observations presented in this work are interesting, further analysis and thorough evaluations are required to justify the claims made in this paper.\n\nExpecting the authors to address the following during rebuttal period: \n \n- Please address and clarify the cons as listed above.\n- Could the authors please provide additional details on how the finetuning is performed in Section 3.1?\n- In Figure1, could the authors clarify the quantity along the x-axis? If it represents iterations, or epochs?\n- Could the authors clarify how the plot for Adv-Train is obtained (in relation to the Std-BN finetune plot)? How were the updates performed on the Adv-Train model to obtain this plot?\n- Given that the parameters of the first BN layer are presented in Figure1, since the convolutional layer that occurs before this BN layer is frozen, it is expected that Plots 1(a) and 1(b) are identical for the two cases, and does not offer additional insight. \n- Could the authors comment on the behaviour of BN parameters in deeper layers of the network when the same fine-tuning experiment is performed?\n- In Figure2, we observe that very few feature maps are indeed changed. Could the authors comment on this? Also, could the authors clarify which layer was used to obtain these figures?\n- In Algorithm 1, could the authors clarify if the number p is sampled uniformly from the [0,1] range?\n- Further, it is not clear which set of BN parameters is used in the crafting of the PGD attack in the proposed algorithm. This small detail is likely to cause dramatic changes in the final outcome of training. As currently presented, it appears as though the BN parameters corresponding to standard training are used in this step.\n- Could the authors show an ablation experiment where the BN parameters corresponding to RobMask are used for attack generation to understand this better?\n- Could the authors clarify if the network parameters $\\theta$ are also adversarially updated (along with W and W\u2019) using the loss on the perturbed sample $x+\\delta$ as presently shown for all experiments? If so, could the authors clarify why the accuracy for eps=0 in Table4 differs from that shown in Table3 (5-6% difference)?\n- Also, it is not completely clear from the algorithm if BN parameters of all layers in the network are updated, or if it is restricted to some specific layer.\n- Could the authors share details used for adversarial training (optimizer, learning rate schedule, number of epochs, validation split, use of early stopping)? These factors play a crucial role in the final robust accuracy achieved and in the trade-off with clean accuracy as well, as often the model obtained at the last epoch of training achieves lower adversarial accuracy compared to intermediate epochs.\n\n########################## Update after rebuttal ##########################\n\nI thank the authors for their detailed response; several concerns have been addressed in the rebuttal. I would encourage the authors to use commonly used practices to improve the robust performance of models in Tables 5 and 6. The use of early-stopping [3] can significantly boost the robust performance, and produce models with better clean accuracy than is presently reported for Adv. Training. I would like to update the score to 5 based on the author's response. Aside from the robust evaluation, the improvement in clean accuracy is of a relatively smaller magnitude given the disproportionate increase in training requirements. Thus, I have not further increased the score.\n\n[3] Rice et al., Overfitting in adversarially robust deep learning, ICML 2020, https://arxiv.org/abs/2002.11569 \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101892, "tmdate": 1606915789180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2196/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review"}}}, {"id": "VHteYphQT6b", "original": null, "number": 8, "cdate": 1605923918249, "ddate": null, "tcdate": 1605923918249, "tmdate": 1605923918249, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "SGZFZJZlSpo", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": "Our main contribution is understanding batch norm in the trade-off and improving generalization, which is different with the listed relevant papers.", "comment": "Thanks for your interests. [1] introduces unlabeled data to achieve a better trade-off. [2] uses early stop PGD to get least adversarial data in the adversarial training. [3] theoretically suggests that it is possible to learn classifiers both robust and highly accurate on real image data. None of the aforementioned methods could achieve a better generalization on clean data. Therefore, different with [1,2,3],  our main contribution is that we study an interesting phenomenon that changing the batch normalization itself with all other weights fixed can control the tradeoff between adversarial robustness and generalization. Therefore, we formulate this finding into adversarial masking and propose RobMask to boost the generalization performance. We will add more discussion in the later revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "ZUp9GnU0j8", "original": null, "number": 5, "cdate": 1605921618147, "ddate": null, "tcdate": 1605921618147, "tmdate": 1605922667200, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "BWhOnU82YNU", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thanks for your insightful comments. Below, we provide detailed responses to your questions. \n\nQ1: About ImageNet results and head-to-head comparison with AdvProp.\n\nA1: Thanks for the great suggestion. However, we couldn\u2019t find the official training code for AdvProp. Also, it is pretty difficult and time-consuming to train EfficientNet in our own servers without TPU support. Therefore, it is very difficult for us to perform a head-to-head comparison with AdvProp, though we acknowledge that it is a great suggestion. As a result, we were only able to conduct comparisons with AdvProp on CIFAR10/CIFAR100 in the submission. \nTo address your concern, recently we have found a pytorch implementation at https://github.com/tingxueronghua/pytorch-classification-advprop for training ResNet on ImageNet. Due to time limit, we have added the ResNet-18 ImageNet experiments in Table 3 in the revision, showing that the proposed method still outperforms AdvProp. Specifically, we achieve +0.38% accuracy improvement over standard training, while AdvProp only achieves +0.03% accuracy improvement based on our experiments. \n\n\nQ2: About head-to-head comparison of training curves and training time.\n\nA2: We have added Figure 4 in the revised paper. Since both RobMask and AdvProp use two forward and one backward pass, they have almost identical training time per epoch. For example, in DenseNet-121, it takes around 870 seconds for Advprop and around 890 seconds for RobMask.  i.e., our model did not take much longer to train.\n\n\nQ3: How many times did you run each experiment?\u201d\n\nA3: We run each experiment three times. We have added the standard deviation in Table 8 in the Appendix.\n\n\nQ4: About the notation \\gamma_i and Figure 2.\n\nA4: Thanks for the suggestions. We have changed the learning rate to \\eta_i, and moved Figure 2 to Appendix.\n\n\nQ5: I believe you are also using PGD attack, but the adversarial perturbation's strength is randomized.\n\nA5: Yes, we set p to be sampled uniformly from [0,1], so the adversarial perturbation\u2019s strength is randomized from 0 to \\epsilon_max.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "3-esqSjWqu", "original": null, "number": 7, "cdate": 1605922142110, "ddate": null, "tcdate": 1605922142110, "tmdate": 1605922142110, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "cm3qgaVHmpX", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": " Response to AnonReviewer2 (Part 2)", "comment": "Q9: Could the authors show results where the maximum constraint is set to 10/255?  \n\nA9: Thanks for the suggestion. We add it in Table 5. Yes, we could see the 8/255\u2019s robust accuracy is improved from 41.7% to 41.69%; however, it sacrifices some clean accuracy. \n\nQ10: Additional details on how the finetuning is performed in Section 3.1.\n\nA10: As stated in the paper, we start from an adversarial/clean trained model. Freezing other parameters except all batch normalization layers, we fine-tuned the network with the same optimizer, learning rate, learning rate scheduler and number of epochs. We will make this clear. \n\nQ11: X-axis in Figure1\n\nA11: The x-axis is across the dimension. The first batch norm as shown in Figure 1 has dims=64 so we have x indexed from 0 to 63.\n\nQ12: Could the authors clarify how the plot for Adv-Train is obtained (in relation to the Std-BN finetune plot)? How were the updates performed on the Adv-Train model to obtain this plot?\n\nA12:  As stated in Figure 1\u2019s title, we conduct standard fine-tuning of only the BN parameters of an adversarially-trained network.  \n\nQ13: Since the convolutional layer that occurs before this BN layer is frozen, it is expected that Plots 1(a) and 1(b) are identical.\n\nA13: Plot 1(a) and 1(b) are based on the running mean and variances trained by only fine-tuning on adversarial examples or clean examples. If the intuition in AdvProp [1] is true, they should be completely different, since they are in different distributions. However, we found only the rescaling weight has changed dramatically while running mean and variances stay unchanged. \n\nQ14: BN in deeper layer.\n\nA14: We show the experiment in Table 9 in Appendix. It could be clearly observed that the rescaling weight has the most significant difference across all the batch normalization layers\u2019 parameters. However, since the deep layer\u2019s mean and variance would be affected by the shallow layers\u2019 rescaling parameter, the result on the deeper layer couldn\u2019t disentangle the effect between normalization and rescaling because it is mixed.\n\nQ15: \u201cvery few feature maps changed In Figure 2. which layer was used to obtain Figure 2?\u201d\n\nA15: In figure 2, we only marked several changes. If you look closely, 16/64=25% feature maps have changed dramatically. In fact, all the feature maps change to some extent, however, we could only observe some in the figure. We extract the feature maps after the first batch normalization and ReLu layer.\n\nQ16: the number p is sampled uniformly from the [0,1] range In Algorithm 1?\n\nA16: Yes, you are correct. We will make this clear. \n\nQ17: \u201cwhich set of BN parameters is used in the crafting of the PGD attack in the proposed algorithm.\u201d\n\nA17: Sorry for the confusion, we use the forward\u2019s BN parameters to generate adversarial attack and to perform adversarial training so the BN parameters are not corresponding to standard training.\n\nQ18: Could the authors show an ablation experiment where the BN parameters corresponding to RobMask are used for attack generation to understand this better?\u201d\n\nA18: Thanks for the suggestion. However, we use the BN parameters on the attack generalization same as training. Could you clarify how the experiments should be done?\n\nQ19: Could the authors clarify if the network parameters \u03b8 are also adversarially updated (along with W and W\u2019) using the loss on the perturbed sample x+\u03b4\u201d \n\nA19: Yes, you are correct. We will make this clear.\n\nQ20: could the authors clarify why the accuracy for eps=0 in Table4 differs from that shown in Table3 (5-6% difference)?\u201d \n\nA20: During attack, we use the mask corresponding to p=1 which has some performance drop since it has a different mask with p=0.\n\nQ21: Also, it is not completely clear from the algorithm if BN parameters of all layers in the network are updated, or if it is restricted to some specific layer.\n\nA21: All BN layers parameters are updated.\n\nQ22: Could the authors share details used for adversarial training (optimizer, learning rate schedule, number of epochs, validation split, use of early stopping)?  \n\nA22: We use SGD optimizer with the cosine learning rate scheduling and trained by 100 epochs. We set the learning rate=0.1. We didn\u2019t use early stop and use the standard validation split same as advprop and RobMask. \n\n\nPlease let us know if you have additional questions, and we are happy to answer them. Thank you!\n\n[1] Ye, Chengxi, et al. \"Network deconvolution.\" arXiv preprint arXiv:1905.11926 (2019). "}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "jcJ6mr0o4y9", "original": null, "number": 6, "cdate": 1605921858277, "ddate": null, "tcdate": 1605921858277, "tmdate": 1605921858277, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "cm3qgaVHmpX", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (Part 1) ", "comment": "Thanks for your insightful and detailed comments. Below, we provide detailed responses to your questions.  However, I want to first clarify our contributions. Our main contribution is in two folds. First, we discover an interesting phenomenon that changing the batch normalization itself with all other weights fixed can control the tradeoff between adversarial robustness and generalization. Second, we formulate this finding into adversarial masking and propose RobMask to boost the generalization performance. As a side contribution, it also brings an additional benefit that robustness-accuracy trade-off is improved.\n\nQ1: About the novelty compared with AdvProp [1].\n\nA1: AdvProp [1] has an assumption that the trade-off is caused by distribution mismatch, i.e., adversarial examples and clean images are drawn from two different domains, therefore training exclusively on one domain cannot well transfer to the other. In Section 3.1, we show the running mean and variance of adversarial and clean examples are almost the same, so their hypothesis is questionable. Instead, we show it is the rescaling parameter together with ReLU function, which is what we call \u201cadversarial masking\u201d, causing the trade-off. Also, as stated, AdvProp cannot use multiple perturbation strengths in their training, which is nontrivial to solve.  Essentially, our method starts from a different standpoint, and we propose a more general method to solve the problem.\n\nQ2: Results of setting k=3 and what the different w_k\u2019s would represent?.\n\nA2: For k=3, we add another dimension so that we could achieve a slightly better accuracy 94.67% for 20 epochs and a comparable accuracy 95.88% for 100 epochs training on ResNet-18 in CIFAR10 dataset.\nEach w_k denotes a \u201cbasis\u201d and all the BN parameters for each different epsilon are based on a linear combination of these bases. In general, we found k=3 does not significantly improve the performance, so we still recommend using k=2. \n\nWe also want to emphasize that RobMask with k=2 is different from Advprop. RobMask considers a series of epsilon, and uses different batch norm (a linear combination of base w_k\u2019s) for each epsilon. In comparison, Advprop only uses a fixed two batch norms, one for clean and one for epsilon perturbed data. Therefore RobMask significantly outperforms Advprop in the experiments. \n\nQ3: About the intuition of the trade-off improvement. \n\nA3: RobMask incorporates a range of perturbations instead of just one. Empirically this leads to a better generalization than Advprop, which brings a better trade-off. \n\n\nQ4: About the training time in DenseNet.\n\nA4: The training time of AdvProp and RobMask is almost the same. Since both RobMask and AdvProp use two forward and one backward pass, they have almost identical training time per epoch. For example, in DenseNet-121, it takes around 870 seconds for Advprop and around 890 seconds for RobMask.  i.e., our model did not take much longer to train.  For the standard training, it is clearly shown in Table 3 that even the 100th epoch result is worse than RobMask at 20 epochs. Giving more epochs will not improve the performance of standard training a lot, you could also refer to https://github.com/kuangliu/pytorch-cifar, where they reported that the peak performance for densenet (with standard training) is 95.04%.\n\nQ5: The clean accuracy on the adv trained model.\n\nA5: We use our own implementation on adversarial trained models, and do not use a lot of tricks that proposed in [2,3]. Also, to be noted, the number listed in [2] is for PreAct ResNet18, which is a different architecture. However, to be noted, the improvement on trade-off is considered as a side contribution of RobMask and the main contribution is we discover an interesting phenomenon that changing the batch normalization itself with all other weights fixed can control the tradeoff between adversarial robustness and generalization and propose RobMask to boost the generalization performance.\n\nQ6: Could the authors provide additional results for k=3 or k=4?\n\nA6: Same as Q2. \n\nQ7: Could the authors clarify the exact BN parameters used in final evaluation on clean and adversarial samples?\n\nA7: Since there are still discrepancies between different u_i, therefore, if we want to obtain the best clean performance, we use p=0. If we want to obtain the best robustness performance, p=1 for the adversarial samples is used for the evaluation. We will make this clear in the revision.\n\n\nQ8: Evaluation on PGD-100 step attack alone is not sufficient\u3002\n\nA8: Thanks for the suggestion. We add the Autoattack results in Table 5. Using Autoattack, robust accuracy of every method is decreasing by 2-3% while the gap between every methods keeps around the same.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "guaF-g7jp6U", "original": null, "number": 4, "cdate": 1605921552956, "ddate": null, "tcdate": 1605921552956, "tmdate": 1605921695507, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "KuEnOsFHYdP", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": " Response to AnonReviewer4", "comment": "Thanks for your encouraging and insightful comments. Below, we provide detailed responses to your questions. \n\nQ1: About the ImageNet results.\n\nA1: Thanks for the great suggestion. However, we couldn\u2019t find the official training code for AdvProp. Also, it is pretty difficult and time-consuming to train EfficientNet in our own servers without TPU support. Therefore, it is very difficult for us to perform a head-to-head comparison with AdvProp, though we acknowledge that it is a great suggestion. As a result, we were only able to conduct comparisons with Advprop on CIFAR10/CIFAR100 in the submission. \nTo address your concern, recently we have found a pytorch implementation at https://github.com/tingxueronghua/pytorch-classification-advprop for training ResNet on ImageNet. Due to time-limit, we have added the ResNet-18 ImageNet experiments in Table 3 in the revision, showing that the proposed method still outperforms AdvProp. Specifically, we achieve +0.38% accuracy improvement over standard training, while AdvProp only achieves +0.03% accuracy improvement based on our experiments. \n\n\nQ2: The proposed hypothesis cannot explain why the standard accuracies also drop when performing adversarial training on models that do not use Batch Normalization.\n\nA2: Thanks for the great question. Since Batch normalization is widely used in nearly all state-of-art neural networks, our analysis could be applied directly. However, we haven\u2019t talked about the robustness of feature extraction in the convolution layer and we leave it as a future work to discuss the tradeoff if there is no batch norm.\n\n\nQ3: About fine-tuning a single convolution layer.\n\nA3: Thanks for the suggestion. We have conducted the experiments only fine-tuning a single convolution layer while freezing others. When starting from a standard trained model and performing adversarial fine-tuning, the clean accuracy drops from 91.97% to 46.46% and robust accuracy only improves marginally from 0.0% to 1%.\nWhen starting from an adversarial trained model and performing standard training, the clean accuracy increase from 78.47% to 81.3% and robust accuracy drops significantly from 48.56% to 4.58%. Therefore, We shows if we only fine-tune on a single convolution layer, we will achieve a very bad tradeoff between clean and robust errors.\nHowever, our main focus here is based on that we can learn robust features from a vanilla standard-trained model.  And we leave the robustness of feature extraction in the convolution layer for the future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "aerGpXHHZMN", "original": null, "number": 3, "cdate": 1605921420860, "ddate": null, "tcdate": 1605921420860, "tmdate": 1605921668831, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "lCBthU0XoNo", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thanks for your encouraging and insightful comments. Below, we provide detailed responses to your questions. \n\nQ1: About the major contribution.\n\nA1: Sorry for the confusion. Our main contribution is in two folds. First, we discover an interesting phenomenon that changing the batch normalization itself with all other weights fixed can control the tradeoff between adversarial robustness and generalization. Second, we formulate this finding into adversarial masking and propose RobMask to boost the generalization performance. As a side contribution, it also brings an additional benefit that robustness-accuracy trade-off is improved.\n\n\nQ2: Compared with Advprop, why is the hypothesis in this paper considered as \u201cnew\u201d?\n\nA2: As stated in Section 4 (Connection with Advprop), Advprop has an assumption that the trade-off is caused by distribution mismatch, i.e., adversarial examples and clean images are drawn from two different domains, therefore training exclusively on one domain cannot well transfer to the other. In Section 3.1, we show the running mean and variance of adversarial and clean examples are almost the same, so the \u201cdistribution mismatch\u201d hypothesis in the AdvProp paper is not completely true. Instead, it is the rescaling parameter together with ReLU function, i.e., what we call \u201cadversarial masking\u201d, causes the trade-off. \n\nFurther,  by exploiting the low rank structure, we propose RobMask to incorporate different perturbation strengths for model training, instead of just one.  RobMask boosts generalization on clean data and achieves a better trade-off between robust and natural accuracy over Advprop.\n\n\nQ3:It is not clear how the adversarial maskings in Section 3.2 are generated.\n\nA3: It is constructed by the first batch normalization (BN) layer\u2019s rescaling parameter with the later ReLU function after the first convolution layer. We obtained them by only fine-tuning the BN layer, the same as the fine-tuning experiment. \n\n\nQ4: The performance gain is minimal.\n\nA4:  To our best knowledge, the improvement over generalization around 1% to 2% should be considered as significant.  For example, works such as Advprop and Network Deconvolution [1] achieving performance improvements around 1% are considered as significant.\n\t\n\nQ5: Consider the evaluations carried out in AdvProp (Xie et al. (2020)).\n\nA5: Thanks for the great suggestion. However, we couldn\u2019t find the official training code for AdvProp. Also, it is pretty difficult and time-consuming to train EfficientNet in our own servers without TPU support. Therefore, it is very difficult for us to perform a head-to-head comparison with AdvProp, though we acknowledge that it is a great suggestion. As a result, we were only able to conduct comparisons with Advprop on CIFAR10/CIFAR100 in the submission. \nTo address your concern, recently we have found a pytorch implementation at https://github.com/tingxueronghua/pytorch-classification-advprop for training ResNet on ImageNet. Due to time-limit, we have added the ResNet-18 ImageNet experiments in Table 3 in the revision, showing that the proposed method still outperforms AdvProp. Specifically, we achieve +0.38% accuracy improvement over standard training, while AdvProp only achieves +0.03% accuracy improvement based on our experiments.\n\n[1] Ye, Chengxi, et al. \"Network deconvolution.\" arXiv preprint arXiv:1905.11926 (2019). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2196/Authors|ICLR.cc/2021/Conference/Paper2196/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923851185, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Comment"}}}, {"id": "SGZFZJZlSpo", "original": null, "number": 1, "cdate": 1605061772714, "ddate": null, "tcdate": 1605061772714, "tmdate": 1605061787856, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Public_Comment", "content": {"title": "Some similar papers about the topic \"tradeoff between robustness and accuracy.\"", "comment": "Dear Authors,\n\nIt is enjoying reading your paper. \nThe paper's topic is \"Understanding Robustness Trade-off for Generalization.\"\n\nRecently there are some similar papers (below) also discussing this tradeoff. Could I know the (dis)similarities between theirs and yours? It would be good if you could at least discuss some :)\n\n[1]  Understanding and mitigating the tradeoff between robustness and accuracy. In ICML, 2020\n[2] Attacks which do not kill training make adversarial learning stronger. In ICML, 2020\n[3] A closer look at accuracy vs. robustness. In NeurIPS 2020"}, "signatures": ["~Jingfeng_Zhang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jingfeng_Zhang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LNtTXJ9XXr", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/Authors", "ICLR.cc/2021/Conference/Paper2196/Reviewers", "ICLR.cc/2021/Conference/Paper2196/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024965001, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Public_Comment"}}}, {"id": "KuEnOsFHYdP", "original": null, "number": 3, "cdate": 1603972314779, "ddate": null, "tcdate": 1603972314779, "tmdate": 1605024266574, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review", "content": {"title": "Nice empirical observation on the trade-off between generalization and robustness", "review": "The paper observes that the rescaling operation in the batch normalization layer and the ReLU activation learn to select different features for standard and adversarial trainings. The authors call this effect \"Adversarial Masking.\" Based on this observation, the authors then propose Rob-Mask that achieves good standard and adversarial accuracies at the same time.\n\nThe trade-off between standard and adversarial accuracies are an important problem in adversarial machine learning. The findings in this paper are really interesting and deserve further investigation from the research community.\nMy biggest concern is that, since it is an empirical paper without rigorous guarantees, I would like to see the experiments on more datasets. The experiments in this paper are only on CIFAR-10 and CIFAR-100, which are similar datasets. Does the finding still hold in other datasets like ImageNet?\nAlso, there are many models that do not use Batch Normalization, but the standard accuracies also drop when doing adversarial training. The proposed hypothesis cannot explain that. One ablation test that I would like to see is whether fine-tunning a single convolutional layer will have similar effect as fine-tunning the BN layer done in this paper.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101892, "tmdate": 1606915789180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2196/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review"}}}, {"id": "lCBthU0XoNo", "original": null, "number": 4, "cdate": 1604941087392, "ddate": null, "tcdate": 1604941087392, "tmdate": 1605024266507, "tddate": null, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "invitation": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review", "content": {"title": "Simple way to turn adversarial training as a regularizer", "review": "Summary:\nThis paper follows the direction of previous work AdvProp (Xie et al. (2020)) and aims to use adversarial training as a regularizer to improve the network generalization on clean data. The authors analyze that the different rescaling operation in the batch normalization layer along with ReLU acts as feature masking/selection layer, which can control the trade-off between adversarial robustness and clean data performance. Unlike AdvProp, which uses different batch normalization layer for clean images and adversarial images at a specific perturbation strength, here the authors propose a technique called RobMask that adapts the rescaling parameters of batch normalization based on the perturbation strength during training. The authors show that such adapting technique is more effective than using different batch normalization layers (as in AdvProp) for each perturbation strength and thus improves the clean data performance on CIFAR10/100.\n\nStrengths: \n+ Motivation and analysis is clear.\n+ Discussed its differences to previous works.\n+ Proposed technique is simple, easy to adapt in the existing setup of adversarial training and addresses the limitations of the previous work AdvProp.\n+ Evaluation is carried out on CIFAR10/100 across different network architectures: ResNet-18, DenseNet-121, Preact ResNet18, ResNeXt-29.\n+ Results on CIFAR-10/100 using four different deep network architectures suggest that this work improves clean data performance than the baselines: standard network training and AdvProp.\n\nWeaknesses:\n-\tThe authors claim about well-balanced robustness trade-off using their method and also claim that their major objective is only to improve network generalization on clean data. There is a little ambiguity regarding the major contribution of this paper. The authors can make this point more clear. \n-\tIsn\u2019t the hypothesis that is stated as \u201cnew\u201d in this work already discussed in AdvProp i.e. using different batch normalization for clean and adversarial images improves network generalization, which in turn draw the conclusion that rescaling operation of batch norm could control the robustness and generalization trade-off. Why this hypothesis considered as \u201cnew\u201d then ?\n-\tThe two learned adversarial maskings discussed in section 3.2, it is not clear how they are generated. \n-\tResults demonstrate that the proposed approach improves generalization but the performance gain is minimal (only 1%-2%) and not so significant compared to the baselines.\nMinor point:\n-\tI understand that the major objective of this work is to improve performance on clean images but not the adversarial robustness. The results demonstrate higher robustness against PGD based adversarial attacks with perturbation strength lower than 8/255 is interesting but not of practical importance since the method requires perturbation strength as an additional input and very specific to PGD based attack. I wouldn\u2019t consider this as major weakness since it is not the primary objective of this work.\n\nFinal thoughts:\nThe proposed method is clearly motivated. Although the performance gains on network generalization are minimal compared to the baselines, this work cleverly addressed the limitations of previous work and extend it with simple modifications. I tend to accept this paper. However, I suggest the authors to also consider the evaluations carried out in AdvProp (Xie et al. (2020)) to improve the significance of their work.\n\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2196/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2196/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization", "authorids": ["~Minhao_Cheng1", "~Zhe_Gan1", "~Yu_Cheng1", "~Shuohang_Wang1", "~Cho-Jui_Hsieh1", "~Jingjing_Liu2"], "authors": ["Minhao Cheng", "Zhe Gan", "Yu Cheng", "Shuohang Wang", "Cho-Jui Hsieh", "Jingjing Liu"], "keywords": ["Adversarial Machine Learning", "Adversarial Robustness", "Adversarial Training", "Generalization"], "abstract": "Adversarial training is a commonly used technique to improve model robustness against adversarial examples. Despite its success as a defense mechanism, adversarial training often fails to generalize well to unperturbed test data. While previous work assumes it is caused by the discrepancy between robust and non-robust features, in this paper, we introduce \\emph{Adversarial Masking}, a new hypothesis that this trade-off is caused by different feature maskings applied. Specifically, the rescaling operation in the batch normalization layer, when combined together with ReLU activation, serves as a feature masking layer to select different features for model training. By carefully manipulating different maskings, a well-balanced trade-off can be achieved between model performance on unperturbed and perturbed data. Built upon this hypothesis, we further propose Robust Masking (RobMask),  which constructs unique masking for every specific attack perturbation by learning a set of primary adversarial feature maskings. By incorporating different feature maps after the masking, we can distill better features to help model generalization. Sufficiently, adversarial training can be treated as an effective regularizer to achieve better generalization. Experiments on multiple benchmarks demonstrate that RobMask achieves significant improvement on clean test accuracy compared to strong state-of-the-art baselines.", "one-sentence_summary": "We introduce a new hypothesis to understand the trade-off between robustness and natural accuracy, and further propose a new method to achieve better generalization using adversarial examples..", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cheng|adversarial_masking_towards_understanding_robustness_tradeoff_for_generalization", "pdf": "/pdf/5eafd989547bf5e640fc45d558c400011626ef20.pdf", "supplementary_material": "/attachment/708c4a3e28f25a4548693e7c8305cfdb7e51fcce.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4bfuNUG_XZ", "_bibtex": "@misc{\ncheng2021adversarial,\ntitle={Adversarial Masking: Towards Understanding Robustness Trade-off for Generalization},\nauthor={Minhao Cheng and Zhe Gan and Yu Cheng and Shuohang Wang and Cho-Jui Hsieh and Jingjing Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=LNtTXJ9XXr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LNtTXJ9XXr", "replyto": "LNtTXJ9XXr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2196/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538101892, "tmdate": 1606915789180, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2196/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2196/-/Official_Review"}}}], "count": 14}