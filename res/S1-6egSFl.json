{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1490203230110, "tcdate": 1487368377205, "number": 158, "id": "S1-6egSFl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1-6egSFl", "signatures": ["~Tiago_Pimentel1"], "readers": ["everyone"], "content": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028639144, "tcdate": 1490028639144, "number": 1, "id": "ryDPuF6ig", "invitation": "ICLR.cc/2017/workshop/-/paper158/acceptance", "forum": "S1-6egSFl", "replyto": "S1-6egSFl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028639668, "id": "ICLR.cc/2017/workshop/-/paper158/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1-6egSFl", "replyto": "S1-6egSFl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028639668}}}, {"tddate": null, "tmdate": 1489436676817, "tcdate": 1489436676817, "number": 2, "id": "Hk6bxt4ie", "invitation": "ICLR.cc/2017/workshop/-/paper158/public/comment", "forum": "S1-6egSFl", "replyto": "rywV8Flie", "signatures": ["~Tiago_Pimentel1"], "readers": ["everyone"], "writers": ["~Tiago_Pimentel1"], "content": {"title": "Updated paper and clarifications", "comment": "Thank you for your feedback. I think this ('state-of-the-art link prediction') was indeed stated poorly from our part, I've updated the paper's abstract, now saying that NBNE achieves results comparable or better than state-of-the-art feature learning algorithms. Instead of specifically stating state-of-the-art at the tasks themselves.\n\nWe compare our algorithm to the baselines in these two problems, i.e. node classification and link prediction, because it\u2019s the usual benchmark when comparing node embedding algorithms. These problems are used for comparisons in Node2Vec (Grover & Leskovec, 2016) and SBNE (Wang et al., 2016), while DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015) evaluate using node classification only.\n\nAll these four methods, and NBNE, are supposed to generate general purpose embeddings, so they are not, nor should be, explicitly optimized for any such test. These chosen tests in tasks with different properties and in different datasets are mainly supposed to 'benchmark' the algorithm.\n\nREFERENCES\n\nAditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855\u2013864, 2016.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701\u2013710, 2014.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067\u20131077, 2015.\n\nDaixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1225\u20131234, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487368378160, "tcdate": 1487368378160, "id": "ICLR.cc/2017/workshop/-/paper158/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper158/reviewers"], "reply": {"forum": "S1-6egSFl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487368378160}}}, {"tddate": null, "tmdate": 1489427983626, "tcdate": 1489427983626, "number": 1, "id": "S1uM0LVjx", "invitation": "ICLR.cc/2017/workshop/-/paper158/public/comment", "forum": "S1-6egSFl", "replyto": "BJY1_1Vse", "signatures": ["~Tiago_Pimentel1"], "readers": ["everyone"], "writers": ["~Tiago_Pimentel1"], "content": {"title": "Updated paper and clarifications", "comment": "Thank you for your feedback, I\u2019m uploading a revised version of the paper which, I think, better describes the way sentences are generated. We would like to point out that, besides having a lower training time, our method is completely unsupervised, while node2vec is semi-supervised. Our method also only depends on a single parameter \u2018n\u2018, which is easier to understand and choose and which can be selected dynamically, by increasing its value until the embeddings start to overfit.\n\nAnother point we would like to make is that choosing how sentences/context are generated in a graph is a fairly complex problem, due to the changing dimensionality in its structure. Unlike text or image, there\u2019s no straight forward way to \u2018read\u2018 it. Also, differences like the one between SkipGram and CBOW are \u2018simple\u2018, since they only change how one word is predicted from the others in already constructed sentences, but create fairly different representations and results, being more efficient when applied to different datasets.\n\nThere was no space to fully state the differences in training time between our method and the baselines, but it was about 100 to 1000x faster than node2vec, when using n=1, n=5 or n=10 on the three datasets (respectively: Astro, Facebook and Blog).\n\nAbout testing against different baselines, to the best of our knowledge, there\u2019s no supervised method for learning representations specific for neither link prediction or node classification. Grover & Leskovec (2016) state that \u201dnone of feature learning algorithms have been previously used for link prediction\u201d. In it, they additionally test their algorithm against common heuristics of the problem, like Common Neighbours and Adamir Adar, strongly beating those baselines. Due to the lack of space in this workshop paper version, we found it was not necessary to compare against these weak baselines.\n\nMost supervised learning algorithms for node classification/link prediction we found use, besides structural knowledge from the graph, node attributes, like sex, age, etc, which we do not use. We compare our algorithm to theirs in these two problems, i.e. node classification and link prediction, because it\u2019s the usual benchmark when comparing node embedding algorithms. These problems are used for comparisons in Node2Vec (Grover & Leskovec, 2016) and SBNE (Wang et al., 2016), while DeepWalk (Perozzi et al., 2014) and LINE (Tang et al., 2015) evaluate using node classification only.\n\nREFERENCES\n\nAditya Grover and Jure Leskovec. Node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855\u2013864, 2016.\n\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 701\u2013710, 2014.\n\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 1067\u20131077, 2015.\n\nDaixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1225\u20131234, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487368378160, "tcdate": 1487368378160, "id": "ICLR.cc/2017/workshop/-/paper158/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper158/reviewers"], "reply": {"forum": "S1-6egSFl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487368378160}}}, {"tddate": null, "tmdate": 1489397729102, "tcdate": 1489397729102, "number": 2, "id": "BJY1_1Vse", "invitation": "ICLR.cc/2017/workshop/-/paper158/official/review", "forum": "S1-6egSFl", "replyto": "S1-6egSFl", "signatures": ["ICLR.cc/2017/workshop/paper158/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper158/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a new method for computing nodes representations in large graphs. The idea is very close to the ideas of other existing papers and consists in transforming nodes+neighbors into sentences, and then to learn a word2vec model on the generated sentences. The originality of the paper is in the way these sentences are generated, using random permutations of nodes. Experimental results are made on both link prediction and node classification problems and show competitive results w.r.t baselines.\n\nThe originality of the approach is quite limited since the only new thing is how the sentences are generated. Moreover, due to the lack of details, I am not sure to exactly understand how the sentes are generated. Adding an example would be nice. The model seems competitive with other unsupervised methods and with a lower training time which is interesting.  But comparisons could be done with supervised methods that have been already proposed, particularly for learning representations for node classification. \n\nPros:\n\u2022\tSimple idea\n\u2022\tLow training time\nCons:\n\u2022\tNot a string contribution\n\u2022\tIncomplete Experimental setting\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489397729932, "id": "ICLR.cc/2017/workshop/-/paper158/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper158/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper158/AnonReviewer1", "ICLR.cc/2017/workshop/paper158/AnonReviewer2"], "reply": {"forum": "S1-6egSFl", "replyto": "S1-6egSFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper158/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper158/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489397729932}}}, {"tddate": null, "tmdate": 1489176110741, "tcdate": 1489176110741, "number": 1, "id": "rywV8Flie", "invitation": "ICLR.cc/2017/workshop/-/paper158/official/review", "forum": "S1-6egSFl", "replyto": "S1-6egSFl", "signatures": ["ICLR.cc/2017/workshop/paper158/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper158/AnonReviewer1"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "Essentially the goal of the contribution is to adapt ideas from Word2Vec to learn node embeddings. I.e., like Node2Vec but borrowing ideas from SkipGrams rather than random walks. This is claimed to lead to faster training times and more general-purpose embeddings.\n\nThe basic idea is to form \"sentences\" based on random permutations of neighbors around some node, so that the ideas from Word2Vec can be adopted. This idea is relatively straightforward and perhaps a little ad-hoc, but makes sense.\n\nThe experiments on a few graphs show improvements on link prediction tasks. These are fine though it's not clear to me whether state-of-the-art link prediction methods are in fact similar to what's being shown, nor is this the task the methods being compared are optimized for. Some more thoroughness would be useful here, though what's shown is sufficient for a workshop paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised and Scalable Algorithm for Learning Node Representations", "abstract": "Representation learning is one of the foundations of Deep Learning and allowed big improvements on several Machine Learning fields, such as Neural Machine Translation, Question Answering and Speech Recognition. Recent works have proposed new methods for learning representations for nodes and edges in graphs. In this work, we propose a new unsupervised and efficient method, called here Neighborhood Based Node Embeddings (NBNE), capable of generating node embeddings for very large graphs. This method is based on SkipGram and uses nodes' neighborhoods as contexts to generate representations. NBNE achieves results comparable or better to the state-of-the-art in three different datasets.", "pdf": "/pdf/d680e60f2b6448aabcc5d26e1c1fa01523ab9306.pdf", "TL;DR": "An unsupervised and efficient method capable of generating node embeddings for very large graphs.", "paperhash": "pimentel|unsupervised_and_scalable_algorithm_for_learning_node_representations", "keywords": ["Unsupervised Learning"], "conflicts": ["dcc.ufmg.br"], "authors": ["Tiago Pimentel", "Adriano Veloso", "Nivio Ziviani"], "authorids": ["tpimentel@dcc.ufmg.br", "adrianov@dcc.ufmg.br", "nivio@dcc.ufmg.br"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489397729932, "id": "ICLR.cc/2017/workshop/-/paper158/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper158/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper158/AnonReviewer1", "ICLR.cc/2017/workshop/paper158/AnonReviewer2"], "reply": {"forum": "S1-6egSFl", "replyto": "S1-6egSFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper158/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper158/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489397729932}}}], "count": 6}