{"notes": [{"id": "HJM4rsRqFX", "original": "B1g46xj6O7", "number": 80, "cdate": 1538087740222, "ddate": null, "tcdate": 1538087740222, "tmdate": 1545355418581, "tddate": null, "forum": "HJM4rsRqFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1xRoumtyE", "original": null, "number": 1, "cdate": 1544267942439, "ddate": null, "tcdate": 1544267942439, "tmdate": 1545354496527, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Meta_Review", "content": {"metareview": "The paper proposes a novel variational inference framework for knowledge graphs which is evaluated on link prediction benchmark sets and is competitive to previous generative approaches.\nWhile the idea is interstnig and technically correct, the originality of the contribution is limited,\nand the paper would be clearly improved by providing a clearer motivation for using generative models instead of standard methods and a experimental demonstration of  the benefits of using a generative instead of a discriminative model,  especially since the standard method perform slightly better in the experiments. Overall, the work is slightly under the acceptance threshold.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Intersting work with slighlty limited originality that would benefit from a clearer motivation. "}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper80/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353344036, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353344036}}}, {"id": "BJgcOJ2BRX", "original": null, "number": 8, "cdate": 1542991729541, "ddate": null, "tcdate": 1542991729541, "tmdate": 1542991729541, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "HyeOCKDBRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "Missing citation issue already addressed", "comment": "Dear Reviewer, \n\nWe thank you for the promising feedback. We have responded to the anonymous comment recently around missing comparisons and stressed the missing references are not crucial to the papers story \u2014- as we focus on different tasks (uni vs multi relational data), they are instead only additional related work, which was already introduced in the original submission when I discussed Kipf\u2019s generative model \u201cVariational graph networks\u201d. \n\nI hope this no longer remains an issue, and appreciate the fact you wish to wait longer before altering the score. \n\nThanks again for your time."}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "HyeOCKDBRQ", "original": null, "number": 6, "cdate": 1542973904277, "ddate": null, "tcdate": 1542973904277, "tmdate": 1542973971595, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "S1efAIW7Am", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "Quality has been improved", "comment": "I find the current version's quality has been improved and I am happy with the answers from the authors, so I am willing to increase my score toward acceptance. However, it seems that more work is needed to have a paper mature enough for the conference (I am thinking on the missing references cited by an anonymous commenter), therefore I would wait the other reviewers comments before deciding how much to increase my score.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "HyekC8PrAm", "original": null, "number": 5, "cdate": 1542973127441, "ddate": null, "tcdate": 1542973127441, "tmdate": 1542973269904, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "r1g2CcuVAX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "GraphVAE and Graphite focus only on uni-relational graph structures (already briefly discussed), whereas we focus purely on multi-relational graph structures. ", "comment": "We thank you for the anonymous comment. However, the papers suggested are uni-relational model --- which have already been included in our discussion on Kipf's [1] generative graph convolution model (S4). We have now included papers [2,3,4,5] in an additional few sentences on generative graph modeling, however, they are not fit for comparison as they do not function on multi-relational data --- thus do not alter the main contributions of this paper. Secondary differences: GraphVAE is focused on generating the full graph structure, whereas we are focused on generating the sub-structures (such as a link). \n\n[1] Kipf, T.N. and Welling, M., 2016. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308.\n[2] Simonovsky, M. and Komodakis, N., 2018. GraphVAE: Towards Generation of Small Graphs Using Variational Autoencoders. arXiv preprint arXiv:1802.03480.\n[3] Grover, A., Zweig, A. and Ermon, S., 2018. Graphite: Iterative generative modeling of graphs. arXiv preprint arXiv:1803.10459.\n[4] Liu, Q., Allamanis, M., Brockschmidt, M. and Gaunt, A.L., 2018. Constrained Graph Variational Autoencoders for Molecule Design. arXiv preprint arXiv:1805.09076.\n[5] De Cao, N. and Kipf, T., 2018. MolGAN: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973."}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "r1g2CcuVAX", "original": null, "number": 1, "cdate": 1542912724041, "ddate": null, "tcdate": 1542912724041, "tmdate": 1542912724041, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "S1efAIW7Am", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Public_Comment", "content": {"comment": "There are a couple of existing works in generative modeling of graphs with variational approaches that are missing a discussion/comparison in the current work:\n\n- GraphVAE https://arxiv.org/pdf/1802.03480.pdf\n- Graphite https://arxiv.org/abs/1803.10459 (this one explicitly addresses the scalability issue of variational graph autoencoder of Kipf&Welling )", "title": "Missing discussion on existing variational approaches to graph generative modeling"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311923405, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "HJM4rsRqFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311923405}}}, {"id": "S1efAIW7Am", "original": null, "number": 4, "cdate": 1542817482424, "ddate": null, "tcdate": 1542817482424, "tmdate": 1542822384197, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "Byl2jPWGjX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "Consistently outperformed existing generative models and introduced a tool to enable large-scale ELBO estimation tailored for knowledge graphs. ", "comment": "The authors greatly appreciate the in-depth feedback. \n\n*As previously mentioned the focus of our work is on comparison to prior generative knowledge base models, there we respectfully disagree with the reviewer's statement of \u201cdo not outperform those of other approaches\u201d, as we outperformed existing generative models. But the significance of our contribution lies in the first intersection of variational inference techniques applied to knowledge graph link prediction, as well as the novel training techniques implemented to scale these probabilistic systems to large-scale KG\u2019s with ease. \n\n*We believe the underlying probabilistic semantics uncovered are showcased through a visual exploration of the learned probabilistic embedding semantics, which has been added into the updated paper. Thanks to your suggestion, we have also added in a discussion of the use of von-Mises distribution as well as the paper which inspired the idea.  \n\n*The index errors have been corrected as well as a reference to Figure 1 and Figure 2 added in their corresponding model derivations. Thank you for pointing them out!\n\n*Formula six now has additional information included, as well as a more straightforward and more specific reformulation to the LFM, Formula seven now expresses the ELBO optimization of LIM with the simplified notation. We have also corrected the proofs in the appendix and now believe the paper to be mathematically sound --- if you disagree please can you specify specific proofs/ equations so we can discuss/ fix the problem? \n\n*We have re-structured the work so that it follows a more typical structure as recommended and included citations to the original datasets and descriptions of what the -257 means (257 relations in the dataset, which is reduced as we only keep the asymmetric relations in order to produce a more challenging dataset). The significance of the results over ComplEX could be proved using confidence margins, however as mentioned earlier we are not focused on beating SotA methods, we are focused on furthering generative knowledge graph research and comparing our method to alternative generative models. \n\n*In Table 1 and 2 the - means that the metric was not reported in the published paper the statistics were referenced from, which I have included in the latex file.\n\n*Proportion in Table 3 represents the ratio of the positive examples which are of that relation's category. The similar points are also now re-iterated across both tables. \n\n*The minor issues are now corrected --- Model A is the Latent Information Model (which was the original name). "}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "SyeWaXbXRX", "original": null, "number": 3, "cdate": 1542816696767, "ddate": null, "tcdate": 1542816696767, "tmdate": 1542816696767, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "SkljN1ID2m", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "The focus of our work is on comparison to prior generative knowledge base models, and on the theoretical tools required. ", "comment": "Thank you for the detailed feedback. Regarding the raised issues; \n\n*The focus of our work is on comparison to prior generative knowledge base models, not on beating SotA neural link prediction methods. We have shown increased performance against previous generative approaches while laying future support for incorporating variational inference techniques with statistical relational learning. That lack of FB15K results in Table 2 was due to the FB15K results not being reported in the referenced articles. \n\n*We have now added in a more explicit description of the underlying assumptions of each model and a brief discussion of the model differences. In essence, the independent assumption on latent variables in LIM leads to a significantly simpler to compute ELBO, requiring fewer approximations to be made. In contrast, the LFM retains dependence across the latent variables.\n\n*We have included more information as to how negative samples are generated --- in prior work [1,2,3,4] this is commonly done by corrupting a positive example. The additional information is in the experimental setup as well as in the section on link prediction (Section 5.1). \n\n\nReferences \n\n[1] Dettmers, T., Minervini, P., Stenetorp, P. and Riedel, S., 2017. Convolutional 2d knowledge graph embeddings. arXiv preprint arXiv:1707.01476.\n\n[2] Nickel, M., Rosasco, L. and Poggio, T.A., 2016, February. Holographic Embeddings of Knowledge Graphs. In AAAI (Vol. 2, No. 1, pp. 3-2).\n\n[3] Trouillon, T., Dance, C.R., Gaussier, \u00c9., Welbl, J., Riedel, S. and Bouchard, G., 2017. Knowledge graph completion via complex tensor factorization. The Journal of Machine Learning Research, 18(1), pp.4735-4772.\n\n[4] Yang, B., Yih, W.T., He, X., Gao, J. and Deng, L., 2014. Embedding entities and relations for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "Hkg787bXRX", "original": null, "number": 2, "cdate": 1542816586763, "ddate": null, "tcdate": 1542816586763, "tmdate": 1542816586763, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "H1l7Wh3hnX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "content": {"title": "We respectfully disagree with the claim that our work does not show much novel contribution. ", "comment": "We thank the reviewer for their comments. We respectfully disagree with the claim that our work does not show too much novel contribution. The probabilistic graphical model approach to knowledge graphs, as well as the training method using Bernoulli sampling to estimate the evidence lower bound, is novel. This novel ELBO approximation will be a crucial technique to scale VI for knowledge graph research as we found using only negative sampling (instead of using the ELBO or ELBO approximation), a typical trick to reduce the negative training examples, leads to a highly unstable training process. The technique trades off exact ELBO computation with an approximation, which is of orders of magnitude faster. Secondly, this ELBO approximation for LIM differs from typical stochastic variational inference techniques, as we are able to compute the full KL divergence term and are only required to approximate the full batches expected likelihood. Our work provides a framework which allows the specification of a family of generative knowledge graph models with any scoring functions that permits MLE of their parameters and any distribution that permits a reparameterization trick, allowing further research in VI to be easily incorporated into knowledge graph modeling. \n\nWe thank the reviewer for the corrections\u2014we corrected the KL divergence notation as well as the predicate index errors.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614140, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJM4rsRqFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper80/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper80/Authors|ICLR.cc/2019/Conference/Paper80/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers", "ICLR.cc/2019/Conference/Paper80/Authors", "ICLR.cc/2019/Conference/Paper80/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614140}}}, {"id": "H1l7Wh3hnX", "original": null, "number": 3, "cdate": 1541356539090, "ddate": null, "tcdate": 1541356539090, "tmdate": 1541534301661, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "content": {"title": "This work proposed variational embedding for knowledge graph inference tasks. However, neither the methods nor the results are really impressive", "review": "\nThis work proposed two variational embedding methods for knowledge graph inference tasks. The experiments show slight improvements compared to other variational embedding methods, e.g., KG2E, TransG, and slight improvement on the WN18 dataset compared with the non-variational method. On the other hand, both the embedding method and the training of variational models used in this work are already well developed. Thus, this work doesn\u2019t show too much novel contribution. However, the reviewer really appreciate the visualization provided in Fig. 4. \nMinor issues:\n1)\tNotation of the KL divergence is not conventional\n2)\tThere are some mistakes of indices for predicates, e.g., in Eq. 7, 8.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "cdate": 1542234542377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335644720, "tmdate": 1552335644720, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkljN1ID2m", "original": null, "number": 2, "cdate": 1541001011077, "ddate": null, "tcdate": 1541001011077, "tmdate": 1541534301456, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "content": {"title": "Interesting variational framework, results not convincing enough", "review": "* The paper proposed a neural variational inference framework for knowledge graph embedding. The paper proposed two models (Latent Fact Model and Latent Information Model) where the neural variational inference is carried out, with competitive results on standard datasets (WN18 and FB15K).\n\n* I am not fully convinced of the advantage of this variational inference approach compared with the optimization approach used in TransE, TransG, DistMult and ComplEx. As can be seen in Table 1, the best performance on WN18-RR and FB15K-257 are obtained without variational inference. In Table 2, performance of the variational inference approach is not as good as other approaches under the MR or Raw Hits metrics. Moreover, performance on FB15K is not reported in Table 2, which makes the result not as complete as Table 1.\n\n* It seems that the main difference between Latent Fact Model and Latent Information Model is the way prior is imposed on h. The authors may want to explain in plain language the differences and the motivation behind that.\n\n* It is unclear how the (\\tau,y) labeled triples are generated, especially for the negative examples with y=0. Is it obtained by randomly corrupting the triples in the knowledge base, as done in other work? It would be better to make this point clear.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "cdate": 1542234542377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335644720, "tmdate": 1552335644720, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byl2jPWGjX", "original": null, "number": 1, "cdate": 1539606436148, "ddate": null, "tcdate": 1539606436148, "tmdate": 1541534301242, "tddate": null, "forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "invitation": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "content": {"title": "Interesting work, not mature for publication at ICLR", "review": "The paper presents two variational inference frameworks for generative models of knowledge graphs. Such models are based respectively on latent fact model and latent information model.\nThe authors argue that with the presented framework the underlying probabilistic semantics can be discovered. Experiments show performances comparable with state-of-art approaches.\n\nUnfortunately, the paper seems to me not clear and rather incomplete in its actual form.\nOverall, the proposal is novel. I cannot decide about significance because results do not outperform those of other approaches. To this extent, the authors should better discuss the results, explaining in more detail why this approach should be used instead of others (scales better, is faster, etc.).\n\nIn the abstract, it is asserted that one can discover underlying probabilistic semantics, but in the corpus of the paper this aspect is not described or mentioned in detail.\nSimilar problem for the reference to von-Mises distribution. This distribution is just named, it is said that the framework can handle such a distribution, but a reference to a paper and/or a short paragraph to explain the sentence are missing. This statement now results to be just information disconnected by the rest of the paper.\n\nIn a similar way, many other points suffer from a poor organization in my opinion.\nWhen describing LIM an error is introduced here that is then copied and pasted throughout the paper: in the productory on p, p is in R not in E. This is a simple typo, but the fact that it is repeated so many times, also in the proof, gives me the feeling that the paper was written at the last moment.\nFigures 1 and 2 are never referred.\n\nFormula 6 must be better explained. If I have not lost something, n is the number of labeled triples, s_c is undefined, b_c is the probability of s_c to be equal to 1, the index i is never used. The paper lacks information here.\n\nAs regards the experimental part, some results are shown in subsection 4.3 called link prediction, others in section 5 called link prediction analysis. This organization does not seem to me to be really optimal. I would suggest creating an experimental section.\nMoreover, the tests should be better explained, the tables are shown without specifying how they are built and how the values are collected. Information is provided in the appendix but could be included in the paper as the maximum limit is of 10 pages (8 suggested but I think an extra half page can be used).\nThe knowledge bases used should be at least cited, I know that freebase and wordnet are well-known but somewhere, in the description of the test, the name should be included. Also to specify the characteristics of the versions (WN18 vs WN18RR). Moreover, what does the value -257 in column 1, row 4 means?\nThen, it is said that Table 1 shows improvements for ComplEx, but such improvements are rather low, is there a way to prove their significance? Otherwise, I would say that the performance is the same for WN18.\nTables 1 and 2 contain cells with '-' value, what does it mean?\n\nDiscussion about table 3 is incomplete in my opinion. First of all, the \"proportion\" column should be described. Also, on one hand, it is true that the _member_meronym is the least accurate and prominent but the most problem may come from _hypernym, which is the most prominent and the accuracy is also low. This fact is highlighted for table 4 but not for table 3.\n\nMinor issues\n- sec 3: references to Miao et al. must be enclosed by brackets\n- sec 4.3: \"We believe this *is* due to ...\"\n- sec 5.2: what is Model A? Also, the sentence seems incomplete.\n\nPros:\n- Novel approach\n\nCons\n- Test results are not convincing\n- The paper is not mathematically sound\n- The paper needs to be re-organized", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper80/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Variational Inference For Embedding Knowledge Graphs", "abstract": "Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in a variety of domains involving high-dimensional data. In this paper, we introduce two generic Variational Inference frameworks for generative models of Knowledge Graphs; Latent Fact Model and Latent Information Model.  While traditional variational methods derive an analytical approximation for the intractable distribution over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions. The new framework can create models able to discover underlying probabilistic semantics for the symbolic representation by utilising parameterisable distributions which permit training by back-propagation in the context of neural variational inference, resulting in a highly-scalable method. Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduces training time at a cost of an additional approximation to the variational lower bound.  The generative frameworks are flexible enough to allow training under any prior distribution that permits a re-parametrisation trick, as well as under any scoring function that permits maximum likelihood estimation of the parameters. Experiment results display the potential and efficiency of this framework by improving upon multiple benchmarks with Gaussian prior representations. Code publicly available on Github.", "paperhash": "cowenrivers|neural_variational_inference_for_embedding_knowledge_graphs", "TL;DR": "Working toward generative knowledge graph models to better estimate predictive uncertainty in knowledge inference. ", "authorids": ["mc_rivers@icloud.com", "p.minervini@ucl.ac.uk"], "authors": ["Alexander I. Cowen-Rivers", "Pasquale Minervini"], "keywords": ["Statistical Relational Learning", "Knowledge Graphs", "Knowledge Extraction", "Latent Feature Models", "Variational Inference."], "pdf": "/pdf/53d66b681a21610179d7c3db8b62c13474b7ff5e.pdf", "_bibtex": "@misc{\ncowen-rivers2019neural,\ntitle={Neural Variational Inference For Embedding Knowledge Graphs},\nauthor={Alexander I. Cowen-Rivers and Pasquale Minervini},\nyear={2019},\nurl={https://openreview.net/forum?id=HJM4rsRqFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper80/Official_Review", "cdate": 1542234542377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJM4rsRqFX", "replyto": "HJM4rsRqFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper80/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335644720, "tmdate": 1552335644720, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper80/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}