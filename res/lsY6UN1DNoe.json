{"notes": [{"id": "lsY6UN1DNoe", "original": "XFqvwr7p-N_", "number": 16, "cdate": 1615310251306, "ddate": null, "tcdate": 1615310251306, "tmdate": 1615313020807, "tddate": null, "forum": "lsY6UN1DNoe", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper16/Authors"], "authors": ["Anonymous"], "keywords": ["Model-Based Reinforcement Learning", "Offline Reinforcement Learning", "Self-supervized adaptation", "dynamics generalization"], "TL;DR": "Augmenting a world model learned from offline data makes it possible for policies to generalize to unseen dynamics.", "abstract": "Reinforcement learning from large-scale offline datasets provides the opportunity to learn policies without potentially unsafe or impractical exploration. Significant progress has been made in the past few years in dealing with the challenge of correcting for differing behavior of the data collection and learned policies. However, little attention has been paid to the potentially changing dynamics when transferring a policy to the online setting, where performance can be up to 90% reduced for existing methods. In this paper we address this problem with Augmented World Models: we augment a learned dynamics model with simple transformations that seek to capture potential changes in physical properties of the robot, leading to more robust policies. We not only train our policy in this new setting, but also provide it with the sampled augmentation as context, allowing it to infer potential changes in the environment. At test time we can then learn the context in a self-supervised fashion, by approximating the augmentation which leads to the new environment. We show in a series of experiments that this simple approach is able to significantly improve the zero-shot generalization of a recent state-of-the-art baseline, often achieving successful policies where the baseline fails.", "pdf": "/pdf/97e825da2d5a243639df2807ff10d1fc17d4ab6d.pdf", "paperhash": "anonymous|augmented_world_models_facilitate_zeroshot_dynamics_generalization_from_a_single_offline_environment", "_bibtex": "@inproceedings{\nanonymous2021augmented,\ntitle={Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=lsY6UN1DNoe},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}