{"notes": [{"id": "9l9WD4ahJgs", "original": "YMn9EQqvXIl", "number": 2299, "cdate": 1601308253405, "ddate": null, "tcdate": 1601308253405, "tmdate": 1614985732431, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4Y3ErIsf1vX", "original": null, "number": 1, "cdate": 1610040406950, "ddate": null, "tcdate": 1610040406950, "tmdate": 1610474003715, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "As of now, automatic data augmentation methods have mostly been proposed for supervised learning tasks, especially classification. This paper introduces automatic data augmentation to deep (image-based) reinforcement learning agents, aiming to make the agents generalize better to new environments. A new algorithm called data-regularized actor-critic (DrAC) is proposed, with three variants that correspond to different methods for automatically finding a useful augmentation: UCB-DrAC, RL2-DrAC, and Meta-DrAC. Promising results are reported on OpenAI\u2019s Procgen generalization benchmark which consists of 16 procedurally generated environments (games) with visual observations. Further experiments have been added in the revised version.\n\n**Strengths:**\n  * This work is among the first attempts that propose an automatic data augmentation scheme for reinforcement learning.\n  * The paper articulates well the problem of data augmentation for reinforcement learning.\n  * The experiment results are generally promising.\n\n**Weaknesses:**\n  * Although the experiment results reported seem promising, there are missing pieces in order to help the readers gain a deeper understanding to justify more thoroughly why the proposed regularization-based scheme works.\n  * Theoretical justification is lacking.\n\nThis is a borderline paper. While it presents some interesting ideas supported empirically by experiment results, the paper in its current form is premature for acceptance since a more thorough, scientific treatment is lacking before drawing conclusions. Moreover, considering that there are many competitive submissions to ICLR, I do not recommend that the paper be accepted. Nevertheless, the authors are encouraged to address the concerns raised to fill the gaps when revising their paper for resubmission in the future.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040406937, "tmdate": 1610474003699, "id": "ICLR.cc/2021/Conference/Paper2299/-/Decision"}}}, {"id": "ODn9f8VMfOk", "original": null, "number": 3, "cdate": 1603668226292, "ddate": null, "tcdate": 1603668226292, "tmdate": 1606760405894, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review", "content": {"title": "Data sugmentation is a nice tool for RL, the authors propose a simple and effective solution but fail to motivate their method or support theoretical claims", "review": "Summary after Discussion Period:\n-----------------------------------------------\nAfter reading the other reviewer's comments and corresponding with the authors, I have become convinced that the author's proposed regularization method is novel and effective, and would recommend this avenue of research be further explored. Yet it has also become clear to me that the author's claims on why their method works are not yet supported by evidence. Further, I don't believe the author's proposed further ablation studies would fix the theory, since such experiments don't address whether their method works by fixing problems with Laskin\u2019s work (as the author's claim) or because it provides a more direct way of enforcing invariance to transformation (as I claim).\n\nSo we're left with a difficult situation, the method and the experiments are good while the theory is lacking. In such a situation both acceptance or rejection seem reasonable. Yet, as per ICLR reviewer guidelines, one should answer three questions for oneself:\n\n - What is the specific question and/or problem tackled by the paper?\n - is the approach well motivated, including being well-placed in the literature?\n - Does the paper support the claims? This includes determining if results, whether theoretical or empirical, are correct and if they are scientifically rigorous.\n\nSince the theory is lacking and the approach is not well motivated, and since the theoretical claims haven't been rigorously supported, I feel as per ICLR guidelines the paper is not yet ready for acceptance.\n\nInitial Review:\n-------------------\n\nSummary In this paper, the authors introduce three approaches for automatically finding an augmentations for any RL task, and a novel regularization scheme to make such augmentations work effectively.\n\nPositive aspects:\n-----------------------\nThe paper\u2019s language is clear and the authors provide a good overview of the problem of data augmentation for reinforcement learning. Furthermore, they nicely explain why data augmentation for RL isn\u2019t as straightforward as augmenting data for supervised learning learning. I believe that data augmentation could be a nice tool in the Reinforcement Learner\u2019s toolbox, and I\u2019m glad to see a paper advancing the idea.\n\n\nMajor Concerns:\n-----------------------\n\nThis paper has not provided sufficient evidence that the author\u2019s proposed way of doing data augmentation is effective. In this paper, there are two main novel methods for doing data augmentation / insights in RL. I will discuss my concerns with both methods separately.\n\nPolicy and Value function Regularization.\n-----------------------------------------------------\n\nThe authors criticize the naive application of transformers in the PPO\u2019s buffer, as done in Laskin et al. (2020), saying that this changes the PPO objective. While I agree that such a naive transformation as in Eq. 2 is problematic, I fail to understand why application of transformation to states in the buffer would result in the Eq. 2, as the transformation would happen to the states being fed into both $\\pi_\\theta$ and $\\pi_{\\theta_\\text{old}}$, resulting in an equation different from Eq. 2. One just needs to save the old policies (and old Advantage function), so that $\\pi_{\\theta_\\text{old}}$ can be applied to transformed states, and not just use the actions from the buffer. This would seem like a straightforward fix.\n\nYet the authors have proposed a different regularization fix which judging the experiments does seem to work, as shown in Figure 2. I suspect it works for another reason: since the regularization forces $V(s) = V(f(s))$ and $\\pi(\\cdot | s) = \\pi(\\cdot | f(s))$ I wonder if this isn\u2019t a method to allow prior knowledge to flow into the policy and value estimation. If the transformation(s) $f_i$ have been chosen such that one can be reasonably sure that true value and policy functions should be invariant to said transformations, then by enforcing $V(s) = V(f(s))$ and $\\pi(\\cdot | s) = \\pi(\\cdot | f(s))$  one is constraining V and \\pi to fit the prior knowledge contained in $f_i$.\n\nSo now we\u2019re comparing apples and oranges, since your method (DrAC) gets to incorporate this prior knowledge, while PPO and RAD don\u2019t, and DrAC\u2019s good performance isn\u2019t surprising.\n\nAutomatic Data Augmentation\n----------------------------------------\n\nHere, given some candidates for data augmentation, the authors propose three methods to discover which candidate work well. Unfortunately, I don\u2019t fully understand the approach. \n\nThe authors are examining a Meta-Reinforcement Learning setting, where one wishes to find a policy which performs well not just on one MDP, but on a whole distribution of MDPs. This leads to an inner-and-outer for-loop like setting, in the inner for loop, the agent does multiple episodes with a single environment, in the outer loop the agent gets new environments.\n\nI had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations, and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment, and how it\u2019s transferred from past environments to future environments.\n\nThe only support given for these methods is the experimental results. Yet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another. So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best.\n\nMinor Comments:\n-------------------------\n$T_m(s\u2019|s,a)$ is the transition function, $R_m(s,a)$ is the reward function: Here, $T$ and $R$ are generally distributions and not functions\n\nEq. 6 is confusing, since $f_*$ can refer to both $f_t$ (function at timestep $t$) and $f_i$ (the $i$-th transformation function). \n\nDo the update with something like $N_t(f) \\leftarrow N_{t-1}(f) + 1$ is a bit clearer, since then it\u2019s the number of times $f$ has been pulled before timestep $t$\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099568, "tmdate": 1606915774543, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2299/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review"}}}, {"id": "QZlDOtzdNda", "original": null, "number": 4, "cdate": 1603697431960, "ddate": null, "tcdate": 1603697431960, "tmdate": 1606702310091, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review", "content": {"title": "Interesting results", "review": "This paper presents a method that utilizes data augmentation for image-based reinforcement learning. The data augmentation is used to regularize the policy and function approximation in the proposed method. In addition, a method for automatically identifying effective ways of data augmentation is proposed. The experimental results show that the proposed method outperforms the baseline methods.\n\nThe study shows that the regularization of policy and function approximation using the transformed images is more effective than training a policy by using the transformed image as a state. Regarding identification of effective ways of data augmentation,  the proposed method seems improve the performance of image-based RL methods, although the proposed approach is simple.\n\nI have some suggestions for improving the paper.\n- The term for regularizing the policy is given in Eq. (3). I think that the first \\pi(a|s) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model. Likewise, the first term in Eq. (4) should have the same subscript as the second term. \n\n- It would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2. \n\n\n== comments after discussions and the paper update ==\n\nI appreciate the authors' efforts to improve the clarity and provide additional results. I believe that the proposed method is now clearly presented and the claims are properly supported by experiments. I raise the score to \"accept\". \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099568, "tmdate": 1606915774543, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2299/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review"}}}, {"id": "6t-9HCFwPM", "original": null, "number": 2, "cdate": 1603600087969, "ddate": null, "tcdate": 1603600087969, "tmdate": 1606253101014, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review", "content": {"title": "An effective framework for automatic data augmentation in deep RL", "review": "#######################################\n\nSummary:\nThis paper tackles the problem of generalization in deep RL via data augmentation. It provides a framework for automatic data augmentation based on UCB, RL^2, or MAML. When UCB is combined with regularization of the policy and value function so that their outputs are invariant to transformations (such as rotation, crop, etc.), it shows improvements over similar algorithms on the ProcGen benchmark. \n\n#######################################\n\nPros:\n1. The paper is well written and the proposed algorithms are straightforward to understand.\n2. UCB-DrAC is shown to be statistically superior to the tested baselines. Surprisingly, when all ProcGen games are taken into account, some existing methods such as Rand-FM are actually worse than PPO.\n3. Ablation studies are able to show that both ingredients of UCB-DrAC are important. UCB is shown to find the best augmentation asymptotically, and the DrAC is shown to be better than PPO and RAD. \n\nCons:\n1. The experiments do not compare the proposed algorithms to DrQ, the algorithm proposed in Kostrikov et al. (2020). Since it also tackles generalization in deep RL through data augmentation, it seems that it should be included as a baseline. Can the authors explain why it was not included?\n2. The proposed algorithms are only combined with PPO. It would be good to have some results for other actor-critic algorithms, such as SAC, to verify if the SOTA behavior holds.\n\n#######################################\n\nOverall:\nI would vote for accepting this paper, due to the strength of its experimental results. The proposed approaches are novel in the data augmentation for generalization in deep RL subfield, although AutoAugment (cited in the paper) also uses RL for choosing data augmentations in the supervised learning case.\n\n#######################################\n\nFurther comments and questions:\n1. It may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode, as text is harder to read.\n2. Is there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO, and not RL2-DrAC and Meta-DrAC? Can it be shown that the former has lower sample complexity than the latter two?\n3. Why is the cycle consistency percentage in Table 2 always low? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories?\n\n#######################################\n\nUpdate after reading other reviews and author responses:\n\nI am happy to keep my score and support accepting this paper. I agree with Reviewer 4 that conducting a more detailed ablation study of the impact of regularizing both the policy and value function (i.e. a comparison with an algorithm like that of Kostrikov et al. (2020)) would improve the paper and hope that it will be included in the final paper. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099568, "tmdate": 1606915774543, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2299/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review"}}}, {"id": "L6TUu14Qj7x", "original": null, "number": 13, "cdate": 1606238381436, "ddate": null, "tcdate": 1606238381436, "tmdate": 1606239269743, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "PPDmresEKt", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Follow-up Response", "comment": "Thank you for your quick response and for the clarifications. We are glad to see that you agree with us on most points and we believe we now better understand the crux of what is preventing you from supporting our paper. We still believe it is based on a misunderstanding regarding our contributions which we aim to explain below.  \n\nIt is our understanding that the main issue preventing you from recommending acceptance of our paper is the fact that it is not crystal clear from our experiments whether the gains of DrAC over RAD are due to fixing the theoretical problem with RAD we emphasize in Section 3.1 or due to providing a stronger signal via the regularization of the policy and value function. However, it seems that you agree this could be tested by running an ablation that only uses $G_\\pi$ to disentangle the effect of the policy regularization (which would fix the problem we point out regarding RAD) from the effect of regularizing both the policy and the value function (which would provide an even stronger invariance signal). This can be done and could have likely been done during the discussion period if we had been aware about this concern. \n\nHowever, we believe that our claims and results still hold even if we do not know exactly where the gains are coming from. The central claim of our paper is that regularizing the policy and value function improves test performance on RL tasks. Hence, we do not see why also using value regularization alongside policy regularization would be a problem if this helps and makes sense in our view (for the reasons mentioned in our previous response regarding that invariance in both the value and the policy is desirable for generalization in RL). A secondary claim is that a naive use of data augmentation in RL as done in RAD leads to certain theoretical problems. We believe we have already supported this claim with our formal explanation in Section 3 and Appendix B and that this could be further supported with empirical experiments using only $G_\\pi$, which we plan to perform.\n\nC1: \"Still, I'm not sure how much an extra week or two would have helped as much work lies in front of the authors to either back up their theory with evidence or rework their theory to be more easily justifiable.\"\nA1: We are sorry to hear about your falling ill. However, we respectfully disagree and we do believe we would have been able to run the additional experiments required to convince you that our claim is well supported by empirical evidence. Alternatively, we could have simply changed some of the wording of the paper (i.e. saying that we make two separate contributions 1) emphasize a theoretical problem with RAD and 2) propose a method for regularizing the policy and value function that does not have this problem without explicitly saying that our method directly fixes RAD's problem) as you also suggested which would have been a minor fix given a reasonable amount of time. We would be happy to rephrase our claims if this makes you more comfortable in supporting our paper. \n\nC2: \"I strongly suspect (and I believe the authors concur) that also using the  term improves performance. But the question is why, when this would seem to contradict the theoretical justification which is that  is the issue with Laskin's method.\"\nA2: We respectfully disagree on this point. We do not believe this would contradict the theoretical justification at all. If only using $G_\\pi$ is better than RAD, but using both $G_\\pi$ and $G_V$ is better than only using $G_\\pi$, then our claim about the problem with RAD still holds and we show additional benefits from also regularizing the value function. \n\nFinally, we do not find a score of 4 to be fully consistent with (at least our understanding of) your current review. While we fully understand the importance of running this ablation study and we believe including them would strengthen our paper (for which we thank you for pointing it out), we believe this is a rather minor issue given that our main claims and results stand even without this ablation. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "PPDmresEKt", "original": null, "number": 12, "cdate": 1606233985259, "ddate": null, "tcdate": 1606233985259, "tmdate": 1606234003482, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "xuYbRYNK3t_", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Some clarifying remarks on my last response", "comment": "C3: \"I think there might be some misunderstanding because this is exactly what we do\"\n\nA3: I see, yes there was a misunderstanding, I appologize. I though that the augmentation would happen in $J_\\text{PPO}$ on line 13 of Alg. 1. Although the authors didn't mention whether augmentation happens here or not, I'd assumed this was the case since data augmentation is in the name of the paper and features prominently throughout. If then no data augmentation happens in $J_\\text{PPO}$, the only place the transformation function $f$ is applied is in the regularization terms $G_\\pi$ and $G_V$. And the paper is about a method for regularizing $\\pi$ and $V$ to conform to pre-existing knowledge of invariance, and isn't so much about data augmentation.\n\nWhich brings me to:\n\nC2.1: \"Could you please explain what other reasons you have in mind?\"\n\nA2.1: Yes, a fair enough request. It would seem to me that the method the authors propose works, but not for the reasons given in the theory section. There, they argue they fix an issue with Laskin's data augmentation while it seems to me that their approach is a pure regularize-to-incorporate-prior knowledge approach. It could be (I'm not sure, but I strongly suspect) this method works better than Laskin's since there the only thing pushing $V$ and $\\pi$ to be invariant is the augmented data. This is a somewhat \"weak\" signal, as it must work its way through many updates of the value function to reach $\\pi$. The author's approach, on the other hand, directly regularizes $\\pi$ and $V$, thus giving a much stronger transformation invariance signal to $\\pi$.\n\nWhich nicely dovetails with your statement:\n\nC1: The regularization of $G_V$ is important in itself for learning a value function\n\nA1: Yes, I think we can agree on this. I also apologize that I didn't respond sooner (I fell ill, no COVID thankfully). This situation and my untimely response is unfair to the authors. Still, I'm not sure how much an extra week or two would have helped as much work lies in front of the authors to either back up their theory with evidence or rework their theory to be more easily justifiable.\n\nC2.2: \"We are unsure what other fixes you would consider to be \u201cstraightforward\u201d and why such fixes would be preferable to our proposed approach\"\n\nFor example, just using the $G_\\pi$ regularization term without $G_V$ is a straightforward fix to the $\\pi(\\cdot\\mid s)\\not=\\pi(\\cdot\\mid f(s))$ issue. If indeed, this issue is the reason why Laskin's method fails, then this fix should suffice and would be preferable since it's simpler.\n\nI strongly suspect (and I believe the authors concur) that also using the $G_V$ term improves performance. But the question is why, when this would seem to contradict the theoretical justification which is that $\\pi(\\cdot\\mid s)\\not=\\pi(\\cdot\\mid f(s))$ is the issue with Laskin's method.\n\nC2.3: \"While further analysis of why a certain method works is always welcome, we believe this is outside the scope of our paper. We have proposed a novel, simple, and effective method and shown undeniable gains over the recently published approach by Laskin et al. 2020...\"\n\nA2.3: On the novel, simple and effective point I can agree. My issue is that the authors also propose a theory for why this approach works, but do not provide evidence supporting this theory. If a paper proposes a theory why something works, then providing evidence why the theory is correct should be very much within the scope of the paper. Further, I have my doubts on the soundness of this theory (which I discussed above).\n\nC4: \"We chose to compare DrAC with RAD since, as far as we know, RAD is the closest method to DrAC\"\n\nA4: I think the comparison to RAD is justified and convincing in the sense that I believe DrAC does indeed work better than RAD. Its just I doubt the theoretical justification, and the theory isn't strongly supported by evidence.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "vqJT1wNsy1N", "original": null, "number": 11, "cdate": 1606225192910, "ddate": null, "tcdate": 1606225192910, "tmdate": 1606228593817, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9u19Az-AAip", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Response to Additional Comments (part 2)", "comment": "C5: \u201cSomething that would help further would be to clarify what the \u201cupdate the policy and value function\u201d on line 7 means.\u201c\n\nA5: We have updated our paper with definitions for the policy and value function in Algorithms 2, 3 and 4, which we hope clarifies your question. The policy and value functions are updated at each interaction in Algorithms 2, 3, and 4, using one DrAC update as described in Algorithm 1. For simplicity, we initially left these details out since it is explained in the text that the agent\u2019s policy and value functions are trained (using DrAC from Algorithm 1) alongside the methods for automatically selecting an augmentation (i.e. UCB, RL2, and Meta which are described in detail in Algorithms 2, 3, and 4).\n\nPlease note that we have also submitted the code along with instructions for how to run it and we plan to open source it. We hope this will allow others to reproduce the results and understand exactly how the algorithms are implemented. \n\nC6: Minor Comment\nA6: We fixed our notation to be consistent and fully defined in the paper. \n\nCognizant that there are only a few hours left to complete the discussion, we have tried to promptly respond to your concerns and have updated the paper to fix some minor notation issues, as you requested. However, we hope that in your final recommendation you will take into account the fact that it was simply not possible for us to run further experiments in the last couple hours of the discussion period. \n\nWe believe our paper introduces a new method, thoroughly compares it against other strong baselines on two challenging and widely used benchmarks, and establishes a new state of the art. While we agree there is always room for more extensive analysis, we ask you to consider that we have put significant effort towards comparing against related methods and relevant ablations. We are confident from our discussion we have made the case to convince you that \"4\" is too low a grade, and we hope that you are willing to change your mind and decide to strongly support our publication. Please note that the scale of 1-10 leaves room for such assessments, e.g. 7 or 8, which both support the paper but state there is some room for improvement. We hope you will agree, and we thank you again for your valuable feedback and for engaging in the discussion process.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "xuYbRYNK3t_", "original": null, "number": 10, "cdate": 1606225144600, "ddate": null, "tcdate": 1606225144600, "tmdate": 1606227773257, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9u19Az-AAip", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Response to Additional Comments (part 1)", "comment": "Thank you for responding to our comments and engaging in the discussion. We believe there are some misunderstandings which we seek to clarify here in the hope that you will feel fully comfortable supporting our paper. \n\nC1: \u201cYet it would seem that the $\\pi(\u22c5\u2223s)\u2260\\pi(\u22c5\u2223f(s))$ issue is fixed by regularizing the $G_\\pi$ term, yet in your work you regularize both $G_\\pi$ and $G_V$, and there\u2019s nothing in the paper showing why \n$G_V$ is important or showing what happens when one regularizes only one of the two terms.\u201d\n\nA1: The regularization of $G_V$ is important in itself for learning a value function that is invariant to certain transformations of the observation. In addition, the learned value function is used to compute the generalized advantage estimate, which is then used to update the policy. Hence, regularizing the value function also indirectly influences the learned policy. As we argue in the paper, learning invariant policies and value functions can be useful for generalizing to new scenarios. Imagine that at test time, you encounter similar observations from training but with different backgrounds (e.g. different colors). In this case, you would like your value function to predict similar expected returns for two observations with the same underlying state but different backgrounds, which can be achieved via training with DrAC with random-conv or color-jitter augmentations. While we would be happy to run an ablation showing the effects of $G_\\pi$ and $G_V$ separately, please note that this is simply unfeasible in the remaining discussion time which ends in a few hours. We are counting on your good faith to trust that we will run these experiments to even more thoroughly evaluate our method and that you will take this into account when making a final recommendation for our paper. Given that this request was only mentioned in the last few hours of the discussion period, there is no way for us to better address it at this point.\n\n\nC2: \u201cWhich brings me back to the original concern, which is that it seems like the authors discard straightforward fixes to the issue in favor of their method, which seem to work well in experiments but I suspect it works for another reason.\u201d\n\nA2: Could you please explain what other reasons you have in mind? We are unsure what other fixes you would consider to be \u201cstraightforward\u201d and why such fixes would be preferable to our proposed approach. While further analysis of why a certain method works is always welcome, we believe this is outside the scope of our paper. We have proposed a novel, simple, and effective method and shown undeniable gains over the recently published approach by Laskin et al. 2020, as well as over other strong baselines and ablations, on two challenging benchmarks for generalization in reinforcement learning. \n\n\nC3: \u201cStill, the paper provides no insight into how much performance is gained by using non-augmented observations while regularizing the policy and value functions such that $V(s)=V(f(s))$ and similar for $\\pi$.\u201d\n\nA3: I think there might be some misunderstanding because this is exactly what we do -- we show how much performance is gained by using non-augmented observations in the buffer while regularizing the policy and value functions. The PPO buffer contains only non-augmented observations, but we use augmented observations to regularize the policy and value function such that $V(s) = V(f(s))$ and $\\pi(s) = \\pi(f(s))$.\n\n\nC4: \u201cTherefore, since Laskin uses data augmentation and here the authors use data augmentation (without showing how important that component is) combined with regularization of the value and policy, I still think we could be comparing apples and oranges; comparing a data augmentation method and a regularization method which forces \\pi and V to conform to pre-existing knowledge of invariance, and that the method works for reasons other than that which the authors describe.\u201d\n\nA4: We believe we have empirically shown how important the regularization component is by comparing DrAC with RAD and demonstrating that DrAC significantly outperforms RAD, by a wide margin in certain cases (see Figure 2). RAD uses data augmentation as part of the PPO buffer while DrAC uses data augmentation as part of the regularization terms. We chose to compare DrAC with RAD since, as far as we know, RAD is the closest method to DrAC. We are unsure what you would consider to be a more convincing experiment to support our claim. We are also unsure what you mean by \u201cthe method works for reasons other than that which the authors describe\u201d. What other reasons do you have in mind and how do you recommend we disentangle the different factors? We are happy to address any concerns you might have if you can provide guidance on how we can alleviate them. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "9u19Az-AAip", "original": null, "number": 9, "cdate": 1606186366723, "ddate": null, "tcdate": 1606186366723, "tmdate": 1606186366723, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "25p-s0SSpRX", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "I thank the authors for the paper updates and clarifying comments.", "comment": "After considering the authors response, I find my concern that the authors haven\u2019t demonstrated why their method works still stands. They claim they\u2019re fixing an issue with Laskin\u2019s work, but don\u2019t provide evidence that the issue in Laskin\u2019s work is the real problem, nor do they address other fairly plausible explanations why the method could work on the experiments they chose. So I still argue for reject, but now with the modifications and clarifying comments I'm less forcefully in favor of rejection\n\nC1: \u201cOne just needs to save the old policies (and old Advantage function), so that $\\pi_{\\theta_\\text{old}}$ can be applied to transformed states, and not just use the actions from the buffer. This would seem like a straightforward fix.\u201d\n\nThank you for you answer, I think I understand the issue more clearly now. You claim the poor performance is as a result of the inaccurate estimates resulting from $\\pi(\\cdot\\mid s) \\not= pi(\\cdot\\mid f(s))$. \n\nYet it would seem that the $\\pi(\\cdot\\mid s)\\not=pi(\\cdot\\mid f(s))$ issue is fixed by regularizing the $G_\\pi$ term, yet in your work you regularize both $G_\\pi$ and $G_V$, and there\u2019s nothing in the paper showing why $G_V$ is important or showing what happens when one regularizes only one of the two terms. Which brings me back to the original concern, which is that it seems like the authors discard straightforward fixes to the issue in favor of their method, which seem to work well in experiments but I suspect it works for another reason.\n\nC2: \u201cSo now we\u2019re comparing apples and oranges, since your method (DrAC) gets to incorporate this prior knowledge, while PPO and RAD don\u2019t, and DrAC\u2019s good performance isn\u2019t surprising.\u201d \n\nYet again, I thank the authors for their answer. Indeed the authors are correct when they write \u201cDrAC does not use any extra knowledge compared to RAD since they both use augmented observations to regularize RL agents.\u201d On this point I stand corrected.\n\nStill, the paper provides no insight into how much performance is gained by using non-augmented observations while regularizing the policy and value functions such that $V(s)=V(f(s))$ and similar for $\\pi$. Therefore, since Laskin uses data augmentation and here the authors use data augmentation  (without showing how important that component is) combined with regularization of the value and policy, I still think we could be comparing apples and oranges; comparing a data augmentation method and a regularization method which forces $\\pi$ and $V$ to conform to pre-existing knowledge of invariance, and that the method works for reasons other than that which the authors describe.\n\nC3: I thank the authors for adding Alg. 2 to the appendix. This greatly helps the reader  understand your approach. Something that would help further would be to clarify what the \u201cupdate the policy and value function\u201d on line 7 means. These functions were not defined before in the algorithm, so it\u2019s unclear if they are defined in Alg. 1, used and then thrown away or if they somehow persist on in Alg. 2.\n\nC4: \u201cYet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another. So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best.\u201d\n\nA4: We never claim that the performance of UCB-DrAC is better than that of DrAC.\n\nI see, fair enough.\n\nMinor Comment:\n\n$J_{\\text{AC}}$ in line 13 of Alg. 1 is not defined. Plus, in Eq. 5 and line 13 of Alg. 1, $J_{\\text{DrAC}}$ gets two different definitions. I would recommend keeping the definition of $J_{\\text{DrAC}}$ consistent and defining $J_{\\text{AC}}$ somewhere."}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "JTL2hMJidBk", "original": null, "number": 7, "cdate": 1605957650370, "ddate": null, "tcdate": 1605957650370, "tmdate": 1605957672518, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Paper Update", "comment": "We would like to thank you again for your constructive feedback which has helped us further improve our paper. We have uploaded a revised draft which contains the following updates:\n\n*. Results on the DeepMind Control Suite with distractor backgrounds (Section 4.5 and Appendix K), where our method still outperforms the baselines.  \n\n*. Pseudocodes and more details for all our proposed methods (Section 3.2 and Appendix D).\n\n*. Comparison with RAD in the robustness analysis (Section 4.4).\n\n*. Explained in greater detail why a naive application of data augmentation in RL can be problematic (Appendix B).\n\n*. Clarified the notation in Sections 3.2 and 3.3.\n\nWe hope these changes, together with our responses below, address your concerns and are sufficient for you to reconsider your assessment of the paper. If you have any remaining concerns, please do not hesitate to let us know so we can discuss them. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "rf8tZ7jnhu", "original": null, "number": 6, "cdate": 1605469271268, "ddate": null, "tcdate": 1605469271268, "tmdate": 1605469271268, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "c2H6vzJKNuC", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Thank you for the suggestion", "comment": "Thank you for these suggestions. We agree they are relevant and we will add them to our related work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "c2H6vzJKNuC", "original": null, "number": 1, "cdate": 1605462864134, "ddate": null, "tcdate": 1605462864134, "tmdate": 1605462864134, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Public_Comment", "content": {"title": "Related works", "comment": "I think this is an interesting paper on one additional form of data augmentation for generalization in RL, which might greatly simplify existing data aug. approaches by making the process automatic. I believe the recent papers $^{1,2}$ would be a great addition to the related works section since they also touch upon data augmentation and generalization in RL. \n$^1$. Stooke, Adam, et al. \"Decoupling representation learning from reinforcement learning.\" arXiv preprint arXiv:2009.08319 (2020)., \n$^2$. Mazoure, Bogdan, et al. \"Deep reinforcement and infomax learning.\" Advances in Neural Information Processing Systems 33 (2020)."}, "signatures": ["~Bogdan_Mazoure1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Bogdan_Mazoure1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors", "ICLR.cc/2021/Conference/Paper2299/Reviewers", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024964075, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Public_Comment"}}}, {"id": "cNmd6a6bmzU", "original": null, "number": 5, "cdate": 1605176195906, "ddate": null, "tcdate": 1605176195906, "tmdate": 1605176935493, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "oeLdbdVTzeX", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "We thank their reviewer for their positive and constructive comments. We particularly appreciate that the reviewer finds our approach to be useful, effective and robust to irrelevant factors. We respond to the reviewer\u2019s comments below.\n\nC1: \u201cI think the primary difference between the proposed approach and existing approaches is the additional regularization losses. This would make the new approach seem incremental.\u201d \n\nA1: We respectfully disagree with the characterization that our approach is \u201cincremental\u201d. On the contrary, we believe that the simplicity of our approach is one of its main strengths since it makes it easier build upon for other members of the community. In addition, even if our method is simple, it shows significant improvements over the previous state-of-the-art on a challenging benchmark for generalization in RL. Moreover, we provide theoretical intuition and empirical evidence for the importance of the additional regularization losses we introduce. As shown in Figure 2, not using these additional losses, the methods may completely fail to learn in certain cases. \n\nC2: \u201cThere is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches.\u201d\n\nA2: Figure 6 and 7 in the Appendix compare our proposed automatic approaches (UCB-DrAC, RL2-DrAC, and Meta-DrAC) with the non-automatic ones (DrAC with the best augmentation). \n\nC3: \u201cThe data augmentation approach is only evaluated in the Procgen benchmark. It would be great if this paper includes more experiments to demonstrate the performance, especially since the proposed method is intended for any RL task.\u201d\n\nA3: We agree with the reviewer that it would be valuable to include experiments on other domains and we will do our best to report back on this by the end of the discussion period. However, we would like to point out that our paper already contains extensive empirical results, with 25 models on 16 different environments, 10 seeds each, accounting to 4000 experiments in total, each taking at least 4 hours on a GPU (after hyperparameter search). Please note that this is far in excess of most other published papers that use Procgen for evaluation, which showed results only on a subset of the environments (e.g. 3 in Laskin et al. 2020, 1 in Igl et al. 2019 and Lee et al. 2020). While we aim to include experiments on another domain by the end of the discussion period, due to computational and time constraints, we expect the comparisons to be more limited in scope (i.e. they will likely contain a smaller number of environments and baselines). \n\nIn light of these clarifications, we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "bLGpVAqVue_", "original": null, "number": 2, "cdate": 1605175447319, "ddate": null, "tcdate": 1605175447319, "tmdate": 1605176910152, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "QZlDOtzdNda", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "We thank the reviewer for their encouraging and constructive feedback. Below we would like to respond to the reviewer\u2019s suggestions for improving our work. \n\nC1: \u201cThe term for regularizing the policy is given in Eq. (3). I think that the first \\pi(a|s) in the KL divergence should also have the subscript \\theta because it seems that the two policies are based on the same model. Likewise, the first term in Eq. (4) should have the same subscript as the second term.\u201d\n\nA1: It is true that the two policies in the equation, \\pi(a|s) and  \\pi(a|f(s, \\nu)) have the same parameters \\theta. The reason we left out the \\theta subscript from \\pi(a|s) is to indicate that we are only backpropagating gradients through  \\pi(a|f(s, \\nu)). This is mentioned right below equation (5). But we appreciate the suggestion and we will make our notation more clear. \n\nC2: \u201cIt would be good to show the Jensen-Shannon divergence and the cycle consistency of RAD in Table 2.\u201d\n\nA2: We agree this would be valuable and we will add these results in the updated draft. \n\nIn light of the promised changes, we would appreciate it if the reviewer would be willing to reconsider their assessment and/or provide us with further feedback.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "25p-s0SSpRX", "original": null, "number": 3, "cdate": 1605175755978, "ddate": null, "tcdate": 1605175755978, "tmdate": 1605176897153, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "ODn9f8VMfOk", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "We thank the reviewer for their feedback and we especially appreciate that the reviewer finds our writing clear and believes that data augmentation for reinforcement learning is an important research direction. We respond to the reviewer\u2019s concerns below.\n \nPolicy and Value Function Regularization\n \nC1: \u201cOne just needs to save the old policies (and old Advantage function), so that \u03c0\u03b8old can be applied to transformed states, and not just use the actions from the buffer. This would seem like a straightforward fix.\u201d\n \nA1: We respectfully disagree with this comment as we believe such a solution would be incorrect and would not solve the problem we emphasize. Please note that the correct estimate of the policy gradient objective used in PPO is the one in equation (1) which does not use the augmented observations at all since we are estimating advantages for the actual observations A(s, a). The probability distribution used to sample advantages is \\pi_{old}(a | s) (rather than \\pi_{old}(a | f(s)) since we can only interact with the environment via the true observations and not the augmented ones (because the reward and transition functions are not defined in this case). Hence, the correct importance sampling estimate uses \\pi(a|s) / \\pi_{old}(a | s). If we understood correctly, what you propose is to use  \\pi(a | f(s)) / \\pi_{old}(a | f(s)), but this is incorrect for the reasons mentioned above. What we argue is that, in the case of RAD, the only way to use the augmented observations f(s) is in the policy gradient objective, whether by  \\pi(a | f(s)) / \\pi_{old}(a | f(s)) as you propose or  \\pi(a | f(s)) / \\pi_{old}(a | s) as RAD uses, but both are incorrect. In contrast, DrAC does not change the policy gradient objective at all which remains the one in equation (2) and instead uses the augmented observations in the additional regularization losses, as shown in equations (3), (4), and (5).\n \nC2: \u201cSo now we\u2019re comparing apples and oranges, since your method (DrAC) gets to incorporate this prior knowledge, while PPO and RAD don\u2019t, and DrAC\u2019s good performance isn\u2019t surprising.\u201d \n \nA2: We respectfully disagree with this interpretation of our work. DrAC does not use any extra knowledge compared to RAD since they both use augmented observations to regularize RL agents. The only difference is that DrAC makes better use of this additional data by explicitly regularizing the policy and value function, while RAD attempts to do this implicitly. Please note that, even if RAD uses additional data, its performance can still be worse than PPO\u2019s, while our method is comparable or better than PPO when using the same extra data as RAD (as shown in Figure 2). \n \nAutomatic Data Augmentation\n\nC3: \u201cI had expected this inner-outer for-loop structure to pe present in the meta learning of augmentations, and for the authors to clearly describe how knowledge about the effectiveness of augmentations is transferred from past episodes on one environment to future episodes in the same environment, and how it\u2019s transferred from past environments to future environments.\u201d\n\nA3: We will add more details about the meta-learning training. Please note that we have also included the code in our submission and we plan to open source it for full transparency. \n\nC4: \u201cYet we see that the performance of DrAC and UCB-DrAC lie within a standard deviation of one another. So the evidence of the effectiveness of UCB-DrAC over the simpler DrAC is weak at best.\u201d\n\nA4: We never claim that the performance of UCB-DrAC is better than that of DrAC. In fact, DrAC with the best augmentation is an upper bound to UCB-DrAC since DrAC uses the best augmentation during the entire training process while UCB-DrAC needs to first find the best augmentation from a given set. The advantage of UCB-DrAC over DrAC is not superior performance but rather the fact that it provides an automatic way of selecting a good augmentation for a given task. As explained in section 3.3, UCB-DrAC reduces the computational resources needed to train an RL agent with data augmentation since it only needs to train a model once, while DrAC needs to train models with each type of augmentation in order to select the best one. Our point is that UCB-DrAC still achieves comparable performance with DrAC trained with the best augmentation, as shown in Figure 3. \n\nC5: Minor Comments.\n\nA5: Thank you for pointing these out. We will change our notation accordingly. \n\nIn light of these clarifications, we would appreciate it if the reviewer confirmed that all their concerns have been addressed and, if so, reconsider their assessment.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "31EU47y8rzd", "original": null, "number": 4, "cdate": 1605176029579, "ddate": null, "tcdate": 1605176029579, "tmdate": 1605176872424, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "6t-9HCFwPM", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "We thank the reviewer for their positive and constructive feedback. We were delighted to hear they found our paper to be well written, our experimental results strong, and our approach novel for generalization in RL. We address your concerns below. \n\nC1: \u201cThe experiments do not compare the proposed algorithms to DrQ, the algorithm proposed in Kostrikov et al. (2020). Since it also tackles generalization in deep RL through data augmentation, it seems that it should be included as a baseline. Can the authors explain why it was not included?\u201d\n\nA1: The reason we do not compare with DrQ is because DrQ uses SAC as a base algorithm, while our method is based on PPO. We chose to use PPO as a base algorithm because it is the state-of-the-art on the Procgen benchmark, which we use to test generalization. In addition, TD(0) methods such as SAC or DrQ have not been shown yet to achieve decent performance on Procgen. Please also note that DrQ was designed for improving sample complexity rather than generalization.  \n\nC2: \u201cThe proposed algorithms are only combined with PPO. It would be good to have some results for other actor-critic algorithms, such as SAC, to verify if the SOTA behavior holds.\u201d\n\nA2: As mentioned above, to the best of our knowledge SAC is not well suited for the Procgen benchmark. While we are happy to run more experiments with other actor-critic algorithms, we would like to point out that our paper already contains extensive empirical results, with 25 models on 16 different environments, 10 seeds each, accounting to 4000 experiments in total, each taking at least 4 hours on a GPU (after hyperparameter search). Please note that this is far in excess of most other published papers that use Procgen for evaluation, which showed results only on a subset of the environments (e.g. 3 in Laskin et al. 2020, 1 in Igl et al. 2019 and Lee et al. 2020). \n\nC3: \u201cIt may be helpful to rewrite some of the algorithms in Section 3.3 in pseudocode, as text is harder to read.\u201d\n\nA3: Thank you for the suggestion. We will add a pseudocode in the paper.\n\nC4: \u201cIs there intuition for why only UCB-DrAC leads to statistically significant improvements over PPO, and not RL2-DrAC and Meta-DrAC? Can it be shown that the former has lower sample complexity than the latter two?\u201d\n\nA4: We are not entirely sure but UCB does have theoretical finite-time regret guarantees for multi-armed bandit problems such as this one (Auer, 2002) while we are not aware of a comparable result for RL^2 or MAML. We also think it is possible that UCB is better suited for our problem formulation while RL^2 and MAML start showing their benefits on more complex problems such as contextual bandits or sequential decision making. We think this is an interesting question for future work.\n\nC5: \"Why is the cycle consistency percentage in Table 2 always low? Is it some idiosyncrasy of the metric or due to the inherent variance in the trajectories?\"\n\nA5: We believe this might indeed be due to the variance in the trajectories. In some cases, there can be more than a single path for maximizing reward and the agent might be prone to choose different paths for different backgrounds of the same level (which could be exacerbated by the fact that many of the backgrounds are highly structured rather uniform).\n \nIn light of these clarifications, we would appreciate it if the reviewer confirmed that all their concerns have been addressed and, if so, reconsider their assessment.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9l9WD4ahJgs", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2299/Authors|ICLR.cc/2021/Conference/Paper2299/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850039, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Comment"}}}, {"id": "oeLdbdVTzeX", "original": null, "number": 1, "cdate": 1603077533204, "ddate": null, "tcdate": 1603077533204, "tmdate": 1605024244211, "tddate": null, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "invitation": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review", "content": {"title": "Tend to accept ", "review": "Summary: \n\nThis paper proposes an automatic data augmentation approach for RL tasks. Specifically, it takes UCB for data selection and introduces two regularization terms for actor-critic algorithms' policy and value function. Then this paper evaluated the approach based on the Procgen benchmark and demonstrated that it outperforms existing methods. It is also shown that the learned policies and representations are robust to irrelevant factors.\n\n\nReasons for score:\n\nOn the one hand, I feel that the proposed approach is incremental (UCB for data selection with the actor-critic algorithm as the RL algorithm plus additional regularization terms). It would also be ideal if more experiments can be included to prove that the proposed approach is effective in most RL tasks. However, on the other hand, based on the current experimentation results, the proposed method seems promising and can be useful for generalization in reinforcement learning.\n\n\nPros\n\n1. The proposal of the automatic data augmentation/selection approach is useful, given that the existing primary methods either rely on expert knowledge or separately evaluate a large number of transformations to find the best one, both of which are expensive.\u00a0\n\n2. The proposed approach achieves SOTA performance in the Procgen benchmark.\n\n3. It shows that the proposed method is robust to irrelevant factors.\u00a0\n\t\n\nCons\n\n1. I think the primary difference between the proposed approach and existing approaches is the additional regularization losses. This would make the new approach seem incremental.\u00a0\n\n2. There is a lack of performance comparison between the automatic data augmentation method and non-automatic approaches.\n\n3. The data augmentation approach is only evaluated in the Procgen benchmark. It would be great if this paper includes more experiments to demonstrate the performance, especially since the proposed method is intended for any RL task.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2299/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2299/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Automatic Data Augmentation for Generalization in Reinforcement Learning", "authorids": ["~Roberta_Raileanu2", "~Maxwell_Goldstein1", "~Denis_Yarats1", "~Ilya_Kostrikov1", "~Rob_Fergus1"], "authors": ["Roberta Raileanu", "Maxwell Goldstein", "Denis Yarats", "Ilya Kostrikov", "Rob Fergus"], "keywords": ["reinforcement learning", "generalization", "data augmentation"], "abstract": "Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to benefit from different types of augmentations and selecting the right one typically requires expert knowledge. In this paper, we introduce three approaches for automatically finding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for actor-critic algorithms. We evaluate our method on the Procgen benchmark which consists of 16 procedurally generated environments and show that it improves test performance by 40% relative to standard RL algorithms. Our approach also outperforms methods specifically designed to improve generalization in RL, thus setting a new state-of-the-art on Procgen. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. ", "one-sentence_summary": "We propose an approach for automatically finding an augmentation, which is used to regularize the policy and value function in order to improve generalization in reinforcement learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "raileanu|automatic_data_augmentation_for_generalization_in_reinforcement_learning", "supplementary_material": "/attachment/77ab2cccc5da44d1c07b8c5f66976a187519930d.zip", "pdf": "/pdf/ae2f7ad5c2753439062781784eaa3039a4ef7b2a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UfvtsbXWoo", "_bibtex": "@misc{\nraileanu2021automatic,\ntitle={Automatic Data Augmentation for Generalization in Reinforcement Learning},\nauthor={Roberta Raileanu and Maxwell Goldstein and Denis Yarats and Ilya Kostrikov and Rob Fergus},\nyear={2021},\nurl={https://openreview.net/forum?id=9l9WD4ahJgs}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9l9WD4ahJgs", "replyto": "9l9WD4ahJgs", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099568, "tmdate": 1606915774543, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2299/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2299/-/Official_Review"}}}], "count": 18}