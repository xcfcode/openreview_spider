{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124463539, "tcdate": 1518462711737, "number": 216, "cdate": 1518462711737, "id": "r1ebvD1wf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r1ebvD1wf", "signatures": ["~Yifan_Wu1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent", "abstract": "A major challenge in understanding the generalization of deep learning is to explain why (stochastic) gradient descent can exploit the network architecture to find solutions that have good generalization performance when using high capacity models. We find simple but realistic examples showing that this phenomenon exists even when learning linear classifiers --- between two linear networks with the same capacity, the one with a convolutional layer can generalize better than the other when the data distribution has some underlying spatial structure. We argue that this difference results from a combination of the convolution architecture, data distribution and gradient descent, all of which are necessary to be included in a meaningful analysis. We provide a general analysis of the generalization performance as a function of data distribution and convolutional filter size, given gradient descent as the optimization algorithm, then interpret the results using concrete examples. Experimental results show that our analysis is able to explain what happens in our introduced examples.", "paperhash": "wu|towards_understanding_the_generalization_bias_of_two_layer_convolutional_linear_classifiers_with_gradient_descent", "keywords": ["generalization analysis", "gradient descent", "convolution"], "_bibtex": "@misc{\n  wu2018towards,\n  title={Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent},\n  author={Yifan Wu and Barnabas Poczos and Aarti Singh},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ebvD1wf}\n}", "authorids": ["yw4@andrew.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu"], "authors": ["Yifan Wu", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We analyze the generalization performance of two layer convolutional linear classifiers trained with gradient descent, motivated by explaining some empirical observations.", "pdf": "/pdf/dfaf30813d6a10404215fb41fcc7a2c5c4d7793b.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582962503, "tcdate": 1520126852768, "number": 1, "cdate": 1520126852768, "id": "H16FsTuuM", "invitation": "ICLR.cc/2018/Workshop/-/Paper216/Official_Review", "forum": "r1ebvD1wf", "replyto": "r1ebvD1wf", "signatures": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer3"], "content": {"title": "an interesting analysis of deep learning combining network architecture, data distribution and gradient descent", "rating": "7: Good paper, accept", "review": "In this paper, the authors contributed towards an understanding of deep learning by considering two layer convolutional linear classifiers trained with gradient descent. A feature of the analysis is that the authors considered a joint effect among network architecture, data distribution and gradient descent. This explains why two models with the same capacity may have different generalization ability. The results are interesting for us to understand the practical performance of deep learning.\n\nMinor comments:\nThe description of 1-D \"images\" is not quite clear. \nFor Task-1stCtrl, do you mean instances x with only one '1' but the instances with y=-1 has '1' on the left-half, while that with y=1 has '1' on the right-half\nFor Task-3rdCtrl, do you mean instances with a single '+1'  and a single '-1' but the order of '+1' and '-1' are different for instances with y=1 and instances with y=-1?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent", "abstract": "A major challenge in understanding the generalization of deep learning is to explain why (stochastic) gradient descent can exploit the network architecture to find solutions that have good generalization performance when using high capacity models. We find simple but realistic examples showing that this phenomenon exists even when learning linear classifiers --- between two linear networks with the same capacity, the one with a convolutional layer can generalize better than the other when the data distribution has some underlying spatial structure. We argue that this difference results from a combination of the convolution architecture, data distribution and gradient descent, all of which are necessary to be included in a meaningful analysis. We provide a general analysis of the generalization performance as a function of data distribution and convolutional filter size, given gradient descent as the optimization algorithm, then interpret the results using concrete examples. Experimental results show that our analysis is able to explain what happens in our introduced examples.", "paperhash": "wu|towards_understanding_the_generalization_bias_of_two_layer_convolutional_linear_classifiers_with_gradient_descent", "keywords": ["generalization analysis", "gradient descent", "convolution"], "_bibtex": "@misc{\n  wu2018towards,\n  title={Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent},\n  author={Yifan Wu and Barnabas Poczos and Aarti Singh},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ebvD1wf}\n}", "authorids": ["yw4@andrew.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu"], "authors": ["Yifan Wu", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We analyze the generalization performance of two layer convolutional linear classifiers trained with gradient descent, motivated by explaining some empirical observations.", "pdf": "/pdf/dfaf30813d6a10404215fb41fcc7a2c5c4d7793b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582962302, "id": "ICLR.cc/2018/Workshop/-/Paper216/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper216/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper216/AnonReviewer1"], "reply": {"forum": "r1ebvD1wf", "replyto": "r1ebvD1wf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper216/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582962302}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582794490, "tcdate": 1520627370421, "number": 2, "cdate": 1520627370421, "id": "Syfn0veKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper216/Official_Review", "forum": "r1ebvD1wf", "replyto": "r1ebvD1wf", "signatures": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer1"], "content": {"title": "I'm not convinced that the artificial classification tasks are representative of real image-classification problems", "rating": "5: Marginally below acceptance threshold", "review": "The paper explores the difference in generalization performance between a linear model and a 1-layer convolutional model using three simple classification tasks, and theoretically and experimentally demonstrate that, on these tasks, the convolutional model generalizes better than the linear model.\n\nI'm not persuaded that the three tasks are realistic. In the first, for example (Task-Cls), the examples are vectors with exactly one nonzero element, with this nonzero being either +1 or -1, which is also the label. The other two tasks are only slightly more complicated, and all three share the property that successful classification is about identifying one or two informative features, instead of aggregating information over the entire feature vector. Some classification tasks are like this, but I think that image classification is almost exactly the opposite: no one pixel, or handful of pixels, suffices to determine the label.\n\nTheorems 1 and 2 demonstrate (with the simplifying assumption that a linear loss is being minimized) that the convolutional model generalizes better than the linear model. I wish that the authors had included an intuitive explanation of why this occurs, beyond the two theorems. It appears to me that, in the case of Task-Cls, the reason is that for the linear model there is only *one* informative feature per example, whereas inserting a width-k convolutional layer causes there to be k informative features, which is effectively reducing the dimension of the problem, and therefore improving generalization performance. Assuming that this intuition is correct, it appears to be a relic of the particular artificial problems under consideration, and is not something that can be easily used to draw conclusions about generalization performance of real models trained on real datasets.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent", "abstract": "A major challenge in understanding the generalization of deep learning is to explain why (stochastic) gradient descent can exploit the network architecture to find solutions that have good generalization performance when using high capacity models. We find simple but realistic examples showing that this phenomenon exists even when learning linear classifiers --- between two linear networks with the same capacity, the one with a convolutional layer can generalize better than the other when the data distribution has some underlying spatial structure. We argue that this difference results from a combination of the convolution architecture, data distribution and gradient descent, all of which are necessary to be included in a meaningful analysis. We provide a general analysis of the generalization performance as a function of data distribution and convolutional filter size, given gradient descent as the optimization algorithm, then interpret the results using concrete examples. Experimental results show that our analysis is able to explain what happens in our introduced examples.", "paperhash": "wu|towards_understanding_the_generalization_bias_of_two_layer_convolutional_linear_classifiers_with_gradient_descent", "keywords": ["generalization analysis", "gradient descent", "convolution"], "_bibtex": "@misc{\n  wu2018towards,\n  title={Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent},\n  author={Yifan Wu and Barnabas Poczos and Aarti Singh},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ebvD1wf}\n}", "authorids": ["yw4@andrew.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu"], "authors": ["Yifan Wu", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We analyze the generalization performance of two layer convolutional linear classifiers trained with gradient descent, motivated by explaining some empirical observations.", "pdf": "/pdf/dfaf30813d6a10404215fb41fcc7a2c5c4d7793b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582962302, "id": "ICLR.cc/2018/Workshop/-/Paper216/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper216/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper216/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper216/AnonReviewer1"], "reply": {"forum": "r1ebvD1wf", "replyto": "r1ebvD1wf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper216/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582962302}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573570568, "tcdate": 1521573570568, "number": 121, "cdate": 1521573570224, "id": "rys6AA0KM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r1ebvD1wf", "replyto": "r1ebvD1wf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent", "abstract": "A major challenge in understanding the generalization of deep learning is to explain why (stochastic) gradient descent can exploit the network architecture to find solutions that have good generalization performance when using high capacity models. We find simple but realistic examples showing that this phenomenon exists even when learning linear classifiers --- between two linear networks with the same capacity, the one with a convolutional layer can generalize better than the other when the data distribution has some underlying spatial structure. We argue that this difference results from a combination of the convolution architecture, data distribution and gradient descent, all of which are necessary to be included in a meaningful analysis. We provide a general analysis of the generalization performance as a function of data distribution and convolutional filter size, given gradient descent as the optimization algorithm, then interpret the results using concrete examples. Experimental results show that our analysis is able to explain what happens in our introduced examples.", "paperhash": "wu|towards_understanding_the_generalization_bias_of_two_layer_convolutional_linear_classifiers_with_gradient_descent", "keywords": ["generalization analysis", "gradient descent", "convolution"], "_bibtex": "@misc{\n  wu2018towards,\n  title={Towards Understanding the Generalization Bias of Two Layer Convolutional Linear Classifiers with Gradient Descent},\n  author={Yifan Wu and Barnabas Poczos and Aarti Singh},\n  year={2018},\n  url={https://openreview.net/forum?id=r1ebvD1wf}\n}", "authorids": ["yw4@andrew.cmu.edu", "bapoczos@cs.cmu.edu", "aarti@cs.cmu.edu"], "authors": ["Yifan Wu", "Barnabas Poczos", "Aarti Singh"], "TL;DR": "We analyze the generalization performance of two layer convolutional linear classifiers trained with gradient descent, motivated by explaining some empirical observations.", "pdf": "/pdf/dfaf30813d6a10404215fb41fcc7a2c5c4d7793b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}