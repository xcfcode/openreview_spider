{"notes": [{"id": "HJxkvlBtwH", "original": "rJM5R9eKDS", "number": 2345, "cdate": 1569439830956, "ddate": null, "tcdate": 1569439830956, "tmdate": 1577168290769, "tddate": null, "forum": "HJxkvlBtwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RISiI_bTQ6", "original": null, "number": 1, "cdate": 1576798746774, "ddate": null, "tcdate": 1576798746774, "tmdate": 1576800889324, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Decision", "content": {"decision": "Reject", "comment": "The paper developed log abstract transformer, square abstract transformer and sigmoid-tanh abstract transformer to certifiy robustness of neural network models for audio. The work is interesting but the scope is limited. It presented a neural network certification methods for one particular type of audio classifiers that use MFCC as input features and LSTM as the neural network layers. This thus may have limited interest to the general readers. \n\nThe paper targets to present an end-to-end solution to audio classifiers. Investigation on one particular type of audio classifier is far from sufficient. As the reviewers pointed out, there're large literature of work using raw waveform inputs systems. Also there're many state-of-the-art systems are HMM/DNN and attnetion based encoder-decoder models. In terms of neural network models, resent based models, transformer models etc are also important. A more thorough investigation/comparison would greatly enlarge the scope of this paper. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719975, "tmdate": 1576800270719, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Decision"}}}, {"id": "rJlkTjrTFB", "original": null, "number": 2, "cdate": 1571802039305, "ddate": null, "tcdate": 1571802039305, "tmdate": 1574201093293, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper presents an end-to-end neural network verifier that is specially designed for audio signal processing to certify the robustness of a system when facing noise perturbation.  The approach is based on abstract transformers to deal with non-linearity in the audio signal processing pipeline and LSTM acoustic model. The authors implement the approach in a so-called \"deep audio certifier\" system and conduct experiments on various datasets and network architectures.  The results seem to be supportive.   The idea is good and the mathematical derivation is meticulous (although appears to be a bit tedious).  This is an interesting paper globally but I have some concerns. \n\n1.  There should be a more thorough introduction on how to verify the robustness of a neural network classifier for noise perturbation.  There should be some background summary such as what are the existing approaches and how to measure the robustness of a neural network classifier, etc..\n\n2. The so-called audio processing pipeline here is actually speech processing pipeline. I am not sure if \"audio\" is the right term in a strict sense. \n\n3.  This paper is closely related to the POPQORN paper. So it is good to see some in-depth discussion and comparison between the two.  One thing that is not clear to me is that the authors claim POPQORN is very time-consuming but DAC is much faster.  I wonder if the authors can elaborate a bit more on this issue.   What exactly makes POPQORN time-consuming in this case? \n\nP.S. rebuttal read.  I will stay with my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574802995137, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Reviewers"], "noninvitees": [], "tcdate": 1570237724159, "tmdate": 1574802995156, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review"}}}, {"id": "SklqlcwuiS", "original": null, "number": 7, "cdate": 1573579250380, "ddate": null, "tcdate": 1573579250380, "tmdate": 1573579250380, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "S1lGzu0hYH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment", "content": {"title": "Reply for Reviewer#1", "comment": "Thank you for the feedback and interest in our work. Below we answer the concerns:\n\n\nQ1: Can you provide background on neural network certification?\n\nA1: Certification of neural networks is an emerging area which aims to prove the robustness of neural networks against adversarial perturbations. For an introduction to the area, we refer the reviewer to related work listed in the introduction, particularly [1] which provides a unifying framework for many of the existing approaches. You can also check the answer we provide to Reviewer 2 for more technical details.\n\nCertification of neural networks is still at an early stage and none of the existing approaches scales to state-of-the-art networks and datasets (e.g. ImageNet). While here we do not directly consider the state-of-the-art architectures you listed, our method for certifying recurrent networks is general and can be applied to those architectures as well. However, certification methods for computer vision tasks have scaled from verifying small networks with 200 neurons to large residual networks with thousands of neurons and we expect the same progress in the audio domain in the future.\n\nOur work is the first one to certify audio classifiers. None of the existing approaches can handle this task and are mostly limited to computer vision. The only alternative approach which can handle recurrent networks, POPQORN, does not scale to the audio benchmarks as we demonstrated in our experiments in Appendix E.\n\n\nQ2: Section 2. Threat model: what kind of noises are using?\n\nA2: Please see the definition of our threat model at the beginning of Section 2.\n\n\n[1] Salman, Hadi, et al. \"A convex relaxation barrier to tight robust verification of neural networks.\", NeurIPS 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxkvlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2345/Authors|ICLR.cc/2020/Conference/Paper2345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142750, "tmdate": 1576860530990, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment"}}}, {"id": "r1gpOKwOiB", "original": null, "number": 6, "cdate": 1573579125047, "ddate": null, "tcdate": 1573579125047, "tmdate": 1573579125047, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "rJlkTjrTFB", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment", "content": {"title": "Reply for Reviewer#2", "comment": "Thank you for the feedback and interest in our work. Below we answer the concerns:\n\n\nQ1: There should be a more thorough introduction on how to verify the robustness of a neural network classifier for noise perturbation.  There should be some background summary such as what are the existing approaches and how to measure the robustness of a neural network classifier, etc..\n\nA1: Thank you for the suggestion, we will extend our background section with the description of existing approaches for certification of neural networks. In the meantime, here we provide short summary: \n\nMost existing state-of-the-art scalable certification methods aim to capture all possible behaviors of a neural network using convex relaxations. In this perspective, we treat the input with the predefined perturbation range as a multi-dimensional polyhedron and pass it through the operations within the network.\nThe most basic approach here uses interval propagation - it maintains the minimum and maximum possible value for each neuron in the network. More recent work (listed in Section 2) presents more elaborate relaxations to capture the propagation of the initial perturbation through the network.\nRobustness certification is performed by checking whether the neuron corresponding to the true label is always greater than neurons corresponding to other labels with respect to the input region. If this is true, then we can establish the network correctly classifies the input under all possible realized perturbations. We will further elaborate on this in the next revision.\n \n\nQ2: The so-called audio processing pipeline here is actually speech processing pipeline. I am not sure if \"audio\" is the right term in a strict sense. \n\nA2: Thank you for pointing this out, we will modify our terminology to more precisely match what we describe in the paper. \n\n\nQ3: Can you provide in-depth discussion and comparison between your approach and POPQORN?\n\nA3: Please see our answer to the main points above where we provide answers to these questions and an in-depth comparison between POPQORN and our approach."}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxkvlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2345/Authors|ICLR.cc/2020/Conference/Paper2345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142750, "tmdate": 1576860530990, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment"}}}, {"id": "Hye17tPOsB", "original": null, "number": 5, "cdate": 1573579031371, "ddate": null, "tcdate": 1573579031371, "tmdate": 1573579031371, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "BJeL0admqr", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment", "content": {"title": "Reply for Reviewer#3", "comment": "Thank you for the feedback and interest in our work. Below we answer the concerns:\n\nQ1: While I found section 3 to be useful to get an intuition of the proposed method, I still feel that it could be condensed a bit to add in additional details. For example, the authors don\u2019t describe \u201cback-substitution\u201d in the work, which I believe should be described in the main text.\n\nA1: We added more details on back-substitution in the last paragraph of Section 3. We have not described back-substitution in full detail in the main text as it is not part of our main contributions. Full details of this algorithm can be found in [1]. We also provide full derivation of bounds in our overview example using back-substitution in Appendix C. \n\n\nQ2: How sensitive was the provability metric to the choice of these 100 test examples?\n\nA2: To check the sensitivity of our results to the choice of 100 samples that we verified we ran the verification on 10 random permutations of our test set (each time with a different seed). We describe this experiment in Appendix F and show the verification results in Figure 9 with error bars indicating the variance. These results show that provability is not significantly affected by the choice of 100 element subset used for verification. In the next revision, we will include these error bars for each plot.\n\n\nQ3: The section on \u201cProvable defense for audio classifiers\u201d was not very clear to me. The authors state that \u201cTo train, we combine standard loss with the worst case loss obtained using interval propagation.\u201d I was not clear on what the modified loss is. Could the authors please clarify this in the text, preferably a mathematical formulation? Also, I\u2019m curious why these experiments are only conducted on the FSDD set, but not on the GSC set. \n\nA3: In Appendix G we have now provided the detailed description of our provable defense procedure, including a mathematical formulation of a modified loss, which closely follows Gowal et al. (2018). We also ran this experiment for GSC network \u2014  these results are also presented in Appendix G.\n\n\nQ4: Why does the interval analysis technique perform so much worse on the GSC set relative to the FSDD set? Could you describe more details about the model architectures used for the two tasks?\n\nA4: Based on our experiments, we believe that certifying GSC is more difficult for two reasons: (i) the main factor which impacts provability is the number of frames \u2014 each additional frame causes accumulation of errors introduced by loose convex relaxations which ultimately makes the certification more likely to fail. This reflects in the fact that GSC, where samples have 19 frames on average, is significantly more difficult to certify than FSDD which has samples with 15 frames on average, and (ii) given that GSC and FSDD have 30 and 10 output classes, respectively, in the case of GSC verifier needs to prove that the final logit of correct class is greater than the logit of all other 29 classes which is naturally more difficult than proving it for only 9 other classes as is the case with FSDD.  \n\n\nQ5: Could you quantify and report the difference in the volume between POPQORN and your approach? Also, why are the approximation volumes not comparable between the two systems? \n\nA5: Please see our answer to the main points above where we provide answers to these questions and in-depth comparison between POPQORN and our approach.\n\n\nQ6: Could you clarify that there is significant body of work which operates directly on the time domain signal?\n\nA6: Yes, thank you for the helpful references. We made a clarification and added the references you suggested.\n\n\n[1] Singh, Gagandeep, et al. \"An abstract domain for certifying neural networks.\" Proceedings of the ACM on Programming Languages 3.POPL (2019): 41."}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxkvlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2345/Authors|ICLR.cc/2020/Conference/Paper2345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142750, "tmdate": 1576860530990, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment"}}}, {"id": "SJeB2PvusB", "original": null, "number": 4, "cdate": 1573578668848, "ddate": null, "tcdate": 1573578668848, "tmdate": 1573578668848, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment", "content": {"title": "Main Points for Common Concerns", "comment": "We thank the reviewers for their comments. We first answer the main points, followed by specific questions:\n\n - Comparison with POPQORN:\n\n  We provide detailed quantitative comparison between our approach and POPQORN in Appendix E. The main takeaways are:\n\n  1) Our method produces bounds strictly better than interval bounds (see Theorem 1). This means the maximum distance between the true function and our bounds cannot grow arbitrarily large. POPQORN offers no such guarantees. Although POPQORN uses gradient descent to optimize for the bounds with minimum volume, there are no convergence or optimality guarantees. This can cause imprecise results in practice \u2014 we found many inputs in our audio benchmarks for which POPQORN produces bounds worse than intervals, please see Figure 6 in Appendix E for one example. Our bounds are strictly better than intervals and do not suffer from this problem.\n\n  2) While we experimented with POPQORN on synthetic inputs and found that it indeed produces relaxations with slightly smaller volume than our method, for realistic inputs which appear in our audio benchmarks it often performs worse. As it is non-comparable to intervals, meaning that all points obtained via intervals are not included inside those obtained via POPQORN and vice-versa, its bounds are often worse than intervals, e.g. see Figure 6.\n\n  3) As POPQORN is relatively slow (108 minutes per sample), we evaluated it only on 10 samples. We plugged their relaxation of tanh * sigmoid instead of our bounds and demonstrated that it results in 0 verified samples on our benchmarks. At the same time, we verify 4 out of these 10 samples (in 29 seconds per sample). We believe the core reason is the existence of many pathological cases as described in the previous point where gradient descent used by POPQORN converges to suboptimal solution which ends up being worse than interval bounds. We note that we contacted the authors to confirm that we are using their framework correctly.\n\n  4) POPQORN is 100 - 1000 times slower than DAC as it relies on optimization via gradient descent whereas DAC produces bounds in constant time. This makes POPQORN less practical for verification at the scale of audio classifiers.\n\n\n - Speedup of the existing implementation:\n\n  We optimized our implementation which improved the runtime of back-substitution. This results in a significant speedup of end-to-end verification: for update depth of 3 now it takes 17.74 seconds per input, compared to 87.92 seconds reported in Table 2. For larger back-substitution depths, the speedup is even larger. We will update running time analysis for experiments in Table 2.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxkvlBtwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2345/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2345/Authors|ICLR.cc/2020/Conference/Paper2345/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142750, "tmdate": 1576860530990, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Authors", "ICLR.cc/2020/Conference/Paper2345/Reviewers", "ICLR.cc/2020/Conference/Paper2345/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Comment"}}}, {"id": "S1lGzu0hYH", "original": null, "number": 1, "cdate": 1571772426171, "ddate": null, "tcdate": 1571772426171, "tmdate": 1572972350498, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes an approach for the certification of speech classification neural networks against adversarial perturbations. The network is based on a simple pipeline starting from MFCC to make an utterance level classifier via a last hidden state of an LSTM acoustic model. This approach can perform analysis through this pipeline. I feel that this paper is very difficult to follow because of the lack of background technique explanations based on neural network certification, and the lack of technical surveys of speech recognition and related areas. This paper requires such major restructuring and more surveys to make it in good shape.\n\nComments\n- The authors only list CTC related techniques as state-of-the-art ASR, but state-of-the-art ASR is still based on the HMM/DNN hybrid system or attention-based encoder-decoder/RNN transducer. They seriously lack the surveys o this area. Also several technical terminologies are not common in the speech recognition are (e.g., automated speech recognition --> automatic speech recognition)\n- \"Additionally, audio systems typically use recurrent architectures (Chiu et al., 2017)\": There are a lot of state-of-the-art ASR systems including TDNN (Kaldi), CNN, and transformer. Again the paper does not have enough surveys.\n- The font size of the characters in figure 1 is too small.\n- I cannot understand why the paper uses MFCC. The community was already moved from MFCC to log mel filterbank. We don't need final DCT.\n- Section 2. Threat model: what kind of noises are using?\n- Page 3, power operation: Either side must be a conjugate to get the power spectrum.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574802995137, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Reviewers"], "noninvitees": [], "tcdate": 1570237724159, "tmdate": 1574802995156, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review"}}}, {"id": "BJeL0admqr", "original": null, "number": 3, "cdate": 1572208077545, "ddate": null, "tcdate": 1572208077545, "tmdate": 1572972350405, "tddate": null, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "invitation": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In this work, the authors study the task of building neural network classifiers for audio tasks which can be certified as being resistant to an adversarial attack. One of the contributions of this work is the development of abstract transformers which can be used for the data processing frontend used in typical audio applications. The work also proposes an abstract transformers for LSTMs which is stated to be much faster to use in practice than previous work. \n\nOverall, this work is interesting and I think it would be a great addition to the conference. The paper is generally well written in the initial sections, and the main ideas are very clearly presented. However, there are a number of missing details, particularly in the final sections which discuss the experimental validation. In its present form, I am rating this work as \u201cweak reject\u201d, but I would increase my scores if the authors can improve the final sections in the revised draft.\n\nMain Comments:\n1. While I found section 3 to be useful to get an intuition of the proposed method, I still feel that it could be condensed a bit to add in additional details. For example, the authors don\u2019t describe \u201cback-substitution\u201d in the work, which I believe should be described in the main text.\n2. A clarification question: When computing provability, the authors state that \u201cWe randomly shuffled the test data and then, for every experiment, inferred labels one by one until the number of correctly classified samples reached 100. We report the number of provably correct samples out of these 100 as our provability.\u201d How sensitive was the provability metric to the choice of these 100 test examples? Was the metric computed by repeatedly sampling 100 test cases, for example?\n3. The section on \u201cProvable defense for audio classifiers\u201d was not very clear to me. The authors state that \u201cTo train, we combine standard loss with the worst case loss obtained using interval propagation.\u201d I was not clear on what the modified loss is. Could the authors please clarify this in the text, preferably a mathematical formulation? Also, I\u2019m curious why these experiments are only conducted on the FSDD set, but not on the GSC set. \n4. Figure 5c. Why does the interval analysis technique perform so much worse on the GSC set relative to the FSDD set?  On a related note, it would also be useful to describe some more details about the model architectures for the two tasks.\n5. The section on \u201cExperimental comparison with prior work\u201d similarly left me with a number of questions. The authors mention that \u201cWe found that, in practice, optimization approach used by POPQORN produces approximations of slightly smaller volume than our LSTM transformer (although non-comparable).\u201d Could these be quantified and reported in the paper. Also, why are the approximation volumes not comparable between the two systems. \n\nMinor comment: It is true that most works in audio classification and speech recognition use processed frontend features such as MFCCs. However, there is also a significant body of work which operates directly on the time-domain signal. Perhaps it would be better to clarify this in the text?\nFor example:\nPascual S, Bonafonte A, Serra J. SEGAN: Speech enhancement generative adversarial network. arXiv preprint arXiv:1703.09452. 2017 Mar 28.\nSainath TN, Weiss RJ, Senior A, Wilson KW, Vinyals O. Learning the speech front-end with raw waveform CLDNNs. In Sixteenth Annual Conference of the International Speech Communication Association 2015.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2345/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wryou@student.ethz.ch", "bmislav@student.ethz.ch", "gsingh@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "title": "Certifying Neural Network Audio Classifiers", "authors": ["Wonryong Ryou", "Mislav Balunovic", "Gagandeep Singh", "Martin Vechev"], "pdf": "/pdf/4c0c258cff5bf1bbbd8f41c7cfcd37934f5e01d3.pdf", "TL;DR": "We present the first approach to certify robustness of neural networks against noise-based perturbations in the audio domain.", "abstract": "We present the first end-to-end verifier of audio classifiers. Compared to existing methods, our approach enables analysis of both, the entire audio processing stage as well as recurrent neural network architectures (e.g., LSTM). The audio processing is verified using novel convex relaxations tailored to feature extraction operations used in audio (e.g., Fast Fourier Transform) while recurrent architectures are certified via a novel binary relaxation for the recurrent unit update. We show the verifier scales to large networks while computing significantly tighter bounds than existing methods for common audio classification benchmarks: on the challenging Google Speech Commands dataset we certify 95% more inputs than the interval approximation (only prior scalable method), for a perturbation of -90dB.", "code": "https://drive.google.com/file/d/13dFJb3hwFaMortWr3D_3Z5H4j6HhurAh/view?usp=sharing", "keywords": ["Adversarial Examples", "Audio Classifier", "Speech Recognition", "Certified Robustness", "Deep Learning"], "paperhash": "ryou|certifying_neural_network_audio_classifiers", "original_pdf": "/attachment/c38c79633d72ca2db88c51f05f78e79d233d8bdc.pdf", "_bibtex": "@misc{\nryou2020certifying,\ntitle={Certifying Neural Network Audio Classifiers},\nauthor={Wonryong Ryou and Mislav Balunovic and Gagandeep Singh and Martin Vechev},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxkvlBtwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxkvlBtwH", "replyto": "HJxkvlBtwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2345/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574802995137, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2345/Reviewers"], "noninvitees": [], "tcdate": 1570237724159, "tmdate": 1574802995156, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2345/-/Official_Review"}}}], "count": 9}