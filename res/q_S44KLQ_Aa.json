{"notes": [{"id": "q_S44KLQ_Aa", "original": "-D-6TdXkz1k", "number": 1614, "cdate": 1601308178894, "ddate": null, "tcdate": 1601308178894, "tmdate": 1611607664892, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WGwJKN_Z1Jn", "original": null, "number": 1, "cdate": 1610040397012, "ddate": null, "tcdate": 1610040397012, "tmdate": 1610473992345, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviewers and AC liked the basic idea of how this paper improves on ALISTA, and the initial scores were high. Because the contributions rely quite a lot on empirical demonstrations, the reviewers asked for more experiments, changes to experiments, and timing results. The revision and rebuttal addressed most of these requests.  The multipath channel estimation problem was interesting though outside the scope of the AC and reviewer's expertise, so it is hard to evaluate how helpful the method is in that particular setting."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040396995, "tmdate": 1610473992327, "id": "ICLR.cc/2021/Conference/Paper1614/-/Decision"}}}, {"id": "4MQehGnqXvC", "original": null, "number": 1, "cdate": 1603697635763, "ddate": null, "tcdate": 1603697635763, "tmdate": 1606298796935, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "SUMMARY:\nThe paper at hand introduces Neurally augmented ALISTA (NA-ALISTA) which is an extension to the previously proposed analytical learned iterative shrinkage threshold algorithm (ALISTA). Both algorithms belong to the class of learned optimization algorithms for solving the compressed sensing problem, i.e., methods that have parameters which are learned via backpropagating through multiple iterations of the algorithm. The key novelty of the NA-LISTA is the LSTM network used to predict thresholds and stepsized used by the algorithm. The experiments show that this adaptive approach improves the performance of ALISTA.\n\nSTRENGTHS:\n1. After reading the paper I have the impression that the proposed method is thouroughly evaluated. The experimental setup is clear and well-described. Interestingly, the performance of their approach does not depend on wether u^(k) or r^(k) is fed into the LSTM-cell.\n2. The results look very promising and the proposed NA-LISTA algorithm seems to consistently outperform the other discussed ISTA variants.\n3. The method is well motivated.\n4. The paper is very clearly written and well positioned in previously existing literature. All notation is introduced beforehand and it is easy to follow.\n\nWEAKNESSES:\n1. The authors claim that the computational time per iteration is not strongly influenced by the forward pass through the LSTM, because of its relatively small architecture. However, I would have liked to see wall-clock time comparison for the different ISTA variants.\n2. No completely novel theoretical insights. The lemma and the threorem are adapted from Liu et al. (2019).\n3. I find the formulations \"An algorithm which approximates such thresholds, resulting in a tighter error bound, is the aim of this paper.\" somewhat misleading, since there is not tighter bound explicitly stated within the paper.\n\nCONCLUSION:\nOverall, I would recommend to accept this submission. The method is solidly justified and the experiments are convincing. I would like to see wall-clock time comparisons in the final manuscript or in the supplemental material, but this is not a major issue in my opinion.\n\nMINOR REMARKS:\n- At some points throughout the paper the definition of variables, e.g., N=500, is not written in math mode.\n- Also some citations are not correctly formatted, e.g. \"Candes, Romberg, Tao and Donoho (Cand\u00e8s et al., 2006; Donoho, 2006)\" should be \"Candes et al. (2006) and Donoho (2006)\".\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114704, "tmdate": 1606915771745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1614/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review"}}}, {"id": "1S6yY7SKxvP", "original": null, "number": 10, "cdate": 1606298780272, "ddate": null, "tcdate": 1606298780272, "tmdate": 1606298780272, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "MRVQizASbI0", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Re: Official Blind Review #4", "comment": "I thank the authors for their follow-up and I appreciate the inclusion of wall-clock time comparisons in the supplementary material. After reading the updated paper, I chose to update my score to 8.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "SSzLNLZcG_B", "original": null, "number": 2, "cdate": 1603880171397, "ddate": null, "tcdate": 1603880171397, "tmdate": 1606278742840, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review", "content": {"title": "Improved method for sparse recovery by augmenting ALISTA with good theoretical and empirical motivation", "review": "### Summary\n\nThe paper shows that augmenting analytic learned ISTA (ALISTA) with a small LSTM that predicts step sizes and thresholds improves empirical performance in terms of sparse reconstruction compared to comparable baselines, especially as the compression ratio increases (i.e. ratio of measurement dimension M to sparse vector dimension N).\n\nThe proposed method is also nicely motivated by an intermediate step in the ALISTA reconstruction error bounds, where predicting thresholds adaptively given knowledge of the L1 error between estimate at the $k$th step and true target $x^*$ can allow use of a tighter error bound.\n\n\n### Strong points\n\nS1: Clear explanation of the method in terms of its relation to prior work and theoretical motivation.\n\nS2: Nice empirical exploration of theoretical motivation (i.e. examining correlation between various quantities of interest in Figures 1 and 2)\n\nS3: The approach seems to achieve superior performance compared to comparable baselines.\n\n### Weak points\n\nW1: Only uses synthetic data for evaluation. This is fine to study the properties of the method, and sparse recovery is a general method, but I think the paper would be stronger if the authors also used the approach for some kind of real world task or application. Or at least used parameters for synthetic data that match a real world task, e.g. maybe something from communications. This would also help motivate the method's superior performance for higher compression ratios.\n\nW2: A bit more empirical validation of the theoretical arguments would strengthen the paper. In particular, Figure 8 shows that the ratio of threshold to step size versus iteration $k$ is roughly proportional to the true L1 error, but looking at the bound in equation (7), I wonder if there's a stronger empirical validation, e.g. would it be possible to compute the coherence $\\tilde{\\mu}$ and check that the ratio is bounded below by coherence times L1 error?\n\n### Recommendation\n\nI think this is a good paper and I recommend acceptance. I would be inclined to increase my score if the weak points I mentioned above were addressed, and depending on answers to my questions below.\nThe paper is clearly written, the method well-motivated both theoretically and empirically, and achieves superior performance compared to competitive and comparable baselines.\n\n### Questions\n\nQ1: how important is the LSTM architecture? Have you tried other types of RNNs, e.g. vanilla RNN or GRU?\n\nQ2: I was a little confused about the LSTM notation. Does c, h \\leftarrow LSTM(c, h, [r, u]) mean that the output is split into these two vectors? A bit more explanation of this notation would be helpful.\n\nQ3: relating to a weak point I mentioned above: is there some motivation for the choice of synthetic data parameters M, N, S, K, H?\n\nQ4: is Figure 3 the mean reconstruction error over the 10k-example test set? Would it make sense to report std devs across examples for each method?\n\n### Other comments:\n\nC1: I found the Figure 1 caption to be a little hard to understand and there's a typo. Adding some punctuation could help: \"=15, (a) and (b), and non sparse vectors .., (c) and (d)\"\n\nC2: Caption of figure 1 says \"whereas there is no obvious correlation for non-sparse vectors.\". I wouldn't say there's no obvious correlation just from looking at the scatter plots. For figures 1 and 2, how about measuring and reporting correlation measures, like Pearson and/or Spearman? I suggest Spearman b/c the correlation doesn't seem entirely linear in the Figure 2 plots. Adding these measures would quantify the degree of correlation.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114704, "tmdate": 1606915771745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1614/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review"}}}, {"id": "DAwXwmVlVQ9", "original": null, "number": 9, "cdate": 1606278720872, "ddate": null, "tcdate": 1606278720872, "tmdate": 1606278720872, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "4dXfVFmZzzk", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Updated review", "comment": "Thanks to the authors for their responses and their effort in adding additional results and improvements to the paper. I read the revised paper and responses, and I appreciate the ablation versus neural network architecture, as well as the experiment with real-world parameters and its detailed description. I feel that all my comments were addressed, and the paper has been improved. As such I chose to increase my score."}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "O7Y0rs7tt47", "original": null, "number": 3, "cdate": 1604033923499, "ddate": null, "tcdate": 1604033923499, "tmdate": 1606253329534, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review", "content": {"title": "An interesting application of LSTM to ALISTA based on empirical observations", "review": "This paper extends the framework of ALISTA, a variant of learned ISTA called Neurally Augmented ALISTA (AG-ALISTA), which significantly reduces the number parameters in the model (down to 2 scalars per layer, one for step size and the other for the threshold in soft-thresholding function). Specifically, the authors use a LSTM to generate these two parameters in each layer along iterations, taking reconstruction error related signals as input. This method is based on (1) the previous previous finding of the relation of the step size and threshold with the $\\ell_1$ signal recovery error; and (2) the empirical observation of the correlation between the $\\ell_1$ signal recovery error and reconstruction error. Experiments in synthetic setting show the superiority of AG-ALISTA over ALISTA and other variants that follow it, especially in settings where the compression ratios are challenging, which is claimed to be more realistic in real-world settings.\n\nPros:\nThe most interesting and novel part of this paper is the use of LSTM for the generation of the step size and threshold parameters. The authors use two types of signals related to reconstruction error as the input to the LSTM, which is based on the restricted isometry due to sparsity and observed to be reasonable. And the observation of \"not weak\" correlation between the $\\ell_1$ recovery error and previous reconstruction error also makes it reasonable to use a LSTM structure. These are all interesting observations. Also, the synthetic experimental results do corroborate the effectiveness of the AG-ALISTA model.\n\nCons:\n- Firstly, it is kind of a pity that the authors do not provide some real-world experiments, e.g. compressive sensing, where the compression ratios are challenging indeed. I think this paper would be strong with taht kind of experiments.\n- I think eqn (11) and (12) hold when the measurements are noiseless; otherwise they are not exactly accurate. However, noiseless experiments are not presented, nor is there discussion about the noisy/noiseless cases.\n- Another concern is that baseline performance of ALISTA with 40dB noises when N=500 seems to be worse than that reported in the previous works. Is it because of the training strategy used? According to the description in this paper, I guess the authors do not use the layerwise training but end-to-end training? This is not clearly stated in the paper.\n\nOthers:\n- In the last paragraph of page 3, \"However, the thresholds that make the error bound tighter vary depending on...\" Either \"tighter\" or \"vary\" should be deleted.\n\nOverall I think this is an interesting paper. I am willing to further raise my score if the authors address my concerns/questions.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114704, "tmdate": 1606915771745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1614/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review"}}}, {"id": "tI1HMZO0ngj", "original": null, "number": 7, "cdate": 1606253311544, "ddate": null, "tcdate": 1606253311544, "tmdate": 1606253311544, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "65Gpz_MJjR", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Re: An interesting application of LSTM to ALISTA based on empirical observations.", "comment": "Thank you for your response, clarifications and the efforts for the new wireless data experiments. It a great job and will be a good complement to the paper. Therefore, I raised my score."}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "MRVQizASbI0", "original": null, "number": 6, "cdate": 1606240823048, "ddate": null, "tcdate": 1606240823048, "tmdate": 1606240823048, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "4MQehGnqXvC", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Official Blind Review #4", "comment": "We thank the reviewer for the helpful feedback. \n\nWe definitely agree with the reviewer\u2019s wish to include a wall-clock time comparison of the algorithms - this can be found in the appendix of the updated version.\n\nWe agree with the reviewer\u2019s assessment that the theoretical insights of NA-ALISTA are largely based on the impressive results from ALISTA. We did our best to highlight their contributions, and to make sure that it is clear that the credit for the Theorem and Lemma belongs to them.\n\nAs for the last point, we clarified the somewhat misleading formulation the reviewer pointed out, now saying that we aim to develop an algorithm that can obtain the adaptive version of the error bound which is tighter for some instances of x*, hoping to improve the overall performance.\n\nWe have also fixed the citation formatting and the math mode the reviewer pointed out.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "4dXfVFmZzzk", "original": null, "number": 5, "cdate": 1606240764708, "ddate": null, "tcdate": 1606240764708, "tmdate": 1606240764708, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "SSzLNLZcG_B", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Improved method for sparse recovery by augmenting ALISTA with good theoretical and empirical motivation.", "comment": "We thank the reviewer for the detailed review. We address the weaknesses brought to attention by the reviewer as well as their questions in the order posed:\n\nW1: We strongly agree that our paper benefits from a real-world scenario. We have included such an experimental setting in the updated version of the paper.\n\nW2: For the empirical verification of the theoretical results the reviewer suggests to verify the Theorem by measuring the coherence of the matrix and checking if the ratio between theta and gamma is greater than the L1 error multiplied by the coherence as suggested in Equation (6), the difference to our current evaluation being the multiplication by the coherence. We agree that this would give a stronger validation, but note that in our experiment in Fig. 8 the settings are N=2000, K=16, M=250 with a generalized coherence mu=0.31 of Phi and W. Using a sparsity of S=50, the Assumption 1 does not hold anymore since we do not fulfill S < (1 + 1/mu)/2 ~= 2.1. Recovery experiments beyond the coherence bound are also the case in previous literature [2], and interestingly the reconstruction still works despite the large S. In fact, in ALISTA [2], the theorem is also only verified via a correlation instead of strict boundedness. \n\nQ1: We appreciate the concern that there is no ablation study on the effectiveness of using an LSTM in NA-ALISTA. In particular it may not have been clear that a recurrent architecture is necessary at all. In the updated version of the paper, in Table 1, we include a comparison to NA-ALISTA with a simple MLP (i.e. no recurrency) and a Vanilla RNN. We find that the MLP performs worse than all recurrent architectures whereas the Vanilla RNN is able to match the performance of the LSTM. This suggests that recurrent architectures are necessary for good performance. However, the Vanilla RNN exhibits strong training instability - sooner or later it always ends up producing NaNs. In fact we were not able to train a Vanilla RNN for N=2000. Training with a smaller learning rate does not seem to mitigate this. We suspect that the training of a Vanilla RNN would become even more brittle as we increase the number of iterations. This is a known issue, and in fact one of the reasons that the LSTM was introduced [1, Section 1, Paragraph 3 and Section 4.8 ].\n\nQ2: We would like to clarify our notation of the LSTM cell in the pseudo-code block. The LSTM cell in itself is a function of three vectors, a hidden state h_k, a cell state c_k and the inputs, in our case a vector v_k  = [r_k,u_k]. Based on the inputs, it acts on its states to produce a new hidden state h_{k+1} and cell state c_{k+1}, which we denoted as \u2018h, c \\leftarrow LSTM(h,c,[r,u])\u2019. These new states are then used as the LSTM input in the next iteration. Since generating an output at a specific iteration can differ depending on the application, this step is not part of the general LSTM framework, so we do not capture it in the LSTM function. This closely mimics the Python code for using LSTM cells in PyTorch, where the LSTM cell is a function that return two values (https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n\nQ3: The main motivation for the choice of parameters was to allow for a fair comparison between our method and existing literature, which established this choice of parameters. In the real-world scenario we included in the updated version of our paper, these values directly match an application in wireless communication.\n\nQ4: This is correct, we average across a 10k test example set. In ALISTA, the measurements are only taken over a set of 1k examples [2, Section 5.1, first paragraph] which we already increased to get a more stable outcome. The reviewers suggestion to also capture the standard deviation is valid since the reconstruction error indeed varies between different test samples. We evaluated the variance of ||x-\\hat{x}|_2/||x||_2 for all methods empirically, but found that it decreases proportionally to the reconstruction error for all of them. Thus we will stick to the common practice of only reporting nMSE without variance in the updated version of the paper.\n\nWe addressed the reviewer\u2019s comments by clarifying the formatting of the Fig 1 caption and corroborating our claims by reporting the Spearman correlation measures as suggested.\n\n[1] Hochreiter, Sepp, and J\u00fcrgen Schmidhuber. \"Long short-term memory.\" Neural computation 9.8 (1997): 1735-1780.\n\n[2] Liu, Jialin, and Xiaohan Chen. \"ALISTA: Analytic weights are as good as learned weights in LISTA.\" International Conference on Learning Representations (ICLR). 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "65Gpz_MJjR", "original": null, "number": 4, "cdate": 1606240669249, "ddate": null, "tcdate": 1606240669249, "tmdate": 1606240669249, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "O7Y0rs7tt47", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: An interesting application of LSTM to ALISTA based on empirical observations.", "comment": "We thank the reviewer for their thorough comments. \n\nWe have included experiments in a compressed sensing scenario which closely follows a real-world setting.\n\nThe reviewer pointed out that the proof from ALISTA [2] we rely on considers the noiseless case. In the LISTA-CPSS paper [1] (by the same authors as ALISTA), specifically Assumption 2 and Theorem 3, properties of desirable sensing matrices to use with LISTA in the presence of noise are proven. This proof of ALISTA [2] very closely follows the proof from LISTA-CPSS, but instead considers the noiseless case \u201cfor simplicity of the proofs\u201d (Page 3, last paragraph), instead showing resilience against noise empirically. We added a comment as a footnote in the revised version of our paper in Section 2. The proofs of AGLISTA [3] and ALISTA-AT [4] are in turn are based on these noiseless proofs of ALISTA, but all also provide experiments only for the noisy setting. We suspect that the reason for these two essential papers not providing experiments in the noiseless setting is that for sparse signals with random normal elements, NMSE below -50 dB essentially becomes indistinguishable from numerical errors. \n\nThe fact that the baseline performance of ALISTA with 40dB noise and N=500 is slightly worse than reported by the original authors may stem from two things: learning rate schedule and the way noise is applied. In ALISTA, an exponential decay of the learning rate is employed, whereas we train with a smaller but constant learning rate. We believe that employing such a scheme and training for longer could improve the performance of all algorithms by a small margin in the SNR=40 case. As for the noise: in ALISTA,  and subsequent works, the standard deviation of the noise is estimated for each dimension of the vectors per batch: https://github.com/VITA-Group/ALISTA/blob/master/utils/train.py#L84-L88. This means that the fluctuation in SNR for each individual target vector depends on the batch size. In our implementation, noise is applied instantaneously, leading to each vector having exactly the prescribed SNR. We do not believe that the difference stems from a layer-wise training procedure: in fact the authors of ALISTA explicitly state that they use end-to-end training [2, page 5, below eq. 16] for the sparse reconstruction experiments. The layer-wise joint training refers to their convolutional sparse coding experiment. In any case, we are confident our paper allows for a fair comparison between the compared algorithms, as all algorithms are trained in exactly the same setting. \n\nWe have also fixed the typo that has been brought to attention. (We clarified the complete paragraph in the new version.)\n\n[1] Chen, Xiaohan, et al. \"Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds.\" Advances in Neural Information Processing Systems (NeurIPS). 2018.\n\n[2] Liu, Jialin, and Xiaohan Chen. \"ALISTA: Analytic weights are as good as learned weights in LISTA.\" International Conference on Learning Representations (ICLR). 2019.\n\n[3] Wu, Kailun, et al. \"Sparse Coding with Gated Learned ISTA.\" International Conference on Learning Representations (ICLR). 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "GSb9GPIaLm", "original": null, "number": 3, "cdate": 1606240591046, "ddate": null, "tcdate": 1606240591046, "tmdate": 1606240591046, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "NoXShSbFsSr", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: A reasonable improvement over LISTA with somewhat insufficient experiments.", "comment": "We thank the reviewer for their feedback.\n\nWe agree with the reviewer that the number of iterations was insufficient to convincingly convey that NA-ALISTA is indeed better - we increased the number of iterations in the updated version of the paper. We also include an analysis of the actual running time in the Appendix. \n\nAs for tuning the other algorithms: In FISTA and ISTA, only lambda has to be tuned, which we did via a grid search (as mentioned on Page 6, Paragraph 3). ALISTA is also rather straightforward to tune: the support selection hyperparameter p has a large effect, and we confirmed that the value proposed by the authors [1] leads to the best performance. Beyond that we found that as long as the initial thresholds aren\u2019t too big to clip all values, ALISTA always reached the same reconstruction performance after enough training. In ALISTA-AT, there is only one more hyperparameter over ALISTA, an epsilon to prevent division by zero. We use the authors\u2019 recommended value. Tuning AGLISTA is a bit more nuanced as another 3 initial learnable parameters have to be tuned: we tuned these per hand to the best of our ability. Note that these are only initial parameters and that stochastic gradient-based learning should lead them to at least a local optimum.\n\nTo address the question about Figures 1 & 2: Indeed Figure 1 (a) and (b) are redundant with the text. Figure 1 (c) and (d) shows that W is not isometric for dense vectors, which directly follows from M<<N. Thus, it is correct that everything in Figure 1 can be inferred from the text - nonetheless we believe the Figure increases readability and thus leave it in the paper. In fact, Reviewer 3 specifically mentioned this.\n\nOn the other hand, Figure 2 shows something much less obvious: even though the matrices involved are an isometry for sparse vectors, this does not necessarily mean that vector norms are preserved through an iteration of the algorithm, i.e. a gradient step with a dynamically computed step size followed by the l1 proximal operation. This relationship, and the fact that it is even preserved across multiple iterations are highlighted by Figure 2.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "lMgakAaa_Di", "original": null, "number": 2, "cdate": 1606240446786, "ddate": null, "tcdate": 1606240446786, "tmdate": 1606240446786, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment", "content": {"title": "Rebuttal Revision", "comment": "We thank reviewers for their comments, all of which provided helpful suggestions to improve the paper. We agree with the comments and did our best to incorporate their feedback. Before addressing each reviewer\u2019s comments individually, we summarize the main improvements of the paper during this author response phase:\n\n\n### Real world setting\nTo some of the reviewers, the choices of problem size M, N, S, and iterations K seemed arbitrary, leading them to wish for a real-world setting. In the updated version of our paper, we have included a setting with real problem sizes from pilot-based multipath channel estimation in wireless communication based on Orthogonal Frequency Division Multiplexing. In this setting, it is particularly important for reconstruction algorithms to require few iterations and use fast transforms (i.e. FFT). Specifically, we focus on an LTE setting with N=256, M=100, S=8 using exponentially decaying power delay profiles. Our method, NA-ALISTA, also performs better than all other evaluated algorithms in this setting. As we make the compression ratio more difficult and set the number of pilots to M=75, the gap by which competitors are outperformed widens.\n### Running until convergence\nReviewers have noted that in some of our evaluated settings, setting the number of iterations K to 16 is not sufficient to see which algorithm reaches the best reconstruction. We agree: in the updated version we have significantly increased the number of iterations for which we show sparse reconstruction results up to K=26. \n### Wall Clock time\nMultiple reviewers noted that our claim that we do not significantly increase computational cost is not sufficiently backed up. The reviewers wished to see a wall clock time comparison of the algorithms. We definitely agree and have included such an analysis in the appendix of the updated version of our paper, corroborating our claims.\n\n\nWe kindly ask the reviewers to have a look at the updated paper, hoping that we could address their questions and uncertainties & incorporate their feedback to improve our paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "q_S44KLQ_Aa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1614/Authors|ICLR.cc/2021/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857733, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Comment"}}}, {"id": "NoXShSbFsSr", "original": null, "number": 4, "cdate": 1604140241051, "ddate": null, "tcdate": 1604140241051, "tmdate": 1605024401346, "tddate": null, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "invitation": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review", "content": {"title": " A reasonable improvement over LISTA with somewhat insufficient experiments.", "review": "This paper adds LSTM to adjust the step size and threshold for LISTA, an optimization algorithm of sparse regression problems.\n\nUsing LSTM to dynamically determine those optimization parameters seems to be reasonable.\nStill, I found the experiments to be a bit insufficient to convince me of its improvement over LISTA or even most vanilla solvers: Figure 3,4,5 all showing MSE to some iterations when only one optimizer reaches its optimality, or even not one reaching optimality. I also think the other algorithms, with the step size tuned better, could be reaching faster convergence, however, the paper fails to mention how the parameters are tuned for those methods. On top of this, no mentions of the actual running time of the optimizer as LSTM could be really slow.\n\nsmall questions:\n\nWhat is the point of the study showed in figure 1&2 as by definition,  as $W$ is already supposed to be an isometric mapping?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1614/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1614/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neurally Augmented ALISTA", "authorids": ["~Freya_Behrens1", "~Jonathan_Sauder2", "~Peter_Jung2"], "authors": ["Freya Behrens", "Jonathan Sauder", "Peter Jung"], "keywords": ["compressed sensing", "sparse reconstruction", "unrolled algorithms", "learned ISTA"], "abstract": " It is well-established that many iterative sparse reconstruction algorithms can be unrolled to yield a learnable neural network for improved empirical performance. A prime example is learned ISTA (LISTA) where weights, step sizes and thresholds are learned from training data. Recently, Analytic LISTA (ALISTA) has been introduced, combining the strong empirical performance of a fully learned approach like LISTA, while retaining theoretical guarantees of classical compressed sensing algorithms and significantly reducing the number of parameters to learn. However, these parameters are trained to work in expectation, often leading to suboptimal reconstruction of individual targets.  In this work we therefore introduce Neurally Augmented ALISTA, in which an LSTM network is used to compute step sizes and thresholds individually for each target vector during reconstruction. This adaptive approach is theoretically motivated by revisiting the recovery guarantees of ALISTA. We show that our approach further improves empirical performance in sparse reconstruction, in particular outperforming existing algorithms by an increasing margin as the compression ratio becomes more challenging.", "one-sentence_summary": "We introduce Neurally Augmented ALISTA, extending ALISTA to compute adaptive parameters to achieve improved recovery of individual sparse target vectors.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "behrens|neurally_augmented_alista", "supplementary_material": "/attachment/e083ae19f13be7615b29dbc524a24b242d9f4fa2.zip", "pdf": "/pdf/cc9fce3481556687c91b3f8f9eeaa58965f7089f.pdf", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nbehrens2021neurally,\ntitle={Neurally Augmented {\\{}ALISTA{\\}}},\nauthor={Freya Behrens and Jonathan Sauder and Peter Jung},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=q_S44KLQ_Aa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "q_S44KLQ_Aa", "replyto": "q_S44KLQ_Aa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538114704, "tmdate": 1606915771745, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1614/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1614/-/Official_Review"}}}], "count": 14}