{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396546782, "tcdate": 1486396546782, "number": 1, "id": "HysthGL_g", "invitation": "ICLR.cc/2017/conference/-/paper372/acceptance", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "While the reviewers find the core idea intriguing, the method needs a clearer explanation and a more thorough comparison to related work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396547289, "id": "ICLR.cc/2017/conference/-/paper372/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396547289}}}, {"tddate": null, "tmdate": 1484578007588, "tcdate": 1484578007588, "number": 5, "id": "rkl1pU58x", "invitation": "ICLR.cc/2017/conference/-/paper372/public/comment", "forum": "HJSCGD9ex", "replyto": "S1p6whZEg", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "writers": ["~Shyam_Upadhyay1"], "content": {"title": "Reply3", "comment": "Thanks for the comments and suggestions. \nWe will simplify the model description per your suggestion. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287603066, "id": "ICLR.cc/2017/conference/-/paper372/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJSCGD9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper372/reviewers", "ICLR.cc/2017/conference/paper372/areachairs"], "cdate": 1485287603066}}}, {"tddate": null, "tmdate": 1484577853798, "tcdate": 1484577853798, "number": 4, "id": "S18ShI5Ig", "invitation": "ICLR.cc/2017/conference/-/paper372/public/comment", "forum": "HJSCGD9ex", "replyto": "BJzLpS74g", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "writers": ["~Shyam_Upadhyay1"], "content": {"title": "Reply2", "comment": "Thank you for your comments. We will improve the writing and experimental comparison in the next version.\n\nWe would like to note that MONO is exposed to same amount of training data, because we ensure that the number of words in the context window are same (by using a bigger context parameter for MONO). Indeed, MONO is unable to process multilingual data, but that is a limitation of the modeling itself. We will try the pseudo monolingual experiment you suggested. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287603066, "id": "ICLR.cc/2017/conference/-/paper372/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJSCGD9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper372/reviewers", "ICLR.cc/2017/conference/paper372/areachairs"], "cdate": 1485287603066}}}, {"tddate": null, "tmdate": 1484577636090, "tcdate": 1484577636090, "number": 3, "id": "rkhDsI5Ll", "invitation": "ICLR.cc/2017/conference/-/paper372/public/comment", "forum": "HJSCGD9ex", "replyto": "B1UhSkIVe", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "writers": ["~Shyam_Upadhyay1"], "content": {"title": "Reply1", "comment": "Thank you for you comments. \nWe will work to make the model description easier to understand. We will also evaluate our embeddings on other downstream tasks  to improve the comparison with existing works."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287603066, "id": "ICLR.cc/2017/conference/-/paper372/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJSCGD9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper372/reviewers", "ICLR.cc/2017/conference/paper372/areachairs"], "cdate": 1485287603066}}}, {"tddate": null, "tmdate": 1482188206526, "tcdate": 1482188206526, "number": 3, "id": "B1UhSkIVe", "invitation": "ICLR.cc/2017/conference/-/paper372/official/review", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/conference/paper372/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper372/AnonReviewer2"], "content": {"title": "Interesting idea, execution needs more work", "rating": "4: Ok but not good enough - rejection", "review": "This paper discusses multi-sense embedddings and proposes learning those by using aligned text across languages. Further, the paper suggests that adding more languages helps improve word sense disambiguation (as some ambiguities can be carried across language pairs). While this idea in itself isn't new, the authors propose a particular setup for learning multi-sense embeddings by exploiting multilingual data.\n\nBroadly this is fine, but unfortunately the paper then falls short in a number of ways. For one, the model section is unnecessarily convoluted for what is a nice idea that could be described in a far more concise fashion. Next (and more importantly), comparison with other work is lacking to such an extent that it is impossible to evaluate the merits of the proposed model in an objective fashion. \n\nThis paper could be a lot stronger if the learned embeddings were evaluated in downstream tasks and evaluated against other published methods. In the current version there is too little of this, leaving us with mostly relative results between model variants and t-SNE plots that don't really add anything to the story.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512606554, "id": "ICLR.cc/2017/conference/-/paper372/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper372/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper372/AnonReviewer1", "ICLR.cc/2017/conference/paper372/AnonReviewer3", "ICLR.cc/2017/conference/paper372/AnonReviewer2"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512606554}}}, {"tddate": null, "tmdate": 1482018242549, "tcdate": 1482018122373, "number": 2, "id": "BJzLpS74g", "invitation": "ICLR.cc/2017/conference/-/paper372/official/review", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/conference/paper372/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper372/AnonReviewer3"], "content": {"title": "needs better comparison", "rating": "4: Ok but not good enough - rejection", "review": "this work aims to address representation of multi-sense words by exploiting multilingual context. Experiments on word sense induction and word similarity in context show that the proposed solution improves over the baseline.\n\nFrom a computational linguistics perspective, the fact that languages less similar to English help more is intriguing. I see following problem with this work:\n- the paper is hard to follow and hard to see what's new compared to the baseline model [1]. A paragraph of discussion should clearly compare and contrast with that work.\n\n- the proposed model is a slight variation of the previous work [1] thus the experimental setup should be designed in a way so that we compare which part helps improvement and how much. thus MONO has not been exposed the same training data and we can't be sure that the proposed model is better because MONO does not observe the data or lacks the computational power. I suggest following baseline: turning multilingual data to monolingual one using the alignment, then train the baseline model[1] on this pseudo monolingual data.\n\n- the paper provides good benchmarks for intrinsic evaluation but the message could be conveyed more strongly if we see improvement in a downstream task.\n\n[1] https://arxiv.org/pdf/1502.07257v2.pdf", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512606554, "id": "ICLR.cc/2017/conference/-/paper372/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper372/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper372/AnonReviewer1", "ICLR.cc/2017/conference/paper372/AnonReviewer3", "ICLR.cc/2017/conference/paper372/AnonReviewer2"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512606554}}}, {"tddate": null, "tmdate": 1481917254662, "tcdate": 1481914309401, "number": 1, "id": "S1p6whZEg", "invitation": "ICLR.cc/2017/conference/-/paper372/official/review", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/conference/paper372/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper372/AnonReviewer1"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the authors propose a Bayesian variant of the skipgram model to learn word embeddings. There are two important variant compared to the original model. First, aligned sentences from multiple languages are used to train the model. Therefore, the context words of a given target word can be either from the same sentence, or from an aligned sentence in a different language. This allows to learn multilingual embedding. The second difference is that each word is represented by multiple vectors, one for each of its different senses. A latent variable z models which sense should be used, given the context.\n\nOverall, I believe that the idea of using a probabilistic model to capture polysemy is an interesting idea. The model introduced in this paper is a nice generalization of the skipgram model in that direction. However, I found the paper a bit hard to follow. The formulation might probably be simplified (e.g. why not consider a target word w and a context c, where c is either in the source or target language. Since all factors are independent, this should not change the model much, and would make the presentation easier). The performance of all models reported in Table 2 & 3 seem pretty low.\n\nOverall, I like the main idea of the paper, which is to represent word senses by latent variables in a probabilistic model. I feel that the method could be presented more clearly, which would make the paper much stronger. I also have some concerns regarding the experimental results.\n\nPros:\nInteresting extension of skipgram to capture polysemy.\nCons:\nThe paper is not clearly written.\nResults reported in the paper seems pretty low.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512606554, "id": "ICLR.cc/2017/conference/-/paper372/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper372/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper372/AnonReviewer1", "ICLR.cc/2017/conference/paper372/AnonReviewer3", "ICLR.cc/2017/conference/paper372/AnonReviewer2"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512606554}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480999398667, "tcdate": 1478288077521, "number": 372, "id": "HJSCGD9ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HJSCGD9ex", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "content": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480999205324, "tcdate": 1480999091861, "number": 2, "id": "Bk32lamQl", "invitation": "ICLR.cc/2017/conference/-/paper372/public/comment", "forum": "HJSCGD9ex", "replyto": "Bk_nFV0zl", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "writers": ["~Shyam_Upadhyay1"], "content": {"title": "Reply to polysemy and morphology in foreign language", "comment": "We have made changes to correct the minor issues in the new revision.\n\n1) Morphology and Foreign Language Polysemy\n  It should be possible to extend our model to incorporate morphology,although we did not attempt it. For bilingual/multilingual training, to handle\n  polysemy in both english and foreign language, we can have a separate dirichlet process (with different parameters) on the foreign language side similar to english.\n\n2) Missing Ru and Es in Table 2? \nNote that the datasets in table 2 and table 3 are different (see caption for tab 3), because in table 3 we wanted to control for change in domain. In our preliminary experiments the corpora used in Table 3 did signficantly worse on the WSI task (avg. ari < 0.45 for each of the 4 languages) than those used for table 2. Indeed, even after pairing languages in Table 3, the avg ARI is still far from Table 2. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287603066, "id": "ICLR.cc/2017/conference/-/paper372/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJSCGD9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper372/reviewers", "ICLR.cc/2017/conference/paper372/areachairs"], "cdate": 1485287603066}}}, {"tddate": null, "tmdate": 1480998968833, "tcdate": 1480998968826, "number": 1, "id": "HyZrgpm7g", "invitation": "ICLR.cc/2017/conference/-/paper372/public/comment", "forum": "HJSCGD9ex", "replyto": "By5Op2JQg", "signatures": ["~Shyam_Upadhyay1"], "readers": ["everyone"], "writers": ["~Shyam_Upadhyay1"], "content": {"title": "Reply to Model Clarification", "comment": "Your interpretation is correct. We will make changes to improve the presentation in the model section.\n\n\nIs the probability distribution P(y | x_e, x_f) is defined as a probability over all the word of the vocabulary, or as the probability of presence (or absence) of the word y given x_e and x_f?\nWe define the probability as over all words in the vocabulary, and compute it using hierarchical softmax.\n\nIn practice, I believe that the word x_f from the \"foreign\" language cannot be aligned with the context word y_e in English.Why not consider a factor Psi between aligned words (to enforce that aligned words have similar representation)?\nWe are not aligning the pivot word x_f (which aligns to english pivot x_e) in the foreign language to the context word y_e in english, but only  predicting y_e using x_f. In principle, we can also enforce the aligned words to have similar representation, using a different factorization than what we presented. \n\nIs polysemy only considered on the English side?\nYes, we only model the polysemy on the english side.\n\nWhat is the difference between the one-sided and full models?\nThe one-sided model does not contain the Psi(y^e, x^f) term which tries to predict the english context using the foreign aligned word, as discussed in Section 4. It is meant to show the value of the Psi(y^e, x^f) term when using the multilingual setting.\n\nWhat is the metric reported in tables 2/3?\nWe report adjusted rand index (see experimental setup Section 5)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287603066, "id": "ICLR.cc/2017/conference/-/paper372/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HJSCGD9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper372/reviewers", "ICLR.cc/2017/conference/paper372/areachairs"], "cdate": 1485287603066}}}, {"tddate": null, "tmdate": 1480737000456, "tcdate": 1480736114463, "number": 2, "id": "By5Op2JQg", "invitation": "ICLR.cc/2017/conference/-/paper372/pre-review/question", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/conference/paper372/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper372/AnonReviewer1"], "content": {"title": "Model clarification", "question": "I found the model section of the paper a bit hard to follow. If I understood correctly, the model is a Bayesian variant of the skipgram model, where the context words are from both the source and the aligned sentences. Word polysemy on the source side is captured by the latent variables z, indicating which vector representation to use for a given context (thus, p(y | z, x) \\propto exp(<u_y, v_x,z>)).\n\nIs the probability distribution P(y | x_e, x_f) is defined as a probability over all the word of the vocabulary, or as the probability of presence (or absence) of the word y given x_e and x_f?\n\nIn practice, I believe that the word x_f from the \"foreign\" language cannot be aligned with the context word y_e in English. Why not consider a factor Psi between aligned words (to enforce that aligned words have similar representation)?\n\nIs polysemy only considered on the English side?\n\nWhat is the difference between the one-sided and full models?\n\nWhat is the metric reported in tables 2/3?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959316340, "id": "ICLR.cc/2017/conference/-/paper372/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper372/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper372/AnonReviewer3", "ICLR.cc/2017/conference/paper372/AnonReviewer1"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959316340}}}, {"tddate": null, "tmdate": 1480636847925, "tcdate": 1480636847920, "number": 1, "id": "Bk_nFV0zl", "invitation": "ICLR.cc/2017/conference/-/paper372/pre-review/question", "forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "signatures": ["ICLR.cc/2017/conference/paper372/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper372/AnonReviewer3"], "content": {"title": "the polysemy & morphology in the foreign language", "question": "- you do not model the polysemy in the foreign language. how can the model be extended in that direction?\n\n- have you considered exploiting the morphology in foreign languages?\n\n- why is table 2 missing Russian and Spanish?\n\n\nMINOR issues:\n- dirichlet process reference has a problem\n- page 6 english is lowercase.\n- table 3 instead of using (1) you can use $\\Delta$"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "abstract": "Word embeddings, which represent a word as a point in a vector space, have become ubiquitous to several NLP tasks. A recent line of work uses bilingual (two languages) corpora to learn a different vector for each sense of a word, by exploiting crosslingual signals to aid sense identification. We present a multi-view Bayesian non-parametric algorithm which improves multi-sense word embeddings by (a) using multilingual (i.e., more than two languages) corpora to significantly improve sense embeddings beyond what one achieves with bilingual information, and (b) uses a principled approach to learn a variable number of senses per word, in a data-driven manner. Ours is the first approach with the ability to leverage multilingual corpora efficiently for multi-sense representation learning. Experiments show that multilingual training significantly improves performance over monolingual and bilingual training, by allowing us to combine different parallel corpora to leverage multilingual context. Multilingual training yields com- parable performance to a state of the art monolingual model trained on five times more training data.", "pdf": "/pdf/29fc6c264a518a2fcb19fae533526ac659dc7e90.pdf", "TL;DR": "Using multilingual context for learning multi-sense embeddings helps.", "paperhash": "upadhyay|beyond_bilingual_multisense_word_embeddings_using_multilingual_context", "keywords": ["Natural language processing"], "conflicts": ["microsoft.com", "virginia.edu", "illinois.edu", "stanford.edu"], "authors": ["Shyam Upadhyay", "Kai-Wei Chang", "James Zou", "Matt Taddy", "Adam Kalai"], "authorids": ["upadhya3@illinois.edu", "kwchang@virginia.edu", "jamesz@stanford.edu", "taddy@microsoft.com", "adum@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959316340, "id": "ICLR.cc/2017/conference/-/paper372/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper372/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper372/AnonReviewer3", "ICLR.cc/2017/conference/paper372/AnonReviewer1"], "reply": {"forum": "HJSCGD9ex", "replyto": "HJSCGD9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper372/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959316340}}}], "count": 12}