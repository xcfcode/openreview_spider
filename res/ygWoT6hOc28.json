{"notes": [{"id": "ygWoT6hOc28", "original": "8GiATu9yQ59", "number": 2051, "cdate": 1601308226002, "ddate": null, "tcdate": 1601308226002, "tmdate": 1614985736098, "tddate": null, "forum": "ygWoT6hOc28", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IqlwFAaATe0", "original": null, "number": 1, "cdate": 1610040402169, "ddate": null, "tcdate": 1610040402169, "tmdate": 1610473998269, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents a useful contribution to the growing literature on uncertainty estimation with deep learning. The review process has significantly helped with strengthening this paper, specifically with the concerns about novelty and sufficient comparisons to existing work. I hope you will continue to improve this work for submission to a future venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040402156, "tmdate": 1610473998252, "id": "ICLR.cc/2021/Conference/Paper2051/-/Decision"}}}, {"id": "awTxN3m2fOi", "original": null, "number": 3, "cdate": 1603857271881, "ddate": null, "tcdate": 1603857271881, "tmdate": 1606819282944, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review", "content": {"title": "A simple extension of Prior networks models to regression. ", "review": "This paper extends Prior networks models, previously introduced for classification, to regression problems.  Prior networks are neural networks whose main target is to \"modelling uncertainty in classification tasks by emulating an ensemble using a single model\".  Standard Prior networks models output the parameters of a Dirichlet probability distribution. This Dirichlet probability distribution then defines a distribution over categorical probability distributions over the different classes. This hierarchical approach allows to better capture uncertainty. The presented approach extends this framework to regression tasks. So, instead of returning the parameters of a Dirichlet distribution, it returns the parameters of a Normal-Wishart distribution, which then defines a probability distribution over Normal distributions, and, in turn, each Normal distribution defines a probability distribution over the value of the target variable.  \n\n\nPros:\n* The presented approach is sound and addresses a relevant problem, which is modelling uncertainty for regression problems. \n* A method for distilling an ensemble model into a single model while maintaining accuracy is also proposed. \n* The proposed approach does not incur in computational and memory overheads like standard deep ensembles. \n* This work properly approaches technical difficulties (such as employing numerical stable precision parametrizations of the Normal-Wishart distribution) that arise in this kind of problems.\n\nCons: \n* The presented approach does not introduce any novel idea or insight. It's a relatively simple extension of a previously published method. \n* The empirical results do not show a clear advantage of the presented approach wrt previously published proposals. \n* The advantage of having a small computational and memory overhead is not properly evaluated with other proposals which also have a small  computational and memory overhead [1] (although this proposal has not been defined for regression problems, the adaptation to regression is as simple as the adaptation of the DeepEnsembles models employed in this work). \n\n\nI can not recommend the acceptation of this paper because I find the originality of the work quite limited. Although the extension of prior networks to regression task is mot really straightforward because of technical issues related to the problem of learning the parameters of a Normal-Wishart distribution. The general strategy to do that exactly matches the previous steps employed when introducing prior networks.  In consequence, this work does not provide any new relevant insight into the problem of modelling uncertainty and learning models with well-calibrated predictions. \n\n\nMinor comments:\n- Eq (14): T parameter is not defined. Temperature? \t\n- Typo at the end of Page 5: [-25,20] --> [-25,-20]\n- ENSM is defined after Table 1. \n- Fix the following reference:\nAndrey Malinin and Mark JF Gales. Reverse kl-divergence training of prior networks: Improved uncertainty and adversarial robustness. 2019. \n\nPost-rebuttal:  I thank  the authors' effort for the improvement of the manuscript following the comments of the different reviewers. I think the overall quality of the paper has really improved. But, after many thoughts, I still think there is a limited novelty in this paper. I have increased my score to 5. But I can not recommend this paper for publication. \n\n  adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1]  Wen, Y., Tran, D., & Ba, J. (2020). BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. arXiv preprint arXiv:2002.06715.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105127, "tmdate": 1606915773214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2051/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review"}}}, {"id": "_GrpdkEvnoU", "original": null, "number": 4, "cdate": 1603875090049, "ddate": null, "tcdate": 1603875090049, "tmdate": 1606743456069, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "Prior Networks (Malinin & Gales, 2018) use Dirichlet prior over categorical predictive distributions to distill ensembles for classification tasks. This paper extends Prior Networks to the regression setting by using a Normal-Wishart prior in order to attempt to match the predictive diversity. The authors define the model and loss terms including analytical derivation and evaluate their proposed approach with synthetic data, UCI datasets and monocular depth estimation. \n\n_Strengths_:\n- The paper is well-written and clearly structured.\n- Most design choices are justified.\n- Simple idea (in a good way!) which seemed to work well, shown by the evaluation.\n\n_Weaknesses_:\n- Most of the work seems to be heavily based on Prior Networks (Malinin & Gales, 2018). Even Section 2.1 seems to be exactly like the Subsection in the paper about Prior Networks. This paper mainly focuses on an extension to the regression task. Therefore, the contribution / novelty of this paper is incremental. However, I still think the authors did a good job to present a general distillation method for regression task. Therefore, I would consider the novelty a minor weakness.\n- I am on the fence about specifying the OOD dataset for learning with the loss in Eq. 8. I believe it is difficult to decide what kind of model to use for generating the OOD dataset, thus, the model choice can lead to large differences in performance. This is not really discussed. Further, the models trained have more data available for training, I believe it is not quite fair to compare against models which only have been trained on in-domain-data.\n- There are no comparisons to other approaches for distillation of regression tasks. I understand, that this paper wants to show a viable general approach for regression distillation, however, this work is not the first one to do so and therefore should consider existing work.\n\n_Overall assessment_: For me, this paper is borderline. The weaknesses, especially the OOD dataset used for training and the lack of comparisons in the evaluation are concerns. However, I like the idea and the execution so therefore, I would recommend a weak accept (6).\n\n_Detailed comments and questions_:\n- OOD data: I have seen that you have an ablation for the degree of regularization on the OOD dataset. However, what about different OOD data? Why choose KITTY and not a different dataset? Were there any large difference in performance?\n- Table 3: I notice that NLL performance of distilled models are better than the actual ensemble, how can this be?\n- OOD detection for monocular depth estimation: Did you also trained the comparing models with the OOD data, e.g. DD?\n- Comparing models: Have you consider comparing your model to other ones, e.g. [1, 2]? This could improve your paper and approach to show that it also consider existing work on regression distillation.\n\n_Post-rebuttal_:\nI really appreciate the authors adding baseline models to the paper and missing citations. I do think this improves the overall paper by a lot. As mentioned already in my paper, I do believe this is a nice idea and executed well, even though novelty might be limited. I am keeping my score and recommending an accept.\n\n[1] Chen, G., Choi, W., Yu, X., Han, T. and Chandraker, M., 2017. Learning efficient object detection models with knowledge distillation. In Advances in Neural Information Processing Systems (pp. 742-751).\n[2] Saputra, M.R.U., de Gusmao, P.P., Almalioglu, Y., Markham, A. and Trigoni, N., 2019. Distilling knowledge from a deep pose regressor network. In Proceedings of the IEEE International Conference on Computer Vision (pp. 263-272).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105127, "tmdate": 1606915773214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2051/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review"}}}, {"id": "uJpIqmq2Bp5", "original": null, "number": 19, "cdate": 1606231368674, "ddate": null, "tcdate": 1606231368674, "tmdate": 1606231368674, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Final revision", "comment": "We added a final revision to fix a mistake in eq. 13 and fix a few typos."}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "yG1VHxH7Mzi", "original": null, "number": 18, "cdate": 1606169065477, "ddate": null, "tcdate": 1606169065477, "tmdate": 1606169327987, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Updated Manuscript Text", "comment": "Dear Reviewers,\n\nAs per the reviewers' and public comments, we have provided additional comparisons to Deep Evidential Regression and Mixture-Density Distillation, as we have described in our previous post. Now we have made changes to the text to reflect the reviewers' other comments. We hope that these changes sufficiently address all of your concerns.\n\nWe describe the changes section by section:\n\nIntroduction (minor changes)\n1. Modified second paragraph to mention evidential approaches\n2. Modified final paragraph to correctly point to roots of idea\n\nRegression Prior Networks (lots of changes)\n1. Sections styles changed for extra space\n2. Added clarifications into discussions of RKL loss function, including role of OOD data, and effect of beta.\n3. Added a discussion of EnD and MD-EnD to ensemble distribution distillation section\n4. Added a final \"Related work\" section, which discusses Deep Evidential Regression and efficient ensemble methods (batch ensemble.\n\nSynthetic Experiment\n1. X-axis range in images widened, so that it is clear from figure C that knowledge uncertainty rises sharply\n\nUCI Experiments (minor tweaks)\n1. Section reworked, experimental protocol clarified.\n2. Added reference to appendix which C3 which discusses PRR\n\nMonocular Depth Estimation (SIGNIFICANTLY reworked for clarity)\n1. Clarified the depth estimation performance metrics and made a forward reference to appendix D1, which they are described.\n2. Added experiments on DER and MD-EnD to both tables 3 and 4\n3. Restructured discussion of OOD experiments. Behaviours explains.\n4. Added forward reference to an additional set of OOD detection experiments in the appendix.\n5. Added forward reference to examples of IN/OOD data so that it is easy to see *exactly* what the models are trying to discriminate between.\n6. Expanded discussion about the difficulty of choosing appropriate OOD data, which highlights that EnD^2 is the superior approach, as it doesn't suffer from this difficulty.\n\nConclusion (Minor clarification at the end)\n\n\n\nWe thank all the reviewers for their effort!\n\nSincerely,\nAuthors\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "wWmPr0CWGN-", "original": null, "number": 16, "cdate": 1606164117886, "ddate": null, "tcdate": 1606164117886, "tmdate": 1606164117886, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "aLiVwZ4XDHC", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Thank you for the stimulating discussion!", "comment": "We've updated the paper and are about to make a post describing all the updates. We've added a discussion about BatchEnsembles and the Multi-head distillation method.\n\nWe appreciate the effort you put into this discussion!\n\nMany thanks,\nAuthors\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "aLiVwZ4XDHC", "original": null, "number": 14, "cdate": 1606128019426, "ddate": null, "tcdate": 1606128019426, "tmdate": 1606128019426, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "V-QYbIwm34", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Response to R4", "comment": "Thanks again for your nice replay!\n\nI think my concerns, in terms of comparison, with other related works are already addressed. The comparison with multi output heads is also reasonable and enough for me. Even though, I think it would be worth to also include the discussion of other methods like BatchEnsembles.\n\nRegarding novelty and relevance of the work, I still have the concerns I rose in my original review. But, I promise I will give new thoughts in the light of other reviewers and all your responses. \n\nThanks for the fruitful discussion. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "EmqRkAeB_sh", "original": null, "number": 13, "cdate": 1605915880828, "ddate": null, "tcdate": 1605915880828, "tmdate": 1605915880828, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ZoIDbGzIXZw", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Response to reviewer 2", "comment": "Thanks for you for your comments!\n\nWe explored the effect of gamma in the depth estimation setting in the appendix, table 11. The effect of beta is primarily to make the the expected (under Normal-Wishart) negative-log-likelihood be a tight bound to the NLL of the expected (vs expected NLL). High beta makes the bound tight and the training to be more accurate. We will add this this to the discussion and make it more clear.\n\nRegarding a discussion of Dirichlet Prior Networks - unfortunately, the space is rather limited to be able to discuss everything in detail. We will try to improve the discussion of RPNs such that it is more self-contained. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "yQTOTbRfj0t", "original": null, "number": 12, "cdate": 1605915380191, "ddate": null, "tcdate": 1605915380191, "tmdate": 1605915642979, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "New results in manuscript", "comment": "Dear All,\n\nWe tried to provide new results in the open review system, but it turns out that it is very poor (and inconsistent across write/preview and what it actually posts) at representing tables. Thus, we have updated TABLE 3 and TABLE 4 in the manuscript, where we have added results for:\n\nDeep Evidential Regression (ArXiv 2019 version) (will only be displayed during rebuttal)\n\nDeep evidential Regression (NeurIPS 2020 pre-proceedings version) [2] \n\nMixture Density Distillation (only NYU Depth V2 so far, KITTI still training...) [3,4]\n\nNOTE - we are still in the process of updating the text. This update is purely intended to demonstrate updated results.\n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" ArXiv. 2019 (version 1)\n\n[2] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[3] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al). \n\n[4] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech."}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "V-QYbIwm34", "original": null, "number": 10, "cdate": 1605909415191, "ddate": null, "tcdate": 1605909415191, "tmdate": 1605915002954, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "QPCxH9YJxku", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Response to R4", "comment": "We are very happy to provide a discussion of various alternative approaches to making ensembles computationally cheaper. Indeed, we\u2019ve found 2 papers on a similar approach to distilling an ensemble into a single model [1,2] by having multiple output heads, where each head is meant to replicate the behaviour of a particular ensemble member. We believe that this is as close a baseline as we can make - it is almost identical in compute to EnD^2, it's also a distillation approach, and it also attempts to preserve ensemble diversity. We have provided these results  in TABLE 3 and TABLE 4 of the updated manuscript. Generally, this works a little better for predictive quality than EnD^2, and worse for OOD detection. Results on Kitti for MDD are not ready yet, but are being calculated.  Do you find these results sufficient? \n\nRegarding BatchEnsembles - we\u2019ve had a closer look, and we currently don\u2019t actually understand how it is cheaper *at run time*. While it is true that a BatchEnsemble model has about as many parameters as a single model *on disk*, at *run time* it trades of increased use of GPU memory (batch is replicated) for efficient use of said GPU. Thus, it may be faster than sequential evaluation of an explicit ensemble, but it certainty is not more memory efficient at run time. We will certainly cite, mention and discuss this range of works. However, implementing it is non-trivial -the libraries you\u2019ve sent us are in Tensorflow, not Pytorch, so we cannot directly carry them over. \n\nIf you insist, we CAN promise to implement BatchEnsembles and add this into the camera ready paper (if this paper is accepted). We would definitely keep this promise, as it would be quite embarrassing to make it publicly and then break it.  \n\n[1] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[2] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "wP-LoPyTYVB", "original": null, "number": 11, "cdate": 1605913792423, "ddate": null, "tcdate": 1605913792423, "tmdate": 1605914930108, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "UjYwY1rS0MR", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Experimental results", "comment": "We initially tried to add table here, but unfortunately it turns out that the system ignores formatting and just dumps numbers. Results are presented in TABLE 3 and TABLE 4 of updated manuscript. \n\nFor all versions of DER (2019 ArXiv and 2020 Neurips) we used a weight of 0.1 on the evidence regulariser. For DER 2020 we checked the implementation of the student NLL against the pytorch version and made sure that everything is correctly parameterised.\n\nThe results show a few things. \n\nFirstly, the old version of DER (ArXiv 2019) doesn't work well, both in terms of predictive performance, and in terms of uncertainty estimation. Which expected, due to the error on the loss.  \n\nSecondly, the new version of DER (NeurIPS 2020 preproceedings) works much better. In terms of predictive performance it is comparable to a single probabilistic DenseDepth model , though still with a minor degradation. In terms of OOD detection performance of models trained on NYU- it  yields rather competitive performance, marginally worse than the ensemble, and outperformed by EnD^2.\n\nThirdly, on KITTI OOD detection the LSUN OOD data is OOD not only because it is indoors, but also because it represents images which are very close to the camera, relative to images seen in Kitti, which features a range of depths. Here, all models, except RPN + RKL, interpret the OOD data as being in-domain using measures of total uncertainty. Using measures of ensemble diversity (knowledge uncertainty), ensembles, RPN-RKL and EnD^2 are able to detect OOD images successfully. Notably, DER does not seem to be able to. The reason for this is that the evidence regulariser biases the DER model tol yield high evidence in regions of low absolute error, and low evidence in regions of high absolute error. As a result, regions which are closer (bottom half of kitty images) always have higher evidence. As LSUN OOD is very close to the camera, the DER model yields high evidence. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "UjYwY1rS0MR", "original": null, "number": 9, "cdate": 1605908064620, "ddate": null, "tcdate": 1605908064620, "tmdate": 1605908064620, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "v2KrN3U9lHY", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Response to A. Amini", "comment": "Thanks for your comment! \n\nRegarding Evidential Deep Learning, particularly [2] - we think it is a rather elegant alternative interpretation for uncertainty estimation, rooted in Dempster-Schafer Theory of evidence, which yields a model which is structurally identical to a Dirichlet Prior Network. However, we are sceptical of the principal claim that this is a reliable single-model uncertainty estimation approach which doesn\u2019t require OOD data or indeed any other approach to enforcing a particular behaviour for OOD inputs which has an understandable mechanism of action. However, that is only our opinion - a rigorous large-scale validation is necessary and clearly would be a useful future direction of investigation. \n\nRegarding your paper (congrats on getting into NeurIPS!). We see our work as being very much an extension and verification of the ideas proposed in [3]. We became aware of your work around the same time as we began ours, however, to the best of our knowledge (until your comment) it was a submission at ICLR2020. Furthermore, upon examination at the time, we determined that the loss function (expectation of square error given samples from the Normal-inverse-Gamma) had an error in the derivation (equations 7-9, derivation 7.1.2 eq. 22-23 in the appendix). As a result, we had no grounds on which to believe in the validity of the results. Looking at the ArXiv submission now - it has not been updated within the last year and still contains the error . \n\nWith respect to the experimental setup - while we both use UCI (which is standard) and NYU Depth v2, our evaluations are quite different. We have used a standard architecture, provided detailed performance comparisons to baselines in depth estimation [4,6], and analysed the properties of several uncertainty measures via ROC-AUC against a range of OOD datasets.\n\nHowever, we have now found your new NeurIPS2020 version in the pre-proceedings (which were released after the ICLR2021 submission deadline), and we see that the mathematical error has now been fixed. In fact, an altogether different loss function (NLL of the student distribution) is used in addition to the evidence regularizer. The results are largely the same. \n\nWe\u2019ve implemented DER both as it is on ArXiv and as it is in the NeurIPS2020 pre-proceedings, and present the results in the next post. Unfortunately, a direct number-for-number comparison to your work is not possible, as there are no summary performance results for your model, and figure 4B contains a range for RMSE which is about 20 times smaller than what is reported in the depth estimation literature [4,6] (you\u2019ve probably scaled something differently). Furthermore, you use a different OOD dataset which seems to be very easy to separate out, as all models achieve a ROC-AUC of about 0.99. \n\nWe will add these results to our paper (omitting the old DER), and cite your work (and the original Evidential work), in our paper. We shall upload an updated version shortly. \n\n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018.\n[3] Uncertainty Estimation in Deep Learning with Application to Spoken Language Assessment, Malinin, 2019. PhD Thesis\n[4] High Quality Monocular Depth Estimation via Transfer Learning (Alhashim & Wonka, 2018)\n[5] HYDRA: PRESERVING ENSEMBLE DIVERSITY FOR MODEL DISTILLATION (Tran et al).\n[6] https://paperswithcode.com/sota/monocular-depth-estimation-on-nyu-depth-v2 ,\n[7] Ensemble Approaches for Uncertainty in Spoken Language Assessment, Wu et al, 2020, Interspeech."}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "QPCxH9YJxku", "original": null, "number": 8, "cdate": 1605574660880, "ddate": null, "tcdate": 1605574660880, "tmdate": 1605574660880, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "7Qx49nQWUAt", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Authors comments", "comment": "Thank you for reply! \n\nI agree deep ensembles can be considered as an upper bound. My point is that there are alternative methods like BatchEnsembles, Rank-1 BNNs, SNGP, etc. (this repo https://github.com/google/uncertainty-baselines contains the references and high-quality open source implementations of all of them) which could be easily adapted for regression and which have much lower time and memory complexity than ensemble methods. In my opinion,  at least one of them should be considered here as relevant baseline, because  only comparing wrt deep ensembles gives the impression that your method is the only available alternative that provides  a big reduction in time and memory complexity  wrt deep ensembles. I think it is fair to show (or at least discuss) that there are other approaches that can be employed here to strongly reduce the memory and time complexity of deep ensembles. \n\nThe lack of novelty of this paper, as acknowledged by other reviewers, puts much more pressure in the empirical evaluation. As I said before, there are well-established prior works which directly address the high memory and time complexity of deep ensembles that can be easily adapted to regression and which, in my opinion, should be considered by this work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "v2KrN3U9lHY", "original": null, "number": 1, "cdate": 1605555240399, "ddate": null, "tcdate": 1605555240399, "tmdate": 1605556929685, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Public_Comment", "content": {"title": "Some missing related work", "comment": "Thanks for submitting this work! In line with many of the reviewer comments regarding novelty, I was also wondering about the relation of the proposed contribution to published evidential deep learning (EDL) approaches [1,2]. Namely, published at NeurIPS this year, Deep Evidential Regression [1] also proposes learning a 1D Normal-Wishart distribution directly to infer representations of uncertainty specifically in the continuous regression domain as well (not classification). The proposed contribution presented here, like [1], also provides experimental results on nearly identical tasks from UCI and on monocular depth estimation. Also, note that deep evidential networks are structurally identical to prior networks (PN), with the only differences being in their respective objective functions (PN additionally require OOD data to train with, EDL does not). Given that a preprint of [1] appeared over a year ago and is now peer-reviewed/published, as well as the foundational work done in the classification domain [2] is over two years old now, I think it would be very helpful for the authors to cite these papers and and discuss their contributions relative to these works. \n\nI also hope this will help orient reviewers to the context for this submission and perhaps to some contributions that may have been missed. \n\n[1] Amini, Alexander, et al. \"Deep evidential regression.\" Advances in Neural Information Processing Systems. 2020.\n\n[2] Sensoy, Murat, et al. \"Evidential deep learning to quantify classification uncertainty.\" Advances in Neural Information Processing Systems. 2018."}, "signatures": ["~Alexander_Amini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Alexander_Amini1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors", "ICLR.cc/2021/Conference/Paper2051/Reviewers", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024966390, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Public_Comment"}}}, {"id": "ZoIDbGzIXZw", "original": null, "number": 7, "cdate": 1605524469238, "ddate": null, "tcdate": 1605524469238, "tmdate": 1605524469238, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "FRAmiZWGSy", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Response to Authors", "comment": "Thank you for your reply.\n\nWhat I was missing is a high-level description of prior networks. A background section on already existing prior networks for e.g. classification could be nice to include.\n\nRegarding the parameters, I was referring to the gamma parameter in Eq. (8) and the beta parameter in Eq. (11). "}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "on7otAuRWqC", "original": null, "number": 6, "cdate": 1605290889631, "ddate": null, "tcdate": 1605290889631, "tmdate": 1605290889631, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "6l-gyq0bIrO", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Comments of Reviewer 3", "comment": "Thank you for your detailed comments! Allow us to address them:\n\nSEC 2.2 - \n  A. Z is indeed a constant that doesn't depend on the parameters of the model. We will make this clear. \n  B. I'm afraid that this isn't the case. The OOD loss doesn't regularise the choice of beta. Rather the OOD loss is supposed to inform the model of regions beyond which it has no understanding of the data. Clearly, this requires one to decide on and choose an OOD dataset, which is non-trivial. \nC. p(y | mu, Lambda) represents a Normal distribution sampled from the Normal-Wishart. \n\nSEC 2.3 - Yes, this is what I mean - the dataset can be seen an empirical distribution to which we minimise KL, or equivalently, maximise likelihood. Phi represents the parameters of the model into which we are distribution-distilling the ensemble. \n\nSEC 3.  ENSM is the Deep Ensemble. Respectfully, the behaviour of the estimates of data uncertainty out of domain is not relevant - data uncertainty is only important in-domain. Indeed, we cannot give any guarantees on the behaviour of data uncertainty in the OOD region. What we actually care about is that estimates of *knowledge uncertainty* increase as we move further out of domain, which is the case (though perhaps not so easy to see from the picture). We will update the picture to make this clearer.\n\nSEC 4   \nA. We will make the experimental protocol clearer in this section. We will add the description of the Prediction Rejection Ratio in the appendix. It shows what part of the best possible error-detection performance our algorithm covers. \n\nB. UCI datasets are very common datasets for evaluation in related works, that\u2019s why we decided to add them despite their simplicity.\n\nC. To obtain train-OOD-data for RKL, we used factor analysis with increased noise and latent variance. This is a simple generative model. We trained it on in-domain data and added noise to the latent variables to generate out-of-domain examples for RKL. This generative model is simple and appropriate for table data, while GANs are not usual for table data. Also, UCI datasets have few examples and small feature spaces, therefore it could be hard to train GANs on them.\n\nD. For the evaluation of OOD-detection performance, we took parts of other UCI datasets as OOD data. We made sure that the OOD-data comes from different domains and feature distributions are different. We felt that this was the best we could, as, to the best of our knowledge, there has been no established research on OOD detection for tabular datasets.\n\nSEC 5\n\nPerformance metrics in table 3 are usual for Monocular Depth Estimation. They describe model performance from different sides and are usually shown in all papers on this topic. A good description of these metrics can be found in the original Monocular Depth Estimation paper \u201cDepth Map Prediction from a Single Image using a Multi-Scale Deep Network\u201d by Eigen et al., in section 4.3. \n\nDelta 1,2,3 shows a percent of predictions such that the maximum of two fractions: (a) between predictions and targets, (b) between targets and predictions is less than corresponding thresholds: 1.25, 1.25^2, and 1.25^3. Rel stands for absolute relative error and log10 for RMSE between logarithms of predictions and targets. These losses show different properties of the model: deltas help to understand confidence intervals of the model, Rel shows the ratio between prediction error and target, and log10 shows error in the log-space. \n\nWe will add the definition of these metrics to the text and  attempt to simplify table 3 as much as possible.\n\nWe fully understand where you are coming from regarding table 4 and figure 3. We will rewrite this section and make it more understandable, it was hard to fit everything into a given space.\n\nRegarding Table 4 and the behaviour of the NWPN (RPN+RKL) model - we hypothesise this is the result of the interaction between the in-domain and OOD training data. It was very hard to get the models to appropriately train. Likely because discrimination between ID/OOD is a very global task (global scene understanding), while depth estimation requires more local data. The tasks are therefore anti-correlated in training. In contrast, EnD$^2$ doesn't suffer from the same problems and only relies on ID training data.\n\nThus, what we aim to show is that: 1) EnD$^2$ can appropriately replicate and surpass the ensemble's OOD performance. 2) NWPN (RPN+RKL) can sometimes do near-perfect OOD detection, but isn't as reliable in this particular task with this choice of OOD data. \n\n\nIn Figure 3 the left image is from KITTI and the right image is from NYU datasets. Using these images we aim to show that error of a prediction correlates with increased uncertainty of the model. Additionally, we wanted to show how the uncertainties of the ensemble and EnD$^2$ model compare, and we can see that the EnD$^2$ model consistently yields higher uncertainties, as it over-estimates the support of the ensemble.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "FRAmiZWGSy", "original": null, "number": 5, "cdate": 1605287201818, "ddate": null, "tcdate": 1605287201818, "tmdate": 1605287227452, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "KMA7obppU9F", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Reviewer 2 Comments", "comment": "Thank you for your comments! We will now address your comments point-by-point:\n\n1. Regarding alternative Bayesian baselines:\n\nThe approach we use to generate ensembles  - Deep Ensembles [4], are already the current go-to SOTA Bayesian approach to uncertainty estimation [1,2,3]. Approaches like Dropout, while also capable of generating ensemble, are shown to be consistently inferior. Variational Inference is typically even worse and has never been successfully scaled to complex tasks such as Depth Estimation, to our knowledge.\n\nOur favoured proposed approach - Ensemble Distribution Distillation for regression, allows us to take a SOTA DeepEnsemble (which is the baseline relative to which we compare) and distill it into a single model, generally preserving most of the ensemble\u2019s gains. This allows us to replicate both the ensemble\u2019s predictive performance as well as uncertainty measures at the computational and memory cost of a single model. Thus, suffer a minor reduction in predictive quality (and no loss in the quality of uncertainty estimates) for an M-fold (where M is the ensemble size) reduction in computational and memory cost relative to the ensemble baseline. \n\n       [1] Can you trust your model\u2019s uncertainty? Evaluating predictive uncertainty under dataset shift.\n\n       [2] Pitfalls of in-domain uncertainty estimation and ensembling in deep learning.\n\n       [3] Deep ensembles: A loss landscape perspective.\n\n       [4] Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\n\n\n2. Regarding additional training parameters - Could you please be more specific, so that we could address your concerns in detail?\n\n3. Regarding the difficulty of understanding the paper - Are there particular changes you would like us to implement which you think would make this paper more accessible? "}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "7Qx49nQWUAt", "original": null, "number": 4, "cdate": 1605287006003, "ddate": null, "tcdate": 1605287006003, "tmdate": 1605287006003, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "awTxN3m2fOi", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Reviewer 4 Comments", "comment": "Thank you for your review! Please allow us to address your concerns:\n\n1. Regarding empirical results:\n\n Could you please elaborate what you would see as a clear advantage? In terms of inference-time compute and memory the M-fold (where M is the ensemble size) advantage over Ensembles is clear. In terms of predictive performance - we outperform single models, and get close to the ensemble. Replicating the ensemble\u2019s predictive performance completely is an upper bound. In terms of OOD Ensemble-Distribution Distilled  RPNs outperform the ensemble. If there some specific comparison you would like us to provide which would convince you?\n\n2. Regarding BatchEnsemble:\n\nBatchEnsembles are interesting, however an efficient implementation of BatchEnsembles is non-trivial and there is no available code in pytorch (The original work was done in Edward). A naive implementation would be as expensive during inference as DeepEnsembles, if not more so, as it may require a larger ensemble to reach the same performance. If you insist, we will explore this approach, but this will likely be infeasible within the time-frame of the rebuttal period. \n\nP.S. Thank you for finding the minor errors. We will fix them. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "DYGxsFPHho-", "original": null, "number": 3, "cdate": 1605286804795, "ddate": null, "tcdate": 1605286804795, "tmdate": 1605286804795, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "_GrpdkEvnoU", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Reply to Review 1 comments.", "comment": "Thank you for your review! Allow us to address your concerns on a point-by-point basis. \n\nREGARDING WEAKNESSES\n\n1. We agree with your concerns regarding the choice of OOD dataset. Defining an appropriate one for classification tasks is already non-trivial - doing so is even more challenging for regression. This is why we place greater emphasis on Ensemble Distribution Distillation - it does not require an OOD dataset and yields superior predictive performance relative to RKL-trained Regression Prior Networks.  \n\nWe will use the extra page to present a discussion regarding difficulties of using an OOD dataset, and will shortly upload an updated manuscript. \n\n2. With regards to regression distillation, we would like to point out that previous work has examined the distillation of a *single model  into a single model*. In our work we consider distillation of *an ensemble of probabilistic models into a single probabilistic model*. Limited prior work has examined this scenario, and it is difficult to provide a sensible baseline . We have attempted to do so through Ensemble Distillation (EnD), though it seems this approach also has its limitations. \n\nIt is, in general, not entirely clear whether combining an ensemble of probabilistic models is better done as an arithmetic or geometric mixture. A full analysis of ensembles of probabilistic regression models deserves an investigation of its own. Furthermore, to our knowledge, probabilistic ensemble distillation for regression has been a generally under-explored area. If you could point us to a more appropriate baseline, we would be happy to consider it!\n \t\nWe will add a detailed discussion of this issue into section 2.3 and upload an updated manuscript shortly. \n\nREGARDING DETAILED COMMENTS\n\n1 We were limited in the compute we had available for this project and decided to focus on the ablation study we did, rather than swapping out OOD datasets. In general, for Depth Estimation, we would like to place greater emphasis on RPNs trained through EnD$^2$, rather than RPNs trained via RKL on OOD datasets.\n\nIndeed, one of the conceptual reasons for not further exploring choice of OOD datasets for RPN+RKL is that we believe (and show) that RPNs+EnD$^2$ to be the superior approach. \n\nWe will clarify this point in an updated manuscript we will shortly upload. \n\n2. We believe this is a result of the fact that the EnD$^2$ will overestimate the support of the ensemble (as a natural consequence of ML training). As a result, it will be less over-confident. \n\n3. We didn\u2019t. To be clear - we intended our main comparison for Depth Estimation to be Ensembles vs  EnD$^2$ . Note that RPNs trained via RKL on OOD data in section 5 suffer degraded predictive performance. On the other hand, RPNs trained via  EnD$^2$ show better predictive performance (relative to EnD, Single models and RPN+RKL). \n\n4. : Thank you for pointing out this work. However, as previously stated, these papers consider the distillation of single model into single model, and thus cannot be used as a meaningful baselines. However we will cite them when discussing the nature of regression distillation and highlighting how our work is different.  \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "e8Fzb_fYVd", "original": null, "number": 2, "cdate": 1605286440475, "ddate": null, "tcdate": 1605286440475, "tmdate": 1605286440475, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment", "content": {"title": "Addressing concerns regarding novelty", "comment": "Dear Reviewers,\n\nAll of you have expressed concerns regarding the novelty and originality of our work. We would like to address this issue and explain why we think this merits a paper. \n\nIn our interactions with other researchers, and especially with industrial ML practitioners, we noticed that many people thought that the correct extension of Prior Networks to regression tasks would be to take a non-probabilistic regression model and place a Normal distribution over the target variable. As is clear from our work, this is not correct. Thus, one of the main motivations for this paper was to address this common misunderstanding and show that the correct way to extend Prior Networks and Ensemble Distribution Distillation to regression tasks.\n\nIn order to convey our message as clearly as possible we explicitly structured the paper around the parallel between Dirichlet and Normal-Wishart Prior Networks to make it absolutely self-evident what the correct approach is. In this regard we seem to have succeeded a little too well, as all of you note how the extension is straightforward and incremental. We would respectfully ask you to consider that this extension is not as evident to the majority of the ML community as we make it seem in this work. Notably, since the publication of the original paper on Dirichlet Prior Networks (Malinin and Gales, 2018), to our knowledge, Prior Networks have not been extended to regression, despite the popularity of the approach for classification. Thus, the value of our work is in extending a powerful uncertainty estimation approach for classification to regression, resolving a common misconception, and clearly presenting the mathematical basis for this extension. \n\nWe address your remaining concerns on a point-by-point basis and will shortly upload an updated manuscript.\n\nSincerely,\nAuthors\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ygWoT6hOc28", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2051/Authors|ICLR.cc/2021/Conference/Paper2051/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852820, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Comment"}}}, {"id": "KMA7obppU9F", "original": null, "number": 1, "cdate": 1603299731653, "ddate": null, "tcdate": 1603299731653, "tmdate": 1605024300393, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review", "content": {"title": "An nice paper based on incremental work", "review": "Summary of the Paper:\n\n        This paper introduces regression prior networks. These are models that aim at capture predictive uncertainty, both epistemic and aleatoric, in the context of regression problems. Regression prior networks can also be used to compress an ensemble of predictors into a single model while keeping the benefits of the ensemble. That is, better predictive performance and uncertainty estimates. The method is validated on several problems from the UCI repository and compared with ensemble methods.\n\nSpecific details:\n\n        I believe that this is a nice paper that illustrates an appealing method for uncertainty estimation in the context of neural networks. My main concern, however, is that it builds heavily on previous work. In particular, prior networks have already been proposed for classification and they have also been used to distill (compress) an ensemble. There is hence not much novelty here, only the extension to regression problems since, previously, only classification problems have been addressed. The use of prior networks for ensemble distillation is also not new. All this questions the novelty of the proposed approach.\n\n        The extension to regression seems to follow very closely the work already carried out for classification. The only difference is that a Normal Wishart distribution is used instead of a Dirichlet distribution.\n\n        The experiments carried out are extensive and consider different tasks involving prediction accuracy and out of distribution data detection. My main concern, however, is that no comparison is carried out with alternative methods to estimate prediction uncertainty such as those of Bayesian neural networks using variational inference or dropout. The authors should comment on the advantages of their method with respect to these techniques.\n\n        The method proposed is also complicated and has several training parameters. The authors give specific values for them, but it is not clear the motivation for them or the sensitivity to their values.\n\n        The paper is clearly written but heavily relies on previous work, making the reading difficult for someone who is not familiar with it. The paper is not self-contained.\n\n        Summing up I believe that this could be an interesting contribution for the conference, suffering from a reduced amount of novelty.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105127, "tmdate": 1606915773214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2051/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review"}}}, {"id": "6l-gyq0bIrO", "original": null, "number": 2, "cdate": 1603729530097, "ddate": null, "tcdate": 1603729530097, "tmdate": 1605024300334, "tddate": null, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "invitation": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review", "content": {"title": "Clarifications needed", "review": "This paper addresses interpretable uncertainty quantification for data driven models. In particular, the authors focus on a sub-class of methods known as Prior Networks and attempt to extend these methods to regression tasks as existing approaches address classification only. The author contribution is thus clearly stated and positioned w.r.t. prior arts and tackle a non-trivial issue.\n\nIn the classification setting, the Dirichlet distribution is pretty much the universal model for the parameters of multinomial distributions. For regression, i.e. continuous r.v., there is no such universal solution and the authors chose to focus on outputs that have a normal distribution. The parameters of this latter are assumed to be normal-Wishart. Although, the proposed method is de facto non-applicable to other types of distributions, it can be argued that this already covers a majority of situations. \n\nThe paper is rather well organized and seems technically sound. This said, a few mathematical details are missing and, most importantly, the experiments are not very convincing. These concerns, also with other minor remarks are detailed below, section by section.\n\nsec 2.2\n\nMaybe give the explicit definition of Z to clarify that is does not depend on network parameters.\nThe presence of the OOD loss term in (8) is a bit artificial as it boils down to regularizing because of the choice of beta. Is this choice systematic ?\nIn (9), how is p(y | mu, Lambda) computed ? Is it a T distribution ?\n\n2.3\n\n(12) lacks clarity : dataset is equal to an empirical distribution... Do you mean p hat is a sum of Dirac ?\nWhat does phi represent ?\n\n3\nThe acronym ENSM is not explained. I believe this corresponds to the deep ensemble. \nThe Prior Networks achieve a form of disambiguation but the quality of it is a bit disappointing compared to ENSM. In particular, data uncertainty raises quickly for out-of-domain inputs. \n\n4\n\nThe presentation of the experimental protocol in 4 lacks clarity thereby impairing the interpretation of the results. The definition of the unconventional performance criteria [(Malinin et al. 2020] must be recalled (at least in an appendix). \nIn addition, as honestly mentioned by the authors, these datasets may not offer sufficiently rich problems to provide interesting comparisons. Besides, the way that OOD data is generated does not seem to necessarily produce inputs that are not covered by the in-domain distribution. Perhaps, the authors could use a \"bad GAN\" to obtain such data points, I mean a GAN where the generator and the discriminator would co-operate instead of being adversaries. If it converges, the generator would produce synthetic inputs that are easy to discriminate, thus far from true inputs. \n\n5\nWhile the dataset used in this section is more challenging, the experiment description is confusing. Again, performance criteria are not sufficiently explained and the general message becomes cryptic. Table 3 is overly complicated, I think RMSE is fairly enough to depict regression performances. Moreover, the definition of some columns are missing.\nIn Table 4, the performances of the methods seem quite unstable. For example, NWPN works fairly well for a given dataset configuration for one knowledge uncertainty criterion but fails miserably using another criterion on the same data.\nOn Fig 3, from what dataset are these image coming from ? Why are these or that object presumably \"unknown\" to the model ?\nI think the whole section deserves some re-writing.\n\nFinal remark : there are a few English mistakes that should be wiped out. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2051/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2051/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regression Prior Networks", "authorids": ["~Andrey_Malinin1", "chervontsev@yandex-team.ru", "iv-provilkov@yandex-team.ru", "~Mark_Gales1"], "authors": ["Andrey Malinin", "Sergey Chervontsev", "Ivan Provilkov", "Mark Gales"], "keywords": ["uncertainty", "prior networks", "regression", "ensemble distribution distillation", "depth estimation."], "abstract": "Prior Networks are a recently developed class of models which yield interpretable measures of uncertainty and have been shown to outperform state-of-the-art ensemble approaches on a range of tasks. They can also be used to distill an ensemble of models via \\emph{Ensemble Distribution Distillation} (EnD$^2$), such that its accuracy, calibration and uncertainty estimates are retained within a single model. However, Prior Networks have so far been developed only for classification tasks. This work extends Prior Networks and EnD$^2$ to regression tasks by considering the Normal-Wishart distribution. The properties of Regression Prior Networks are demonstrated on synthetic data, selected UCI datasets and a monocular depth estimation task, where they yield performance competitive with ensemble approaches.", "one-sentence_summary": "Development of Prior Networks and Ensemble Distribution Distillation for Regression Tasks", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "malinin|regression_prior_networks", "pdf": "/pdf/d160e7d6173cdebf940ce18312f1b6e50cead309.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=zttNivMlGv", "_bibtex": "@misc{\nmalinin2021regression,\ntitle={Regression Prior Networks},\nauthor={Andrey Malinin and Sergey Chervontsev and Ivan Provilkov and Mark Gales},\nyear={2021},\nurl={https://openreview.net/forum?id=ygWoT6hOc28}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ygWoT6hOc28", "replyto": "ygWoT6hOc28", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2051/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105127, "tmdate": 1606915773214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2051/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2051/-/Official_Review"}}}], "count": 23}