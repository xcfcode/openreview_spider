{"notes": [{"id": "vsyJQYFKlD8", "original": "ERS2h13VcD", "number": 6, "cdate": 1615310248887, "ddate": null, "tcdate": 1615310248887, "tmdate": 1615313018186, "tddate": null, "forum": "vsyJQYFKlD8", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "Causal Inference Q-Network: Toward Resilient Reinforcement Learning", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper6/Authors"], "authors": ["Anonymous"], "keywords": ["Deep Reinforcement Learning", "Generative Reinforcement Learning", "Causal Inference", "Learning from Noisy Labels"], "TL;DR": "We propose a causal inference based generative DRL algorithm called causal inference Q-network (CIQ) with auxiliary labels (interferences) toward resilient learning.", "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may receive faulty observation by abrupt interferences such as black-out, frozen-screen, and adversarial perturbation. How to design a resilient DRL algorithm against these rare but mission-critical and safety-crucial scenarios is an important yet challenging task. In this paper, we consider a generative DRL framework training with an auxiliary task of observational interferences such as artificial noises.\nUnder this framework, we discuss the importance of the causal relation and propose a causal inference based DRL algorithm called causal inference Q-network (CIQ).\nWe evaluate the performance of CIQ in several benchmark DRL environments with different types of interferences as auxiliary labels.\nOur experimental results show that the proposed CIQ method could achieve higher performance and more resilience against observational interferences.", "pdf": "/pdf/e4ad060a5e0ec586bd6d82089753538c9fd0b29a.pdf", "paperhash": "anonymous|causal_inference_qnetwork_toward_resilient_reinforcement_learning", "_bibtex": "@inproceedings{\nanonymous2021causal,\ntitle={Causal Inference Q-Network: Toward Resilient Reinforcement Learning},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=vsyJQYFKlD8},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}