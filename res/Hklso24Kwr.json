{"notes": [{"id": "Hklso24Kwr", "original": "BJeCteMWwB", "number": 165, "cdate": 1569438883290, "ddate": null, "tcdate": 1569438883290, "tmdate": 1583912045725, "tddate": null, "forum": "Hklso24Kwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "CC45ERa-dC", "original": null, "number": 1, "cdate": 1580781608202, "ddate": null, "tcdate": 1580781608202, "tmdate": 1581034783665, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Public_Comment", "content": {"title": "Results on Permuted MNIST", "comment": "I don't know the exact process the authors followed for training permuted mnist task. If they used Zenke et. al like architecture (2 100x100 hidden layers) and 20 epochs or less of training - an individual task by itself barely reaches 97-98% test accuracy under the mentioned optimization parameters.\nThe code isn't available in the public domain and seeing results of this Deepmind paper on the same experiments- https://arxiv.org/pdf/1901.11356.pdf makes me wonder how the authors actually went about the experimentation. For e.g. the same method's results (FRCL) are overstated in your results than the original work. The authors should clarify what additional tricks, if any, they used for such 'boost' in performances of all methods.\nSurprisingly, none of the reviewers seem to notice it either."}, "signatures": ["~Prakhar_Kaushik1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Prakhar_Kaushik1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hklso24Kwr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504213025, "tmdate": 1576860562307, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper165/-/Public_Comment"}}}, {"id": "WuWzFh5Dsw", "original": null, "number": 8, "cdate": 1580867950029, "ddate": null, "tcdate": 1580867950029, "tmdate": 1580867950029, "tddate": null, "forum": "Hklso24Kwr", "replyto": "CC45ERa-dC", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment", "content": {"title": "Resp.", "comment": "The same architecture (as Zenke et al.) was used as a basis to enable fair comparison. Apparently, the method's characteristics lead to changes in how the parameters are computed and therefore the results change. Results of 98% in Zenke et al. improved to 99% in CLAW, based on the same architecture. This is similar to the improvement by VCL with coresets on the same data; the results therein were higher than what was originally reported in Zenke et al. . CLAW goes one step further. \nRegarding the performance on the first task on Permuted MNIST, this dataset follows a random permutation process. Zenke et al. used the same architecture but the outcome of the random permutation process is not necessarily the same. What matters is that the same outcome of the permutation process is used here throughout all the compared methods. "}, "signatures": ["ICLR.cc/2020/Conference/Paper165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hklso24Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper165/Authors|ICLR.cc/2020/Conference/Paper165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175385, "tmdate": 1576860528432, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment"}}}, {"id": "zoRa96wO-F", "original": null, "number": 1, "cdate": 1576798689118, "ddate": null, "tcdate": 1576798689118, "tmdate": 1576800946002, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposes a new variational-inference-based continual learning algorithm with strong performance.\n\nThere was some disagreement in the reviews, with perhaps the one shared concern being the complexity of the proposed method. One reviewer brought up other potentially related work, but this was convincingly rebutted by the authors. Finally, one reviewer had an issue with the simplicity with the networks in the experiments, but the authors rightly pointed out that the architectures were simply designed to match those from the baselines.\n\nContinual learning has been an active area for quite some time and convincingly achieving SOTA in a new way is a strong contribution, and will be of interest to the community. Progress in a field is sometimes made by iteratively simplifying an initially complex solution, and this work lays in a brick in that direction. For these reasons, I recommend acceptance.\n\n\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717034, "tmdate": 1576800267236, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper165/-/Decision"}}}, {"id": "rkx_G-XciB", "original": null, "number": 2, "cdate": 1573691663772, "ddate": null, "tcdate": 1573691663772, "tmdate": 1573817292179, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Sygqf-m0Fr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We thank the reviewer for the welcome feedback, which we will/do incorporate into the revised version. \n\n- The sequential task setting for continual learning has very little to add in practice, and to other branches of machine learning: it has little to say for domains where continual learning problems occur naturally, such as reinforcement learning, GAN training, multi-agent learning; all of these domains need continual learning solutions:\n\n- A: As mentioned in the paper, our architecture as well as the modeling ideas can be utilised in the multi-task learning paradigm.\n\nIn addition, with fairly simple changes, the methods presented in the paper can be generalised from the classification setting to some of the paradigms mentioned by the reviewer, namely conditional generation in GANs, reinforcement learning (RL) and generally environments that change over time (e.g. due to the behaviour of other agents, etc).\n\nWe also intend to extend the ideas presented in this paper in some of these learning paradigms in future work. One of the key differences between continual learning and some of these paradigms is that in the former the learner is aware of the boundaries between tasks, unlike with RL for instance. We have already begun to work on extensions of CLAW based on the harder case where the learner needs to discover such boundaries based on the uncertainty. In other words, and similar to what was developed by Farquhar and Gal, \"Towards robust evaluations of continual learning\", 2018, we test the ability of model uncertainty to distinguish a task boundary. Thanks to our variational inference modeling framework, there is potential to use uncertainty estimates to discover task boundaries, which is one step in the direction of sequential decision-making paradigms other than continual learning.\n \n\n- Complexity of CL algorithms:\n- A: This is another very good point. This is one of the reasons we have been keen on developing and presenting the ideas in a \u201cmodular\u201d framework, separating the modeling from the inference ideas. This eventually makes it straightforward to improve one particular module in the future, due to its minimal entanglement with the other modules of the framework. We also believe that one of the advantages of shedding light on the relevance of each of the principal parameters (like what we empirically provided via the ablative analysis in Section D in the Appendix) is to make the overall presentation more unequivocal.\n \n\n- 'episodic memory' baselines:\n- A: Methods developed here are orthogonal to memory-based methods, and it would therefore be simple to combine with them. In principle, this is similar to the coreset version of VCL (the coreset is analogous to an episodic memory that retains important training data points from previous tasks). The inference framework in CLAW also naturally allows for the integration and storage of an episodic memory. We have added this clarification to the text.\n \n\n- Progress across a more diverse and widely relevant set of continual learning challenges:\n - A: We agree that in general the current baselines in continual learning could be improved and made more real-world relevant. We have tried to provide a thorough analysis: we have performed a range of experiments in line with previously published relevant papers, and have aimed at carrying out a set of experiments that is richer than the average (compared to our 5 examined datasets, some of the seminal, top-tier publications in the CL literature considered fewer datasets, for example EWC: 2 datasets, VCL: 3 datasets, RCL: 3 datasets & LTG: 3 datasets).\n \nWe agree with your point. As mentioned earlier, we hope to make further use of the uncertainty estimates and their high fidelity to move on to other sequential decision-making paradigms with no task boundaries. The more stringent assumption that the learner is not aware of when the changes occur in the task distribution has also been highlighted by the seminal continual learning paper \u201cProgress and compress\u201d, ICML 2018, as a cornerstone in the direction of moving from continual learning to more real-life problems (and also a future objective of theirs), which we believe that we have a strong basis of thanks to the variational inference foundation of CLAW. "}, "signatures": ["ICLR.cc/2020/Conference/Paper165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hklso24Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper165/Authors|ICLR.cc/2020/Conference/Paper165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175385, "tmdate": 1576860528432, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment"}}}, {"id": "BJe7P1mcjr", "original": null, "number": 1, "cdate": 1573691227077, "ddate": null, "tcdate": 1573691227077, "tmdate": 1573692165959, "tddate": null, "forum": "Hklso24Kwr", "replyto": "B1xod8Xe9S", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank the reviewer for the welcome feedback, which we will/do incorporate into the revised version. \n\n- The motivation of this paper is a task-specific weight adaptation mechanism, which seems a simple version of [1]:\n- A: We respectfully disagree. The methodology we propose in CLAW is completely different from [1], i.e. \"Learn to Grow\". They both try and adapt the architecture, and use similar terminology for the \"reuse\", \"adaptation\" and \"new\" options. However, the methodologies used to achieve the adaptation in the two papers are completely different. CLAW introduces a novel algorithm where the adaptation itself becomes part of the variational inference procedure, whereas \u201cLearn to Grow\u201d is based on neural architecture search. CLAW is the first method to offer CL architecture adaptation without having to expand the architecture with new layers or new neurons. In the beginning (regardless of the number of tasks), our framework adds solely (a total of) 3 parameters per neuron, and from this point onwards there is no need to add any further parameters along with the introduction of new CL tasks (no matter how many tasks).\n\n- Motivation of the design of b^T as s/(1+e^{-a^T})-1:\n- A: The variable b^T constrains the range of the neuron adaptation. This is useful to limit overfitting, especially when dealing with noisy data. Instead of keeping the adaptation values unconstrained, more sensitive to noise and prone to overfitting, the unconstrained adaptation variable a^T is constrained with the corresponding b^T. As such, rather than varying in the interval [-inf, inf], the range of adaptation values becomes [0, s], which also facilitates the optimisation. Though in a different context, this design is similar to Swietojanski and Renals [3] (reference added to the revised version). As was already mentioned in our original version, the addition of $1$ is to facilitate the usage of a probability distribution while still keeping an adaptation range allowing for the attenuation or amplification of each neuron's contribution.\n\nThe (differentiable) logistic function is well-known to be powerful and flexible in bounding (constraining) its domain within a certain range while still counting on a relatively low number of adaptation parameters. We empirically tried other options including: rigidly projecting the unconstrained adaptation values into a certain interval (e.g. [-s, s]), using a logistic function without learning its maximum s (as in the ablative analysis in Section D), and using a fixed maximum (e.g. 2/(1+e^{-a^T})-1). Of these options, and as expected, using the logistic function empirically proved to be more effective.  \n\n [3] P. Swietojanski and S. Renals. Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models. IEEE Spoken Language Technology Workshop (SLT), 2014. \n\n- Comparison with [2]:\n- A: We have performed comparisons with the interesting \"online meta-learning\" algorithm (to be added to the revised version). CLAW performs significantly better in terms of classification accuracy, achieved reduction in catastrophic forgetting and the achieved degree of positive forward transfer. Having said that, we also have to highlight the fact that the principal goals of the \u201conline meta-learning\u201d paper are rather different since they are focussed around meta-learning and ensuring that online meta-learning, and especially the follow the meta-leader (FTML) algorithm, can be successfully applied to non-stationary learning problems.\nHere are the results of the comparisons between CLAW and the FTML algorithm presented in the online meta-learning paper:\n\nClassification accuracy:\n                                             | CLAW       FTML\nCIFAR-100 (after 19 tasks) | 95.6%      81.3%\nCIFAR-100 (after 20 tasks) | 95.6%      80.2%\n\nOmniglot (after 49 tasks)  | 84.5%      79.7%\nOmniglot (after 50 tasks)  | 84.6%      78.5%\n \nCatastrophic forgetting:\n                                             | CLAW      FTML\nCIFAR-100 (after 19 tasks)  | 95.4%     82.4%\nCIFAR-100 (after 20 tasks)  | 95.4%     81.8%\n\nOmniglot (after 49 tasks)   | 75.2%     68.4%\nOmniglot (after 50 tasks)   | 75.1%     68%\n \nPositive forward transfer:\n                                                      | CLAW      FTML\nCIFAR-100 (after 18 other tasks)| 95.3%     78.6%\nCIFAR-100 (after 19 other tasks)| 95.6%     78.7%\n\nOmniglot (after 48 other tasks)   |   85.2%      79.4%\nOmniglot (after 49 other tasks)   |   86.1%      79.7%\n \nWe acknowledge that this is one of the methods that one can compare to, and thank the reviewer for pointing it out. However, in the original version of the paper, we have compared to 7 CL algorithms. We believe this represents a thorough analysis that goes beyond the comparisons found in many published papers (for instance EWC, VCL and P&C compared theirs with 2, 4 and 4 other algorithms, respectively), although we acknowledge there is a rapid growth of literature in this area. "}, "signatures": ["ICLR.cc/2020/Conference/Paper165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hklso24Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper165/Authors|ICLR.cc/2020/Conference/Paper165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175385, "tmdate": 1576860528432, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment"}}}, {"id": "SJg-iM7qsS", "original": null, "number": 3, "cdate": 1573692056916, "ddate": null, "tcdate": 1573692056916, "tmdate": 1573692056916, "tddate": null, "forum": "Hklso24Kwr", "replyto": "HJgp2lKpYB", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We thank the reviewer for the welcome feedback, which we will/do incorporate into the revised version.\n\n- ResNet:\n- A: We have tested the CLAW algorithm on standard architectural choices that are widely used in the continual learning literature in order to provide a fair comparison that does not conflate performance gains from improved feature extractors with performance gains from better continual learning. \n\nThis is in line with many papers in continual learning (e.g. Progress & Compress, EWC, VCL, FRCL, etc) as well as similar paradigms like few-shot learning and meta-learning. This has been explicitly mentioned in previous seminal continual learning papers like the Progress & Compress algorithm by Schwarz et al., ICML 2018, and has also been explicitly discussed in few-shot learning papers like \"Meta-learning probabilistic inference for prediction\" by Gordon et al., ICLR 2019 and \"A closer look at few-shot classification\" by Chen et al., ICLR 2019. The latter discussed the role of deeper feature extractors in these learning scenarios. Of course, we also agree that combining CLAW with more powerful baseline architecture can further boost the performance. \n \n\n- Ablation study:\n- A: We have highlighted the introduction of an efficient inference mechanism which efficiently controls the following in a data-driven way: whether or not to adapt a neuron, and the magnitude of adaptation. In Section D in the Appendix, we have provided ablations exhibiting the relevance of each of the proposed adaptation parameters via comparing the performance of CLAW to the following cases:\n\n1) When the magnitude of the maximum allowed adaptation is not learnt.\n2) When adaptation always happens, i.e. the binary variable denoting the adaptation decision is always activated.\n3) When adaptation never takes place.\n\nThe results empirically demonstrate the relevance of each part and parameter of the proposed adaptation procedure. What else do you believe is missing?\n \n\n - Training time:\nIn Section C in the original version of the paper, we had provided the number of epochs of CLAW. In addition, we also provided the algorithmic complexity of CLAW near the end of Section 3 in the original version. \n\nWe appreciate the benefits of looking at timings, though they are not often reported in related literature. We have just added to the revised version (Section E) values of the wall-clock run time (in seconds) after finishing training in each of the six experiments. VCL and CLAW converge more quickly than the other methods (more details can be found in Section E). \n\nWe highlight the fact that one of the reasons why we were keen on designing the whole adaptation process to be part of the proposed, amortised variational inference procedure is the run-time efficiency.\n\n\n - Dividing the sample into two halves:\n- A: One of the advantages of the VAE-based CLAW is the reduced variance (see e.g. Kingma and Welling, \u201cAuto-encoding variational Bayes\u201d, ICLR 2014 and Kingma et al., \u201cVariational dropout and the local reparameterization trick\u201d, NIPS 2015). This is supported by the small standard error values in Section B in the Appendix. This is empirically demonstrated via the results of a dataset like Omniglot. Omniglot consists of 50 realistic tasks depicting 50 alphabets, each consisting of handwritten characters. The small standard error values achieved by CLAW on Omniglot (as well as the other datasets) show that the model is not prone to this vulnerability.\n \n\n- Why are the VCL variants with CNN not compared?\n- A: This was because VCL does not originally enable the use of CNNs. As mentioned in Section 4, we have enabled the utilisation of CNNs in CLAW due to the adoption of the local reparameterisation trick. We have just adopted the local reparameterisation trick to VCL as well, and added these results to the revised version. \n\n\n- Figure 1 (e):\n- A: Thanks. We have fixed this typo in the revised version. "}, "signatures": ["ICLR.cc/2020/Conference/Paper165/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hklso24Kwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper165/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper165/Authors|ICLR.cc/2020/Conference/Paper165/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175385, "tmdate": 1576860528432, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper165/Authors", "ICLR.cc/2020/Conference/Paper165/Reviewers", "ICLR.cc/2020/Conference/Paper165/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Comment"}}}, {"id": "HJgp2lKpYB", "original": null, "number": 1, "cdate": 1571815604873, "ddate": null, "tcdate": 1571815604873, "tmdate": 1572972630497, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposed a novel probabilistic continual learning approach which automatically learn an optimal adaptation for arriving tasks while maintaining the performance of the past tasks. CLAW learns element-wise weight masking per task task with respect to the several learnable parameters. \nCLAW fundamentally based on Variational Continual Learning, but it outperforms benchmarks on diverse dataset even without additional coreset. However, the ablation study and analysis on the model is weak and authors only show experimental observations. Also, the experiments are performed on old architectures. Then, it needs to show the model consistently outperform on recent deep network architectures, such as ResNet.\n\n\nI have several questions,\n\n- How about the training time / convergence rate of the CLAW compared to other methods? \n\n- If the model need to divide the sample into two halves, isn't the model vulnerable when there are only a few number of samples with high variance? This situation is quite natural on realistic problem, like Imagenet.\n\n- Why are the VCL variants with CNN not compared?\n\n- There might be used a wrong plots in Figure 1 (e). It doesn't make sense that all methods show equal accuracy on task 46.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654869734, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper165/Reviewers"], "noninvitees": [], "tcdate": 1570237756072, "tmdate": 1575654869747, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Review"}}}, {"id": "Sygqf-m0Fr", "original": null, "number": 2, "cdate": 1571856657703, "ddate": null, "tcdate": 1571856657703, "tmdate": 1572972630455, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper introduces CLAW, a complex but effective approach to continual learning with strong performance in the sequential task learning setting, as demonstrated on a number of standard benchmarks.\n\nI recommend acceptance because:\n- While conceptually similar to VCL, CLAW is convincingly shown to have superior performance across standard benchmarks and measures. The evaluation is thorough across the board, as far as I can tell.\n- Forward transfer is shown to be substantially better compared to other methods. Experiments with long sequences of tasks (Omniglot, CIFAR-100) are particularly telling.\n- Overall, the balance between not forgetting and still learning new tasks seems particularly favourable for the proposed method. This has been an elusive goal of continual learning research, hence the importance of accepting this work.\n\n\nHere are some good reasons why an adversarial reviewer would reject this paper:\n- The sequential task setting for continual learning has very little to add in practice, and to other branches of machine learning: it has little to say for domains where continual learning problems occur naturally, such as reinforcement learning, GAN training, multi-agent learning; all of these domains need continual learning solutions; while progress on standard benchmarks is important, we may be overfitting to these benchmarks.\n- The paper is well written but the method is rather complex and presumably non-trivial to tune. This is actually characteristic of several top competing methods on these benchmarks; getting the last bit of performance seems to require this complexity, but it also makes it that much harder to generalize such methods beyond these benchmarks. For example, exploiting well partitioned datasets into different tasks and known task labels is a good starting point, but once such information is not available all bets are off. Acceptance means encouraging work on these benchmarks; is this really what we should do?\n- It's perfectly tractable to store some small amount of old data for these problems in an 'episodic memory', so one could claim that an entire class of relevant baselines is missing, e.g. A-GEM, iCaRL, etc.\n\n\nLuckily, I am not an adversarial reviewer, but I want to see progress across a more diverse and widely relevant set of continual learning challenges."}, "signatures": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654869734, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper165/Reviewers"], "noninvitees": [], "tcdate": 1570237756072, "tmdate": 1575654869747, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Review"}}}, {"id": "B1xod8Xe9S", "original": null, "number": 3, "cdate": 1571989107465, "ddate": null, "tcdate": 1571989107465, "tmdate": 1572972630413, "tddate": null, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "invitation": "ICLR.cc/2020/Conference/Paper165/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a new continual learning method. The model is based on the probabilistic model and variational inference. To show the effectiveness of the proposed model, the authors do experiments on several benchmarks.\n\nThe model lacks enough technical novelty. The motivation of this paper is a task-specific weight adaptation mechanism, which seems a simple version of [1]. In addition, the design of b^T as s/(1+e^{-a^T})-1 is not well-motivated. It is better to explain more.\n\nIn addition, if the authors compare the performance on both negative transfer (forgetting) and forward transfer (performance on the new task). I suggest the authors compare with [2], which also focuses on the performance of new tasks.\n\nMinor:\n1. The font of figures and its legends are small.\n\n[1] Li, Xilai, et al. \"Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting.\" ICML\u201919.\n\n[2] C Finn et al. \u201cOnline Meta-learning\u201d ICML\u201919\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper165/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tah47@cam.ac.uk", "han.zhao@cs.cmu.edu", "ret26@cam.ac.uk"], "title": "Continual Learning with Adaptive Weights (CLAW)", "authors": ["Tameem Adel", "Han Zhao", "Richard E. Turner"], "pdf": "/pdf/cbfaaa87615669fc9bfef8f514f89a0bc42084bc.pdf", "TL;DR": "A continual learning framework which learns to automatically adapt its architecture based on a proposed variational inference algorithm. ", "abstract": "Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting. ", "keywords": ["Continual learning"], "paperhash": "adel|continual_learning_with_adaptive_weights_claw", "_bibtex": "@inproceedings{\nAdel2020Continual,\ntitle={Continual Learning with Adaptive Weights (CLAW)},\nauthor={Tameem Adel and Han Zhao and Richard E. Turner},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Hklso24Kwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ca5433cb200702540ad221417c288f4c45264dee.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hklso24Kwr", "replyto": "Hklso24Kwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper165/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575654869734, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper165/Reviewers"], "noninvitees": [], "tcdate": 1570237756072, "tmdate": 1575654869747, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper165/-/Official_Review"}}}], "count": 10}