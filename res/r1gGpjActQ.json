{"notes": [{"id": "r1gGpjActQ", "original": "Hke0bTt5t7", "number": 781, "cdate": 1538087865849, "ddate": null, "tcdate": 1538087865849, "tmdate": 1545355432989, "tddate": null, "forum": "r1gGpjActQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJx6yBXrxN", "original": null, "number": 1, "cdate": 1545053413009, "ddate": null, "tcdate": 1545053413009, "tmdate": 1545354483527, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Meta_Review", "content": {"metareview": "\n\n+ sufficiently strong results\n\n+ a fast / parallelizable model\n\n\n- Novelty with respect to previous work is not as great (see AnonReviewer1 and AnonReviewer2's comments)\n\n- The same reviewers raised concerns about the discussion of related work (e.g., positioning with respect to work on knowledge distillation). I agree that the very related work of Roy et al should be mentioned, even though it has not been published it has been on arxiv since May.\n\n- Ablation studies are only on smaller IWSLT datasets, confirming that the hints from an auto-regressive model are beneficial (whereas the main results are on WMT)\n\n-  I agree with R1 that the important modeling details (e.g., describing how the latent structure is generated) should not be described only in the appendix, esp given non-standard modeling choices.  R1 is concerned that a model which does not have any autoregressive components (i.e. not even for the latent state) may have trouble representing multiple modes.  I do find it surprising that the model with non-autoregressive latent state works well however I do not find this a sufficient ground for rejection on its own. However, emphasizing this point and discussing the implication in the paper makes a lot of sense, and should have been done.  As of now, it is downplayed. R1 is concerned that such model may be gaming BLEU: as BLEU is less sensitive to long-distance dependencies, they may get damaged for the model which does not have any autoregressive components.  Again, given the standards in the field, I do not think it is fair to require human evaluation, but I agree that including it would strengthen the paper and the arguments.\n\n\nOverall, I do believe that the paper is sufficiently interesting and should get published but I also believe that it needs further revisions / further experiments.\n\n\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "needs more work"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper781/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353090174, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353090174}}}, {"id": "HJg04FBCAQ", "original": null, "number": 8, "cdate": 1543555381533, "ddate": null, "tcdate": 1543555381533, "tmdate": 1544373931896, "tddate": null, "forum": "r1gGpjActQ", "replyto": "SyxK4plKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Author Response - Regarding Mode Breaking", "comment": "Dear reviewer:\n\nTo address the concern of mode breaking, we make some discussion here. \n\n1. To some extent, a machine translation system doesn\u2019t require to model **multiple modes** and is not evaluated by whether the model can generate **multiple modes**. \n\nMachine translation is a real application which aims at providing the correct translation to users, but not providing multiple diverse translations. For example, Google Translate just provides the best translation result from a set of translation candidates, but not a set of translation results with multiple modes. This is a bit different from other generative modeling tasks such as music generation (WaveNet and parallel-WaveNet).\n \nThis claim can also be justified from the evaluation metric of machine translation. In the test data, we usually have bilingual sentence pairs, one side is called the source sentence, the other side is called the reference sentence (ground truth target sentence). Sometimes one source sentence has multiple reference sentences. When we have a translation output, the translated sentence will be compared to each reference sentence and use the **maximum** BLEU score as the correctness of the translation. That is being said, although we have multiple references with multiple modes, we just use BLEU score between the translation results and the most similar reference, but do not evaluate the diversity.\n\n2. Sentence-level knowledge distillation can map  **multiple modes** to a **single mode**. \n\nOur work, Gu et al. and Kaiser et al. (Roy et al.) use sentence-level knowledge distillation(KD), which is super effective in all the works. Sentence-level KD can be considered as using the auto-regressive model output instead of the reference. We have explicitly mentioned this in the paper (\"Following Gu et al. (2017); Kim & Rush (2016), we replace the target sentences in all datasets by the decoded output of the teacher models.\"). As observed by [1], ART model outputs are more stable and the patterns are clearer.\n \nIn the example that the reviewer suggests \u201dsuppose our dataset consists of sequences of numbers, where with probability 0.5 the sequence is sorted in ascending order and with probability 0.5 it is sorted in descending order\u201d. We assume the ART model can well capture such modes and further assume there are some training errors: the ascending order sequence is predicted with probability 0.501 while the other one is predicted with probability 0.499. Once we use the greedy search algorithm (e.g., beam search) for distillation, the ART model output is always the in-ascending-order one which has a relatively larger probability.\n \nFrom the above discussion, we can see that although the original dataset may be **multiple-mode**, the distilled dataset is reduced to **single mode** by using KD. **single mode** dataset is much easier to train non-autoregressive models. That is also a reason why our simple z works.\n\n3. Regarding appendix\n\nWe move the model details to appendix due to the paper length requirements.  Per the reviewer's request, we are willing to move any parts back to the main body if they are considered to be important to the paper quality. Furthermore, we are also willing to release our reproducible codes and models for testing all the tasks mentioned in the paper.\n\nWe believe the performance of our non-autoregressive model is significant and we hope the discussions above address the concerns of the reviewers and ACs. \n\n[1] Ott, Myle, Michael Auli, David Granger, and Marc'Aurelio Ranzato. \"Analyzing uncertainty in neural machine translation.\" ICML 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "S1lIxAuFyV", "original": null, "number": 11, "cdate": 1544289773902, "ddate": null, "tcdate": 1544289773902, "tmdate": 1544340467953, "tddate": null, "forum": "r1gGpjActQ", "replyto": "rke_YdcHkN", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Re: Regarding Mode Breaking ", "comment": "While we fully get your points, we have to say that we respectfully disagree with your opinions and we are afraid that you have misunderstandings on the research area of neural machine translation.\n \n- \u201cI would argue, that the ability of the model to capture said diversity is an important factor to decide whether to deploy such a model.\u201d\nWe have worked on neural machine translation for years and have involved in developing a popular public translation service. We have also communicated with multiple translation teams (including both big Internet companies and small startups).  According to our own experiences and the messages from other groups, the ability to capture such diversity is indeed an unimportant factor to consider while deploying a model, at least at the current stage. According to our knowledge, the most important factors to consider are accuracy, inference latency, and then the model size, which are also hot research topics in neural machine translation. While we agree that the ability to capture diversity is an interesting research problem, it is not the first priority to consider in real-world machine translation systems and not our focus in this work.\n \n- \u201can imperfect, real-world MT model that is unable to capture multiple modes might output an erroneous translation (corresponding to one mode)\u201d\nWhile it is not clear whether an erroneous translation is caused by the inability to capture such diversity, we do believe that improving translation accuracy (e.g., in terms of BLEU) can somehow address the problem. This is exactly our focus in this paper.\n \n- \u201cWhat if one of the modes that the model misses is the correct one, while the mode it converges on is an incorrect mode due to noise in the dataset?\u201d\nGood point. Our model cannot model noise in the dataset. Our model aims to converge to the major mode of the training data (this is also the case for most MT models). If the majority of the translations of a certain source sentence in the training data is incorrect, our model will converge to the incorrect mode and output an incorrect translation. For this case, even if a model can well capture multiple modes, it is still very likely to output an incorrect translation, because the incorrect mode is the major one in the training data and a real-world MT system only output one translation usually corresponding to the major mode. That is, the ability to capture multiple modes *does not* help to solve this problem. Furthermore, if one uses a complete random model for MT, one can eventually get the correct translation result by asking the model repeatedly. However, this kind of \u201cmultimode\u201d is definately not we want for a real-world MT model.\n \n- \u201cthis model may be able to game the BLEU score which does not evaluate a model on diversity, I still think this approach to non-autoregressive translation is a hack and has limited practical usefulness due to it's inability to sufficiently capture multiple modes in the translation data\u201d\nWe are surprised that you say we \u201cgame the BLEU\u201d because BLEU \u201cdoes not evaluate a model on diversity\u201d. According to this judgement rule, most (maybe >90%) research on neural machine translation, including those most influential ones such as LSTM with attention [1], ConvS2S [2] and Transformer [3], will be meaningless, because their primary contribution is to improve the translation quality in terms of BLEU. In other words, they also \u201cgame BLEU score which does not evaluate a model on diversity\u201d. We are afraid this statement might be a negative bias towards our work and the whole area of neural machine translation, in which BLEU is the most widely used metric and improving BLEU is one of the most important goals. \u201cBLEU does not reflect diversity\u201d does not mean \u201cBLEU is not a good measure for translation quality\u201d.\n\n\n\n[1] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. ICLR 2015.\n[2] Gehring J, Auli M, Grangier D, et al. Convolutional sequence to sequence learning. ICML 2017.\n[3] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. NeurIPS 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "rke_YdcHkN", "original": null, "number": 10, "cdate": 1544034431889, "ddate": null, "tcdate": 1544034431889, "tmdate": 1544034431889, "tddate": null, "forum": "r1gGpjActQ", "replyto": "HJg04FBCAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Re: Regarding Mode Breaking", "comment": "\"To some extent, a machine translation system doesn\u2019t require to model **multiple modes** and is not evaluated by whether the model can generate **multiple modes**. \"\n\nI am fully aware of what Machine Translation is - sure to some extent the MT model is not evaluated on it's ability to capture multiple modes in the data. However, I would argue, that the ability of the model to capture said diversity is an important factor to decide whether to deploy such a model. For example, an imperfect, real world MT model that is unable to capture multiple modes might output an erroneous translation (corresponding to one mode), and the confused user on querying the model again is faced with the same wrong translation because your model is unable to capture the multiple modes in a messy, noisy real-world translation dataset.\n\n\"Our work, Gu et al. and Kaiser et al. use sentence-level knowledge distillation(KD), which is super effective in all the works. \"\n\nIf you read the papers carefully you will find that sentence-level knowledge distillation was not used in Kaiser et al., but in Roy et al. which you refuse to cite, even though as R3 pointed out is on arxiv since May 2018. \n\nRegarding the example I pointed out, as you yourself acknowledge the NART model will be unable to capture the two modes present in the dataset. What if one of the modes that the model misses is the correct one, while the mode it converges on is an incorrect mode due to  noise in the dataset? \n\nOn the other hand the models of Kaiser et al., Roy et al., are able to capture this diversity because of the autoregressive prior fit on the latents z. While, this model may be able to game the BLEU score which does not evaluate a model on diversity, I still think this approach to non-autoregressive translation is a hack and has limited practical usefulness due to it's inability to sufficiently capture multiple modes in the translation data, due to the inexpressivity of the choice of z's the authors use. Hence my rating still stands."}, "signatures": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "rylRpqlCAm", "original": null, "number": 7, "cdate": 1543535301771, "ddate": null, "tcdate": 1543535301771, "tmdate": 1543535301771, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1x-Ew4pC7", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Concerns persist", "comment": "I have read the author rebuttal carefully as well as the updated version of their paper. I do believe the paper is well motivated and is addressing an important problem. However, I still have the following concerns about this work, which I feel the authors have not been able to address. \n\n+ Do not understand how the z's are being used: Firstly, the authors define the formulation of z's in the appendix, while the definition of z's is quite critical and can dictate whether or not the approach could work. Further, I do not see a clear explanation of how the z's are being used by the decoder. Reviewers are not required to read the Appendix in judging the work and I wonder if the other reviewers who are positive about the work, understand the details of this approach? To me it seems that the choice of z is critical in determining if this method could work, since no mode-breaking can happen in the part where the model produces the y's given x's and z's. \n\n+ Mode breaking: I do not understand how the proposed approach is breaking the multiple modes that can arise in a translation problem. Here is a concrete example - suppose our dataset consists of sequences of numbers, where with probability 0.5 the sequence is sorted in ascending order and with probability 0.5 it is sorted in descending order. This dataset has exactly two modes. An autoregressive model can solve this mode breaking problem because the second token depends on the first token, and so on. The non-autoregressive model proposed by Kaiser et al, can also solve this problem since the second latent depends on the first latent and so on.\n\nHowever, it is not at all clear to me how the current approach would work in this case. In principle if the z's were some disentangled representation of the targets, then I could imagine that this might work - but the z's proposed by the authors is a fixed linear combination of the embeddings of the source and depend only on the length of the targets. The z's are independent of each other and the y's are independent given the z's and x's. So I do not understand where mode-breaking could be happening?\n\nSo while this approach may be working on the WMT'En-De dataset where the sequence sizes are small, I do not see this approach generalizing to harder multimodal translation datasets and longer sequences. Thus my original rating stands."}, "signatures": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "ByeImig4AX", "original": null, "number": 4, "cdate": 1542880029658, "ddate": null, "tcdate": 1542880029658, "tmdate": 1542880056179, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Revision to the Paper", "comment": "Thanks all reviewers for their valuable comments. We updated a new version of the paper by including the following discussions in the Appendix:\n\n1. We discuss the previous related works on knowledge distillation in Appendix B.\n\n2. We include extra experiments to show the effectiveness of our proposed method in Appendix D."}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "rkgOEYeNCX", "original": null, "number": 3, "cdate": 1542879535740, "ddate": null, "tcdate": 1542879535740, "tmdate": 1542879535740, "tddate": null, "forum": "r1gGpjActQ", "replyto": "SyxK4plKnX", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Author Response", "comment": "Thanks for the review! We believe there are some misunderstandings here. We respond to the concerns as below. We will also post our source codes and trained models for verification after the double-blind review period.\n\n1. Regarding the design choice of z\n\nThe reviewer considers that in a non-autoregressive translation model, \u201cz_j does not depend on z_1, ..., z_{j-1}\u201d and \u201cz_1, z_2, ..., z_{T_y} depends only on the length of y\u201d are unreasonable and red flags. However, before our paper, such setting has been shown to work in non-autoregressive translation. We believe the reviewer\u2019s understanding to this might be incorrect.\n\nGu et al. [1] and we choose to generate z using non-autoregressive ways and ours is a further simplification of [1]. In [1], The hidden z_1, \u2026, z_{T_y} (the \u201cfertility\u201d module) are also mutually independently generated, and have an only limited dependency on y. Please note that the simplicity of z does not mean that the model will definitely suffer from poor translation quality. Although the hidden z is simple, the model itself is a deep neural network, consisting of different components (self attention layer, encoder to decoder attention layer, positional attention layer) that enable the model to learn a complex mapping from x and z to y. \n\nWe believe a simple design choice of z is enough and do not list it as a major contribution of our work. Our main technical contribution is to improve the model performance by a more well-designed training algorithm from teachers. Our experimental results show that by our carefully designed training algorithm, a non-autoregressive model with a simple z can achieve near autoregressive performance, while benefiting from the speedup brought by the little overhead of such a simple z. \n\n2. Regarding \"orders of magnitude faster\" related works\n\nThanks for pointing out the recent work from Roy et al. [3], which is also an ICLR submission this year. By checking their paper, we can see that the speedup of Roy et al. [3] is not \"orders of magnitude faster\". It is only 4.08x when reaching the highest performance, comparing to 17.8x in our work. It is not true that the model by Roy et al. is \"orders of magnitude faster\".\n\nThe main reason is that they choose to use a more complex z using an autoregressive module. Such overhead of z will greatly hurt their speedup, which also contradicts with the initial purpose of introducing non-autoregressive modeling. We believe the translation quality of our model (25.2 for WMT En-De, 29.52 for WMT De-En) is significant given the large speedup of our model.\n \n[1] Gu, Jiatao, et al. \"Non-autoregressive neural machine translation.\" ICLR 2018\n[2] Lee, Jason, Elman Mansimov, and Kyunghyun Cho. \"Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.\" EMNLP 2018\n[3] Roy, Aurko, et al. \"Theory and Experiments on Vector Quantized Autoencoders.\" arXiv 2018\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "SJg6ougVAX", "original": null, "number": 2, "cdate": 1542879397042, "ddate": null, "tcdate": 1542879397042, "tmdate": 1542879397042, "tddate": null, "forum": "r1gGpjActQ", "replyto": "rklofD7c3X", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Author Response", "comment": "Thanks for the review! We have added discussions on related references and KD to the paper.\n\nIn machine translation, the state-of-the-art model uses beam search and thus we follow to use it in the baseline and make comparisons. Batch size 1 for decoding is a common practice when comparing the speedups of non-autoregressive translation models [1, 2, 3].  We follow the practice of previous works to make a fair comparison. Setting batch size 1 and studying the efficiency is also reasonable. Just consider the applications where the translation computation is done on a portable device (e.g. offline translation app on a smartphone). In such a scenario, the user inputs one sentence and expects the translation result.\n\n[1] Gu, Jiatao, et al. \"Non-autoregressive neural machine translation.\" ICLR 2018\n[2] Lee, Jason, Elman Mansimov, and Kyunghyun Cho. \"Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.\" EMNLP 2018\n[3] Kaiser, \u0141ukasz, et al. \"Fast Decoding in Sequence Models Using Discrete Latent Variables.\" ICML 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "Hkx6vOg40m", "original": null, "number": 1, "cdate": 1542879333108, "ddate": null, "tcdate": 1542879333108, "tmdate": 1542879333108, "tddate": null, "forum": "r1gGpjActQ", "replyto": "SkxNVPkkTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "content": {"title": "Author Response", "comment": "Thanks for the review! The main contribution of the paper is to show that by our proposed hint-based training algorithm, a simple non-autoregressive model without a complex submodule can reach competitive performance near an autoregressive model, while still being orders of magnitude faster. We think the results and findings are significant and will be helpful to future works in this direction.\n\nWe conduct the following two experiments according to the suggestions:\n\n- According to our study, the proposed algorithm reduces the percentage of repetitive words by more than 20% in IWSLT De-En task. \n\n- We filter out all the sentences whose lengths are at least 40 in the test set of IWSLT De-En, and test the baseline model and the model trained with hints on the subsampled set. It turns out that our model outperforms the baseline model by more than 3 points in term of BLEU (20.63 v.s. 17.48). Note that the incoherent patterns like repetitive words are a common phenomenon among sentences of all lengths, rather than a special problem for long sentences.\n\nAs stated in the paper, it is quite difficult to find a uniform fair measure for comparing the speed of non-autoregressive models. Since the speedup of non-autoregressive models comes from their fine-grained parallelism, traditional metrics like FLOPs do not fit. Absolute metrics like latency highly depends on underlying hardware and code implementations. Therefore, we use speedup in our paper as a better relative measure for a fair comparison.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615933, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1gGpjActQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper781/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper781/Authors|ICLR.cc/2019/Conference/Paper781/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers", "ICLR.cc/2019/Conference/Paper781/Authors", "ICLR.cc/2019/Conference/Paper781/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615933}}}, {"id": "SkxNVPkkTQ", "original": null, "number": 3, "cdate": 1541498668310, "ddate": null, "tcdate": 1541498668310, "tmdate": 1541533696138, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "content": {"title": "good results, okay paper", "review": "In this paper, the authors propose an extension to the Non Autoregressive Translation model by Gu et. al, to improve the accuracy of Non autoregressive models as compared to the autoregressive translation models.\nThe authors propose using hints which can occur as\n1. Hidden output matching by incurring a penalty if the cosine distance between the representation differ according to a threshold. The authors state that this reduces same output word repetition which is common for NART models\n2. Reducing the KL divergence between the attention distribution of the teacher and the student model in the encoder-decoder attention part of the model.\n\nWe see experimental evidence from 3 tasks showing the effectiveness of this technique.\n\nThe strengths of this paper are the speedup improvements of using these techniques on the student model while also improving BLEU scores. \nThe paper is easy to read and the visualisations are useful.\n\nThe main issue with this paper is the delta contribution as compared to the NART model is Gu et. al. The 2 techniques, although simple, don't make up for technical novelty.\nIt would also be good to see more analysis on how much the word repetition reduces using these techniques quantitatively, and performance especially on longer length sequences.\n\nAnother issue is the comparison of latency measurements for decoding. The authors state that the hardware and the setting under which the latency measurements are done might be different as compared to previous numbers. Though still impressive speedup improvements, it somehow becomes fuzzy to understand the actual gains.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "cdate": 1542234378609, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335801006, "tmdate": 1552335801006, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rklofD7c3X", "original": null, "number": 2, "cdate": 1541187346818, "ddate": null, "tcdate": 1541187346818, "tmdate": 1541533695927, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "content": {"title": "Good results, although knowledge distillation and its use in non-autoregressive NMT should be discussed better.", "review": "This paper proposes to distill knowledge from intermediary hidden states and\nattention weights to improve non-autoregressive neural machine translation.\n\nStrengths:\n\nResults are sufficiently strong. Inference is much faster than for\nauto-regressive models, while BLEU scores are reasonably close.\n\nThe approach is simple, only necessitating two auxiliary loss functions during\ntraining, and rescoring for inference.\n\nWeaknesses:\n\nThe discussion of related work is deficient. Learning from hints is a variant\nof knowledge distillation (KD). Another form of KD, using the auto-regressive\nmodel output instead of the reference, was shown to be useful for non-autoregressive\nneural machine translation (Gu et al., 2017, already cited). The authors mention using\nthat technique in section 4.1, but don't discuss how it relates to their work. [1] should\nalso probably be cited.\n\nHu et al. [2] apply a slightly different form of attention weight distillation.\nHowever, the preprint of that paper was available just over one month before the\nICLR submission deadline.\n\nQuestions and other remarks:\n\nDo the baselines use greedy or beam search?\n\nWhy batch size 1 for decoding? With larger batch sizes, the speed-up may be\nlimited by how many candidates fit in memory for rescoring.\n\nPlease fix \"are not commonly appeared\" on page 4, section 3.1.\n\n[1] Kim, Yoon and Alexander M. Rush. \"Sequence-Level Knowledge Distillation\" EMNLP. 2016.\n[2] Hu, Minghao et al. \"Attention-Guided Answer Distillation for Machine Reading Comprehension\" EMNLP. 2018", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "cdate": 1542234378609, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335801006, "tmdate": 1552335801006, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxK4plKnX", "original": null, "number": 1, "cdate": 1541111089409, "ddate": null, "tcdate": 1541111089409, "tmdate": 1541533695699, "tddate": null, "forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "invitation": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "content": {"title": "Some concerns", "review": "This work proposes a non-autoregressive Neural Machine Translation model which the authors call NART, as opposed to an autoregressive model which is referred to as an ART model. The main idea behind this work is to leverage a well trained ART model to inform the hidden states and the word alignment of NART models. The joint distribution of the targets y given the inputs x, is factorized into two components as in previous works on non-autoregressive MT: an intermediate z which is first predicted from x, which captures the autoregressive part, while the prediction of y given z is non-autoregressive. This is the approach taken e.g., in Gu et al, Kaiser et al, Roy et al., and this also seems to be the approach of this work.  The authors argue that improving the expressiveness of z (as was done in Kaiser et al, Roy et al), is expensive and so the authors propose a simple formulation for z. In particular, z is a sequence of the same length as the targets, where the j^{th} entry z_j is a weighted sum of the embedding of the inputs x (the weights depend in a deterministic fashion on j) . Given this z, the model predicts the targets completely non-autoregressively. However, this by itself is not entirely sufficient, and so the authors also utilize \"hints\": 1) If the pairwise cosine similarity between two successive hidden states in the student  NART model is above a certain threshold, while the similarity is lower than another threshold in the ART model, then the NART model incurs a cost proportional to this similarity 2) A KL term is used to encourage the distribution of attention weights of the student ART model to match that of the teacher NART model. These two loss terms are used in different proportions (using additional hyperparameters) together with maximizing the likelihood term.\n\nQuality: The paper is not very well written and is often hard to follow in parts. Here are some examples of the writing that feel awkward:\n\n--  Consequently, people start to develop Non-AutoRegressive neural machine\nTranslation (NART) models to speed up the inference process (Gu et al., 2017; Kaiser et al., 2018;\nLee et al., 2018). \n\n-- In order to speed up to the inference process, a line of works begin to develop non-autoregressive\ntranslation models. \n\nOriginality: The idea of using an autoregressive teacher model to improve a non-autoregressive translation model has been used in Gu et al., Roy et al., where knowledge distillation is used. So knowledge distillation paper from Hinton et al., should be cited. Moreover, the authors have missed comparing their work to that of Roy et al. (https://arxiv.org/abs/1805.11063), which greatly improves on the work of Kaiser et al., and almost closes the gap between a non-autoregressive model and an autoregressive model (26.7 BLEU vs 27 BLEU on En-De) while being orders of magnitude faster. So it is not true that:\n\n-- \"While the NART models achieve significant speedup during inference (Gu et al., 2017), their accuracy\nis considerably lower than their ART counterpart.\"\n\n-- \"Non-autoregressive translation (NART) models have suffered from low-quality translation results\"\n\nSignificance: The work introduces the idea of using hints for non-autoregressive machine translation. However, I have a technical concern: It seems that the authors complain that previous works like Kaiser et al, Roy et al, use sophisticated submodules to help the expressiveness of z and this was the cause for slowness. However, the way the authors define z seems to have some problems:\n\n- z_j does not depend on z_1, ..., z_{j-1}, so where is the autoregressive dependencies being captured?\n- z_1, z_2, ..., z_{T_y} depends only on the length of y, and does not depend on y in any other way. Given x, predicting z is trivial and I don't see why that should help the model f(y | z, x) help at all? \n- Given such a trivial z, one can just assume that your model is completely factorial i.e. P(y|x) = \\prod_{i} P(y_i|x) since the intermediate z has no information on the y's except it's length.\n\nThis is quite suspicious to me, and it seems that if this works, then a completely factorial model should work as well if we only use the \"hints\" from the ART teacher model. This is a red flag to me, and I am finding this hard to believe.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper781/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hint-based Training for Non-Autoregressive Translation", "abstract": "Machine translation is an important real-world application, and neural network-based AutoRegressive Translation (ART) models have achieved very promising accuracy. Due to the unparallelizable nature of the autoregressive factorization, ART models have to generate tokens one by one during decoding and thus suffer from high inference latency. Recently, Non-AutoRegressive Translation (NART) models were proposed to reduce the inference time. However, they could only achieve inferior accuracy compared with ART models. To improve the accuracy of NART models, in this paper, we propose to leverage the hints from a well-trained ART model to train the NART model. We define two hints for the machine translation task: hints from hidden states and hints from word alignments, and use such hints to regularize the optimization of NART models. Experimental results show that the NART model trained with hints could achieve significantly better translation performance than previous NART models on several tasks. In particular, for the WMT14 En-De and De-En task, we obtain BLEU scores of 25.20 and 29.52 respectively, which largely outperforms the previous non-autoregressive baselines. It is even comparable to a strong LSTM-based ART model (24.60 on WMT14 En-De), but one order of magnitude faster in inference.", "keywords": ["Natural Language Processing", "Machine Translation", "Non-Autoregressive Model"], "authorids": ["lizhuohan@pku.edu.cn", "dihe@microsoft.com", "fetia@microsoft.com", "taoqin@microsoft.com", "wanglw@cis.pku.edu.cn", "tyliu@microsoft.com"], "authors": ["Zhuohan Li", "Di He", "Fei Tian", "Tao Qin", "Liwei Wang", "Tie-Yan Liu"], "TL;DR": "We develop a training algorithm for non-autoregressive machine translation models, achieving comparable accuracy to strong autoregressive baselines, but one order of magnitude faster in inference.  ", "pdf": "/pdf/1b4a07fd6e3d0fe9980093a28d25f5cda6ad8856.pdf", "paperhash": "li|hintbased_training_for_nonautoregressive_translation", "_bibtex": "@misc{\nli2019hintbased,\ntitle={Hint-based Training for Non-Autoregressive Translation},\nauthor={Zhuohan Li and Di He and Fei Tian and Tao Qin and Liwei Wang and Tie-Yan Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1gGpjActQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper781/Official_Review", "cdate": 1542234378609, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1gGpjActQ", "replyto": "r1gGpjActQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper781/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335801006, "tmdate": 1552335801006, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper781/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}