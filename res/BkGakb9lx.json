{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396408111, "tcdate": 1486396408111, "number": 1, "id": "SkeWhGUOg", "invitation": "ICLR.cc/2017/conference/-/paper167/acceptance", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper was fairly well received by the reviewers in terms of the underling idea but the fact that a very specialized problem was the focus of the paper held back reviewers from giving stronger ratings. The question of what sorts of baselines would be reasonable was discussed extensively as the reviewers felt that other credible baselines should be included. The authors argue certain baselines are not appropriate but they were not able to clearly sway the reviewers to a more positive rating based on their response to this issue. We recommend a workshop invitation for this paper.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396408642, "id": "ICLR.cc/2017/conference/-/paper167/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396408642}}}, {"tddate": null, "tmdate": 1485427206667, "tcdate": 1485427206667, "number": 9, "id": "HJ1GfUvvg", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "rkTJXsyDe", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Answer to about generic GAN", "comment": "Thank you for your response. You proposed two baselines, a cGAN version and an\nunconstrained GAN. As we previously explained (see general rebuttal letter),\nthese are no viable or credible baselines.\n\nAs you mentioned correctly, the cGAN approach requires labeled data. We do not\nhave such a dataset. Annotating enough data for this approach to represent\na credible baseline would be prohibitively expensive. Remember that we have\nmultiple labels such as 3D orientation and 12 binary labels for each sample.\nAnnotating one sample can easily take 2 minutes. If we would annotate 20 samples\nfor every possible ID, it would take a person 341 days. If we would have this\nkind of labeled data, we would directly use it to train the decoder network. The\nscarcity of labeled data is the main motivation for our approach and we think\nthere exist many similar problems that can be solved with the RenderGAN\napproach.\n\nSecondly, you proposed an \u201cunconstrained\u201d generator network as a baseline. We\nargue in our rebuttal:\n\n> One of our early approaches was to add an offset to the 3D model, i.e.\n> x = t + g(t) where x is the synthesized image, t is an image from the 3D model,\n> and g an unconstrained generator. However, in our experiments, the generator\n> learned to synthesize realistic images but ignored the given template\n> t completely. Thus, no valid labels of the synthetic images could be collected.\n\nAgain, we cannot use a generic GAN to add missing image characteristics. This\nintuitive notion is incorrect. The GAN generates images that deviate from the\ninput labels. The images might display a different ID or a different spatial\norientation compared to the input labels. Our experiments then led us to the\nidea of introducing constrained augmentations that ensure that the labels remain\nvalid in the output image. This is the central idea of the paper and as far as\nwe see a straight-forward solution to the problem stated above.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1484923621119, "tcdate": 1484923621119, "number": 2, "id": "rkTJXsyDe", "invitation": "ICLR.cc/2017/conference/-/paper167/official/comment", "forum": "BkGakb9lx", "replyto": "SJ9NXznSe", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "content": {"title": "about generic GAN", "comment": "Could a generic GAN indeed not be applied? One could train a conditional GAN conditioned on the ID, same way as class-conditional GANs are trained, that is, for example, by feeding the ID both to the generator and the discriminator. I would expect this to fail horribly, but it might in principle work. One would need labeled data to train it, though.\n\nAlternatively, in a more realistic scenario, one might keep the \"rendering\" part, but replace everything else by a generic network which takes this clean rendered image and some noise as input, and aims to generate a realistic marker image from that. This might not even require labeled data.\n\nOverall, I think these are somewhat reasonable baselines, and they would make the paper more complete, but I would not expect them to work well, or at least they would be much harder to tune.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703086, "id": "ICLR.cc/2017/conference/-/paper167/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703086}}}, {"tddate": null, "tmdate": 1484228421417, "tcdate": 1484228421417, "number": 8, "id": "B16Sw-BUl", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Updated paper", "comment": "We uploaded a new version of the paper based on the peer review feedback.\n\n* Clarified that an unconstrained GAN is not a suitable baseline.\n* Added additional references pointed out by reviewer 2.\n* Extracted a related work section to improve the overall clarity and also stated our contribution explicitly.\n* Various modifications to improve clarity."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484228321001, "tcdate": 1478262713875, "number": 167, "id": "BkGakb9lx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkGakb9lx", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "content": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": ["Hk-bBcWYe"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483707809791, "tcdate": 1483641871382, "number": 7, "id": "HJDMVfhSl", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "HkosYQzEx", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Rebuttal Reviewer 2", "comment": "Thank you very much for your review. Your comments have helped to improve the\npaper.  We think your main point of critique was that we did not evaluate our\nsystem sufficiently:\n\n> \u201cHowever, I won\u2019t champion the paper as the evaluation could be improved.\n> A critical missing baseline is a comparison against a generic GAN. Without this\n> it\u2019s hard to judge the benefit of the more structured GAN. \u201c\n\nSince similar comments were made by the other reviewers, we formulated a general\nresponse to this point. Please refer to the \u201cgeneral rebuttal\u201d in which we give\na detailed rationale of why a generic GAN cannot be used as a baseline as you\n(and others) suggested. We think this is a crucial point representing the\ncentral motivation of the RenderGAN method.\n\n> \u201cAlso, it would be worth seeing the result when one combines generated and\n>  real images for the final task.\u201d\n\nIn previous experiments, we used a mixture of real vs. generated data (5/95 and\n50/50). The real data was augmented to prevent overfitting as described in\nAppendix C in the paper. Both experiments did not result in significant\nimprovements, and we therefore decided not to include them in the paper. We\nagree that this might be interesting and we will update the manuscript\naccordingly.\n\nThank you for pointing out additional references. We will include both of them in our next update.\n\n> \u201cThe problem domain (decoding barcode markers on bees) is limited.  It would\n> be great to see this applied to another problem domain, e.g., object detection\n> from 3D models as shown in paper reference [A], where direct comparison against\n> prior work could be performed. \u201c\n\nWe are currently working on more popular problem domains. We, however, feel\nthat, although the problem has limited applicability for the general reader, it\nnonetheless represents a complex problem with analogies to other, potentially\nmore popular problems, like e.g. pose estimation for cars or faces.\n\n> \u201cI found the writing to be somewhat vague throughout.  For instance, on first\n> reading of the introduction it is not clear what exactly is the contribution of\n> the paper.\u201d\n\nThank you for this feedback. We are currently revising the manuscript to improve\noverall clarity and will update the paper in the following days.\n\nThank you also for your minor comments, the manuscript will be updated with\nappropriate changes.\n\n> \u201cFig 3 - Are these really renders from a 3D model? The images look like 2D\n> images, perhaps spatially warped via a homography.\u201d\n\nYes, the bee marker is represented by a 3D mesh which is rotated in 3D space\naccording to the parameters and then projected to the image plane. Here is\na link to the code (https://goo.gl/9UWuhi). This is a very simple 3D model and\nwe believe this is one of the advantages of the RenderGAN method. Only\na rudimentary object and camera model are required. All the rest of the imaging\nprocess, lighting, shadows, blur, reflections, compression artifacts, etc. is\nlearned by the RenderGAN.\n\n> \u201cPage 3: \"chapter\" => \"section\".\u201d\n \nChanged accordingly.\n\n> \u201cIn Table 2, what is the loss used for the DCNN?\u201d\n\nThe loss is the mean log-loss for the classification of the individual bits of\nthe barcode marker. We will include this in the caption of the figure in the\nnext revision.\n\n> \u201cFig 9 (a) - The last four images look like they have strange artifacts. \n> Can you explain these?\u201d\n\nWe mentioned them briefly in the text (Section 3, Training):\n\n> In some cases, the generator creates artifacts that destroy the images. As\n> these bad images are scored unrealistic by the discriminator without exception,\n> we can discard them for the training of the supervised algorithm.\n\nWe suspect that during training the generator can deceive the discriminator by\nproducing out of distribution samples i.e. very unrealistic images. The\ndiscriminator learns quickly to score them as fake, and now the gradient for the\ngenerator might be too small to escape this local minimum. However, this is only\na hypothesis which we did not investigate further.  The images with the strange\nartifacts are rated unrealistic by the discriminator without exception.\nTherefore, we can sort them out before training the DCNN.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1483641830913, "tcdate": 1483641830913, "number": 6, "id": "rJJxNM3Bx", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "SJVdBxG4x", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Rebuttal Reviewer 1", "comment": "\nThank you very much for your review. You summarized our work well, and your\nfeedback contributed significantly to improving our paper.\n\n> As Reviewer3 points out, it would be interesting to analyze if restricting GAN\n> to a fixed set of transformations is necessary here, [...].\n\nPlease see our general rebuttal for a detailed explanation why the\ntransformations are necessary and cannot be left out. We will revise the\nmanuscript to make this point clearer.\n\n> The downside is that experiments are limited to a fairly simple and\n> not-widely-known domain of honeybee marker classification.\n\nWe agree, there are more complex datasets e.g. ImageNet. However, our problem,\ni.e. extraction of the marker\u2019s ID and rotation in space, was unsolvable with\nstate-of-the-art supervised learning methods due to the time-consuming nature of\nmanual labeling.  We are currently working on more popular problem domains. We,\nhowever, feel that, although the problem has limited applicability for the\ngeneral reader, it nonetheless represents a complex problem with analogies to\nother, potentially more popular problems, like pose estimation for cars or\nfaces.\n\n> The authors should tone down their claims such as \u201cOur method is an improvement\n> over previous work  <...> Whereas previous work relied on real data for training\n> using pre-trained models or mixing real and generated data, we were able to\n> train a DCNN from scratch with generated data that performed well when tested on\n> real data. \u201c. This is not a fair comparison: the domain studied by authors in\n> this work is much simpler than what was studied in these previous works, so this\n> comparison is not appropriate.\n\nWe fully agree and will change the paragraph accordingly.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1483641742435, "tcdate": 1483641742435, "number": 5, "id": "ByLcQf3Sx", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "HJ-3KJzEe", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Rebuttal Reviewer 3", "comment": "\nThank you very much for your review. Your pre-review questions already helped to\nimprove the paper. However, we disagree with several points raised in your\nreview.\n\n> The main novelty are parametric modules that emulate different transformations\n> and artefact that allow to match the natural appearance.  [...] the proposed\n> method is more model driven that previous GAN models. But does it pay off? how\n> would a traditional GAN approach perform?  [...] The key proposal of the\n> submission seems parameterised modules that can be trained to match the real\n> data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs.\n\nWe disagree with your summary that our main contribution are augmentation\nfunctions that allow a GAN to learn the real data distribution. In fact, the\naugmentation functions are specifically designed to constrain the generative\ncapabilities of the model. A generic GAN cannot be used as a baseline for our\nmethod. Please refer to our general rebuttal for a more thorough reply.\n\n> Using a render engine to generate the initial sample appearance if of limited novelty.\n\nThe point of using a render engine is not to generate an initial appearance but\nto allow collecting a _labeled_ sample that subsequently is rendered more\nrealistic by a constrained GAN. To the best of our knowledge, this has never\nbeen done before.\n\n> The answers of the authors only partially addresses the point.\n\nThis is incorrect. We explained in our pre-review reply that a generic GAN\ncannot be used as a baseline. Furthermore, we elaborated on this point in our\ngeneral rebuttal. Could you please clarify your point if you feel that we still\ndidn\u2019t address your questions completely?\n\n> The authors reply that plenty of such augmentation was used and more details are\n> going to be provided in the appendix. it would have been appreciated if such\n> information was directly included in the revision - so that the procedure could\n> be directly checked. right now - this remains a point of uncertainty.\n\nIt was already included in the revision from 9 Dec 2016 which is based on your\npre-review feedback (see appendix C page 14).\n\n> The authors do evaluate the effect of hand tuning the transformation stages vs.\n> learning them. it would be great to also include results of including/excluding\n> stages completely\n\nWe agree, excluding different transformation stages would shed light on the\nrelative importance of each stage, and this would be an interesting aspect. We\nwill try to include this in the manuscript by the submission deadline. For now,\nthe differences in performance obtained by hand-designed and learned\naugmentations can be used as a rough proxy for the importance of the individual\naugmentations. For example, there is a large improvement in validation\nperformance when replacing the hand-designed lighting augmentation with\na learned one. We added a respective remark to the latest revision (9 Dec).\n\n> - and also reporting how much the initial jittering of the data helps.\n\nIt is not clear to us what you mean with initial jittering of the data. Would\nyou mind clarifying this point?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1483641650487, "tcdate": 1483641650487, "number": 4, "id": "SJ9NXznSe", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "General Rebuttal", "comment": "\nThank you very much for your reviews. Your feedback helped to improve the\nmanuscript significantly, and we are preparing a revised version of the\nmanuscript with changes outlined either below or in our responses to each\nreviewer. Multiple valid points of criticism were raised during the review\nprocess and have already been worked into the current version of the document.\nFor example, we included hand-designed augmentations for comparison with the\nlearned ones.\n\nHowever, in two of the three reviews there seems to be a major misunderstanding\nthat we would like to clarify here. Since this relates to the central finding of\nour paper, we would like to provide a detailed response to this point. We hope\nthat, in the light of this fact, the reviewer\u2019s rating of our contribution\u2019s\nimportance and novelty will be reconsidered.\n\n> Reviewer 2: \u201cA critical missing baseline is a comparison against a generic GAN.\n> Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also,\n> it would be worth seeing the result when one combines generated and real images\n> for the final task.\u201d\n\n> Reviewer 3: \u201c [...] the proposed method is more model driven that previous GAN\n> models. But does it pay off? how would a traditional GAN approach perform? [...]\n> The answers of the authors only partially addresses the point. The key proposal\n> of the submission seems parameterised modules that can be trained to match the\n> real data distribution. but it remains unclear why not a more generic\n> parameterisation can also do the job. E.g. a neural network - as done in regular\n> GANs. The benefit of introducing a stronger model is unclear.\u201d\n\nThe main point of critique here is that a comparison with a generic GAN\n(Goodfellow et al. 2014) is missing. This comment implies that both methods (GAN\nand RenderGAN) share the same task domain, which is incorrect. The task we\naddress is generating _labeled_ data. We emphasize that we do not refer to the\nbinary class label but rather to higher dimensional labels. In our example\nscenario, this corresponds to images of bee markers and their respective bit\nconfiguration (its ID) and rotations in 3D space. A generic GAN cannot generate\nlabels, it learns to generate realistic images _without_ labels. Ultimately, we\nwant to train a convnet (\u2018decoder network\u2019) in a supervised setting to map an\nimage to its respective labels. Thus, we need labeled samples and hence,\na conventional GAN cannot be used as a baseline! Stated formally: the RenderGAN\nsamples from the joint distribution p(l, x) of labels l and data x whereas\na conventional GAN can only sample from the data distribution p(x).\n\nThere are two alternative approaches to our RenderGAN, one being a conventional\n3-dimensional rendering pipeline that can be used to generate images of bee\nmarkers with known ID and spatial orientation. Secondly, one could train the\ndecoder network with manually labeled data. Both approaches have been\nimplemented and tested against the RenderGAN and do not perform satisfyingly. To\nimprove both alternatives\u2019 performance one would need to either tune the\nrendering pipeline to match the details of the real world imaging process, or\nlabel more data manually. Both measures are time-consuming and do not generalize\nwell when changing parts of the imaging process (lighting, cameras, compression,\netc.) or the marker design.\n\nOur approach is to extend a generic GAN by adding several network modules, the\nfirst being the network equivalent of a simple 3D model. Secondly, we learn\na number of parameterised augmentation functions. We would like to point out\nthat this approach was _not_ chosen to improve the generative capabilities of\nthe network but to constrain it in such a way that the image produced by the GAN\nis correct with respect to the labels fed into the network. In our use case,\neach image produced by the GAN has to preserve the given bit pattern and\nrotation in space provided by the 3D model for the labels to remain valid. This\npoint was already addressed in our paper and the pre-review questions:\n\n> Paper Introduction: \u201c[...] We constrain the augmentation of the images such that\n> the high-level information represented by the 3D model is preserved. The\n> RenderGAN framework allows us to generate images of which the labels are known\n> from the 3D model, and that also look strikingly real due to the GAN framework.\n> The training procedure of the RenderGAN framework does not require any labels.\n> We can generate high-quality, labeled data with a simple 3D model and a large\n> amount of unlabeled data.\u201d\n\n> Our reply to Reviewer 3: \u201c[...] The payoff is that we can generate labeled data\n> with only a simple 3D model and unlabeled data. You are right. A DCGAN\n> architecture can model all mentioned effects, even affine transformations. We\n> trained a DCGAN on the data, and the quality of the synthesized images is\n> similar. However, no labels can be collected in the conventional GAN framework.\n> [...]\u201d\n\nAll reviewers question the necessity of the constraints we introduced. One of\nour early approaches was to add an offset to the 3D model, i.e. x = t + g(t)\nwhere x is the synthesized image, t is an image from the 3D model, and g an\nunconstrained generator. However, in our experiments, the generator learned to\nsynthesize realistic images but ignored the given template t completely. Thus,\nno valid labels of the synthetic images could be collected. Since a decoder\nnetwork cannot be trained without labels, this approach cannot be used as\na baseline. We will revise our paper to clarify that an unconstrained GAN is not\na suitable baseline for our task.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1481943872627, "tcdate": 1481943459192, "number": 3, "id": "HkosYQzEx", "invitation": "ICLR.cc/2017/conference/-/paper167/official/review", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer2"], "content": {"title": "The proposed model has potential merits, but the paper is missing a critical baseline in the evaluation.", "rating": "5: Marginally below acceptance threshold", "review": "This paper addresses the problem of decoding barcode-like markers depicted in an image.  The main insight is to train a CNN from generated data produced from a GAN.  The GAN is trained using unlabeled images, and leverages a \"3D model\" that undergoes learnt image transformations (e.g., blur, lighting, background).  The parameters for the image transformations are trained such that it confuses a GAN discriminator.  A CNN is trained using images generated from the GAN and compared with hand-crafted features and from training with real images.  The proposed method out-performs both baselines on decoding the barcode markers.\n\nThe proposed GAN architecture could potentially be interesting.  However, I won\u2019t champion the paper as the evaluation could be improved.\n\nA critical missing baseline is a comparison against a generic GAN.  Without this it\u2019s hard to judge the benefit of the more structured GAN.  Also, it would be worth seeing the result when one combines generated and real images for the final task. \n\nA couple of references that are relevant to this work (for object detection using rendered views of 3D shapes):\n\n[A] Xingchao Peng, Baochen Sun, Karim Ali, Kate Saenko, Learning Deep Object Detectors from 3D Models; ICCV, 2015.\n\n[B] Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views. Francisco Massa, Bryan C. Russell, Mathieu Aubry. CVPR 2016.\n\nThe problem domain (decoding barcode markers on bees) is limited.  It would be great to see this applied to another problem domain, e.g., object detection from 3D models as shown in paper reference [A], where direct comparison against prior work could be performed.  \n\nI found the writing to be somewhat vague throughout.  For instance, on first reading of the introduction it is not clear what exactly is the contribution of the paper.  \n\nMinor comments:\n\nFig 3 - Are these really renders from a 3D model?  The images look like 2D images, perhaps spatially warped via a homography.  \n\nPage 3: \"chapter\" => \"section\".\n\nIn Table 2, what is the loss used for the DCNN?\n\nFig 9 (a) - The last four images look like they have strange artifacts. Can you explain these?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676306, "id": "ICLR.cc/2017/conference/-/paper167/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper167/AnonReviewer3", "ICLR.cc/2017/conference/paper167/AnonReviewer1", "ICLR.cc/2017/conference/paper167/AnonReviewer2"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676306}}}, {"tddate": null, "tmdate": 1481930091721, "tcdate": 1481930091721, "number": 2, "id": "SJVdBxG4x", "invitation": "ICLR.cc/2017/conference/-/paper167/official/review", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an approach to generating synthetic training data for deep networks, based on rendering 3D models and learning additional transformations with adversarial training. The approach is applied to generating barcode-like markers used for honeybee identification. The authors demonstrate that a classifier trained on synthetic data generated with the proposed approach outperforms both training on (limited) real data and training on data with hand-designed augmentations. \n\nThe topic of the paper \u2014 using machine learning (in particular, adversarial training) for generating realistic synthetic training data \u2014 is very interesting and important. The proposed method looks reasonable, and the paper is written well. The downside is that experiments are limited to a fairly simple and not-widely-known domain of honeybee marker classification. While I am sure this is an important task by itself, in order to demonstrate general applicability of the method and to allow comparison with existing techniques, experiments on some standard and/or realistic datasets would be very helpful. Overall, I recommend acceptance, but encourage the authors to perform experiments on more datasets.\n\nI appreciate that the authors added a baseline with manually designed transformations. This strengthens the paper.\n\nAs Reviewer3 points out, it would be interesting to analyze if restricting GAN to a fixed set of transformations is necessary here, and which transformations are most important. Perhaps this would provide some guidelines for designing sets of transformations for more complicated scenarios.\n\nThe authors should tone down their claims such as \u201cOur method is an improvement over previous work  <...> Whereas previous work relied on real data for training using pre-trained models or mixing real and generated data, we were able to train a DCNN from scratch with generated data that performed well when tested on real data. \u201c. This is not a fair comparison: the domain studied by authors in this work is much simpler than what was studied in these previous works, so this comparison is not appropriate.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676306, "id": "ICLR.cc/2017/conference/-/paper167/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper167/AnonReviewer3", "ICLR.cc/2017/conference/paper167/AnonReviewer1", "ICLR.cc/2017/conference/paper167/AnonReviewer2"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676306}}}, {"tddate": null, "tmdate": 1481927081177, "tcdate": 1481927081177, "number": 1, "id": "HJ-3KJzEe", "invitation": "ICLR.cc/2017/conference/-/paper167/official/review", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer3"], "content": {"title": "RenderGAN: Generating Realistic Labeled Data", "rating": "6: Marginally above acceptance threshold", "review": "The submission proposes an interesting way to match synthetic data to real data in a GAN type architecture.\nThe main novelty are parametric modules that emulate different transformations and artefact that allow to match the natural appearance.\n\nseveral points were raised during the discussion:\n\n1. the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\nThe answers of the authors only partially addresses the point. The key proposal of the submission seems parameterised modules that can be trained to match the real data distribution. but it remains unclear why not a more generic parameterisation can also do the job. E.g. a neural network - as done in regular GANs. The benefit of introducing a stronger model is unclear. Using a render engine to generate the initial sample appearance if of limited novelty.\n\n\n2. how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\nThe authors reply that plenty of such augmentation was used and more details are going to be provided in the appendix. it would have been appreciated if such information was directly included in the revision - so that the procedure could be directly checked. right now - this remains a point of uncertainty.\n\n3. How do the different stages (\\phis) effect performance? which are the most important ones?\nThe authors do evaluate the effect of hand tuning the transformation stages vs. learning them. it would be great to also include results of including/excluding stages completely - and also reporting how much the initial jittering of the data helps.\n\nWhile there is an interesting idea of (limited) novelty to the paper, there are some concerns about evalations and comparisons as outlined above. In addition, only success on a single dataset/task is shown. Yet the task is interesting and seems challenging. Overall, this remains makes only a weak recommendation for acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512676306, "id": "ICLR.cc/2017/conference/-/paper167/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper167/AnonReviewer3", "ICLR.cc/2017/conference/paper167/AnonReviewer1", "ICLR.cc/2017/conference/paper167/AnonReviewer2"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512676306}}}, {"tddate": null, "tmdate": 1481290056397, "tcdate": 1481290056391, "number": 3, "id": "ryl8W4umg", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "Update paper", "comment": "We updated our paper based on the feedback from the pre-review questions.  We\nincluded handmade augmentation in the evaluation.  We also retrained the DCNN on\nthe real data. Thanks for the feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1480960440269, "tcdate": 1480960420254, "number": 2, "id": "Hk2sFXQQe", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "S1NOaOkXl", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "pre-review answer", "comment": "> the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\n\nThe payoff is that we can generate labeled data with only a simple 3D model\nand unlabeled data. You are right. A DCGAN architecture can model all mentioned\neffects, even affine transformations. We trained a DCGAN on the data, and\nthe quality of the synthesized images is similar. However, no labels can be collected in the conventional GAN framework. The latent space is too entangled, i.e. there is no simple relationship of the labels to the latent space. A conditional GAN can be used to generated labeled data but also requires labeled data for training.\n\n\n> how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\n\nIs your question about augmenting the images of the 3D model or the real data?\nFor the 3D model, see our answer to question 2. from reviewer number 1.\nThe real data that is used for training is highly augmented. This is mentioned in the result section (random translations, rotations, shear transformations, histogram scaling, and noise). We will include the specific parameters used to augment the data in the appendix. \n\n\n> How do the different stages (\\phis) effect performance? Which are the most important ones?\n\nWe agree that this should be addressed in the paper. We are currently evaluating the effects of the different stages on the network performance by replacing learned augmentations (\\phis) with hand-designed augmentations (see also our reply to reviewer 1 / question 2). We will update the paper shortly to include this evaluation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1480960347705, "tcdate": 1480960347698, "number": 1, "id": "rkNPFmmme", "invitation": "ICLR.cc/2017/conference/-/paper167/public/comment", "forum": "BkGakb9lx", "replyto": "ByZazmk7x", "signatures": ["~Leon_Sixt1"], "readers": ["everyone"], "writers": ["~Leon_Sixt1"], "content": {"title": "pre-review answer", "comment": "> 1. Would it make sense to use a differentiable renderer (like OpenDR by Loper & Black) in your framework ?\n\n\nThanks for the suggestion. Yes, it would make sense. In future work, this could make it possible to partially infer the 3D model from data.\n\n\n> 2. Have you tried a baseline approach to synthetic data generation with simple parametric distributions of orientations and augmentations, hand-designed or estimated with simple statistics? Basically, do you really need a GAN to  learn them in this relatively simple scenario, or can a human quickly make up   a reasonable distribution of transformations which will perform as well?\n\n\nWe initially tried a very basic approach to augment the 3D model data with only noise and shifting of the pixel intensities. A model trained on this data did not generalize to real data.\nBased on your suggestion, we implemented hand designed augmentations similar to the ones learned by the GAN. Preliminary results indicate a network trained on this data overfits much more than when trained on the GAN data. We will update the paper shortly. \n\n\n> 3. Do you think your work is related to Style and Structure Adversarial Networks\n   https://arxiv.org/pdf/1603.05631.pdf , Spatial Transformers\n   https://arxiv.org/abs/1506.02025 ?\n\n\nIn the Style and Structure Adversarial Networks paper, a GAN is adapted to not only generate\nimages but also to capture the structure representation in the form of a normal map.\nContrary to our work, their training procedure requires large amounts of labeled data.\nThey rely on 200K normal maps and matching real images from\nthe NYU Depth Dataset V2.  Still, Wang and Gupta successfully learn to separate\nstyle from structure. Something we are also able to achieve with the RenderGAN\nframework. The 3D model would be the structure and the augmentations the style.\nWe think this work is related to our work and we should mention it.\n\nSpatial Transformers (STs) learn an affine transformation of\nconvolutional feature maps. It was shown that this can improve the performance\non rotated or otherwise deformed data.\nOur approach is only related to STs such that STs could\nbe used as an augmentation function. For example, in our application the 3D model directly incorporates\ntranslation and rotation, it should instead be possible to learn them with a ST.\nIn future work, ST could help to reduce the complexity of the 3D model.\n\n\n> 4. Have you had a chance to run experiments on other datasets?\n\nCurrently, we are investigating to apply the RenderGAN framework on a different\ndataset but we have no results yet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287703210, "id": "ICLR.cc/2017/conference/-/paper167/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkGakb9lx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper167/reviewers", "ICLR.cc/2017/conference/paper167/areachairs"], "cdate": 1485287703210}}}, {"tddate": null, "tmdate": 1480719723982, "tcdate": 1480719723976, "number": 2, "id": "S1NOaOkXl", "invitation": "ICLR.cc/2017/conference/-/paper167/pre-review/question", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer3"], "content": {"title": "pre-review questions", "question": "- the proposed method is more model driven that previous GAN models. But does it pay off? how would a traditional GAN approach perform? The mentioned effects like blur, lighting and background could also potentially be modelled by upsamling network that directly predicts the image. I would assume that blur and lighting can be modelled by convolutions. transformations to some extend by convolutions - or spatial transformer networks.\n- how does it compare to traditional data augmentation techniques, e.g. noise, dropout, transformations. you are linking to keras code - where data augmentation is readily available and could be tested (ImageDataGenerator)\n- How do the different stages (\\phis) effect performance? which are the most important ones?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959428075, "id": "ICLR.cc/2017/conference/-/paper167/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper167/AnonReviewer1", "ICLR.cc/2017/conference/paper167/AnonReviewer3"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959428075}}}, {"tddate": null, "tmdate": 1480696504986, "tcdate": 1480696504981, "number": 1, "id": "ByZazmk7x", "invitation": "ICLR.cc/2017/conference/-/paper167/pre-review/question", "forum": "BkGakb9lx", "replyto": "BkGakb9lx", "signatures": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper167/AnonReviewer1"], "content": {"title": "pre-review questions", "question": "Hello,\n\nAn interesting idea! A couple of questions:\n\n1. Would it make sense to use a differentiable renderer (like OpenDR by Loper & Black) in your framework ?\n\n2. Have you tried a baseline approach to synthetic data generation with simple parametric distributions of orientations and augmentations, hand-designed or estimated with simple statistics? Basically, do you really need a GAN to learn them in this relatively simple scenario, or can a human quickly make up a reasonable distribution of transformations which will perform as well ?\n\n3. Do you think your work is related to Style and Structure Adversarial Networks https://arxiv.org/pdf/1603.05631.pdf , Spatial Transformers https://arxiv.org/abs/1506.02025 ?\n\n4. Have you had a chance to run experiments on other datasets? \n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "RenderGAN: Generating Realistic Labeled Data", "abstract": "Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable performance on many computer vision tasks.  Due to their large parameter space, they require many labeled samples when trained in a supervised setting. The costs of annotating data manually can render the use of DCNNs infeasible.  We present a novel framework called RenderGAN that can generate large amounts of realistic, labeled images by combining a 3D model and the Generative Adversarial Network framework. In our approach, image augmentations (e.g. lighting, background, and detail) are learned from unlabeled data such that the generated images are strikingly realistic while preserving the labels known from the 3D model.  We apply the RenderGAN framework to generate images of barcode-like markers that are attached to honeybees. Training a DCNN on data generated by the RenderGAN yields considerably better performance than training it on various baselines. ", "pdf": "/pdf/1eec1561c58f263ff8ff3fb0fac669559d09b991.pdf", "TL;DR": "We embed a 3D model in the GAN framework to generate realistic, labeled data.", "paperhash": "sixt|rendergan_generating_realistic_labeled_data", "conflicts": ["fu-berlin.de"], "keywords": ["Unsupervised Learning", "Computer vision", "Deep learning", "Applications"], "authors": ["Leon Sixt", "Benjamin Wild", "Tim Landgraf"], "authorids": ["leon.sixt@fu-berlin.de", "benjamin.wild@fu-berlin.de", "tim.landgraf@fu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959428075, "id": "ICLR.cc/2017/conference/-/paper167/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper167/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper167/AnonReviewer1", "ICLR.cc/2017/conference/paper167/AnonReviewer3"], "reply": {"forum": "BkGakb9lx", "replyto": "BkGakb9lx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper167/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959428075}}}], "count": 17}