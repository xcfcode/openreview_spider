{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487838043939, "tcdate": 1478295234565, "number": 467, "id": "S1_pAu9xl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1_pAu9xl", "signatures": ["~song_han1"], "readers": ["everyone"], "content": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396600341, "tcdate": 1486396600341, "number": 1, "id": "B1x63f8Ox", "invitation": "ICLR.cc/2017/conference/-/paper467/acceptance", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper describes a method for training neural networks with ternary weights. The results are solid and have a potential for high impact on how networks for high-speed and/or low-power inference are trained.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396600873, "id": "ICLR.cc/2017/conference/-/paper467/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396600873}}}, {"tddate": null, "tmdate": 1484983279921, "tcdate": 1484983279921, "number": 10, "id": "BydehKxvl", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "rk-BOtQIx", "signatures": ["~song_han1"], "readers": ["everyone"], "writers": ["~song_han1"], "content": {"title": "added motivation", "comment": "We have added a section discussing the motivation to the paper:\n\nReducing the size of a DNN model makes the deployment on edge devices easier.\nFirst, a smaller model means less overhead when exporting models to clients. Take autonomous driving for example; Tesla periodically copies new models from their servers to customers\u2019 cars. Smaller models require less communication in such over-the-air updates, making frequent updates more feasible. Another example is on Apple Store; apps above 100 MB will not download until you connect to Wi-Fi. It\u2019s infeasible to put a large DNN model in an app. The second issue is energy consumption. Deep learning is energy consuming, which is problematic for battery-constrained mobile devices. As a result, iOS 10 requires iPhone to be plugged with charger while performing photo analysis. Fetching DNN models from memory takes more than two orders of magnitude more energy than arithmetic operations. Smaller neural networks require less memory bandwidth to fetch the model, saving the energy and extending battery life. The third issue is area cost. When deploying DNNs on Application-Specific Integrated Circuits (ASICs), a sufficiently small model can be stored directly on-chip, and smaller models enable a smaller ASIC die."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1484451961484, "tcdate": 1484451961484, "number": 9, "id": "HkWteduLe", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "rk-BOtQIx", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "Motivations", "comment": "Thanks for your review. After reading your review, we went through our paper and found indeed lack of motivations for our work. We are working on more background about the field of neural network compression and the motivation of using our TTQ method. We will soon upload a revised version of the paper.\n\nChenzhuo"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1484451699129, "tcdate": 1484451699129, "number": 8, "id": "ryodyOuLg", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "Code Release", "comment": "Dear all,\n\nRecently we released our code on GitHub (https://github.com/czhu95/ternarynet). The repository includes everything you need to replicate all the experiments described in our paper and you are welcomed to conduct further experiments on our work.\n\nWe also thank all the reviewers for your comments!\n\nChenzhuo"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1484130361213, "tcdate": 1484130361213, "number": 4, "id": "rk-BOtQIx", "invitation": "ICLR.cc/2017/conference/-/paper467/official/comment", "forum": "S1_pAu9xl", "replyto": "ByKdvG7Ux", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer4"], "content": {"title": "References?", "comment": "If the mentioned research was properly referenced in the paper I would be willing to reconsider my evaluation. As of now the paper does not provide a significant motivation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564814, "id": "ICLR.cc/2017/conference/-/paper467/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564814}}}, {"tddate": null, "tmdate": 1484101489752, "tcdate": 1484101489752, "number": 3, "id": "ByKdvG7Ux", "invitation": "ICLR.cc/2017/conference/-/paper467/official/comment", "forum": "S1_pAu9xl", "replyto": "r17NNyvrx", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "content": {"title": "Respectfully disagree on relevance to the field.", "comment": "Efficient model compression techniques that don't sacrifice (and possibly enhance) real-time performance are hugely important to industrial applications, and are the focus of a lot of industrial research at the moment. I would be weary of discouraging research on the topic by qualifying the whole problem as niche research."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564814, "id": "ICLR.cc/2017/conference/-/paper467/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564814}}}, {"tddate": null, "tmdate": 1483301930702, "tcdate": 1483301930702, "number": 4, "id": "r17NNyvrx", "invitation": "ICLR.cc/2017/conference/-/paper467/official/review", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer4"], "content": {"title": "Somewhat interesting, but incremental work lacking sufficient practical motivation", "rating": "3: Clear rejection", "review": "The paper shows a different approach to a ternary quantization of weights.\nStrengths:\n1. The paper shows performance improvements over existing solutions\n2. The idea of learning the quantization instead of using pre-defined human-made algorithm is nice and very much in the spirit of modern machine learning.\n\nWeaknesses:\n1. The paper is very incremental.\n2. The paper is addressed to a very narrow audience. The paper very clearly assumes that the reader is familiar with the previous work on the ternary quantization. It is \"what is new in the topic\" update, not really a standalone paper. The description of the main algorithm is very concise, to say the least, and is probably clear to those who read some of the previous work on this narrow subject, but is unsuitable for a broader deep learning audience.\n3. There is no convincing motivation for the work. What is presented is an engineering gimmick, that would be cool and valuable if it really is used in production, but is that really needed for anything? Are there any practical applications that require this refinement? I do not find the motivation \"it is related to mobile, therefore it is cool\" sufficient.\n\nThis paper is a small step further in a niche research, as long as the authors do not provide a sufficient practical motivation for pursuing this particular topic with the next step on a long list of small refinements, I do not think it belongs in ICLR with a broad and diversified audience.\n\nAlso - the code was not released is my understanding.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483301931377, "id": "ICLR.cc/2017/conference/-/paper467/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2", "ICLR.cc/2017/conference/paper467/AnonReviewer1", "ICLR.cc/2017/conference/paper467/AnonReviewer4"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483301931377}}}, {"tddate": null, "tmdate": 1482936323513, "tcdate": 1482936323513, "number": 3, "id": "H1jbg8bBx", "invitation": "ICLR.cc/2017/conference/-/paper467/official/review", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer1"], "content": {"title": "Trained Ternary Quantization", "rating": "7: Good paper, accept", "review": "This paper presents new way for compressing CNN weights. In particular this paper uses a new neural network quantization method that compresses network weights to ternary values.\nThe group has recently published multiple paper on this topic, and this one offers possibly the lowest returns I have seen. Only a fraction of percentage in ImageNet. Results on AlexNet are of very little interest now, given the group already showed this kind of older style-network can be compressed by large amounts. \nI also would have liked to see this group release code for the compression, and also report data on the amount of effort required to compress: flops, time, number of passes, required original dataset, etc. This data is important to decide if a compression is worth the effort.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483301931377, "id": "ICLR.cc/2017/conference/-/paper467/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2", "ICLR.cc/2017/conference/paper467/AnonReviewer1", "ICLR.cc/2017/conference/paper467/AnonReviewer4"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483301931377}}}, {"tddate": null, "tmdate": 1482415077955, "tcdate": 1482415077955, "number": 7, "id": "HJAJ2LFEx", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "why two different scaling parameters for positive and negative weights?", "comment": "I noticed that in Figure 2, the two quantization factors for the same layer are always almost the same after convergence, what is the intuition that you need different scaling parameters for positive and negative weights? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1481910290669, "tcdate": 1481910226212, "number": 2, "id": "BkcRws-4l", "invitation": "ICLR.cc/2017/conference/-/paper467/official/review", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This work presents a novel ternary weight quantization approach which quantizes weights to either 0 or one of two layer specific learned values. Unlike past work, these quantized values are separate and learned stochastically alongside all other network parameters. This approach achieves impressive quantization results while retaining or surpassing corresponding full-precision networks on CIFAR10 and ImageNet.\n\nStrengths:\n\n- Overall well written and algorithm is presented clearly.\n- Approach appears to work well in the experiments, resulting in good compression without loss (and sometimes gain!) of performance.\n- I enjoyed the analysis of sparsity (and how it changes) over the course of training, though it is uncertain if any useful conclusion can be drawn from it.\n\nSome points:\n\n- The energy analysis in Table 3 assumes dense activations due to the unpredictability of sparse activations. Can the authors provide average activation sparsity for each network to help verify this assumption. Even if the assumption does not hold, relatively close values for average activation between the networks would make the comparison more convincing.\n\n- In section 5.1.1, the authors suggest having a fixed t (threshold parameter set at 0.05) for all layers allows for varying sparsity (owed to the relative magnitude of different layer weights with respect to the maximum). In Section 5.1.2 paragraph 2, this is further developed by suggesting additional sparsity can be achieved by allowing each layer a different values of t. How are these values set? Does this multiple threshold style network appear in any of the tables or figures? Can it be added?\n\n- The authors claim \"ii) Quantized weights play the role of \"learning rate multipliers\" during back propagation.\" as a benefit of using trained quantization factors. Why is this a benefit? \n\n- Figure and table captions are not very descriptive.\n\nPreliminary Rating:\nI think this is an interesting paper with convincing results but is somewhat lacking in novelty. \n\nMinor notes:\n- Table 3 lists FLOPS rather than Energy for the full precision model. Why?\n- Section 5 'speeding up'\n- 5.1.1 figure reference error last line\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483301931377, "id": "ICLR.cc/2017/conference/-/paper467/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2", "ICLR.cc/2017/conference/paper467/AnonReviewer1", "ICLR.cc/2017/conference/paper467/AnonReviewer4"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483301931377}}}, {"tddate": null, "tmdate": 1481718627427, "tcdate": 1481718627421, "number": 6, "id": "HJivs2RQe", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "SkUPszR7g", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "reply", "comment": "Thanks a lot for your review!\nFor the weaknesses that you pointed out, I'd like to make some comment:\n1) The main difference between our method and BWN/TWN is we use asymmetric and independent scaling factors Wn and Wp. By independent we mean there are not calculated from latent weights (BWN and TWN take E(|w| as the scaling factor) but totally learned using gradient descent. \n2) I admit it is a flaw of our work that we didn't quantize the first and last layer. However, I want to emphasize that we didn't quantize these two layers for comparison groups either, which ensures fairness.\n3) Despite we don't have large accuracy advantage over BWN on ImageNet, our latest experiments on ResNet-18 displayed a notable improvement (66.7% top1, please see comments below for more information) over BWN (60.8% top1). We are willing to add our ResNet experiments to the paper.\n4) We implemented our AlexNet model based on DoReFa-Net implementation (https://github.com/ppwwyyxx/tensorpack/) and cited their baseline accuracy. \n5) We are willing to add ResNet results to the paper.\n6) As the last paragraph of introduction session states, multiplications can be precomputed with data-fetching process, therefore no multipliers are needed during forward computation. Furthermore, as ternary weights can be regarded as binary weights + zeros, the large portion of zeros can be skipped, and positive/negative weights can be implemented with multiplexers. \n\nWe will soon enrich our experiments and revise the paper. Thanks again for your review!\n\nChenzhuo"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1481687992808, "tcdate": 1481677661640, "number": 1, "id": "SkUPszR7g", "invitation": "ICLR.cc/2017/conference/-/paper467/public/review", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Incremental improvement, experimental issues", "rating": "3: Clear rejection", "review": "This paper presents a ternary quantization method for convolutional neural networks. All weights are represented by ternary values multiplied by two scaling coefficients. Both ternary weights and the scaling coefficients are updated using back-propagation. This is useful for CNN model compression. Experiments on AlexNet show that the proposed method is superior Ternary-Weight-Networks (TWN) and DoReFa-Net. This work has the following strengths and weaknesses.\n\nStrengths:\n(1). Good results are shown on CIFAR-10 dataset.\n\n(2). Massive energy saving of the ternary weights comparing to 32-bit weights.\n\n(3). It is well written, and easy to understand.\n\nWeaknesses:\n\n(1). It seems that this work is an incremental improvement on the existing works. The main difference from Binary-Weight-Networks (BWN) proposed in XNOR-net[1] is using ternary weights instead of binary weights, while ternary weights have been used by many previous works. Both BWN and the proposed method in this paper learn the scaling factors during training. Comparing to Ternary-Weight-Networks (TWN), the main difference is that two independent quantization factors are used for positive and negative weights, while TWN utilizes the same scaling factor for all weights. \n\n(2). In the experiment, the authors did not process the first conv layer and the last fully-connected layer. The results of processing all layers should be given for fair comparison. \n\n(3). In the experiment, comparison with BWN is not reported. In [1], the top-1 and top-5 error of AlexNet on ImageNet of BWN is 43.2% and 20.6%, which is comparable with the method proposed in this paper (42.5% and 20.3%). However, the BWN only uses binary weights and all layers are binarized including the first conv layer and the last fully-connected layer. \n\n(4). For the baseline method of full precision alexnet (with BN), the reported accuracy seems to be too low (44.1% top-1 error). Commonly, using batch normalization can boost the accuracy, while the reported accuracy are much lower than alexnet without batch normalization. On the other hand, the error rates of pre-trained model of alexnet (with BN) reported by the official MatConvNet [2] are 41.8% and 19.2%. \n\n(5). The proposed method should be evaluated on the original AlexNet, whose accuracy is publicly available for almost all deep learning frameworks like caffe. Moreover, more experiments should be added on ImageNet, like VGG-S, VGG-16, GoogleNet or ResNet.\n\n(6). In previous methods such as XNOR-net and TWN, most of the 32-bit multiply operation can be replaced by addition by using binary or ternary weights. However, the proposed method in this paper utilizes independent scaling factors for positive and negative weights. Thus it seems that the multiply operation can not be replaced by addition. \n\n\nReferences:\n[1] Mohammad R, Vicente O, Joseph R, Ali F. XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks. ECCV 2016\n[2] http://www.vlfeat.org/matconvnet/pretrained/#imagenet-ilsvrc-classification\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1482512904380, "id": "ICLR.cc/2017/conference/-/paper467/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu", "ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs", "(anonymous)"], "cdate": 1482512904380}}}, {"tddate": null, "tmdate": 1481167063603, "tcdate": 1481167063597, "number": 5, "id": "BJg1ZULQe", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "SJV7aQr7x", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "TTQ ResNet18 on ImageNet", "comment": "We are still working on TTQ ResNet-18 on ImageNet. Based on following conditions we obtained the above results:\n1. Yes we left firt conv and last fc layer un-quantized.\n2. Using Wn and Wp initialized to ones seems to be too big for ImageNet, therefore we scaled down Wn and Wp by the mean absolute value of latent weights of each layer.\n3. We trained the model from scratch.\n\nChenzhuo"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1481092379977, "tcdate": 1481092379908, "number": 4, "id": "SJV7aQr7x", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "rysgf9X7g", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "implement", "comment": "hi,\n\nI want to re-implement the the TTQ models with a ResNet-18 on ImageNet, in this paper, you did not quantize the AlexNet weights of the first convolutional layer and the last fully-connected layer to ternary weight, what about ResNet-18\uff1fThe details is really important to me."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1480987122903, "tcdate": 1480987122897, "number": 3, "id": "rysgf9X7g", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "ryX3eo1mg", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "ResNet from scratch", "comment": "Yes we did experiment to train a ResNet18 from scratch on CIFAR-10 and ImageNet\n1) On CIFAR-10, training from scratch results in around 0.1% accuracy loss compared to fine-tuned model. The accuracy drop various with hyper-parameter t or r (section 3.2).\n2) On ImageNet, our TTQ models (from scratch) show better results compared to TWN (fine-tuned). For ResNet18, we reached error rate of 34.7% (TWN gets 38.2%); For ResNet18-B, we reached 33.3% (TWN gets 34.7%). \n\nChenzhuo"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "ddate": null, "writable": true, "revisions": false, "tmdate": 1480752469609, "tcdate": 1480752257584, "number": 2, "replyCount": 0, "id": "H1qthgeQx", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "AlexNet on imagenet", "comment": "I noticed that when you implement AlexNet on imagenet\uff0cthat keep the first layer and the the last layer with 32 bit float point\uff0cwhy not transform all layers into ternary? if all layers are ternary weights, Compared to original TWN, How much to improve the accuracy\uff1f  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1480728746589, "tcdate": 1480728746584, "number": 2, "id": "ryX3eo1mg", "invitation": "ICLR.cc/2017/conference/-/paper467/pre-review/question", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer2"], "content": {"title": "ResNet Training", "question": "Have you experimented with training ResNet from scratch (on either CIFAR-10 or ImageNet)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959265917, "id": "ICLR.cc/2017/conference/-/paper467/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959265917}}}, {"tddate": null, "tmdate": 1480705890608, "tcdate": 1480705890603, "number": 1, "id": "H1iDPByXe", "invitation": "ICLR.cc/2017/conference/-/paper467/public/comment", "forum": "S1_pAu9xl", "replyto": "SJQBIcdGg", "signatures": ["~Chenzhuo_Zhu1"], "readers": ["everyone"], "writers": ["~Chenzhuo_Zhu1"], "content": {"title": "Source code release", "comment": "We do have a plan to release the source code. The expected date should be around Feb. 2017."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564946, "id": "ICLR.cc/2017/conference/-/paper467/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1_pAu9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper467/reviewers", "ICLR.cc/2017/conference/paper467/areachairs"], "cdate": 1485287564946}}}, {"tddate": null, "tmdate": 1480701896662, "tcdate": 1480701896655, "number": 1, "id": "S1b0vEymg", "invitation": "ICLR.cc/2017/conference/-/paper467/official/review", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "content": {"title": "Simple premise, well argued, useful result, exhaustive analysis.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper studies in depth the idea of quantizing down convolutional layers to 3 bits, with a different positive and negative per-layer scale. It goes on to provide an exhaustive analysis of performance (essentially no loss) on real benchmarks (this paper is remarkably MNIST-free).\n\nThe relevance of this paper is that it likely provides a lower bound on quantization approaches that don't sacrifice any performance, and hence can plausibly become the approach of choice for resource-constrained inference, and might suggest new hardware designs to take advantage of the proposed structure.\n\nFurthermore, the paper provides power measurements, which is really the main metric that anyone working seriously in that space cares about. (Nit: I don't see measurements for the full-precision baseline).\n\nI would have loved to see a SOTA result on ImageNet and a result on a strong LSTM baseline to be fully convinced.\nI would have also liked to see discussion of the wall time to result using this training procedure.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483301931377, "id": "ICLR.cc/2017/conference/-/paper467/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2", "ICLR.cc/2017/conference/paper467/AnonReviewer1", "ICLR.cc/2017/conference/paper467/AnonReviewer4"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483301931377}}}, {"tddate": null, "tmdate": 1480267322822, "tcdate": 1480267322818, "number": 1, "id": "SJQBIcdGg", "invitation": "ICLR.cc/2017/conference/-/paper467/pre-review/question", "forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "signatures": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper467/AnonReviewer3"], "content": {"title": "Source code?", "question": "Any plans to release the source code to reproduce these experiments?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trained Ternary Quantization", "abstract": "Deep neural networks are widely used in machine learning applications. However, the deployment of large neural networks models can be difficult to deploy on mobile devices with limited power budgets. To solve this problem, we propose Trained Ternary Quantization (TTQ), a method that can reduce the precision of weights in neural networks to ternary values. This method has very little accuracy degradation and can even improve the accuracy of some models (32, 44, 56-layer ResNet) on CIFAR-10 and AlexNet on ImageNet. And our AlexNet model is trained from scratch, which means it\u2019s as easy as to train normal full precision model. We highlight our trained quantization method that can learn both ternary values and ternary assignment. During inference, only ternary values (2-bit weights) and scaling factors are needed, therefore our models are nearly 16\u00d7 smaller than full- precision models. Our ternary models can also be viewed as sparse binary weight networks, which can potentially be accelerated with custom circuit. Experiments on CIFAR-10 show that the ternary models obtained by trained quantization method outperform full-precision models of ResNet-32,44,56 by 0.04%, 0.16%, 0.36%, respectively. On ImageNet, our model outperforms full-precision AlexNet model by 0.3% of Top-1 accuracy and outperforms previous ternary models by 3%.", "pdf": "/pdf/8534394d7c823fa401a91f5d642e0fd63b51263b.pdf", "TL;DR": "Ternary Neural Network with accuracy close to or even higher than the full-precision one", "paperhash": "zhu|trained_ternary_quantization", "keywords": ["Deep learning"], "conflicts": ["stanford.edu"], "authors": ["Chenzhuo Zhu", "Song Han", "Huizi Mao", "William J. Dally"], "authorids": ["zhucz13@mails.tsinghua.edu.cn", "songhan@stanford.edu", "huizi@stanford.edu", "dally@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959265917, "id": "ICLR.cc/2017/conference/-/paper467/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper467/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper467/AnonReviewer3", "ICLR.cc/2017/conference/paper467/AnonReviewer2"], "reply": {"forum": "S1_pAu9xl", "replyto": "S1_pAu9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper467/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959265917}}}], "count": 21}