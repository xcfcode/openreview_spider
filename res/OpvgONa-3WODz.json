{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363459200000, "tcdate": 1363459200000, "number": 2, "id": "pC-4pGPkfMnuQ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OpvgONa-3WODz", "replyto": "OpvgONa-3WODz", "signatures": ["Guillaume Desjardins, Razvan Pascanu, Aaron Courville, Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you to the reviewers for the helpful feedback. The provided references will no doubt come in handy for future work.\r\n\r\nTo all reviewers:In an effort to speedup run time, we have re-implemented a significant portion of the MFNG algorithm. This resulted in large speedups for the diagonal approximation of MFNG, and all around lower memory consumption. Unfortunately, this has delayed the submission of a new manuscript, which is still under preparation. The focus of this new revision will be on: \r\n(1) reporting mean and standard deviations of Fig.1 across multiple seeds.\r\n(2) a more careful use of damping and the use of annealed learning rates.\r\n(3) results on a second dataset, and hopefully a second model family (Gaussian RBMs).\r\n\r\nIn the meantime, we have uploaded a new version which aims to clarify and provide additional technical details, where the reviewers had found it necessary. The main modifications are:\r\n* a new algorithmic description of MFNG\r\n* a new graph which analyzes runtime performance of the algorithm, breaking down the run-time performance between the various steps of the algorithm (sampling, gradient computation, matrix-vector product, and MinRes iterations).\r\nThe paper should appear shortly on arXiv, and can be accessed here in the meantime:\r\nhttp://brainlogging.files.wordpress.com/2013/03/iclr2013_submission1.pdf\r\n\r\nAn open-source implementation of MFNG can be accessed at the following URL.  \r\nhttps://github.com/gdesjardins/MFNG.git\r\n\r\nTo Anonymous 7e2e:There are numerous advantages to sampling from parallel chains (with fewer Gibbs steps between samples), compared to using consecutive (or sub-sampled) samples generated by a single Markov chain. First, running multiple chains guarantees that the samples are independent. Running a single chain will no doubt result in correlated samples which will negatively impact our estimates of the gradient and the metric. Second, simulating multiple chains is an implicitly parallel process, which can be implemented efficiently on both CPU and GPU (especially so on GPU). The downside however is in increase in memory consumption.\r\n\r\nTo Anonymous 77a7:\r\n\r\n>> In terms of motivation, the new algorithm aims to attenuate the effect of ill-conditioned Hessians, however that claim is weakened by the fact that the experiments seem to still require the centering trick.\r\n\r\nSince ours is a natural gradient method, it attenuates the effect of ill-conditioned probability manifolds (expected hessian of log Z, under the model distribution), not ill-conditioning of the expected hessian (under the empirical distribution). It is thus possible that centering addresses the latter form of ill-conditioning. Another hypothesis is that centering provides a better initialization point, around which the natural gradient metric is better-conditioned and thus easier to invert. More experiments are required to answer these questions.\r\n\r\n>> Also, reproducibility would be improved with pseudocode (including all tricks used) was provided in the appendix (or a link to an open-source implementation, even better).\r\n\r\nOur source code and algorithmic description should shed some light on this issue. The only 'trick' we currently use is a fixed damping coefficient along the diagonal, to improve conditioning and speed up convergence of our solver. Alternative forms of initialization and preconditioning were not used in the experiments.\r\n\r\n>> Is there a good reason to limit section 2.1 to a discrete and bounded domain chi?\r\n\r\nThese limitations mostly reflect our interest with Boltzmann Machines. Generalizing these results to unbounded domains (or continuous variables) remains to be investigated.\r\n\r\n>> Hyper-parameter tuning is over a small ad-hoc set, and finally chosen values are not reported.\r\n\r\nThe results of our grid-search have been added to the caption of Figure 1."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "decision": "conferencePoster-iclr2013-conference", "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "pdf": "https://arxiv.org/abs/1301.3545", "paperhash": "desjardins|metricfree_natural_gradient_for_jointtraining_of_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Guillaume Desjardins", "Razvan Pascanu", "Aaron Courville", "Yoshua Bengio"], "authorids": ["guillaume.desjardins@gmail.com", "r.pascanu@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362379800000, "tcdate": 1362379800000, "number": 3, "id": "dt6KtywBaEvBC", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OpvgONa-3WODz", "replyto": "OpvgONa-3WODz", "signatures": ["anonymous reviewer 77a7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "review": "This paper introduces a new gradient descent algorithm that combines is based on Hessian-free optimization, but replaces the approximate Hessian-vector product by an approximate Fisher information matrix-vector product. It is used to train a DBM, faster than the baseline algorithm in terms of epochs needed, but at the cost of a computational slowdown (about a factor 30).  The paper is well-written, the algorithm is novel, although not fundamentally so. \r\n\r\nIn terms of motivation, the new algorithm aims to attenuate the effect of ill-conditioned Hessians, however that claim is weakened by the fact that the experiments seem to still require the centering trick. Also, reproducibility would be improved with pseudocode (including all tricks used) was provided in the appendix (or a link to an open-source implementation, even better).\r\n\r\n\r\nOther comments:\r\n\r\n* Remove the phrase 'first principles', it is not applicable here.\r\n\r\n* Is there a good reason to limit section 2.1 to a discrete and bounded domain X?\r\n\r\n* I'm not a big fan of the naming a method whose essential ingredient is a metric 'Metric-free' (I know Martens did the same, but it's even less appropriate here).\r\n\r\n* I doubt the derivation in appendix 5.1 is a new result, could be omitted.\r\n\r\n* Hyper-parameter tuning is over a small ad-hoc set, and finally chosen values are not reported.\r\n\r\n* Results should be averaged over multiple runs, and error-bars given.\r\n\r\n* The authors could clarify how the algorithm complexity scales with problem dimension, and where the computational bottleneck lies, to help the reader judge its promise beyond the current results.\r\n\r\n* A pity that it took longer than 6 weeks for the promised 'next revision', I had hoped the authors might resolve some of the self-identified weaknesses in the meanwhile."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "decision": "conferencePoster-iclr2013-conference", "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "pdf": "https://arxiv.org/abs/1301.3545", "paperhash": "desjardins|metricfree_natural_gradient_for_jointtraining_of_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Guillaume Desjardins", "Razvan Pascanu", "Aaron Courville", "Yoshua Bengio"], "authorids": ["guillaume.desjardins@gmail.com", "r.pascanu@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362294960000, "tcdate": 1362294960000, "number": 1, "id": "o5qvoxIkjTokQ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OpvgONa-3WODz", "replyto": "OpvgONa-3WODz", "signatures": ["anonymous reviewer 7e2e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "review": "This paper presents a natural gradient algorithm for deep Boltzmann machines. The authors must be commended for their extremely clear and succinct description of the natural gradient method in Section 2. This presentation is particularly useful because, indeed, many of the papers on information geometry are hard to follow. The derivations are also correct and sound. The derivations in the appendix are classical statistics results, but their addition is likely to improve readability of the paper.\r\n\r\nThe experiments show that the natural gradient approach does better than stochastic maximum likelihood when plotting estimated likelihood against epochs. However, per unit computation, the stochastic maximum likelihood method still does better. \r\n\r\nI was not able to understand remark 4 about mini-batches. Why are more parallel chains needed? Why not simply use a single chain but have longer memory. I strongly think this part of the paper could be improved if the authors write down the pseudo-code for their algorithm. Another suggestion is to use automatic algorithm configuration to find the optimal hyper-parameters for each method, given that they are so close.\r\n\r\nThe trade-offs of second order versus first order optimization methods are well known in the deterministic case. There is is also some theoretical guidance for the stochastic case. I encourage the authors to look at the following papers for this:\r\n\r\nA Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets. N. Le Roux, M. Schmidt, F. Bach. NIPS, 2012. \r\n\r\nHybrid Deterministic-Stochastic Methods for Data Fitting.\r\nM. Friedlander, M. Schmidt. SISC, 2012. \r\n\r\n'On the Use of Stochastic Hessian Information in Optimization Methods for Machine Learning' R. Byrd, G. Chin and W. Neveitt, J. Nocedal.\r\nSIAM J. on Optimization, vol 21, issue 3, pages 977-995 (2011).\r\n\r\n'Sample Size Selection in Optimization Methods for Machine Learning'\r\nR. Byrd, G. Chin, J. Nocedal and Y. Wu. to appear in Mathematical Programming B (2012).\r\n\r\nIn practical terms, given that the methods are so close, how does the choice of implementation (GPUs, multi-cores, single machine) affect the comparison? Also, how data dependent are the results. I would be nice to gain a deeper understanding of the conditions under which the natural gradient might or might not work better than stochastic maximum likelihood when training Boltzmann machines.\r\n\r\nFinally, I would like to point out a few typos to assist in improving the paper:\r\n\r\nPage 1: litterature should be literature\r\nSection 2.2 cte should be const for consistency.\r\nSection 3: Avoid using x instead of grad_N in the linear equation for Lx=E(.)  This causes overloading. For consistency with the previous section, please use grad_N instead.\r\nSection 4: Add a space between MNIST and [7].\r\nAppendix 5.1: State that the expectation is with respect to p_{\theta}(x).\r\nAppendix 5.2: The expectation with respect to q_\theta should be with respect to p_{\theta}(x) to ensure consistency of notation, and correctness in this case.\r\nReferences: References [8] and [9] appear to be duplicates of the same paper by J. Martens."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "decision": "conferencePoster-iclr2013-conference", "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "pdf": "https://arxiv.org/abs/1301.3545", "paperhash": "desjardins|metricfree_natural_gradient_for_jointtraining_of_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Guillaume Desjardins", "Razvan Pascanu", "Aaron Courville", "Yoshua Bengio"], "authorids": ["guillaume.desjardins@gmail.com", "r.pascanu@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362012600000, "tcdate": 1362012600000, "number": 4, "id": "LkyqLtotdQLG4", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "OpvgONa-3WODz", "replyto": "OpvgONa-3WODz", "signatures": ["anonymous reviewer 9212"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "review": "The paper describes a Natural Gradient technique to train Boltzman machines.  This is essentially the approach of Amari et al (1992) where the Fisher information matrix is expressed in which the authors estimate the Fisher information matrix L with examples sampled from the model distribution using a MCMC approach with multiple chains.  The gradient g is estimated from minibatches, and the weight update x is obtained by solving Lx=g with an efficient truncated algorithm.  Doing so naively would be very costly because the matrix L is large.  The trick is to express L as the covariance of the Jacobian S with respect to the model distribution and take advantage of the linear nature of the sample average to estimate the product Lw in a manner than only requires the storage of the Jacobien for each sample. \r\n\r\nThis is a neat idea.  The empirical results are preliminary but show promise.  The proposed algorithm requires less iterations but more wall-clock time than SML.  Whether this is due to intrinsic properties of the algorithm or to deficiencies of the current implementation is not clear."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "decision": "conferencePoster-iclr2013-conference", "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "pdf": "https://arxiv.org/abs/1301.3545", "paperhash": "desjardins|metricfree_natural_gradient_for_jointtraining_of_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Guillaume Desjardins", "Razvan Pascanu", "Aaron Courville", "Yoshua Bengio"], "authorids": ["guillaume.desjardins@gmail.com", "r.pascanu@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358406900000, "tcdate": 1358406900000, "number": 65, "id": "OpvgONa-3WODz", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "OpvgONa-3WODz", "signatures": ["guillaume.desjardins@gmail.com"], "readers": ["everyone"], "content": {"title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines", "decision": "conferencePoster-iclr2013-conference", "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for training Boltzmann Machines. Similar in spirit to the Hessian-Free method of Martens [8], our algorithm belongs to the family of truncated Newton methods and exploits an efficient matrix-vector product to avoid explicitely storing the natural gradient metric $L$. This metric is shown to be the expected second derivative of the log-partition function (under the model distribution), or equivalently, the variance of the vector of partial derivatives of the energy function. We evaluate our method on the task of joint-training a 3-layer Deep Boltzmann Machine and show that MFNG does indeed have faster per-epoch convergence compared to Stochastic Maximum Likelihood with centering, though wall-clock performance is currently not competitive.", "pdf": "https://arxiv.org/abs/1301.3545", "paperhash": "desjardins|metricfree_natural_gradient_for_jointtraining_of_boltzmann_machines", "keywords": [], "conflicts": [], "authors": ["Guillaume Desjardins", "Razvan Pascanu", "Aaron Courville", "Yoshua Bengio"], "authorids": ["guillaume.desjardins@gmail.com", "r.pascanu@gmail.com", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 5}