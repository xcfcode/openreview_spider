{"notes": [{"id": "r1lfga4KvS", "original": "H1xcTYRHvS", "number": 331, "cdate": 1569438954441, "ddate": null, "tcdate": 1569438954441, "tmdate": 1577168276408, "tddate": null, "forum": "r1lfga4KvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tFQQpRi6Ja", "original": null, "number": 1, "cdate": 1576798693527, "ddate": null, "tcdate": 1576798693527, "tmdate": 1576800941932, "tddate": null, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "invitation": "ICLR.cc/2020/Conference/Paper331/-/Decision", "content": {"decision": "Reject", "comment": "This paper explores extending k-means to allow to clusters with non-convex shapes.\n\nThis paper introduces a new algorithm, relying on empirical comparisons to illustrate its contribution. The main issue with the paper is that the empirical claims do not support that the new method is indeed better. The paper claims the new method outperforms the competitors in most cases. However, the original submission reported median performance and when the authors provided mean performance and additional baseline methods (at the reviewers' request) there appear to be little evidence to support the claim. In addition there are no measures of significance provided. The authors provided no commentary to help the reviewers understand the new results. There might be some important speed gains at the cost of final performance, but on the evidence provided we are not able to evaluate the cost in final performance.\n\nThe text changes size after section 5.3 and is 9% smaller. Watch out for this formatting issue in future submissions\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712438, "tmdate": 1576800261821, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper331/-/Decision"}}}, {"id": "BJlI6ipjqB", "original": null, "number": 4, "cdate": 1572752318002, "ddate": null, "tcdate": 1572752318002, "tmdate": 1574822250265, "tddate": null, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "invitation": "ICLR.cc/2020/Conference/Paper331/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "The paper considers extending the k-means algorithm to allow for finding clusters with non-convex shapes. Particularly, it uses an existing theoretical framework (Extreme Value Theory) to maps a Euclidean space into what it calls the extreme value space, and proposes two Extreme Value k-means algorithms: GEV k-means and GPD k-means. It then provides some empirical results demonstrating their approach.\n\nI have some concerns with the paper's claimed novelties, its empirical evaluation, and its overall presentation, and thus am initially recommending a weak reject. To raise my score, the following concerns should be clarified or addressed:\n\n1) An initial concern is with the claimed novelty of the work. A paper by Li et al. (2012) also uses Extreme Value Theory (EVT) to improve k-means using the Generalized Extreme Value (GEV) distribution. Their algorithm is also called GEV k-means, and is based on an observation that the squared distance from a point to its closest center follows the GEV distribution for large numbers of clusters. From this, it doesn't appear to be the first time that EVT has been used to improve k-means, and it would be good for the authors to contrast their methods in the context of existing work in this direction.\n\n2) Is it generally applicable to measure similarity based on the probability of being an extreme value, compared with classic metrics like Euclidean distance? In other words, would it always be better to do this or are there clear counterexamples where you would not want to measure similarity based on this?\n\n3) In the empirical evaluation, it is said that 10 independent runs were performed, and the maximum result of the 10 runs was reported. I believe it would be more informative to report the mean, or expected performance of the algorithm, as well as some statistic about the mean to ensure any differences are significant. It is not clear whether this maximum can be expected or reproduced, and can negatively be interpreted as the algorithm having considerably higher variability- in other words, it could be the case that the minimum of the 10 runs for the EV methods was also lower than the minimums of classic k-means. Were any statistical tests done to ensure that the larger maximum over the runs was not by chance?\n\n4) The paper has many frequent, but minor, grammatical and spelling errors. As such, it is possible to get the overall message (and didn't strongly impact my score), but it does detract from the paper's overall presentation and quality.\n\n-----\n\nPost-rebuttal:\n\nThank you for your clarifications regarding 1) and 2), and for now reporting the mean. However, there still does not seem to be any statistical significance testing. Further, after reporting the mean and comparing with more methods, the method doesn't seem to perform as well as previously reported. This makes the concern in 2) more prevalent- if the method is not generally applicable, and are likely to help on a problem-specific basis, it would be more informative to characterize *when* one might expect the method to perform better, and support this with empirical results. Based on this, I am maintaining my score, but think the work is interesting and encourage the authors to improve on the paper!", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575395201863, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper331/Reviewers"], "noninvitees": [], "tcdate": 1570237753686, "tmdate": 1575395201877, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper331/-/Official_Review"}}}, {"id": "B1xAsmc3FH", "original": null, "number": 1, "cdate": 1571754918407, "ddate": null, "tcdate": 1571754918407, "tmdate": 1572972608763, "tddate": null, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "invitation": "ICLR.cc/2020/Conference/Paper331/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose an improved k-means clustering algorithm by using a new similarity measurement method based on extreme value theory. Extreme value theory can better capture the data distribution and thus better handle non-convex situations. Experiments on extensive datasets are conducted to show the effectiveness of the proposed method.\n\nThe paper is well written and easy to understand. However there are some concerns:\n1. There are many clustering methods based on geodesic distance of data points (manifold learning), which are supposed to be better at capturing the non-convex data distribution. Comparisons are suggested.\n2. The datasets used in the experiments are relatively small (n<=50000, and k<=10), what about more clusters?\n3. The results of different methods may be unstable to initialization, more tests and average results are suggested.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575395201863, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper331/Reviewers"], "noninvitees": [], "tcdate": 1570237753686, "tmdate": 1575395201877, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper331/-/Official_Review"}}}, {"id": "SJldIO6ntS", "original": null, "number": 2, "cdate": 1571768400102, "ddate": null, "tcdate": 1571768400102, "tmdate": 1572972608728, "tddate": null, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "invitation": "ICLR.cc/2020/Conference/Paper331/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper claims to improve the \u201cclustering ability\u201d of k-means by measuring the similarity between samples and centroids by something known as Extreme Value Theory (EVT). The paper in its current form is extremely difficult to follow. I highlight some of the difficulty: \n\n1. The notion of \u201cclustering ability\u201d of k-means is not well defined in the paper. So, in general there does not seem to be a clear theoretical objective that is being optimised by the suggested algorithm.\n\n2. The algorithm itself is not clear. For example, the block maximum sequence M^j = {M_1, \u2026, M_m} is not clearly understood. What is threshold u_j? \n\nGiven the above, I am unable to evaluate the contribution of the paper. It will help the paper if the authors clearly formalise what is it that they are trying to achieve with their algorithm. After this, there needs to be a much more clear description of the algorithm. I think the above will also help decide whether section 3.2 on Extreme Value Theory is really required for the discussion. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575395201863, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper331/Reviewers"], "noninvitees": [], "tcdate": 1570237753686, "tmdate": 1575395201877, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper331/-/Official_Review"}}}, {"id": "SJglh4GrcB", "original": null, "number": 3, "cdate": 1572312231839, "ddate": null, "tcdate": 1572312231839, "tmdate": 1572972608685, "tddate": null, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "invitation": "ICLR.cc/2020/Conference/Paper331/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper utilizes EVT to improve k-means, which aims to address the problem of clustering data of nonconvex shape. GEV k-means and GPD k-means are proposed as two kind of  Extreme Value k-means. A method for accelerating Euclidean distance computation has also been proposed to solve the bottleneck of k-means.\nThe proposed idea is novel and the paper is well written. Experimental reaultts are also good to me.\nThere are two concerns which I need the authors to address:\n1. Since the authors claim that they propose to speed up the computation of the Euclidean distances in k-means. However, there is no time cost comparison in the experiments.\n2. Some other variants of k-means should be added for experimental comparison."}, "signatures": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper331/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["sxzheng18@fudan.edu.cn", "yxhou@fudan.edu.cn", "yanweifu@fudan.edu.cn", "jffeng@fudan.edu.cn"], "title": "Extreme Value k-means Clustering", "authors": ["Sixiao Zheng", "Yanxi Hou", "Yanwei Fu", "Jianfeng Feng"], "pdf": "/pdf/f66f977827a975c0124021b647b49ec3b5ee2229.pdf", "TL;DR": "This paper introduces Extreme Value Theory into k-means to measure similarity and proposes a novel algorithm called Extreme Value k-means for clustering.", "abstract": "Clustering is the central task in unsupervised learning and data mining. k-means is one of the most widely used clustering algorithms. Unfortunately, it is generally non-trivial to extend k-means to cluster data points beyond Gaussian distribution, particularly, the clusters with non-convex shapes (Beliakov & King, 2006). To this end, we, for the first time, introduce Extreme Value Theory (EVT) to improve the clustering ability of k-means. Particularly, the Euclidean space was transformed into a novel probability space denoted as extreme value space by EVT. We thus propose a novel algorithm called Extreme Value k-means (EV k-means), including GEV k-means and GPD k-means. In addition, we also introduce the tricks to accelerate Euclidean distance computation in improving the computational efficiency of classical k-means. Furthermore, our EV k-means is extended to an online version, i.e., online Extreme Value k-means, in utilizing the Mini Batch k-means to cluster streaming data. Extensive experiments are conducted to validate our EV k-means and online EV k-means on synthetic datasets and real datasets. Experimental results show that our algorithms significantly outperform competitors in most cases.", "keywords": ["unsupervised learning", "clustering", "k-means", "Extreme Value Theory"], "paperhash": "zheng|extreme_value_kmeans_clustering", "original_pdf": "/attachment/38559f24d4164f70e527efd6b871527975183d9f.pdf", "_bibtex": "@misc{\nzheng2020extreme,\ntitle={Extreme Value k-means Clustering},\nauthor={Sixiao Zheng and Yanxi Hou and Yanwei Fu and Jianfeng Feng},\nyear={2020},\nurl={https://openreview.net/forum?id=r1lfga4KvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1lfga4KvS", "replyto": "r1lfga4KvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper331/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575395201863, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper331/Reviewers"], "noninvitees": [], "tcdate": 1570237753686, "tmdate": 1575395201877, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper331/-/Official_Review"}}}], "count": 6}