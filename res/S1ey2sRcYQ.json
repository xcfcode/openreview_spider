{"notes": [{"id": "rylSgPw367", "original": null, "number": 4, "cdate": 1542383340718, "ddate": null, "tcdate": 1542383340718, "tmdate": 1545497126031, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "rJefZTGLnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "content": {"title": "Thank you for your constructive comments", "comment": "We thank you for your thoughtful comments. Here are answers to the concerns raised by the review. We complement these answers with a revised submission (appendix and paper)\n\nWall-clock time for the CelebA with $10$ binary attributes takes 0.13 seconds for Gumbel-Softmax and 0.06 seconds for Gumbel-Max, when the discrete latent space is structured (spin glass model). More generally: Gumbel-Softmax uses in its forward computation: (i) one computation of $\\theta$ (getting as input a softmax) and (ii) $|{\\cal Z}|$ computations of $\\phi$ for each $z \\in {\\cal Z}$ (due to the normalization of the softmax). In its backward computation it uses: (i) one gradient for $\\theta$ and (ii) $|{\\cal Z}|$ computations derivatives of $\\phi$ (due to the normalization of the softmax), for each $z \\in {\\cal Z}$. In contrast, Gumbel-Max uses in its forward computation (i) one computation of $\\theta$ (getting as input the argmax) and (ii) max computation over $\\phi$. In its backward computation it requires another max operation (total of two max operations), now with $\\theta$ (when the $\\theta$ is decomposable, as in our discrete product space approximation, this is as efficient as computing the maximum over $\\phi$) and two gradient computations of $\\phi$. \n\nThe computational complexity of the max operation is sometimes much less than the computational complexity of the normalization constant since the max operation does not necessarily requires going over all $z \\in {\\cal Z}$ in all cases. This happens in structured prediction models, where the max operation can be computed with integer linear solvers efficiently. Structured prediction models are important in practice since they capture correlations between labels (as in the CelebA problem).   \n\nThe work [1] in its one dimensional form consdiers $\\sum_z e^{\\phi_v(x,z)} \\theta(x,z)$ (assuming the normalization constant is one for simplicity). Comparing to Gumbel-Softmax and Gumbel-Max, it requires in its forward computation $|{\\cal Z}|$ computations of $\\phi$ and $\\theta$ and in its backward computation $|{\\cal Z}|$ computations of $\\phi$ and $\\theta$ gradients. We agree that such a comparison is in place, we will compare the works.\n\nWe agree that Eq 9 can be computed similarly to Eq 4, Eq 5, it will result in REINFORCE for Gumbel-Max: The update rule of variational Bayes in its discrete setting is $\\sum_z e^{\\phi_v(x,z)} \\theta(x,z) \\nabla_v \\phi_v(x,z)$ (assuming the normalization constant is one for simplicity). When doing it in the perturbation space, this will lead to REINFORCE update rule with respect to perturbation models, and the gradient is  $ \\mathbb{E}_{\\gamma \\sim g} [ \\nabla \\log(g(\\gamma)) \\nabla \\phi(x,z^{\\phi + \\gamma }) \\theta(x,z^{\\phi + \\gamma})] $ where $g$ is the probability density function of the perturbation. This representation perhaps gives some insight for the variance of REINFORCE (multiplying the gradient by a log-gradient and $\\theta$, compared to our (biased) approach.\n\nWe added a plot (Figure 6 in the Appendix), also experimenting with depth and different loss functions. When $n$ increases the gap becomes smaller and the difference is negligible (and random).\n\nThe variance experiment was conducted as follows: the network was trained over all the samples until convergence, then computed the encoder gradients 1000 times (with 1000 different Gumbels)  over one sample and computed the variance over the 1000 gumbels.  We will add a comparison to Gumbel-Softmax with temperature and the bias as well. however, we are not sure that $\\epsilon$ is the right equivalent to temperature in Gumbel-Softmax. Instead, we think that the analogue is the perturbation variance: the temperature signifies the max-argument, as happens when the variance of the perturbation approach zero. $\\epsilon$ in our setting inserts the gradient signal\n\nIn our experiments, we focused on the settings of Gumbel-Softmax, to be able to compare to the previous methods. For structured setting we worked on CelebA but limited ourselves to small number of discrete variables to be able to compare to Gumbel-Softmax. In retrospect, we should have emphasized the structured encoder setting, where our method excels (when we compute two max operations instead of summing over all structures), and we will elaborate on that.\n\nIn semi-supervised setting, we can plug any loss. In the MNIST task it sometimes perform better and sometimes worse. Our setting was chosen from computational aspect, since it allowed us to set the perturbed prediction to the true label.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614887, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper674/Authors|ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614887}}}, {"id": "BJxSXov1RQ", "original": null, "number": 5, "cdate": 1542581020618, "ddate": null, "tcdate": 1542581020618, "tmdate": 1545496997995, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "S1gDwY7Na7", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "content": {"title": "Thank you for your insightful suggestions", "comment": "We thank you for your time and effort. Many of your comments helped us to improve the submission. We uploaded a new manuscript with all suggestions. \n\nWe agree that citing Song 2016 will solve these issues, but we think that these issues are a result of poor notation rather than a mistake: we tried to condense Danskin theorem to one line, and implicitly work with product spaces that decouple the dependencies of $\\hat z$, it was a mistake as it made our derivation unclear. \n\nThe first equation in Theorem 1:\nThis is a convolution between a smooth function and a non-smooth function, and therefore it is smooth. Take for example a two dimensional Gaussian random variable with mean $\\mu$. The expectation of their max is a smooth function. Analytically, it equals to $C\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty  e^{\\|\\gamma - \\mu\\|^2/2} \\max\\{\\gamma_1,\\gamma_2\\} d \\gamma_1 d \\gamma_2$. While $\\max \\{\\gamma_1,\\gamma_2\\}$ certainly depends on $\\gamma$ through $\\mu$, the integral is a smooth function of $\\mu$. We defined $g(\\gamma)$ more precisely above equation 3 to show it is a product space so it decouples dependencies of $\\theta$ within $\\hat z$: The notation $g(\\hat \\gamma - \\epsilon \\theta - \\phi_v )$ implicitly uses the independent product space $\\prod_{\\hat z} g(\\hat \\gamma(x,\\hat z) - \\epsilon \\theta(x,\\hat z) - \\phi_v(x,\\hat z) )$. We borrowed this notation from Gaussian random variables, where in this case it is $e^{\\|\\gamma - \\mu\\|^2/2}  = \\prod_{\\hat z} e^{(\\gamma(\\hat z) - \\mu(\\hat z))^2/2}$. \n\n\"We turn to prove Equation 8\" paragraph:\nWe agree we chose poor notation. The function $\\max_{\\hat z} \\{\\epsilon \\theta(x,\\hat z) + \\phi_v(x,\\hat z) + \\gamma(\\hat z) \\}$ is the maximum of linear functions of $\\epsilon$. Therefore Danskin Theorem (Proposition 4.5.1 in Convex Analysis and Optimization by Bertsekas) states that $\\partial_\\epsilon(\\max_{\\hat z} \\{\\epsilon \\theta(x,\\hat z) + \\phi_v(x,\\hat z) + \\gamma(\\hat z) \\}) = \\theta(x,z^{\\epsilon \\theta + \\phi_v + \\gamma})$ whenever the $\\arg \\max$ is unique. Since the $\\arg \\max$ is unique with probability one, we can continue without problems, thus overcoming the general position condition in McAllester et al 2010 and the regularity conditions in Song et al. 2016.\n\nClarifications: \n* Annealing: We set the annealing rate to be 1e-3 to follow Jang et al. We stop at $0.1$ to avoid gradient blowup.\n\n* Semi-supervised: we referred to general loss functions, beyond the log-loss. We agree that the log loss is a natural choice but there are recent cases where semi-supervised is important and log-loss cannot capture the structures accurately. Such losses can extend Corro and Titov's \"Differentiable Perturb-and-Parse: Semi-Supervised Parsing with a Structured Variational Autoencoder\"} \n\n* We used 500 examples in CelebA\n\n* We will fix the biography\n\n* We will add the bias in our bias/variance tradeoff\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614887, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper674/Authors|ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614887}}}, {"id": "Hygbflpce4", "original": null, "number": 1, "cdate": 1545420809222, "ddate": null, "tcdate": 1545420809222, "tmdate": 1545420884773, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "rJg3iGZLgN", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Public_Comment", "content": {"comment": "I enjoy reading the gradient estimation methods with multiple evaluations in the paper! \nI would like to point our concurrent work which also provides low variance, unbiased gradient estimator for discrete latent variables which may serve as a proper comparison.  https://openreview.net/pdf?id=S1lg0jAcYm", "title": "Related reference"}, "signatures": ["~Mingzhang_Yin1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mingzhang_Yin1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311780127, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1ey2sRcYQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311780127}}}, {"id": "S1ey2sRcYQ", "original": "rkx81-I9KX", "number": 674, "cdate": 1538087847058, "ddate": null, "tcdate": 1538087847058, "tmdate": 1545355439096, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJg3iGZLgN", "original": null, "number": 1, "cdate": 1545110179769, "ddate": null, "tcdate": 1545110179769, "tmdate": 1545354478205, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Meta_Review", "content": {"metareview": "The paper presents a novel gradient estimator for optimizing VAEs with discrete latents, that is based on using a Direct Loss Minimization approach (as initially developed for structured prediction) on top of the Gumble-max trick. This is an interesting and original alternative to the use of REINFORCE or Gumble Softmax. The approach is mathematically well detailed, but exposition could be easier to follow if it used a more standard notation. After clarifications by the authors, reviewers agreed that the main theorerm is correct. The proposed method is shown empirically to converge faster than Gumbel-softmax, REBAR, and RELAX baselines in number of epochs. However, as questioned by one reviewer, the proposed method appears to require many more forward passes (evaluations) of the decoder for each example.\u00a0Authors replied by highlighting that an argmax can be more computationally efficient than softmax (in cases when the discrete latent space is structured), and also clarified in the paper their use of an essential computational approximation they make for discrete product spaces. These are important aspects that affect computational complexity. But they do not address the question raised about using significantly more decoder evaluations for each example. A fair comparison for sampling based gradient estimation methods should rest on actual number of decoder evaluations and on resulting timing. The paper currently does not sufficiently discuss the computational complexity of the proposed estimator against alternatives, nor take this essential aspect into account in the empirical comparisons it reports. \nWe encourage the authors to refocus the paper and fully develop and showcase a use case where the approach could yield a clear a computational advantage, like the structured encoder setting they mentioned in the rebuttal.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Original approach, but with unclear computational benefit "}, "signatures": ["ICLR.cc/2019/Conference/Paper674/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper674/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353130955, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353130955}}}, {"id": "HygdSa5rJE", "original": null, "number": 13, "cdate": 1544035647559, "ddate": null, "tcdate": 1544035647559, "tmdate": 1544035647559, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "r1eRfDcrA7", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "content": {"title": "Reply", "comment": "I slightly increased my score because of some of the clarifications. However I believe some points still haven't been addressed in the paper:\n\n1. I think the discussion about the computational complexity should be mentioned in the paper.\n\n2. As mentioned by the reviewer 3, the evaluation of the bias of gumbel-softmax is really noisy so I believe that either you're not using enough samples or there is something wrong. Also you compute the bias with respect to the REINFORCE estimator, I think the methods should actually compared to the \"true\" gradient computed by $\\sum_k \\nabla_c p(z=k) \\theta(x,z=k)$. Also as a note I believe there is a closed form expression for the bias of your method. \n\n3. This is just a note but if you consider the gradient of $\\sum_k p(z=k) \\theta(x,z=k)$ then the method you propose is equivalent to taking the directional derivative of p(z=k) along the vector \\theta(x), which is equal to the gradient when epsilon goes to zero. The \"mean-field\" approximation of the decoder you propose is then equivalent to choosing a different direction for the directional derivative. "}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614887, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper674/Authors|ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614887}}}, {"id": "S1gDwY7Na7", "original": null, "number": 3, "cdate": 1541843294881, "ddate": null, "tcdate": 1541843294881, "tmdate": 1543996896120, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "content": {"title": "Worthwhile and interesting paper, but exposition could use some work (rating maintained after author feedback).", "review": "This paper proposes combining the Gumbel-max trick and \"direct loss optimization\" for variance reduction in VAEs with discrete latent variables. This is a natural combination (in hindsight), since the Gumbel-max trick turns sampling into non-differentiable optimization, and direct loss optimization provides a way to optimize the expected value of a non-differentiable loss. The paper is well-written for the most part and is backed by good experimental results. However it like some of the mathematical details and some of the exposition could be greatly improved.\n\nI think there are several mistakes in the reasoning presented in the proof of Theorem 1 (see detailed comments below). Theorem 1 in the current paper seems to me to be a special case of Theorem 1 in (Song 2016), where the expectation over data is replaced by an expectation over the Gumbel variable gamma. If I've understood correctly, it seems like it would be more correct and concise to simply cite that paper with some explanatory comments.\n\nThe word \"direct\" occurs quite a lot in the paper. It sometimes seemed misplaced. For example for \"The direct differentiation of the resulting expectation\" in the introduction, in what sense is the differentiation direct, and what would non-direct differentiation be?\n\nIn section 3, that's not the meaning of the term \"exponential family\".\n\nThe re-use of theta and psi as both model parameters and the log probability density / distribution is unnecessarily confusing.\n\nA small point, but in \"the challenge in generative learning is to reparameterize and optimize (2)\", the authors assume that q has analytic expression for the second KL term in (1). That's often the case but definitely not always. Also, even if this KL term has an analytic expression, it is not always better to use it (see Duvenaud \"Sticking the landing...\").\n\nIn (3), the usual notation is P(x = i) where x is the random variable and i is its possible value, whereas in (3) the random variable z^{\\phi + \\gamma} appears on the right of the equals sign.\n\nThe first paragraph of section 4.1 and (4) and (5) are just a simple application of the law of total expectation, and it would be simpler and clearer to state that.\n\n\"gradient of the decoder\" should be \"gradient of the decoder log probability\" (or log prob density depending on preference). Similarly with \"the decoder is a smooth function\". The decoder is a conditional probability distribution (at least according to my understanding of conventional usage).\n\nIn the first equation in the proof of Theorem 1, it seems as though the authors are using the standard change of variables formula for integrals. However the new variable \\hat{\\gamma} depends on \\hat{z} through \\theta, so I don't see how it's valid to ignore the max in the way the present paper does. One way to see that something is wrong is the fact that the integrand on LHS has \\hat{z} as a bound variable only, whereas the integrand on RHS has \\hat{z} as both a bound variable (inside the max) and a free variable (since \\theta depends on \\hat{z}, though strangely that is not written in the equation). What is the value of \\hat{z} used for \\theta on RHS?\n\nThere's a missing [] after \\partial_\\epsilon in the third line of the paragraph starting \"We turn to prove Equation (8)\".\n\nIn the same line, I don't see why the two expectations are equal. It seems to me that the differentiation w.r.t. epsilon ignores the fact that changing epsilon occasionally changes z^{\\epsilon \\theta + \\phi_v + \\gamma} in a discontinuous way. The term being differentiated has both a continuous-in-epsilon component and a piecewise-constant-in-epsilon component, and the latter appears to have been ignored. While the gradient of a piecewise constant function is zero almost everywhere, the occasional large changes (which could be thought of as delta functions) still can make a large contribution to the overall expression once we take the expectation. To look at it another way, if the reasoning here is correct, why can't the same argument be used on the RHS of (8), first to take the derivative inside the expectation and subsequently to compute the derivative as zero, since the inner term is a piecewise constant function of v? Yet clearly the RHS of (8) is not always zero.\n\nAround \"However when we approach the limit, the variance of the estimate increases...\", I think it would be extremely helpful to explain that for small epsilon, we occasionally obtain a large gradient (and otherwise zero), while for large epsilon we often obtain a moderate non-zero gradient. That gives some insight into the effect of epsilon, and why the variance is larger for small epsilon.\n\nAny reason not to plot the bias in right Figure 1, which is ostensibly about the bias-variance trade-off?\n\nI didn't follow the meaning of the diagram or caption for left Figure 1.\n\nIn (11), I wasn't sure whether S included the supervised examples or not (i.e. whether S_1 was disjoint from or a subset of S). If disjoint, shouldn't the KL term be included, or the expectation-over-gamma term be changed to use ground truth z? I guess I was unclear on the form of loss used for the supervised data, and unclear on the motivation for this choice.\n\nIn the last sentence of section 5.1, should \"chain rule\" be \"variance reparameterization trick\"?\n\nIn section 5.2, it would be helpful to mention what mean field means in terms of the variational distribution q (namely q(z | x) = \\prod_i q(z_i | x) ). Also, the term \"mean field\" is not conventionally used for general distributions (such as the decoder here) as far as I'm aware, only for variational distributions. \"Conditionally independent\" might be clearer.\n\nWhat does \"for which we can approximate z^{...} efficiently\" refer to?\n\nIn section 6.1, what is the annealing rate? Also, the minimal epsilon is set to 0.1. Is epsilon changed as training progresses according to some schedule?\n\n\"The main advantage of our framework is that it seamlessly integrates semi-supervised learning\" seems like an overstatement. Wouldn't semi-supervised learning be relatively straightforward to incorporate into any form of VAE? And why not just use log p(x, z) for updating the decoder parameters and log q(z | x) for updating the encoder parameters?\n\nHow many labeled examples were used for the CelebA semi-supervised learning?\n\nSome bibliography typos. For example, no capitalization throughout (e.g. \"gumbel\" instead of \"Gumbel\"). Also lots of arxiv preprints cited when published papers exist (e.g. Jang 2017 should be ICLR 2017 not arxiv preprint).\n\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "cdate": 1542234405810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777701, "tmdate": 1552335777701, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJefZTGLnQ", "original": null, "number": 1, "cdate": 1540922618019, "ddate": null, "tcdate": 1540922618019, "tmdate": 1543989494089, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "content": {"title": "An principled approach with weak empirical results", "review": "This work proposes a new (biased) gradient estimator to learn a discrete auto-encoders. Similarly to the gumbel-softmax estimator this paper proposes to use the gumbel-max trick and the reparametrization trick but instead of relaxing the argmax by a softmax, the authors derive a formula for the gradient based on direct loss optimization to compute the gradient through the argmax.\n\nPros:\n- The approach is well motivated and the proof of theorem 1 which gives the formula of the new gradient estimator seems correct.\n\nCons:\n- The principle downside of the proposed approach is that it requires to compute the value of the objective for several values of z, which makes it more computationally expensive than gumbel-softmax. Could the author compare the different estimators in terms of running time instead of epoch for example in fig2. it seems like RELAX would perform similarly or better in terms of wall-clock time.\n\n- [1] also proposed an estimator that requires evaluating the objective for different values of z, and showed that it is unbiased and optimal (lowest variance). I think the authors should mention this related work and how their approach differs. I also think the author should compare their work to [1].\n\n- Since both gumbel-softmax and the proposed approach are biased, could the authors give some intuitions on why they believe their approach is better.\n\n- I believe the expectation of the right-hand side of equation (9) can be computed in closed form by using a formula similar to eq (4) and (5), which replace the expectation by a sum over the possible values of z. This will lead to a gradient estimator with no variance, can the author comment on this ?\n\n- I think the bias induced by the mean-field approximation of the decoder should be investigated more thoroughly. Could the authors plot the gap as a function of n for example ? What happens if we also increase the number of category ? (there is a typo in this section it should be k^n instead of n^k) ? Can they compare to gumbel-softmax, is there a threshold at which gumbel-softmax becomes better ?\n\n- It's not clear on what setting is the variance plotted in fig 1. is computed ? Is it computed on the discrete VAE experiment ? if so how many latent variables and category ? Could the bias also be provided ? Could it be compared to gumbel-softmax with varying temperature ?\n\n- The experiments are a bit toyish, it's not clear what happens when the task are more complex, the architecture for the encoder and decoder are deeper or the latent space is bigger. In particular the authors only consider linear encoder and decoder when comparing the ELBO of different methods.\n\n- In the semi-supervised settings what happens if we don't set the perturbed level to the true label ?\n\nConclusion:\nThe experiments are quite toyish and the approach is more computationally expensive than gumbel-softmax. More experiments should be done to clearly show the advantage of this method compared to gumbel-softmax.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "cdate": 1542234405810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777701, "tmdate": 1552335777701, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJeixZ5NJ4", "original": null, "number": 12, "cdate": 1543966962777, "ddate": null, "tcdate": 1543966962777, "tmdate": 1543966962777, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "BJedFgq41V", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "content": {"title": "Reviewer response (continued)", "comment": "For the annealing rate, I actually meant what is the annealing rate in the sense of what does it mean as a phrase? Even if defined in Jang et al. it would be useful to briefly restate here.\n\nAlso, I still didn't see any explanation as to whether epsilon changed as training progresses according to some schedule?\n\nThanks for your clarification regarding the log loss. \"The main advantage of our framework is that it seamlessly integrates semi-supervised learning\" is still an overstatement, since many forms of VAE can incorporate semi-supervised learning easily. Also, some of the clarification should go in the paper, not just these review notes.\n\nThe bibliography still contains a ton of miscapitalizations, e.g. \"john wiley & sons\". Please fix.\n\nIn figure 1, I suggest using more than 500 samples. The GSM plot is so noisy it is hard to compare the two biases, and the trade-off is hard to see at the moment since the bias appears essentially flat for \"direct\" due to the noisiness and the scale of the y axis.\n\nFor the law of total expectation point, I actually meant that the entire first paragraph of section 4.1 and (4) and (5) are just a simple application of the law of total expectation, and could be replaced by simply stating that and the left side of (4) and the right side of (5).\n\nThere were a number of other comments that did not appear to be addressed:\n\nThe re-use of theta and psi as both model parameters and the log probability density / distribution is unnecessarily confusing.\n\nA small point, but in \"the challenge in generative learning is to reparameterize and optimize (2)\", the authors assume that q has analytic expression for the second KL term in (1). That's often the case but definitely not always. Also, even if this KL term has an analytic expression, it is not always better to use it (see Duvenaud \"Sticking the landing...\").\n\nIn (11), I wasn't sure whether S included the supervised examples or not (i.e. whether S_1 was disjoint from or a subset of S). If disjoint, shouldn't the KL term be included, or the expectation-over-gamma term be changed to use ground truth z? I guess I was unclear on the form of loss used for the supervised data, and unclear on the motivation for this choice.\n\nThe additional sentence after \"for which we can approximate z^{...} efficiently\" is very opaque and does not really make explicit what the authors are referring to. Explicit equations would be helpful, please!"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614887, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper674/Authors|ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614887}}}, {"id": "BJedFgq41V", "original": null, "number": 11, "cdate": 1543966848386, "ddate": null, "tcdate": 1543966848386, "tmdate": 1543966939779, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "BJxSXov1RQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "content": {"title": "Reviewer response", "comment": "I appreciate the authors' feedback, and much of it was very helpful. However, I don't feel like the authors did a very good job of addressing my comments.\n\nRegarding Theorem 1, after carefully reviewing the authors' comments and going through some working of my own, I now believe each of the claims in Theorem 1 is correct, and some of my confusion was indeed solely due to non-standard notation rather than incorrect reasoning. It is quite a nice way to derive (10)! However the presentation still leaves a huge amount to be desired even after the authors' improvements. Firstly, the differentiating-under-the-integral argument justifies differentiating under the integral for the expression on the right side of the first equation in the proof, but not for the expression on the left side, which is what is actually used. Please clarify the reasoning here. Secondly, for Folland (1999), Theorem 2.27, we require df/dt to exist (in their notation), but it does not exist everywhere (only almost everywhere) for the max function according to a strict definition of derivative. Please clarify why this theorem actually applies here. Also, what precisely is the integrable function which bounds the derivative? The Danskin argument is used to justify the derivation of equation (8), but the derivation of equation (9) also requires a similar derivative-of-max argument, and Danskin is not completely straightforward to apply in this case since the part inside the max is not convex.\n\nI would suggest that it might be easier to first define the integral of $g(\\gamma) max_z{v(z) + \\gamma(z)} d\\gamma$, derive the derivatives of this with respect to $v$, then use this to derive both (8) and (9).\n\nA few other small points in the proof of Theorem 1. It might be worth reiterating that $g$ is the *multivariate* Gumbel pdf to stress that the integral is not 1D. Using the limits -infty and infty also seems strange, and suggests a 1D integral. I would suggest omitting those limits. The notation $\\theta$ makes sense now; I had not realized it was intended to be a vector with components indexed by $\\hat{z}$. I would suggest using the more standard subscript notation $\\theta_{\\hat{z}}$ or just $\\theta_i$ for vectors. The notation $\\theta$ also doesn't work very well as it drops the dependence on $x$ (though I understand this is not relevant for the proof). The change of variables in the first equation in the proof also now makes sense to me. It would be helpful to state the result being used that the convolution of a smooth function and a non-smooth function is smooth, and also state this result precisely; it is not true in general for non-compactably-supported functions if I understand correctly. Incidentally it would also be helpful to precisely define \"smooth\". The meaning I presume is intended (infinitely continuously differentiable) is fairly standard but other definitions are also used. For \"differentiating under the integral, now with respect to v\" in the paragraph starting \"We turn to prove Equation (8)\", there is actually no need to differentiate under the integral to obtain the desired result, which is good because if I understand correctly differentiating under the integral here would not be valid. Also, \"taking the derivative with respect to $\\epsilon = 0$\" is awkward shorthand for \"taking the derivative with respect to $\\epsilon$ and setting $\\epsilon$ to 0\"; I would use the more explicit version. Also, in \"Applying a change of variable...\", there is a missing hat on $z$.\n\nI still think it's worth mentioning that the key result (10) can also be derived using Theorem 1 in the Song paper (briefly alluding to any regularity conditions needed). This would help give additional confidence that the result is correct.\n\nFinally on Theorem 1, I think it would be helpful for the reader's intuition to state why Folland (1999) Theorem 2.27 does not apply to differentiating under the integral of expressions like the right side of (8). Intuitively to me the difference is that the gradient of the integrand in (8) is a delta function, whereas in (6) the gradient of the integrand was merely discontinuous, but bounded. Some clarifying statement would be helpful for the reader to understand when the sort of reasoning the authors are using is valid or not."}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614887, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1ey2sRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper674/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper674/Authors|ICLR.cc/2019/Conference/Paper674/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers", "ICLR.cc/2019/Conference/Paper674/Authors", "ICLR.cc/2019/Conference/Paper674/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614887}}}, {"id": "BkewQsl62Q", "original": null, "number": 2, "cdate": 1541372702859, "ddate": null, "tcdate": 1541372702859, "tmdate": 1541533785774, "tddate": null, "forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "content": {"title": "A significant contribution", "review": "The authors propose a method to apply the reparametrization trick when the random variables of interest are discrete. Their technique is based on a formulation of the objective function in terms of Gumbel-Max operators. They propose a derivation of the gradient in terms of an auxiliary variable \\epsilon, such that the resulting gradient estimate is biased but the bias is reduced as \\epsilon approaches zero, at the cost of increasing variance. Experiments are performed with VAE including discrete latent variable models. The authors show how their method converges faster than other baselines formed by estimators of the gradient given by the REBAR, RELAX and Gumbel-soft-max methods. In experiments with semi-supervised VAEs, their method outperforms the Gumbel softmax method in terms of accuracy and objective function.\n\nQuality:\n\nThe theoretical derivations seem rigorous and the experiments performed clearly indicate that the proposed method can outperform existing baselines.\n\nClarity:\n\nThe paper is clearly written and easy to read. I found that the network architecture shown in the left of Figure 1 a bit confusing and needs to be explained more clearly.\n\nSignificance:\n\nThe experimental results clearly show that the proposed method can outperform existing baselines and that the proposed contribution is significant.\n\nNovelty:\n\nThe proposed method is novel up to my knowledge. This is the first time I have seen the proposed theoretical derivations, which are significantly different from previous approaches.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper674/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder", "abstract": "Reparameterization of variational auto-encoders is an effective method for reducing the variance of their gradient estimates. However, when the latent variables are discrete, a reparameterization is problematic due to discontinuities in the discrete space. In this work, we extend the direct loss minimization technique to discrete variational auto-encoders. We first reparameterize a discrete random variable using the $\\arg \\max$ function of the Gumbel-Max perturbation model. We then use direct optimization to propagate gradients through the non-differentiable $\\arg \\max$ using two perturbed $\\arg \\max$ operations.\n", "keywords": ["discrete variational auto encoders", "generative models", "perturbation models"], "authorids": ["guy_lorber@campus.technion.ac.il", "tamir.hazan@technion.ac.il"], "authors": ["Guy Lorberbom", "Tamir Hazan"], "pdf": "/pdf/3ddfa8f3ab7ea91f6e06dc5dfe4a138f1614bc81.pdf", "paperhash": "lorberbom|direct_optimization_through_\\arg_\\max_for_discrete_variational_autoencoder", "_bibtex": "@misc{\nlorberbom2019direct,\ntitle={Direct Optimization through $\\arg \\max$  for  Discrete Variational Auto-Encoder},\nauthor={Guy Lorberbom and Tamir Hazan},\nyear={2019},\nurl={https://openreview.net/forum?id=S1ey2sRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper674/Official_Review", "cdate": 1542234405810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1ey2sRcYQ", "replyto": "S1ey2sRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper674/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335777701, "tmdate": 1552335777701, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper674/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}