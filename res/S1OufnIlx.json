{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396318354, "tcdate": 1486396318354, "number": 1, "id": "SyUioGLdx", "invitation": "ICLR.cc/2017/conference/-/paper35/acceptance", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device. \n \n The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions. \n \n In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396318894, "id": "ICLR.cc/2017/conference/-/paper35/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396318894}}}, {"tddate": null, "tmdate": 1484679134761, "tcdate": 1484678474428, "number": 6, "id": "B1fLrJnUx", "invitation": "ICLR.cc/2017/conference/-/paper35/public/comment", "forum": "S1OufnIlx", "replyto": "rJ4Lg0bNe", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "writers": ["~Alexey_Kurakin1"], "content": {"title": "Reply to the review", "comment": "Thanks for the review.\n\nIt's worth noting that the two papers mutually \"scooped\" each other.\nWhile Sharif et al submitted to a conference earlier, our work has been publicly available longer and has more citations.\nAccording to Google scholar on Jan 17, 2017, our article has been cited 9 times ( https://scholar.google.com/scholar?cites=263531058904899909 ), while that of Sharif et al has been cited 4 times ( https://scholar.google.com/scholar?cites=10006479087737690195 ). One of these 4 citations to Sharif et al is the citation we added in November."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287755849, "id": "ICLR.cc/2017/conference/-/paper35/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1OufnIlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper35/reviewers", "ICLR.cc/2017/conference/paper35/areachairs"], "cdate": 1485287755849}}}, {"tddate": null, "tmdate": 1484678426460, "tcdate": 1484678426460, "number": 5, "id": "SyzXr128x", "invitation": "ICLR.cc/2017/conference/-/paper35/public/comment", "forum": "S1OufnIlx", "replyto": "rk2M7kzNe", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "writers": ["~Alexey_Kurakin1"], "content": {"title": "Reply to the review", "comment": "Thanks for the review.\nWe would like to address some of the cons you mentions.\n\n\nWe acknowledge that there are existing papers which describe various adversarial attacks (including white-box and black box attacks) on various machine learning systems (including neural networks).\nHowever we would like to emphasize that the main difference of our work and prior work is the fact that the attacker in prior work has direct access to inputs of the classifier thus can fine-tune all input values, while in our work we have shown that adversarial attacks are possible even when data is perceived through the sensors (like a camera). Overall this observation is important for practical machine learning systems operating in real world.\nAs we mentioned in the paper, the closest work to ours is Sharif et al. However their work has somewhat different scope and also became publicly available after our paper.\n\n\n\nRegarding whether the size of perturbation is small or not.\nWe agree that there is no good quantitative threshold assessing whether a perturbation is small or not and this is a very subjective measure. In this sense our primary criterion was whether the perturbation was large enough to interfere with human recognition of the image.\n\nAlso we have run additional experiments with eps=2 and 4 (results are added to Tables 1, 2 and 3 in the paper). These experiments are showing the same trend as experiments with higher epsilon - adversarial images may remain misclassified after \u201cphoto transformation\u201d. In particular, for eps=2 and 4 about half of \u201cFast\u201d adversarial examples remain misclassified after \u201cphoto transformation\u201d (see destruction rate in Table 3).\n\nArguably adversarial examples with eps=4 and 2 are small enough to be hard to notice (examples of adversarial image with eps=4 can be found in Figure 5). So hopefully the fact that physical adversarial examples exist even for such small epsilon should address your concern.\n\n\n\nRegarding \u201cSome hypotheses proposed in the paper based on one-shot experiments seems too rushy,\u201d we\u2019d be happy to provide further support for any particular hypothesis you think is invalid.\n\n\n\nRegarding \u201cthe results of this paper seems not really improving the understanding of the adversarial example phenomenon\u201d: this paper is an empirical paper and not a theoretical paper, but we\u2019ve found important empirical observations that theory should aim to explain, such as the reduced transferability of iterative adversarial examples.\nIn addition the paper helps to advance research on adversarial examples by showing that they also exist in the physical world.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287755849, "id": "ICLR.cc/2017/conference/-/paper35/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1OufnIlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper35/reviewers", "ICLR.cc/2017/conference/paper35/areachairs"], "cdate": 1485287755849}}}, {"tddate": null, "tmdate": 1484678345804, "tcdate": 1484678345804, "number": 3, "id": "ryf0NkhIe", "invitation": "ICLR.cc/2017/conference/-/paper35/public/comment", "forum": "S1OufnIlx", "replyto": "SyapBiWNe", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "writers": ["~Alexey_Kurakin1"], "content": {"title": "Reply to the review", "comment": "Thanks for the review.\n\nWe would like to address the significance of the paper.\nWe agree that we did not propose radically novel methods or explanations of the adversarial example problem.\nAt the same time the observations made in this paper are significant for practical ML applications because they show how real world machine learning systems are susceptible to adversarial attacks similar to \u201cdigital-only\u201d machine learning systems.\n\nThus the novelty and significance of this paper is in uncovering a new risk for practical real-world machine learning systems. Or in other words it could be considered as a security paper with a new vulnerability disclosed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287755849, "id": "ICLR.cc/2017/conference/-/paper35/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1OufnIlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper35/reviewers", "ICLR.cc/2017/conference/paper35/areachairs"], "cdate": 1485287755849}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484678242596, "tcdate": 1478046320019, "number": 35, "id": "S1OufnIlx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1OufnIlx", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": ["HJGU3Rodl"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481925395568, "tcdate": 1481925395568, "number": 3, "id": "rk2M7kzNe", "invitation": "ICLR.cc/2017/conference/-/paper35/official/review", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/conference/paper35/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper35/AnonReviewer2"], "content": {"title": "Experimental paper with some interesting observations. Overall its contribution is incremental", "rating": "5: Marginally below acceptance threshold", "review": "The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. \n\nPros:\n1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.\n2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. \n\nCons:\n1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.\n2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.\n3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721054, "id": "ICLR.cc/2017/conference/-/paper35/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper35/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper35/AnonReviewer1", "ICLR.cc/2017/conference/paper35/AnonReviewer3", "ICLR.cc/2017/conference/paper35/AnonReviewer2"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721054}}}, {"tddate": null, "tmdate": 1481920588427, "tcdate": 1481920588427, "number": 2, "id": "rJ4Lg0bNe", "invitation": "ICLR.cc/2017/conference/-/paper35/official/review", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/conference/paper35/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper35/AnonReviewer3"], "content": {"title": "incremental but still good to know", "rating": "6: Marginally above acceptance threshold", "review": "In some sense, the Sharif et al. work \"scooped\" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721054, "id": "ICLR.cc/2017/conference/-/paper35/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper35/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper35/AnonReviewer1", "ICLR.cc/2017/conference/paper35/AnonReviewer3", "ICLR.cc/2017/conference/paper35/AnonReviewer2"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721054}}}, {"tddate": null, "tmdate": 1481909700710, "tcdate": 1481909700710, "number": 1, "id": "SyapBiWNe", "invitation": "ICLR.cc/2017/conference/-/paper35/official/review", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/conference/paper35/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper35/AnonReviewer1"], "content": {"title": "Experimental paper. Well written. Solid and interesting experiments. Not sure if contribution is significant enough.", "rating": "6: Marginally above acceptance threshold", "review": "Description.\nThe paper investigates whether adversarial examples survive different geometric and photometric image transformations,\nincluding a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.\nThe paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network.  In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.   \n\nStrong  points.\n- adversarial examples are an interesting phenomenon that is worth detailed investigation.\n- the paper is well written and presented.\n- Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.\n- Experiments are well done and solid.\n\nWeak points:\n- Probably the main negative point is the amount of novelty and contribution. The paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations. Apart from that there is no other main contribution / novelty. While the experiments are solid and well-done, this seems borderline.\n\nDetailed evaluation.\nOriginality:\n- the main contribution of this work is the experimental evaluation showing (and quantifying) how adversarial examples behave under various image transformations.  \n\nQuality:\n- The shown experiments are solid and well done.\n\nClarity:\n- The paper is well written and clear.\n\nSignificance:\n- The findings and shown experiments are interesting, but I not sure if the scale and amount of contribution is significant enough for the main conference track. \n\nOverall:\nExperimental paper. Well written. Solid experiments. Not sure if contribution is significant enough.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721054, "id": "ICLR.cc/2017/conference/-/paper35/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper35/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper35/AnonReviewer1", "ICLR.cc/2017/conference/paper35/AnonReviewer3", "ICLR.cc/2017/conference/paper35/AnonReviewer2"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721054}}}, {"tddate": null, "tmdate": 1481694837916, "tcdate": 1481694837905, "number": 2, "id": "ByAd0I0me", "invitation": "ICLR.cc/2017/conference/-/paper35/public/comment", "forum": "S1OufnIlx", "replyto": "ryMXRcJ7e", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "writers": ["~Alexey_Kurakin1"], "content": {"title": "Reply to reviewer's question", "comment": "Yes, you are correct that this method is not guaranteed to cause a\nmisclassification. We do not claim that it is guaranteed to cause a\nmisclassification. Indeed, we report the accuracy of the model on such\nadversarial examples.\n\nMain motivations of Iter L.L. method were following.\nFirst of all, to find strong adversarial method which would produce non-trivial misclassification, as opposed to fast method which tend to produce less interesting misclassification (like one breed of dog confused with another breed of dog).\nSecond, to keep computational cost manageable.\n\nIter L.L. method achieved both of those goals.\nIn addition we observed that for large enough \\epsilon this method actually was able to force class label to become least likely class. In particular for \\epsilon = 16 more than 90% of all adversarial examples were classified as least likely class y_{LL}.\nFor \\epsilon=8 about 80% of all adversarial images were classified as y_{LL}\n\nAdv_Alpha and Adv_Loss from [1] are shown to produce strong adversarial examples as could be seen from Figure 1 from that paper.\nHowever these two methods are not suitable for our needs for the following reasons:\n1. Both of them are using L_2 norm to construct adversarial examples. Which means that adversarial examples would have fractional pixel values when converted to range [0, 255]. Given that we\u2019re saving and printing adversarial images, all fractional values will be rounded up, and this would lead to decrease of quality of adversarial examples.\nSection 2 of [1] mentions how to use Adv_Alpha method with L_{\\infty} loss, however accuracy on such adversarial examples was not reported in [1]\n2. Adv_Alpha method required to compute Jacobian matrix (i.e. compute gradient for each class) and then solve linear equation for each of class. When dataset has a lot of classes(ImageNet has 1000 distinct classes) this become prohibitively expensive from the computational point of view. On the other iterative least likely class from our paper need to compute gradient only up to 20 times.\n\n\n> Also, what is the actual perturbation of the basic iterative method in the results of Figure 2? When \\epsilon is large, the actual perturbation may be much smaller than \\epsilon.\n\nFor epsilon from 2 to 16 (which we have used in our experiments) L_{\\infty} norm of perturbation was equal or close to actual value of the epsilon. \nFor examples, for \\epsilon=8, L_{\\infty} norm of adversarial perturbation was 8 for all images from validation set. For \\eps=16 more than 98% of generated adversarial perturbations had L_{\\infty} norm equal to 16.\n\nIf you visually look at adversarial images produced by Fast and Iter L.L methods with same epsilon you might have an impression that perturbation of Iter L.L. adversarial images is smaller compared to Fast adversarial images.\nThis is explained by the fact that Fast method modifies all pixels of the image by the same absolute value (but with different sign). At the same time iterative methods are modifying pixels by different value. And only one pixel has to be modified by epsilon for L_{\\infy} norm to be equal epsilon."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287755849, "id": "ICLR.cc/2017/conference/-/paper35/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1OufnIlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper35/reviewers", "ICLR.cc/2017/conference/paper35/areachairs"], "cdate": 1485287755849}}}, {"tddate": null, "tmdate": 1481694637394, "tcdate": 1481694637389, "number": 1, "id": "SJB2a8Cml", "invitation": "ICLR.cc/2017/conference/-/paper35/public/comment", "forum": "S1OufnIlx", "replyto": "SkhI1qP7x", "signatures": ["~Alexey_Kurakin1"], "readers": ["everyone"], "writers": ["~Alexey_Kurakin1"], "content": {"title": "Reply to reviewer's question", "comment": "> Can you provide more details about this timeline?\n\nOur work appeared on arXiv on 8 July 2016, see timestamp of the first version at https://arxiv.org/abs/1607.02533 \n\nSharif's work was submitted to the CCS2016 conference, which had submission deadline 23 May 2016 and acceptance notification on 22 July 2016 (see https://www.sigsac.org/ccs/CCS2016/call-for-papers/ ).\n\nIt seems that they made their work publicly available only in the beginning of November ( https://www.reddit.com/r/hackernews/comments/5auig2/real_and_stealthy_attacks_on_stateoftheart_face/ ). At that time we have learned about it and decided that we should add a citation.\n\n> Other than the three-point description of differences, how does this work differ from the earlier conference submission?\n\n\tThe two works are somewhat different in spirit.\n\tThe goal of our work was to test whether existing forms of adversarial examples have an effect when displayed in the physical world. The goal of the work by Sharif et al was to develop a practical attack against a face recognition system.\n\tSharif et al developed an approach to printing out paper glasses frames that could be attached to real glasses. The design on the paper glasses is chosen to fool a face recognition system. In this case, the goal was primarily to demonstrate that this specific exploit was possible. The number of classes in the machine learning system is small (it can recognize the co-authors of the paper and a few celebrities) and some new development of adversarial example construction techniques was necessary to make the attack viable using only glasses frames.\n\tIn our work, the goal is to measure how well existing adversarial example techniques transfer to the real world. This meant that our focus was on evaluating large numbers of adversarial examples from large numbers of categories (the 1,000 imagenet categories) rather than on developing new adversarial example construction techniques.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287755849, "id": "ICLR.cc/2017/conference/-/paper35/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1OufnIlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper35/reviewers", "ICLR.cc/2017/conference/paper35/areachairs"], "cdate": 1485287755849}}}, {"tddate": null, "tmdate": 1481248596457, "tcdate": 1481248596451, "number": 2, "id": "SkhI1qP7x", "invitation": "ICLR.cc/2017/conference/-/paper35/pre-review/question", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/conference/paper35/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper35/AnonReviewer3"], "content": {"title": "expanding on the Sharif et al. connection", "question": "The submission says, \"The most similar work to this paper is Sharif et al. (2016), which appeared publicly after our work but had been submitted to a conference earlier.\" Can you provide more details about this timeline? Other than the three-point description of differences, how does this work differ from the earlier conference submission?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481248596942, "id": "ICLR.cc/2017/conference/-/paper35/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper35/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper35/AnonReviewer2", "ICLR.cc/2017/conference/paper35/AnonReviewer3"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481248596942}}}, {"tddate": null, "tmdate": 1480729125474, "tcdate": 1480728089635, "number": 1, "id": "ryMXRcJ7e", "invitation": "ICLR.cc/2017/conference/-/paper35/pre-review/question", "forum": "S1OufnIlx", "replyto": "S1OufnIlx", "signatures": ["ICLR.cc/2017/conference/paper35/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper35/AnonReviewer2"], "content": {"title": "About the least-likely class method and the basic iterative method", "question": "Based on my understanding, no matter how large \\epsilon is, the method proposed in section 2.3 can not guarantee that the adversarial sample will be misclassified as y_{LL}. Actually, it can't even guarantee that the image will be misclassified, no matter how large \\epsilon is. See the discussion in Proposition 2 of the paper of Huang et al. [1] I am wondering if the target label actually appears in your experiments. Also, is this effect taken into account in the experiment settings? On the other hand, if the motivation of the LL method is to find a strong adversarial method (as shown in Figure 2), the alpha method in [1] may be a better one.\n\nAlso, what is the actual perturbation of the basic iterative method in the results of Figure 2? When \\epsilon is large, the actual perturbation may be much smaller than \\epsilon. \n\n[1] Huang, R., Xu, B., Schuurmans, D., & Szepesv\u00e1ri, C. (2015). Learning with a strong adversary. CoRR, abs/1511.03034."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Adversarial examples in the physical world", "abstract": "Most existing machine learning classifiers are highly vulnerable to adversarial examples.\nAn adversarial example is a sample of input data which has been modified\nvery slightly in a way that is intended to cause a machine learning classifier\nto misclassify it.\nIn many cases, these modifications can be so subtle that a human observer does\nnot even notice the modification at all, yet the classifier still makes a mistake.\nAdversarial examples pose security concerns\nbecause they could be used to perform an attack on machine learning systems, even if the adversary has no\naccess to the underlying model.\nUp to now, all previous work has assumed a threat model in which the adversary can\nfeed data directly into the machine learning classifier.\nThis is not always the case for systems operating in the physical world,\nfor example those which are using signals from cameras and other sensors as input.\nThis paper shows that even in such physical world scenarios, machine learning systems are vulnerable\nto adversarial examples.\nWe demonstrate this by feeding adversarial images obtained from a cell-phone camera\nto an ImageNet Inception classifier and measuring the classification accuracy of the system.\nWe find that a large fraction of adversarial examples are classified incorrectly\neven when perceived through the camera.", "pdf": "/pdf/e8eca92cff8015f8d0ec8db450144a10b1b9bdab.pdf", "paperhash": "kurakin|adversarial_examples_in_the_physical_world", "conflicts": ["google.com", "openai.com"], "keywords": ["Supervised Learning", "Computer vision"], "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481248596942, "id": "ICLR.cc/2017/conference/-/paper35/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper35/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper35/AnonReviewer2", "ICLR.cc/2017/conference/paper35/AnonReviewer3"], "reply": {"forum": "S1OufnIlx", "replyto": "S1OufnIlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper35/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481248596942}}}], "count": 12}