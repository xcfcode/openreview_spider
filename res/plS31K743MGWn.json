{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1394190780000, "tcdate": 1394190780000, "number": 2, "id": "MgcjPCG7jsM-N", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "PoYaL8FrgDN65", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Just to let you know that an updated version incorporating your feedback has been posted on arxiv. Thanks again for your effort in reviewing the paper!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394190780000, "tcdate": 1394190780000, "number": 1, "id": "ttlj83NGjhko6", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "OA1iAwN-D5AL9", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Just to let you know that an updated version incorporating your feedback has been posted on arxiv. Thanks again for your effort in reviewing the paper!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394190780000, "tcdate": 1394190780000, "number": 1, "id": "EHRL0DwvPfEBx", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "gVSNgqxnTcVC-", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Just to let you know that an updated version incorporating your feedback has been posted on arxiv. Thanks again for your effort in reviewing the paper!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1394190720000, "tcdate": 1394190720000, "number": 2, "id": "gboKgWa1rzbml", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "ZMlu0TaNxgHzF", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Just to let you know that an updated version incorporating your feedback has been posted on arxiv. Thanks again for your effort in reviewing the paper!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393554360000, "tcdate": 1393554360000, "number": 2, "id": "1jsWz5vwWlzRt", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "OA1iAwN-D5AL9", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks again for your further comments. For equation 10, typically, we choose a fixed number of backpropagation steps T (which is typically used in BPTT), so that this 1/T will be absorbed into the step-size. That is, if we do gradient descent on a function J = 1/T J1, we will have w_t = w_{t-1} - mu*(grad J) = w_{t-1} - mu*(grad 1/T J1) = w_{t-1} - (mu/T) grad J1. In other words, we are effectively using a step-size of mu/T to J1 that is the sum instead of average. Of course, I agree with you that if T changes over different training sequences, then it will make a difference. \r\n\r\nRegrading the claims and presentation of the paper, we will incorporate the changes into the paper so that the contribution of the paper is re-stated as we discussed. We will update the paper arxiv soon. Also, in future work, we will do experiments on the synthetic data too to further check the behavior of the algorithm there."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393191000000, "tcdate": 1393191000000, "number": 1, "id": "OA1iAwN-D5AL9", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "E6cFvWOmFRv42", "signatures": ["anonymous reviewer 3c88"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I am still confused about equation 10. I've checked the reference you pointed to, but I could not find the division by T in that paper (e.g. eq. 3 in that paper is just a sum). I do feel that if you are running your model over sequences of different length, you are weighting the error done at each step differently (according to the length of the given sequence) which is not exactly what you want. \r\n\r\nRegarding point 1. I guess I agree with what you are doing to do, and from that perspective is fine. I would tend to agree that there might be a sufficiently large chunk of problems on which long term correlations are irrelevant and you only need RNNs that can deal with short term very well (which in my opinion is what you get when training a model under the constraint that the echo state property is respected). But I'm not finding the paper exactly stating that. You can see phrases like : 'Below, we describe a more rigorous and effective approach to learning RNNs developed in this work' on page 5 bottom, just before you start introducing the new method.  In the conclusion you say: 'Fourth, we show experimentally that\r\nthe new training method motivated by rigorous optimization methodology performs better than the previous methods Mikolov et al. (2011); Pascanu et al. (2013) of learning RNNs using heuristical\r\nrules of truncating gradients during the BPTT procedure.' To me this states that the new method is superior overall compared to other techniques. I'm saying this is probably not true, since you drastically reduce the family of models you are allowing yourself to visit during training. And more exactly you are reducing yourself to those kind of models that can not deal with long term information very well. I might be wrong but I do not see any evidence in the paper against this. E.g. could a model trained this well solve the synthetic datasets from 'On the difficulty of training RNNs' ? Can you get as good solutions as those there?  If not, you should use a more mild language in your claims. If not for any other reason, just because it will probably confuse future readers, expecting some different from this method."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392880080000, "tcdate": 1392880080000, "number": 1, "id": "Phz0PJ58do_fI", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "PoYaL8FrgDN65", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your reading of our paper and useful comments. We will take your feedback into revised manuscript. Also, we tested the plain RNN training algorithm with traditional gradient clipping technique on TIMIT with the same DNN features. The best phone error rate on the test set is found to be between 19.05%-20.5% over a wide range of the threshold values where the best tuned clipping threshold is around 1.0 which corresponds to the error rate of 19.05%. This is higher than the 18.91% from our primal-dual method. Thus, using the new method presented in the paper, we do not need to tune the hyper-parameter of clipping threshold while obtaining lower errors. In addition, we also observe that the proposed primal-dual BPPTT converges faster than the traditional BPTT with gradient clipping. And an important observation from this comparison is that by imposing such a constraint on the recurrent weights, it does not restrict the performance of RNN at least on TIMIT dataset. We will test our algorithm on the dataset you suggested for the long-term dependency performance in future work. Again, we appreciate your feedback on our paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392878700000, "tcdate": 1392878700000, "number": 2, "id": "kg59kX839cksb", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "gVSNgqxnTcVC-", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and kind feedback on our paper. In the revised manuscript, we will improve the presentation in Sec. 3 according to our suggestion. Also, we would like to clarify that the projection onto the set of ||W||_1<r with matrix L1-norm is not scaling it down when it exceeds the threshold. Instead, it is actually a soft-thresholding operation and the threshold has to be computed by solving a nonlinear equation (see  page 188 of N. Parikh and S. Boyd, \u201cProximal Algorithm\u201d Foundations and Trends in Optimization.). Although the nonlinear equation is one-dimension and it can be solved by bi-section, it is still computational expensive."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392878640000, "tcdate": 1392878640000, "number": 1, "id": "E6cFvWOmFRv42", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "dyoYR1N2YCRQu", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your kind feedback on the paper. Please find our response to your main comments below.\r\n\r\n1.\tThe Echo-state property is a sufficient condition (instead of necessary condition) for avoiding exploding gradient. That is, once it is satisfied, there would be no exploding gradient problem. In this paper, our main point is that with such a sufficient condition/constraint, we can train RNN in a principled way that avoids the exploding gradient naturally and performs well on the TIMIT dataset. We are not claiming that this is the only correct way to train RNN. And of course, there could be possibly better approach that is able to exploit the parameter space that goes beyond this constraint. What we are trying to convey here is that, with such constraint, it can be relatively easy to train the RNN in a principled way, and also attain a satisfactory performance level.\r\n\r\n2.\tRegarding the question \u201cEq. (10). You do not want to take an average over the sequence length. That will bias you towards short sequences \u201c, there might be a misunderstanding here. The cost in (10), either written in 1/T sum_{t=1}^T J_t(y_t,d_t) or sum_{t=1}^T J_t(y_t,d_t) are used to measure how good the predicted labels {y_1, y_2, \u2026, y_T} matches the true labels {d_1, d_2, \u2026, d_T} over t=1,\u2026,T. This cost is a standard formulation for training RNN in other literatures too (e.g., R. Pascanu, T. Mikolov, Y. Bengio, \u201cOn the difficulty of training recurrent neural networks\u201d, 2013). And T here means the number of back propagation steps through time.\r\n\r\n3.\tRegarding the question \u201cMinimizing the sum of each column is not the same as minimizing only the column with the maximal sum. You are putting pressure on certain columns even when they are not the maximal one.\u201d, we apologize for for the possible confusion. We would like to clarify that we are not minimizing the maximum sum of the columns. Instead, we are imposing constraint on the maximum sum of the columns (||W||_{inf} < gamma) when minimizing the negative cross entropy between the predicted labels and the true labels. And another thing we would like to clarify is that this is equivalent to imposing constraint on the sum of each column. In other words, max{s_1, \u2026, s_N} < r \u21d4 s_1<r, s_2<r, \u2026 s_N<r, where s_k is the absolute sum of the k-th column. To see this, first we note that the left hand side implies the right-hand side immediately since the max of {s_1, \u2026 s_N} being less than r means each of them will be less than r. Next, we can also see the right-hand side means the left-hand side because if each of {s_1,\u2026, s_N} is less than r, then the maximum of them will also be less than r. In other words, these two conditions are equivalent. Moreover, this technique is a standard approach that is widely used in optimization (e.g., S. Boyd and L. Vandenberghe \u201cConvex Optimization\u201d, page 150-151)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392878520000, "tcdate": 1392878520000, "number": 1, "id": "77lR7U3aJP-jB", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "plS31K743MGWn", "replyto": "ZMlu0TaNxgHzF", "signatures": ["Jianshu Chen"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We would like to thank the reviewer for the kind review and feedback on the paper. We include our response to the major points of the review below:\r\n\r\n1.\tIt is true that both ARMA and the bidirectional RNN use information from past and future. However, the bidirectional RNN uses information from future by letting its hidden layers depend on the hidden states at future. On the other hand, the ARMA is made to depend on future by letting its input include future. So these two methods uses information from future in two different ways and our ARMA method is much simpler and is equally effective.\r\n\r\n2.\tThe \u201ctandem\u201d architecture mentioned by the reviewer --- concatenating the posterior output (i.e., the top layer) of a neural network with the MFCC features as new inputs to an HMM recognition --- was actually first proposed in 2000 (Hermansky, Hynek; Ellis, Daniel P. W. and Sharma, Sangita \u201cTandem connectionist feature stream extraction for conventional GMM-HMM systems,\u201d ICASSP, 2000, much earlier than 2012, which added also the bottleneck features. We exploit the DNN features in a much different way. First, we take the hidden layer of the DNN as the features, which are shown experimentally in our work to be much better than the top-layer posterior outputs as in the standard \u201ctandem\u201d method. Second, the hidden layer of the DNN is shown experimentally in our work to be much better features than the hidden layers below (similar to the bottleneck features). Third, rather than using the GMM-HMM as the separate sequence classifier, we use the RNN as the sequence classifier.\r\n\r\n3.\tThe main motivation of the paper is to propose a method to train RNN in a principled. We just use sigmoid neuron as an example. It can also be extended to the ReLU case.\r\n\r\n4.\tWe will add the reference suggested by the reviewer in the revised manuscript and also other revisions suggested by the reviewer."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392171780000, "tcdate": 1392171780000, "number": 4, "id": "PoYaL8FrgDN65", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "plS31K743MGWn", "replyto": "plS31K743MGWn", "signatures": ["Justin Bayer"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "(Disclaimer: It is quite a while since I read this paper carefully and just wanted to put my comments here before the review phase closes.)\r\n\r\nI think that this work has several interesting data points for research on recurrent networks. \r\n\r\n(1) The regulariser is an idea which had to be tried. While it does not work well on raw signals the results with DNN features are in the leading pack, if I am correct (see suggestion below). \r\n\r\n(2) It is easy to say in hindsight that the neglected parameter volume is a reason for the performance. But interestingly, DNN preprocessing somehow can make up for this. I do not know of any work which shows static (i.e. time step wise) preprocessing to ease the long term dependency problems of RNNs.\r\n\r\nSuggestions/Questions:\r\n- add a comparative table of TIMIT results \r\n- try the method on data sets with long term dependencies (see [1]) to validate whether the method has detrimental effects, as one would expect.\r\n- how well does a carefully trained plain RNN with no regularizer do on the DNN features?\r\n\r\n[1] I know there is a better reference by (Hochreiter & Schmidhuber), but this one has to work for now: Martens, James, and Ilya Sutskever. 'Learning recurrent neural networks with hessian-free optimization.' Proceedings of the 28th International Conference on Machine Learning (ICML-11). 2011."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391862780000, "tcdate": 1391862780000, "number": 3, "id": "gVSNgqxnTcVC-", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "plS31K743MGWn", "replyto": "plS31K743MGWn", "signatures": ["anonymous reviewer a863"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "review": "This work presents a method for training RNNs that achieves good results on TIMIT.   The method applies\r\na deep recurrent neural network similar to Graves et al. (ICASSP 2013) and achieves good results on TIMIT.  The main\r\nnovelty here is the introduction of a new training method which enforces a constraint of a certain\r\neasily-computable matirx norm using lagrange multipliers.  As a result, the network improves its\r\nperformance from 19.05% to 18.86%.   The work also introduces a number of RNN variants that get their\r\ninformation from a fairly wide range of frames. \r\n\r\nThe main idea is related to previous analyses of the exploding gradient problem.  The approach taken\r\nby this work is to force the RNN's weights to be small at all times, thus ensuring that the RNN\r\nnever has exploding gradinets.  \r\n\r\nThere are several weaknesses in the paper.  First, it is quite verbose, spending pages on standard\r\ndefinitions of RNNs and derivations of their learning rules.  Second, the paper chose to use\r\nlagrange multipliers, with another lengthy derivation, but did not compare with the much simpler\r\nprojected gradient descent, where we simply shrink any weights that become too large (i.e.,\r\nif ||w||_1 is too large, scale it down until it is of the right size).  And third, the improvement\r\nover previous work is quite small."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391821440000, "tcdate": 1391821440000, "number": 2, "id": "dyoYR1N2YCRQu", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "plS31K743MGWn", "replyto": "plS31K743MGWn", "signatures": ["anonymous reviewer 3c88"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "review": "Summary:\r\n------------\r\n\r\nThe paper introduces a primal-dual method for training RNNs as well as a few other structural changes to the base model. The algorithm is tested on the TIMIT dataset.\r\nComments:\r\n--------------\r\n\r\n I think my main observation about the paper is that it misinterprets the exploding gradient problem/vanishing gradient problem as well as the echo state property.\r\n\r\nSimply put, comparing the norm of the recurrent weight with 1 (for tanh) just gives either a necessary condition for gradients to explode. That means that there is a large volume of parameter configuration that do not satisfy this condition and yet the gradients do not explode.\r\nThis constraint, in some sense, over-restricts the capacity of the model. It is true that the empirical evidence shows that the model still performs well (arguably better than the unconstrained one) but this is just a data point (a single dataset) and is hard to draw conclusion such as the new method has `superior performance`. It is far from clear to me that this is true, I would argue that by excluding that large volume of possible values, with some positive probability this new learning algorithm is inferior on several tasks.\r\nThe echo state property is also an approximation of what exactly we want from the model, as most of the analysis of recurrent networks. Basically the echo state property assumes that if **no input** is presented the, now, dynamical system will converge to a point attractor at the origin. In reality we care about the behavior of the model in the presence of input. This is mathematically much more difficult to analyze though there is some effort done e.g.Manjunath, G., Jaeger, H. (2013): Echo State Property Linked to an Input: Exploring a Fundamental Characteristic of Recurrent Neural Networks.\r\n\r\nFurther more, I would argue that the model trained with the primal-dual method will suffer a lot more from the vanishing gradient problem and it is potentially one of the crucial factor that makes the ARMA variant outperform the other variations (as it explicitly uses a time window)\r\n\r\n\r\nOther comments\r\n--------------------\r\n(1) I feel wordings like 'superior performance', 'demonstrate the effectiveness' can be misleading. Providing the number 18.86% in the abstract, without a point of comparison, is also not very useful.\r\n(2) I'm not sure I understand why stacked RNN suffer from an overfitting problem ? And would there be a reason, if they do suffer, for this overfitting to not be addressed by say weight noise or some other regularization ?\r\n (3) Eq. (10). You do not want to take an average over the sequence length. That will bias you towards short sequences\r\n(4) Most of the equation on page 5 are not very useful (showing the cost and gradients for softmax and cross-entropy vs linear units and square error). In general I feel there are more equation then necessary, making the text harder to read\r\n(6) Minimizing the sum of each column is not the same as minimizing only the column with the maximal sum. You are putting pressure on certain columns even when they are not the maximal one.\r\n(7) There are hardly any details about the experiments that you run."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391808960000, "tcdate": 1391808960000, "number": 1, "id": "ZMlu0TaNxgHzF", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "plS31K743MGWn", "replyto": "plS31K743MGWn", "signatures": ["anonymous reviewer ce6f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "review": "Pro: \r\n\u2022\tThe concept of using AR/ARMA for RNN training, and formulating this as a convex optimization problem, and interesting.\r\nCons:\r\n\u2022\tA new idea (AR/ARMA) is introduced as yet another technique to deal with the vanish gradient problems when training RNNs. You touch in the related work on how this is different than other ideas in this space. I would have liked to see empirical comparisons in the experimental section, at least with a few of the ideas (i.e. Martens\u2019 HF approach) to better understand the value of this technique.\r\n\u2022\tARMA seems similar to the bidirectional RNN proposed by Graves (ICASSP 2013) and experiments or discussion comparing this should be done.\r\n\u2022\tThe experimental results are not convincing. \r\no\tDNNs trained with filter-banks on TIMIT are around 20% (see Abdel-rahman Mohamed, George Dahl, Geoffrey Hinton, 'Acoustic Modeling using Deep Belief Networks'. Accepted for publication in IEEE Trans. on Audio, Speech and Language Processing) while your RNN results are around 28%\r\no\tIt seems that most of the gains you are getting are by using DNN-based features as input in to the RNN. The idea of extracting features from the DNN and then training another network is not new in speech recognition (see Z. T\u00fcske, M. Sundermeyer, R. Schl\u00fcter, and H. Ney. \u201cContext-Dependent MLPs for LVCSR: TANDEM, Hybrid or Both?\u201d, in Proc. Interspeech, September 2012). What happens if you train your DNN in the same way, how would this compare to the RNN?\r\n\u2022\tReferences could be improved, citing the most relevant papers in the field rather. Will indicate this in my comments below.\r\n\r\nHere are my comments per section:\r\n\u2022\tPage 1, section 1: When describing papers that cut recognition errors for speech, the main papers are Hinton 2012, Dahl 2011 which you have. In addition, pls include F. Seide\u2019s DNN SWB paper from Interspeech 2011, B. Kingbury\u2019s HF paper from Interspeech 2012 and T. Sainath\u2019s paper from ICSASSP 2013. These 3 papers showed the biggest impacts in reducing WER across many LVCSR tasks. Pls remove Deng2013b.\r\n\u2022\tPage 1, section 1: Its not clear from the intro how AR/ARMA ideas address the vanishing gradient issues with RNNs discussed in para 1. Pls elaborate on this more\r\n\u2022\tSection 2, page 2: Oriyal Vinayls RNN paper should be cited as well along with Maas, Graves,  work.\r\n\u2022\tSection 2, page 2: in describing DNNs ability to extract high-level features, Yann Lecun has a paper on this and should be cited: Y.LeCun,\u201cLearningInvariantFeatureHierarchies,\u201dinEuropeanCon- ference on Computer Vision (ECCV). 2012, vol. 7583 of Lecture Notes in Computer Science, pp. 496\u2013505, Springer.\r\n\u2022\tSection 3.2, page 3: Why did you use sigmoid instead of ReLU ,which has been shown extensively to work much better on TIMIT\r\n\u2022\tSection 3.2, page 3: you are move eq (3) as you don\u2019t use it in the paper\r\n\u2022\tSection 3.2, page 3: The ARMA model, which looks into the future, related to the biodirectinal RNN proposed by Graves (ICASSP 2013). The motiviation seems similar and thus differences should be clarified.\r\n\u2022\tSection 3.3, page 4: most of this is review from other papers and does not need 2 pages of explanation. Just summarize the main high level points so more of the paper can be focused in Section 4 on your novel contributions\r\n\u2022\tSection 4, page 6: I\u2019m not sure why you say RNNs require relatively short memory of just 30 frames. Oriyal Vinyals\u2019 RNN paper shows that 60 is more reasonable. In addition, graves uses an LSTM framework which allows for more long-range dependencies which he shows is better than an RNN.\r\n\u2022\tSection 5, page 9: The TIMIT paper to be cited is Kai Fu Lee\u2019s paper which describes the standard protocol, not (Hintin 2012 and Deng 2013a): K. F. Lee and H. W. Hon, \u201cSpeaker-independent Phone Recognition \u2028Using Hidden Markov Models,\u201d IEEE Transacations on Acoustics, \u2028Speech and Signal Processing, vol. 37, pp. 1641\u20131648, 1989 \r\n\u2022\tSection 5, page 10: I\u2019m not convinced by the experimental results, as indicated above. Further comparison should be done with other RNN training methods, as well as with stronger DNN baselines."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387533120000, "tcdate": 1387533120000, "number": 67, "id": "plS31K743MGWn", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "plS31K743MGWn", "signatures": ["chenjianshu@gmail.com"], "readers": ["everyone"], "content": {"title": "A Primal-Dual Method for Training Recurrent Neural Networks Constrained by the Echo-State Property", "decision": "submitted, no decision", "abstract": "We present an architecture of a recurrent neural network (RNN) with a fully-connected deep neural network (DNN) as its feature extractor. The RNN is equipped with both causal temporal prediction and non-causal look-ahead, via auto-regression (AR) and moving-average (MA), respectively. The focus of this paper is a primal-dual training method that formulates the learning of the RNN as a formal optimization problem with an inequality constraint that guarantees stability of the network dynamics. Experimental results demonstrate the effectiveness of this new method, which achieves 18.86% phone recognition error on the TIMIT benchmark for the core test set. The results also show that the proposed primal-dual training method produces lower recognition errors than the popular RNN methods developed earlier based on the carefully tuned threshold parameter that heuristically prevents the gradient from exploding.", "pdf": "https://arxiv.org/abs/1311.6091", "paperhash": "chen|a_primaldual_method_for_training_recurrent_neural_networks_constrained_by_the_echostate_property", "keywords": [], "conflicts": [], "authors": ["Jianshu Chen", "Li Deng"], "authorids": ["chenjianshu@gmail.com", "deng629@gmail.com"]}, "writers": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 15}