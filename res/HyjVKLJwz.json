{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573621227, "tcdate": 1521573621227, "number": 328, "cdate": 1521573620896, "id": "rypxJk19z", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HyjVKLJwz", "replyto": "HyjVKLJwz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict Responsibly: Increasing Fairness by Learning to Defer", "abstract": "When machine learning models are used for high-stakes decisions, they should predict accurately, fairly, and responsibly. To fulfill these three requirements, a model must be able to output a reject option (i.e. say \"``I Don't Know\") when it is not qualified to make a prediction. In this work, we propose learning to defer, a method by which a model can defer judgment to a downstream decision-maker such as a human user. We show that learning to defer generalizes the rejection learning framework in two ways: by considering the effect of other agents in the decision-making process, and by allowing for optimization of complex objectives. We propose a learning algorithm which accounts for potential biases held by decision-makerslater in a pipeline. Experiments on real-world datasets demonstrate that learning\nto defer can make a model not only more accurate but also less biased. Even when\noperated by highly biased users, we show that\ndeferring models can still greatly improve the fairness of the entire pipeline.", "pdf": "/pdf/d3c6975dd33d3ce19c7a3a1379b4148b0dd37583.pdf", "TL;DR": "Incorporating the ability to say I-don't-know can improve the fairness of a classifier without sacrificing too much accuracy, and this improvement magnifies when the classifier has insight into downstream decision-making.", "paperhash": "madras|predict_responsibly_increasing_fairness_by_learning_to_defer", "_bibtex": "@misc{\nmadras2018predict,\ntitle={Predict Responsibly: Increasing Fairness by Learning to Defer},\nauthor={David Madras, Toniann Pitassi, Richard Zemel},\nyear={2018},\nurl={https://openreview.net/forum?id=SJUX_MWCZ},\n}", "keywords": ["Fairness", "IDK", "Calibration", "Automated decision-making", "Transparency", "Accountability"], "authors": ["David Madras", "Toniann Pitassi", "Richard Zemel"], "authorids": ["david.madras@mail.utoronto.ca", "zemel@cs.toronto.edu", "toni@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730162406, "tcdate": 1518459187636, "number": 185, "cdate": 1518459187636, "id": "HyjVKLJwz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HyjVKLJwz", "original": "SJUX_MWCZ", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Predict Responsibly: Increasing Fairness by Learning to Defer", "abstract": "When machine learning models are used for high-stakes decisions, they should predict accurately, fairly, and responsibly. To fulfill these three requirements, a model must be able to output a reject option (i.e. say \"``I Don't Know\") when it is not qualified to make a prediction. In this work, we propose learning to defer, a method by which a model can defer judgment to a downstream decision-maker such as a human user. We show that learning to defer generalizes the rejection learning framework in two ways: by considering the effect of other agents in the decision-making process, and by allowing for optimization of complex objectives. We propose a learning algorithm which accounts for potential biases held by decision-makerslater in a pipeline. Experiments on real-world datasets demonstrate that learning\nto defer can make a model not only more accurate but also less biased. Even when\noperated by highly biased users, we show that\ndeferring models can still greatly improve the fairness of the entire pipeline.", "pdf": "/pdf/d3c6975dd33d3ce19c7a3a1379b4148b0dd37583.pdf", "TL;DR": "Incorporating the ability to say I-don't-know can improve the fairness of a classifier without sacrificing too much accuracy, and this improvement magnifies when the classifier has insight into downstream decision-making.", "paperhash": "madras|predict_responsibly_increasing_fairness_by_learning_to_defer", "_bibtex": "@misc{\nmadras2018predict,\ntitle={Predict Responsibly: Increasing Fairness by Learning to Defer},\nauthor={David Madras, Toniann Pitassi, Richard Zemel},\nyear={2018},\nurl={https://openreview.net/forum?id=SJUX_MWCZ},\n}", "keywords": ["Fairness", "IDK", "Calibration", "Automated decision-making", "Transparency", "Accountability"], "authors": ["David Madras", "Toniann Pitassi", "Richard Zemel"], "authorids": ["david.madras@mail.utoronto.ca", "zemel@cs.toronto.edu", "toni@cs.toronto.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730162406, "tcdate": 1509136414403, "number": 869, "cdate": 1518730162397, "id": "SJUX_MWCZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJUX_MWCZ", "original": "SkrQdzWCW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Predict Responsibly: Increasing Fairness by Learning to Defer", "abstract": "When machine learning models are used for high-stakes decisions, they should predict accurately, fairly, and responsibly. To fulfill these three requirements, a model must be able to output a reject option (i.e. say \"``I Don't Know\") when it is not qualified to make a prediction. In this work, we propose learning to defer, a method by which a model can defer judgment to a downstream decision-maker such as a human user. We show that learning to defer generalizes the rejection learning framework in two ways: by considering the effect of other agents in the decision-making process, and by allowing for optimization of complex objectives. We propose a learning algorithm which accounts for potential biases held by decision-makerslater in a pipeline. Experiments on real-world datasets demonstrate that learning\nto defer can make a model not only more accurate but also less biased. Even when\noperated by highly biased users, we show that\ndeferring models can still greatly improve the fairness of the entire pipeline.", "pdf": "/pdf/d3c6975dd33d3ce19c7a3a1379b4148b0dd37583.pdf", "TL;DR": "Incorporating the ability to say I-don't-know can improve the fairness of a classifier without sacrificing too much accuracy, and this improvement magnifies when the classifier has insight into downstream decision-making.", "paperhash": "madras|predict_responsibly_increasing_fairness_by_learning_to_defer", "_bibtex": "@misc{\nmadras2018predict,\ntitle={Predict Responsibly: Increasing Fairness by Learning to Defer},\nauthor={David Madras and Toniann Pitassi and Richard Zemel},\nyear={2018},\nurl={https://openreview.net/forum?id=SJUX_MWCZ},\n}", "keywords": ["Fairness", "IDK", "Calibration", "Automated decision-making", "Transparency", "Accountability"], "authors": ["David Madras", "Toniann Pitassi", "Richard Zemel"], "authorids": ["david.madras@mail.utoronto.ca", "zemel@cs.toronto.edu", "toni@cs.toronto.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}