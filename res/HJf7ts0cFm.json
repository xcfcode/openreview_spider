{"notes": [{"id": "HJf7ts0cFm", "original": "B1l3kE55KQ", "number": 431, "cdate": 1538087803027, "ddate": null, "tcdate": 1538087803027, "tmdate": 1545355392294, "tddate": null, "forum": "HJf7ts0cFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJxYy9ieg4", "original": null, "number": 1, "cdate": 1544759776925, "ddate": null, "tcdate": 1544759776925, "tmdate": 1545354518810, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Meta_Review", "content": {"metareview": "the authors propose to incorporate an additional layer between the consecutive steps in LSTM by introducing a radial basis function layer (with dot product kernel and softmax) followed by a linear layer to make LSTM similar to or better at (by being more explicit) capturing DFA-like transition. the motivation is relatively straightforward, but it does not really resolve the issue of whether existing formulations of RNN's cannot capture such transition. since this was not shown theoretically nor intuitively, it is important for empirical evaluations to be thorough and clearly show that the proposed approach does indeed outperform the vanilla LSTM (with peepholes) when the capacity (e.g., the number of parameters) matches. unfortunately it has been the consensus among the reviewers that more thorough comparison on more conventional benchmarks are needed to convince them of the merit of the proposed approach.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "RBFN is back; a bit more work necessary"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper431/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353220537, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper431/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353220537}}}, {"id": "rJxDU40zC7", "original": null, "number": 5, "cdate": 1542804558738, "ddate": null, "tcdate": 1542804558738, "tmdate": 1542804558738, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "content": {"title": "The end of the discussion period is approaching", "comment": "Dear reviewers, \n\nThe period during which we can address your comments and make changes to the submission is coming to an end. We wanted to make sure that we have addressed your main concerns in a sufficient manner. Please let us know if you have additional suggestions for improvements. We'll be happy to incorporate those into the paper.  Thank you again for your valuable reviews and for considering our responses and revisions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper431/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620296, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJf7ts0cFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper431/Authors|ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620296}}}, {"id": "SJlckk2taX", "original": null, "number": 4, "cdate": 1542205153976, "ddate": null, "tcdate": 1542205153976, "tmdate": 1542205153976, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "SJeM_VnbaX", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you very much for your insightful review. \n\nWe have uploaded a revision of the paper which we hope addresses all of your concerns. \n\nLet us address your comments and questions one by one.\n\n\nComment (1): \u201cNo experiments on widely used benchmarks for RNNs (e.g. language modeling, arithmetic tasks (for instance see Zaremba and Sutskever, 2015) ). Have you tried this by any chance?\u201d\n\nSequential MNIST and IMDB are benchmarks commonly used for RNNs, specifically when assessing their memorization behavior. However, based on your comment and the comments of your fellow reviewers, we have added experiments for a commonly used language modeling dataset. We have added the results in Appendix C. In short, we outperform the vanilla LSTM and the LSTM with peephole connections. However, we cannot outperform the state of the art on this dataset which typically uses some form of attention mechanism. Please note that our objective is not primarily to outperform the state of the art on all benchmark datasets. Rather, we want to better understand the ways in which we can interpret and improve the learning behavior of RNNs. Previous work with this motivation has often not conducted any experiments on non-synthetic datasets (e.g., [2]).\n\nIn [1] the task used to evaluate memorization capabilities of LSTMs is the \u201cCopy task\u201d which was also mentioned by reviewer 2. Please note that the Palindrome task (recognizing ww^{-1}) also requires memorization of the entire sequence w. Based on your comment, we have included a reference to [1] in the paper. Also note that, similar to [1,2], we have used curriculum learning in the experiments for BP and Palindrome for all methods to have a fair comparison. \n\n\nComment (2): \u201cTheorems 3.1 and 3.2 are presented without proof. Will be good to at least include it in the appendix.\u201d\n\nWe have included full proofs in a new appendix (Appendix A). \n\n\nComment (3): \u201cIMDB experiments: you claim that SR-LSTM and SR-LSTM-p have \"superior\" extrapolation capabilities than vanilla LSTMs. However, as SR-LSTM and SR-LSTM-p give far lower train error rate, it's not strictly fair to claim that they extrapolate better to longer sequences than encountered during training time.\u201d\n\nThanks for pointing out that we left enough room for misunderstanding the results. The error rates listed in Table 4 are for sequences of length 100 and 200 when training on truncated sequences of length 10. Hence, we trained the models on truncated sequences of length 10 and tested the models on the training data (with length 100 and 200) and the test data (with length 100 and 200). Hence, the training error is not the error on the truncated sequences of length 10. We hope that this clarifies the experimental results listed in Table 4. \n\nWhile it is true that the SR-LSTM and SR-LSTM-p have more parameters than the LSTM and LSTM-p (due to the addition of the centroids), your observation that the state-regularized RNNs should be more constrained is absolutely correct. The training error on the truncated sequences of length 10 is indeed lower for the vanilla LSTMs. \n\n\nComment (4): \u201cMNIST experiments: please include results for SR-LSTM\u201d\n\nWe have included the results for the SR-LSTM in the revised version of the paper.\n\nWe have also corrected the typo you mentioned in \u201cMinor comments\u201d \n\nWe appreciate your insightful review which, as you can tell, has allowed us to improve the paper. Please let us know if there is anything else we can do. \n\n\n[1] Zaremba and Sutskever, Learning to Execute, 2015\n[2] Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. 2018.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620296, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJf7ts0cFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper431/Authors|ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620296}}}, {"id": "H1xGBniFp7", "original": null, "number": 3, "cdate": 1542204473812, "ddate": null, "tcdate": 1542204473812, "tmdate": 1542204473812, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "Hyel1qtb67", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you very much for the encouraging and helpful review. \n\nFirst of all, all points you are raising are good observations and have helped us to improve the paper. We have uploaded an additional revised version.\n\nLet us address your concerns one by one.\n\n\nComment 1: \"It would be interesting to train both models with specific sequence length and then keep testing them with longer sequence length and compare the performance.\u201d\n\nIf we don\u2019t misunderstand your suggestion, this is what we did with the experiments using the BP (Table 1), Palindrome (Table 3), and IMDB data (Table 4). In all of these cases, we have trained the model on shorter sequences (or sequences with smaller nesting depth as in BP), and then evaluated the trained models on longer sequences. With these experiments, we want to show that state-regularized LSTMs with peephole connections tend to extrapolate better to more complex/longer sequences even when trained on shorter sequences only. Please let us know if we misunderstood your comment.\n\n\nComment 2: \u201cTheorem 3.1 and 3.2 have no proofs. Please make them as notes rather than theorems.\u201d\n\nWe completely agree and have updated both the statements of the theorems and added detailed proofs to the appendix (new Appendix A). \n\n\nComment 3: \u201cWhat do different colors in Figure 6 stands for?\u201d\n\nEach color corresponds to one dimension in the respective state vectors (hidden or cell state). In Figure 6, we visualize the hidden and cell states each having 10 dimensions. For instance, the red line in Figure 6(d) was highlighted by us to show that this particular value of the cell state vector of the SR-LSTM-p memorizes the number of encountered open parentheses. \n\n\nComment 4: \u201cIn the MNIST task authors claim that they have significant improvement when compared to LSTM. I am not sure if that is accurate. Also, why do you compare SR-LSTM-p only with LSTM? What is the performance of LSTM-p? Please report that as well.\u201d\n\nThe SR-LSTM-p leads to an accuracy improvement of 1.5 percentage points on sequential MNIST and 0.9 percentage points on permuted MNIST. These differences have been considered significant in previous work. However, we are not insisting on the word \u201csignificant\u201d and are happy to use simply \u201cimprovement.\u201d We have changed this in the revised version.\n\nComment 5: \u201cEven in table 3, can you please report the performance of LSTM-p?\u201d\n\nBased on your comments (4) and (5), we have included additional experimental results for the LSTM-p. We report results of the LSTM-p on all datasets including IMDB and MNIST. Moreover, we have conducted an additional set of experiments on a language modeling dataset and have included this in Appendix C. All LSTM-p results have either been added to the tables in the main part of the paper or have been added to the said appendix. \n\n\nMinor comment: \u201cFig 6 is not referred anywhere.\u201d\n\nThank you. We have fixed the reference to the Figure in the paper. \n\n\nWe want to thank you for acknowledging the merit of our contribution as being in line with work that attempts to better understand what and how RNNs learn. The main aim of our work is not to tune state-regularized LSTMs to outperform state of the art methods on benchmark datasets. Rather, we want to understand and develop a mechanism that allows one to encourage RNNs to operate more like automata with external memory. That\u2019s why we have focused on visualizations such as Figure 6-8. Moreover, we believe that the proposed method allows one to extract DFAs representing the state transition behavior and supports novel ways to interpret the working of RNNs on text and visual data. \n\nAgain, thank you. Please let us know if there is anything else we can do to improve the paper. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620296, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJf7ts0cFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper431/Authors|ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620296}}}, {"id": "HJg-saGEpQ", "original": null, "number": 2, "cdate": 1541840280556, "ddate": null, "tcdate": 1541840280556, "tmdate": 1541840280556, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJxlpD793m", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you very much for your helpful review. \n\nDue to your remarks, we have conducted additional experiments on a commonly used language modeling dataset. These results can be found in Appendix C. We want to emphasize that, while we also outperform the vanilla LSTMs on said dataset, a number of previous papers (e.g., [1]) have shown that the ability to model long-range dependencies is not so important in language modeling (i.e., next word prediction). A better way to handle long-range dependencies can improve the results only modestly since the majority of predictions can be accurately made based on a few previous words. It is for this reason that prior work has often used IMDB and sequential MNIST because the ability to model long-range dependencies is more crucial for these datasets. Please also note that we see our paper in line with previous work on DFA extraction [2, 3] which is why we chose the BP and Tomita datasets. Moreover, the Palindrome language we use in the paper is similar to the Copy problem as it is also required to memorize all the input tokens. It is therefore very similar to the copy task that you mention in your review. The results in Table 3 show that the sr-lstm-p outperforms the vanilla LSTM on the Palindrome data. \n\nPrevious work has shown that LSTMs are better than GRUs at counting due to their ability to memorize counts in the cell state. For instance, in [2] it was shown that LSTMs perform much better than GRUs on simpler counting tasks (simpler than e.g. the BP language we use).  In Figure 6, 12, and 13 we also show that the cell state is indeed used as memory in the sr-lstm-p. Hence, there is evidence that indeed, the cell state is used as memory in LSTMs. With our work, we want to encourage RNNs to mainly use the cell state for memorization. \n\nWe have added additional results (Appendix C) for the LSTM with peephole connections and without state-regularization. We agree that this is important. We show that the sr-lstm-p outperforms the lstm-p. \n\nBased on your helpful input, we have also included a more thorough description of what we mean when we say that vanilla RNNs often operate more like DFAs. In short, DFAs memorize everything about the current input sequence in the current state and do not have access to additional memory components. RNNs (without state regularization) tend to do the same based on empirical results. Indeed, in Figure 6 we show that most of the memorization is accomplished with the hidden state and to a lesser extent by the cell state. There is also evidence from previous work. For instance, in [3] was shown that LSTMs trained on BP behaved more like DFAs. \n\nWe fully agree with your comment that Theorems 3.1 and 3.2 were vaguely formulated. We have improved the formulation of the theorems and added proofs of both theorems (new Appendix A). \n\nWe have also included a few more citations in the related work section. We would highly appreciate pointers to additional LSTM regularization methods that we could add to the discussion. \n\nWe have fixed all the issues you have listed under \u201cminor points.\u201d We have added a citation to Theano, we have fixed the typo, and added a sentence describing the state drift behavior first observed in [4].\n\nFinally, we want to emphasize that the main contribution we see in state-regularized LSTMs is to gain a better understanding of the workings of RNNs and the ways one could encourage them to behave more like automata with external memory. It is not our intention to show that these models outperform all state of the art methods. We have put more emphasis on understanding what and how exactly RNNs learn (by e.g. creating visualizations such as Figure 6) and less on tuning them to outperform the state of the art. Please note that this is an up and coming research theme with the recent BlackboxNLP (https://blackboxnlp.github.io/) workshop having more than 600 attendees. \n\nThank you again for your helpful review. We hope you take our response and revised paper into account. Please let us know if there is anything else we can do to improve the paper. \n\n\n[1] Micha\u0142 Daniluk, Tim Rockt\u00e4schel, Johannes Welbl, and Sebastian Riedel. Frustratingly short attention spans in neural language modeling. 2017b.\n[2] Gail Weiss, Yoav Goldberg, and Eran Yahav. On the practical computational power of finite precision rnns for language recognition. 2018.\n[3] Gail Weiss, Yoav Goldberg, and Eran Yahav. Extracting automata from recurrent neural networks using queries and counterexamples. 2018.\n[4] Zheng Zeng, Rodney M Goodman, and Padhraic Smyth. Learning finite state machines with self-clustering recurrent networks. 1993.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620296, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJf7ts0cFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper431/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper431/Authors|ICLR.cc/2019/Conference/Paper431/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers", "ICLR.cc/2019/Conference/Paper431/Authors", "ICLR.cc/2019/Conference/Paper431/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620296}}}, {"id": "SJeM_VnbaX", "original": null, "number": 3, "cdate": 1541682281946, "ddate": null, "tcdate": 1541682281946, "tmdate": 1541682281946, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "content": {"title": "Interesting approach on embedding finite state space transitions into RNN dynamics. General empirical usefulness remains to be seen.", "review": "This paper proposes a novel architecture and regularization technique for RNN, where the hidden state of an RNN is one of (or a soft weighted average of) a finite number of learnable clusters. This has two claimed benefits: (1) extracting finite state automata from an RNN is much simpler, and (2) forces RNN to operate like an automata and less like finite state machines. The authors make (1) immediately clear, and show (2) with empirical results.\n\nMajor comments:\n\n(1) No experiments on widely used benchmarks for RNNs (e.g. language modeling, arithmetic tasks (for instance see Zaremba and Sutskever, 2015) ). Have you tried this by any chance?\n\n(2) Theorems 3.1 and 3.2 are presented without proof. Will be good to at least include it in the appendix.\n\n(3) IMDB experiments: you claim that SR-LSTM and SR-LSTM-p have \"superior\" extrapolation capabilities than vanilla LSTMs. However, as SR-LSTM and SR-LSTM-p give far lower train error rate, it's not strictly fair to claim that they extrapolate better to longer sequences than encountered during training time. \n\nIs the number of parameters held constant across 3 models? I'm struggling to understand why the training performance of the proposed models is significantly better than pure LSTM. For SR-LSTM-P I can see this being the case (the peephole connections effectively increase the hidden state size), but why does SR-LSTM (whose hidden states should be more constrained than pure LSTMS) perform better than LSTM during training? This makes me wonder whether SR-LSTM and SR-LSTM-P have higher capacity than LSTM somehow.\n\n(4) MNIST experiments : please include results for SR-LSTM\n\nMinor comments:\n\n(1) page 8 : MNIST imagse -> images", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "cdate": 1542234463208, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335722589, "tmdate": 1552335722589, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyel1qtb67", "original": null, "number": 2, "cdate": 1541671384149, "ddate": null, "tcdate": 1541671384149, "tmdate": 1541671384149, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "content": {"title": "Good paper on regularizing the hidden state of LSTM to make sure it uses the cell state properly.", "review": "\nSummary:\n\nThis paper is based on the observation that LSTMs use the hidden state to memorize information and the cell state (memory) is not fully utilized. To encourage the LSTM to utilize the cell state, authors constraint the hidden state to a set of centroid states and learn to transition between these centroids in a soft way. Authors demonstrate their model in learning simple regular and context-free languages and also in a couple of non-synthetic tasks. The proposed model also has some interpretability of internal state transitions.\n\nMajor comments:\n\n1.\tThe main claim of the paper is that SR-LSTM can extrapolate to longer sequences, unlike LSTM. However, the sequence lengths considered are too small. It would be interesting to train both models with specific sequence length and then keep testing them with longer sequence length and compare the performance. If SR-LSTM behaves like a DPDA, then with larger cell state, the performance should not drop as you increase the sequence length till the capacity of the cell state.\n\n2.\tTheorem 3.1 and 3.2 have no proofs. Please make them as notes rather than theorems.\n\n3.\tWhat do different colors in Figure 6 stands for?\n\n4.\tIn the MNIST task authors claim that they have significant improvement when compared to LSTM. I am not sure if that is accurate. Also, why do you compare SR-LSTM-p only with LSTM? What is the performance of LSTM-p? Please report that as well.\n\n5.\tEven in table 3, can you please report the performance of LSTM-p?\n\nEven though the paper does not show strong empirical performance in real-world tasks, I would still recommend for accepting this paper for its contributions in understanding RNNs better, provided authors answer to question 1, 4, and 5.\n\n\nMinor comments:\n\n1.\tFig 6 is not referred anywhere.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "cdate": 1542234463208, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335722589, "tmdate": 1552335722589, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxlpD793m", "original": null, "number": 1, "cdate": 1541187512007, "ddate": null, "tcdate": 1541187512007, "tmdate": 1541534001067, "tddate": null, "forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "invitation": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "content": {"title": "Interesting idea, but shines only on specifically designed benchmarks, needs more experiments on well established datasets", "review": "The paper proposes an RNN architecture inspired from deterministic pushdown automata. An RNN is extended to use soft attention at every time step to choose from several learnable centroids.\n\nIn general, the paper is well written and the proposed model is theoretically grounded. Unfortunately, the proposed approach shines only on specifically designed benchmarks. It is not a surprise that a CF can be learned by an architecture very similar to DPDA (with addition of learnable parameters). There is a number of specifically designed tasks to test long-term memorization, such as copy/addition, etc. Furthermore, RNNs are mostly used for natural language processing tasks. This paper only conducts experiments on IMDB sentiment analysis ignoring better benchmarked tasks, such as language modelling.\n\nIt is not absolutely clear why authors claim that cell is playing the role of memory. It is always possible to rewrite LSTM formulas with h' which is concatenation of hidden state h and cell c. Results on \"peephole connection\"-inspired SR-LSTM-p should be benchmarked against an LSTM with peephole connections.\n\nThe claim repeated several times that RNNs operate like DFAs, not DPDAs. This is an important point in the paper and should be verbalized more. Does it mean that it is easier to learn regular languages with RNNs?\n\nWhile intuitive, theorems 3.1-3.2 are very vague to be theorems. Otherwise, they should be proven or provided a sketch of proof. For example, how do you formalize \"state dynamics\"?\n\nThe quality of writing of the related work section is worse that the rest of the paper. Authors should explore more other hidden state regularization methods. And, perhaps, give less attention to stochastic RNNs since the final version of the proposed model is not stochastic.\n\nTo summarize, this paper provides an interesting direction but lacks in terms of experimentation and global coherence of what is claimed and what is shown.\n\nMinor points:\n- Citation of Theano is missing\n- Give a sentence explaining what is hidden state \"drifting\"\n- a-priori -> a priori", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper431/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "State-Regularized Recurrent Networks", "abstract": "Recurrent networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work poorly on sequences requiring long-term memorization, despite having this capacity in principle. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell applications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We show that state-regularization (a) simplifies the extraction of finite state automata modeling an RNN's state transition dynamics, and (b) forces RNNs to operate more like automata with external memory and less like finite state machines.", "keywords": ["recurrent network", "finite state machines", "state-regularized", "interpretability and explainability"], "authorids": ["dr.rer.nat.chengwang@gmail.com", "mathias.niepert@neclab.eu"], "authors": ["Cheng Wang", "Mathias Niepert"], "TL;DR": "We introduce stochastic state transition mechanism to RNNs, simplifies finite state automata (DFA) extraction, forces RNNs to operate more like automata with external memory, better extrapolation behavior and interpretability.", "pdf": "/pdf/b9076c54d2f1f0b845a2ae7198b0781826bddabd.pdf", "paperhash": "wang|stateregularized_recurrent_networks", "_bibtex": "@misc{\nwang2019stateregularized,\ntitle={State-Regularized Recurrent Networks},\nauthor={Cheng Wang and Mathias Niepert},\nyear={2019},\nurl={https://openreview.net/forum?id=HJf7ts0cFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper431/Official_Review", "cdate": 1542234463208, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJf7ts0cFm", "replyto": "HJf7ts0cFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper431/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335722589, "tmdate": 1552335722589, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper431/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}