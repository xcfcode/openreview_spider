{"notes": [{"id": "BkMq0oRqFQ", "original": "B1gQV_8KKX", "number": 923, "cdate": 1538087890499, "ddate": null, "tcdate": 1538087890499, "tmdate": 1545355419011, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BylkZeRLyN", "original": null, "number": 1, "cdate": 1544114166997, "ddate": null, "tcdate": 1544114166997, "tmdate": 1545354496119, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Meta_Review", "content": {"metareview": "This paper interprets batch norm in terms of normalizing the backpropagated gradients. All of the reviewers believe this interpretation is novel and potentially interesting, but that the paper doesn't make the case that this helps explain batch norm, or provide useful insights into how to improve it. The authors have responded to the original set of reviews by toning down some of the claims in the original paper, but haven't addressed the reviewers' more substantive concerns. There may potentially be interesting ideas here, but I don't think it's ready for publication at ICLR.\n\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "a new interpretation of batch norm, but not clear what we gain from it"}, "signatures": ["ICLR.cc/2019/Conference/Paper923/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper923/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353032878, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper923/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper923/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper923/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353032878}}}, {"id": "BkglmsZY0Q", "original": null, "number": 3, "cdate": 1543211800028, "ddate": null, "tcdate": 1543211800028, "tmdate": 1543211800028, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "BJxGyPqla7", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "content": {"title": "Thank you for your thoughtful criticism, and especially for your kind comments.", "comment": "Dear Paper923 AnonReviewer3,\nThank you for your thoughtful criticism, and especially for your kind comments. We have toned down our language broadly in our revision, and removed all mentions of a \"unified view.\" We agree deeply with your note on needing more focus in the experiments; we would have liked to followup with a cleaner, more convincing application. Sadly, we could not deliver this under the resource constraints. "}, "signatures": ["ICLR.cc/2019/Conference/Paper923/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611787, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMq0oRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper923/Authors|ICLR.cc/2019/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611787}}}, {"id": "B1eSC5-K0X", "original": null, "number": 2, "cdate": 1543211725368, "ddate": null, "tcdate": 1543211725368, "tmdate": 1543211725368, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "ByghVRki27", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "content": {"title": "We have dialed back our language in the abstract, the TLDR, and the main body of the text to reflect your perspective on our work.", "comment": "Dear Paper923 AnonReviewer2\nThank you for your criticisms.  We have dialed back our language in the abstract, the TLDR, and the main body of the text to reflect your perspective on our work. We apologize for the typos in the earlier version, and we have been more diligent in this update. Also, we have clarified some of the language around the downstream affine transformation. Ignoring the affine transform is done without loss of generality, in the sense that they can be absorbed into the rest of the network without impacting our view of the gradients of the gaussian normalization. Our experiments were meant to be toy-examples of how one might better understand what happens to the gradient regression under adjustments to BN; ideally, we would like to design a new normalization that outperforms switch normalization, but we have not been able to do that here."}, "signatures": ["ICLR.cc/2019/Conference/Paper923/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611787, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMq0oRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper923/Authors|ICLR.cc/2019/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611787}}}, {"id": "HyxHscZKA7", "original": null, "number": 1, "cdate": 1543211677139, "ddate": null, "tcdate": 1543211677139, "tmdate": 1543211677139, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "SJg5feZqnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "content": {"title": "We have made notable adjustments to our language in the abstract, the TLDR, and the main body of the text, in light of your review.", "comment": "Dear Paper923 AnonReviewer3,\nThank you for your criticisms.  We have made notable adjustments to our language in the abstract, the TLDR, and the main body of the text, in light of your review. Regarding concerns related to ignoring the affine transform downstream of the gaussian normalization, we have rephrased the text to emphasize that it is done without loss of generality. This is WLOG in the sense that the affine transform after gaussian normalization can be absorbed into the rest of the network. Also, we would like to emphasize that we think of division by the standard deviation during training as a non-affine transform. One way to make that division affine is to use the BN running-variance instead of the batch variance during training -- but this alternative generally known (but not well stated in literature) to lead to poor performance."}, "signatures": ["ICLR.cc/2019/Conference/Paper923/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611787, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkMq0oRqFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper923/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper923/Authors|ICLR.cc/2019/Conference/Paper923/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers", "ICLR.cc/2019/Conference/Paper923/Authors", "ICLR.cc/2019/Conference/Paper923/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611787}}}, {"id": "BJxGyPqla7", "original": null, "number": 3, "cdate": 1541609177771, "ddate": null, "tcdate": 1541609177771, "tmdate": 1541609177771, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "content": {"title": "The discussion and conclusion drawn from the experimental part are quite arguable. In my opinion, the paper would gain much more impact if the view developed in this paper were illustrated by more convincing experiments.", "review": "The authors propose a new interpretation of the batch normalization step inside a neural network.\nThe main result shows that the backpropagation of the gradient of some loss function through a batch normalization can be seen as a scaled residual of a least square linear fit. This new interpretation is extended to other normalization technics used in the literature and thus give a \"unified\" view of such methods. \n\nThe idea is simple yet very interesting and well introduced. The theoretical results are good and the proofs are well written and easy to follow.\n\nHowever the arguments brought forward by this new vision of batch normalization in applications look light (see sections 3.3, 4.1, 4.2). A more detailed interpretation of this new vision on a single application and its impact would have been preferred than numerous applications as it is done in this paper. \nNot all the existing normalization methods have been extended with success yet, this makes this unified vision a bit less convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper923/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "cdate": 1542234345938, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833952, "tmdate": 1552335833952, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByghVRki27", "original": null, "number": 2, "cdate": 1541238324360, "ddate": null, "tcdate": 1541238324360, "tmdate": 1541533575833, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "content": {"title": " It has not been sufficiently demonstrated that the new perspective regarding batch normalization presented in this work is actually useful for either improving or explaining BN.", "review": "The primary technical contribution comes from Section 2, where it is demonstrated that the normalized back-propagated gradients obtained from a BN layer can be viewed as the residuals of the gradients obtained without BN regressed via a simple two-parameter model of the activations.  In some sense though this result is to be expected, since centering data (i.e., removing the mean as in BN) can be generically viewed as computing the residuals after a least squares fit of a single constant, and similarly for de-trending with respect to a single independent variable, in this case the activations.  So I'm not sure that Theorem 1 is really that much of an insightful breakthrough, even if it may be nice to work through the precise details in the specific case of a BN layer and the relationship to gradients.\n\nBut beyond this a larger issue is as follows:  This paper is framed as taking a step in explaining why batch normalization (BN) works so well.  For example, even the abstract mentions this as an unsettled issue in motivating the proposed analysis.  However, to me the interpretation of BN as introducing a form of least squares fit does not really extend our understanding of why it actually works better in practice, and this is the biggest disconnect of the paper.  The new perspective presented might be another way to interpret BN layers, but it unfortunately remains mostly unanswered exactly why this new perspective is relevant in actually explaining BN behavior.\n\nThe presented normalization theory is also used to motivate heuristic modifications to standard BN schemes.  For example, the paper proposed concatenating BN with a layer normalization layer, demonstrating some modest improvement on CIFAR-10 data.  But again, I don't see how viewing these normalization schemes as least-squares residuals motivates such concatenation any more than the merits of the original versions themselves.  Moreover, it is not even clear that BN+LN is in fact generally better since only a single data set is considered.  There are also no comparisons against competing BN modifications such as switch normalization (Luo et al. 2018) which also involves a hybrid method combining aspects of LN and BN.  Why not compare against approaches like this?\n\nTo conclude, in Section 6 the paper asks \"Why do empirical improvements in neural networks with BN keep the gradient-least-squares residuals and drop the explained portion?\"  But this question is not at all answered but rather deferred to future work.  For me this was a disappointment as this would seem to be an essential ingredient for actually developing a meaningful theory for why BN is helpful in practice.\n\n\nOther comments:\n\n* The analysis from Section 2, including Theorem 1, assume that the BN parameters c and b can be ignored (presumably this means fixing c = 1 and b = 0).  I did not carefully check the details, but do all the same derivations and conclusions still seamlessly go through when these parameters have general values that deviate from this standard initialization?  If not, then I don't really see what is the practical relevance, since once learning begins, both b and c will typically shift to arbitrary values.  Below eq. (1) it states that c and b are only ignored for clarity, but then later I did not see any subsequent discussion to handle the general case, which is what would be actually needed for explaining BN behavior in practice.\n\n* Please run a speck-checker.  Example, \"On some leve, the matrix gradient ...\"\n\n* The paper cites (Lipton and Steinhardt, 2018) in arguing that reasons for the effectiveness of BN are lacking.  Indeed (Lipton and Steinhardt, 2018) criticize the original BN paper for conflating speculation with explanation, or more precisely, framing speculation about why BN should be helpful as an actual true explanation without clear evidence.  But to me this submission is hovering somewhere in the same category, speculating that regressing away certain portions of the gradient could be useful but never really providing concrete evidence for why this should offer an improvement. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper923/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "cdate": 1542234345938, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833952, "tmdate": 1552335833952, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJg5feZqnQ", "original": null, "number": 1, "cdate": 1541177362457, "ddate": null, "tcdate": 1541177362457, "tmdate": 1541533575584, "tddate": null, "forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "content": {"title": "A premature paper proposing a novel interpretation of Batch Normalisation", "review": "The paper aims at a better understanding of the positive impacts of Batch Normalisation (BN) on network generalisation (mainly) and  convergence of learning. First, the authors propose a novel interpretation of the BN re-parametrisation. They show that an affine transform of the variables with their local variance (scale) and mean (shift) can be interpreted as a decomposition of the gradient of the objective function into a regressor assuming that the gradient is parallel to the variables (up to a shift) and the residual part which is the gradient w.r.t. to the new variables. In the second part of the paper, authors review various normalisation proposals (differing mainly in the subset of variables over which the normalisation statistics is computed) as well as the known empirical findings about the dependence of BN on the batch size. The paper presents an experiment that combines two normalisation variants. A further experiment strives at regularising BN for small batch sizes.\n\nUnfortunately, it remains unclear what questions precisely the authors answer in the second part of the paper and, what is more important, how they are related to the novel interpretation of BN presented in the first part. This interpretation holds for any function and can be possibly seen as a gradient pre-conditioning. However, the authors do not \"extend\" it towards the gradients w.r.t. the network parameters and do not consider the specifics of the learning objectives (a sum of functions, each one depending on one training example only). The main presented experiment combines layer normalisation with standard batch normalisation for a convolutional network. The first one normalises using the statistics over channel and spatial dimensions, whereas the second one uses the statics over the batch and spatial dimensions. The improvements are rather marginal, but, what is more important, the authors do not explain how and why this proposal follows from their new interpretation of BN.\n\nOverall, in my view, this paper is premature and not appropriate for publishing at ICLR in its present form.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper923/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Normalization Gradients are Least-squares Residuals", "abstract": "Batch Normalization (BN) and its variants have seen widespread adoption in the deep learning community because they improve the training of deep neural networks. Discussions of why this normalization works so well remain unsettled.  We make explicit the relationship between ordinary least squares and partial derivatives computed when back-propagating through BN. We recast the back-propagation of BN as a least squares fit, which zero-centers and decorrelates partial derivatives from normalized activations. This view, which we term {\\em gradient-least-squares}, is an extensible and arithmetically accurate description of BN. To further explore this perspective, we motivate, interpret, and evaluate two adjustments to BN.", "keywords": ["Deep Learning", "Normalization", "Least squares", "Gradient regression"], "authorids": ["liu.yi.pei@gmail.com"], "authors": ["Yi Liu"], "TL;DR": "Gaussian normalization performs a least-squares fit during back-propagation, which zero-centers and decorrelates partial derivatives from normalized activations.", "pdf": "/pdf/b37504a8f6cad67daa4ca5a89ddf53116d7a0d52.pdf", "paperhash": "liu|normalization_gradients_are_leastsquares_residuals", "_bibtex": "@misc{\nliu2019normalization,\ntitle={Normalization Gradients are Least-squares Residuals},\nauthor={Yi Liu},\nyear={2019},\nurl={https://openreview.net/forum?id=BkMq0oRqFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper923/Official_Review", "cdate": 1542234345938, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkMq0oRqFQ", "replyto": "BkMq0oRqFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper923/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335833952, "tmdate": 1552335833952, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper923/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}