{"notes": [{"id": "r1ztwiCcYQ", "original": "H1ewJ8ucKQ", "number": 284, "cdate": 1538087777218, "ddate": null, "tcdate": 1538087777218, "tmdate": 1545355422472, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJx3F58uAm", "original": null, "number": 1, "cdate": 1543166595863, "ddate": null, "tcdate": 1543166595863, "tmdate": 1545354492975, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Meta_Review", "content": {"metareview": "This paper studies a variational formulation of the loss minimization to study the solution that generalizes the most. An expectation of the loss wrt a Gaussian distribution is minimized to find the mean and variance of the Gaussian distribution. As the variance goes to zero, we recover the original loss, but for a higher value of variance, the loss may be convex. This is used to study the generalizability of the landscape.\n\nBoth objective and solutions of the paper are unclear and not communicated well. There is not enough citation to previous work (e.g., Gaussian homotopy exactly considers this problem, and there are papers that study the convexity of the expectation of the loss function). There are no experimental results either to confirm the theoretical finding.\n\nAll the reviewers struggle to understand both the problem and solutions discussed in this paper. I believe that the paper could become useful if reviewers' feedback is taken seriously to improve the paper.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "A good problem, but not well executed and communicated "}, "signatures": ["ICLR.cc/2019/Conference/Paper284/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper284/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353271181, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper284/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper284/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper284/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353271181}}}, {"id": "SJlehN5PA7", "original": null, "number": 1, "cdate": 1543115944477, "ddate": null, "tcdate": 1543115944477, "tmdate": 1543435987151, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "content": {"title": "Thank you for your input. Your questions will be answered below in a few comments.", "comment": "This note is in support of the claim of the paper regarding generalization for critical point solution.\n\nWhile general proof is still in work below we consider toy one-dimensional examples that explains how generalization error may be lower at critical point: we show that absolute value of  expectation of generalization error for critical point solution is lower than for \u201ctrivial\u201d solutions.\n\nTo warm up let\u2019s consider a linear case: loss per data sample L=(w-a)^2, where w is weight and a is sample specific value.\nThen loss per sample for m IID samples from data set M is L_m=sum_n{(w-a_n)^2}/m=w^2-2w(sum_n{a}/m)+sum_n{a_n^2}/m.\nWe will use square brackets for average over an unknown sample distribution: E(z)=[z].\nThen expectation of the loss at a given weight w is\nE(L)(w)=w^2-2w[a]+[a^2].\nGeneralization error for a given weight is a difference between loss for data set M and expected loss: gerr(w)=L_m-E(L)=-2w(sum_n{a}/m-[a])+sum_n{a^2}/m-[a^2].\nNote that for a given weight w the expectation of the generalization error is zero: [gerr(w)]=0.\n\nLet\u2019s consider a solution that minimizes loss for a data set M:\nw_m=argmin(L_m(w))=sum_n{a}/m.\n\nNow, the expectation of the generalization error for that solution is not zero and equals\n[gerr(w_m)]=-2[(a-[a])^2]/m. Note that it is negative and goes to zero when m goes to infinity.\n\nLet\u2019s consider a simple non-linear case.\nThe loss below is selected to simplify derivations and still good enough to make a statement.\n\nLoss per sample is L(w)=(w^2-a^2)^2. Loss for m-point data set M is L_m(w)=sum_n{(w^2-a_n^2)^2}/m.\n\nMinimum loss solutions are w_m^2=sum_n{a_n^2}/m. Also w_m=0 is a local maximum of the loss.\nThe generalization error at weight w is gerr(w)=2w^2([a^2]-sum_n{a_n^2}/m)+(sum_n{a_n^4}/m-[a^4]).\nThe expectation of the generalization error at a given weight is zero: [gerr(w)]=0.\nHowever, the expectation of the generalization error for minimum loss solutions of the sampled data set is not zero:\n[gerr(w_m)]=-2[(a^2-[a^2])^2]/m. It is negative and goes to zero for large m.\nLet\u2019s call this error -Err_m, so Err_m=2[(a^2-[a^2])^2]/m.\n\nNow, let\u2019s consider the effective loss by averaging the losses over Gaussian distribution with given (mu,s) - mean and std.\nWe use angular brackets for Gaussian averages: <L_m(w)> and <[L(w)]>.\nInstead of dependency on weight these averages are functions of mu and s.\n\nThen generalization error for a given mu, s is\ngerr(mu,s)=<L_m(w)>-<[L(w)]>=-2mu^2(sum_n{a_n^2}/m-[a^2])-2s^2(sum_n{a_n^2}/m-[a^2])+(sum_n{a^4}/m-[a^4]).\nAgain, at given mu,s the expectation of generalization error [gerr(mu,s)] is zero.\n\nLet\u2019s consider solutions that minimize the effective loss for data set M w.r.t. mean and std:\n\nd<L_m>/dmu=4mu(mu^2-(sum_n{a_n^2}/m-3s^2))=0,\n d<L_m>/ds=4s(3mu^2-(sum_n{a_n^2}/m-3s^2))=0.\n\nThere are trivial solutions that minimize the effective loss: s_m=0, mu_m^2=sum_n{a_n^2}/m.\nAnd the critical point solution is: 3s_m^2=sum_n{a_n^2}/m, mu_m=0.\n\nFor trivial solutions the expectation of the generalization error [gerr(mu_m,s_m=0)]=-Err_m  - same as found above.\nFor the critical point solution the expectation of the generalization error [gerr(mu_m=0,s_m)]=-Err_m/3.\n\nNow we can see that for the critical point the absolute value of the expectation of the generalization error is three times smaller than the error for trivial solutions.\n\nWhile this is not a proof, it is a good example that supports the claim of the paper. The proof will be provided when it is finalized.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606746, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1ztwiCcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper284/Authors|ICLR.cc/2019/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606746}}}, {"id": "r1g4PM0uCX", "original": null, "number": 2, "cdate": 1543197276226, "ddate": null, "tcdate": 1543197276226, "tmdate": 1543198823590, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "HkgcijgT27", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "content": {"title": "Answers to questions 1-5. Please see a note regarding generalization error above.", "comment": "Answers to the questions 1 and 4. It indeed requires more detailed explanation.\n\nThe approach used in paper for finding a computable variational approximation based on the following consideration.\n\nLet\u2019s x to be random variable with probability distribution p(x). We want to compute averages for some A(x): <A>_p.\nIt is given by integral <A>_p=int_x{p(x)A(x)}, which is in general intractable. Instead we can use another more manageable probability distribution q(x) introduced via identity:\n<A>_p=int_x{q(x)A(x)p(x)/q(x)}=<(p/q)A>_q.\n\nWe can get approximation of that average by replacing ratio (p(x)/q(x)) by its mean for distribution q if variance of p/q is small enough:\n\np/q -> mean_q(p/q)=<p/q>_q = 1;\nvar_q(p/q)=<(p/q)^2-1>_q=<p/q>_p - 1. \n\nThe mean of p/q is equal to one. To make a good approximation we need to minimize variance of (p/q) by using free parameters of distribution q(x|params). The problem is that <p/q>_p is still intractable. One approach is to use and minimize lower bound of <p/q>_p:\n<p/q>_p >= exp(<log(p/q)>_p) > 1. Here <log(p/q)>_p is a positive KL divergence that we must minimize by selecting appropriate parameters of the distribution q. To do that we need to compute the integral <log(q)>_p=int_x{p(x) log(q(x))}, which for general p(x) is not possible.\n\nWe will solve this problem by replacing p with its approximation sequentially one almost identical  factor at the time.\n\nRemember, that our original problem is to compute integral over weights: int_w{p0(w)p1(w)}, where p0 is prior and p1 is a product of model probabilities over all data samples.\n\nThat integral is an average of p1 with probability distribution p0: <p1>_p0.\n\nWe will sequentially update prior:\n\nFirst, we factorize p1: p1=(p1^(1/T))^T={exp(log(p1)/T)}^T=(1+log(p1)/T)^T, where T >>1 is large.\nBy normalizing product of prior p0 and a single factor (1+log(p1)/T) we make a probability r0=p0(1+log(p1)/T)/<1+log(p1)/T>_p0 and then approximate r0 with q:\n\n<p1>_p0=<{(1+log(p1)/T)^T}(p0/q)>_q=<{(1+log(p1)/T)^(T-1)}{r0/q}>_q *<(1+log(p1)/T)>_p0 = \n<{(1+log(p1)/T)^(T-1)}>_q * <(1+log(p1)/T)>_p0.\n\nHere we replaced r0/q with its mean <r0/q>_q=1, which is one. To make it working we need to minimize var(r0/q)=<(r0/q)^2>_q-1=<r0/q>_r0-1, by selecting parameters of q. Minimizing lower bound of the var(r0/q) is equivalent to minimizing KL divergence \n<log(r0/q)>_r0 which in turn is equivalent to maximizing <(1+log(p1)/T)*log(q)>_p0.\n\nFinding q allows to represent initial integral <p1>_p0=<(1+log(p1)/T)^T>_p0 as <(1+log(p1)/T)^(T-1)>_q, that looks like an original one only with number of factors reduced by one and prior p0 is replaced by prior q.\n\nBy repeating this we have recurrent updates: q_t -> q_(t+1), where q_0=p0 and parameters of each new q_t are found by maximizing <(1+log(p1)/T)*log(q_(t+1))>_q_t by parameters of q_(t+1).\n\nThe updates repeat exactly T times to approximate initial integral <p1>_p0. We call one such update an epoch because p1 is a product over probabilities of data samples p1=prod_n{p1_n} and log(p1)=sum_n{log(p1_n)} and one such update is a full batch update. If we will do one update per data sample then to complete computing the approximation to the integral it will take N times T updates. Here N is a number of samples and T is a number of epochs.\n\n============\n\n2. Why Gaussian?\n\nWe can use any probability distribution for variational inference that allows to compute. For the purpose of the paper we can use any distribution with efficient sampling and which has enough parameters to approximate a true distribution.\n\nFreedom to select an approximate distribution is the same freedom we use to select a model for learning statistics of data. We use Gaussian for its special properties and usability for continuous weights. For binary networks we will use another distribution.\n\nWe use approximation where each weight is independent, so a whole weight distribution is a product over all weight component distributions. This approach allows to compute efficiently.\n\nIf we would like to go beyond a variational approximation we could use MCMC with the approximation developed in the paper as a proposal distribution.\n\n===============\n\n3. Indexing \n\nn=1..N is index for data samples. t=1..NxT is index for total NxT iterations. Index t goes over all N data samples T times. In one iteration contribution of only one of T fractions of one of N samples is used. Hope this is clear now.\n\n==============\n\n4. See above in 1.\n\n=============\n\n5. This formula for P0 is the prior of weight and product is taken over each weight component. That product is not related to a following paragraph.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606746, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1ztwiCcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper284/Authors|ICLR.cc/2019/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606746}}}, {"id": "ryxFKHR_0Q", "original": null, "number": 3, "cdate": 1543198080860, "ddate": null, "tcdate": 1543198080860, "tmdate": 1543198080860, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "rJlqb64qn7", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "content": {"title": "Please see answers to some of your questions in notes above.", "comment": "1. The reviewer is right about missing proper literature review.\n2. Please see above the answer to that question.\n3. Hope the provided clarifications in notes above are adding proper mathematical presentations.\n4. The conducted experiments are not fitting the capacity of the paper and will be published separately."}, "signatures": ["ICLR.cc/2019/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606746, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1ztwiCcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper284/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper284/Authors|ICLR.cc/2019/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers", "ICLR.cc/2019/Conference/Paper284/Authors", "ICLR.cc/2019/Conference/Paper284/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606746}}}, {"id": "HkgcijgT27", "original": null, "number": 3, "cdate": 1541372833867, "ddate": null, "tcdate": 1541372833867, "tmdate": 1541534124875, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "content": {"title": "Study of the landscape of the effective loss", "review": "Summary: Non-convex learning problems can have multiple solutions with different generalization properties, thus it is important to find solutions that generalize well. The goal of this paper is to derive an algorithm for finding a solution to the learning problem with the best possible generalization properties. This is achieved by using a Bayesian approach in which the parameters w (e.g., weights of a network) are random variables and the effective loss (integral wrt to w) is minimized in lieu of the usual loss. The paper assumes that each component of the weight vector w is Gaussian and derives a formula for updating the mean and covariance of said Gaussians (this is an SGD method). The paper claims that the resulting effective loss is convex for large variances (sigma > threshold), nonconvex for small variances (sigma < threshold), and converges to original loss as sigma goes to zero. The paper also claims that when sigma=0 there are trivial solutions that are unstable as data changes, but that when sigma=threshold (assuming this is what is meant by end of convexity) there are non-trivial solutions that are less sensitive to data changes and hence the most generalizable.\n\nComments: the goal of the paper (finding minima that generalize well) is an excellent one. But the paper is not clearly written and appears to oversell the contribution. In particular, the title speaks about SGD, dropout, generalization and critical points \u201cat the end of convexity\u201d. Naturally, a reader is inclined to think that the paper will study SGD and dropout for deep learning and analyze generalization properties of the solutions found by those methods. In reality, there is very little in the paper about SGD, dropout, and generalization. The connection with SGD is merely because the method for updating mu and sigma is an SGD method. The connection with dropout is mentioned in passing in one paragraph and it is not very clear. The connection with generalization is claimed but never quite explained (there are no generalization bounds in the paper). As far as understand, the paper considers the minimization of the effective loss, uses a Gaussian approximation for computing the effective loss, and focuses primarily on the characterization of convexity as a function of sigma as well as a characterization of the critical points depending on whether the effective loss is convex (sigma above a threshold) or not (sigma below the threshold). The main claim appears to be that critical points at the critical threshold lead to solutions that generalize well, but a detailed explanation of why this is the case isn't given. If my digest of the paper is the correct one, then modifying the title, abstract and intro to make this clear would have helped a lot. \n\nBeyond the high-level lack of clarity about the contribution of the paper, the writing lacks precision and rigor, and many things are undefined (though one can figure them out after reading many times back and forth). Specifically: \n\n1) It is not explained why the probability of each training sample can be expressed as a product of factors close to 1, with the product taken over the epochs.\n\n2) It is not explained why each factor can be modeled as a product of Gaussians\n\n3) At nearly the top of page 3, a product over n is substituted by a product over t, with x_n replaced by x_t and so forth, but the total number of products goes up from N to TxN. What is the value of y_{NT} and x_{NxT}? Do the authors mean that y_n should have been replaced by y_{n,t} and we now have two indices? Or do the authors mean that the same mini batch of N samples is reused, and so indices should be corrected accordingly? \n\n4) It is not clear why replacing R_t/Q_{t+1} by 1 is an adequate approximation.\n\n5) At the top of equation 4, there is a product, but no index wrt which the product is taken. Right after it says the index is t, but there is no t in the expression. Should mu_0 be mu_t and similarly for sigma? \n\nIn short, a promising direction, but the contribution of the paper appears to be over claimed and the writing of the paper needs significant improvement before the paper can be accepted for publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "cdate": 1542234496805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689933, "tmdate": 1552335689933, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlqb64qn7", "original": null, "number": 2, "cdate": 1541192961774, "ddate": null, "tcdate": 1541192961774, "tmdate": 1541534124630, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "content": {"title": "VARIATIONAL SGD", "review": "Summary of the paper:\nThe paper proposes an algorithm to find solution to the maximum likelihood problem that could generalize well. The paper argues from the point of view that purely optimizing over the likelihood could result in solution that corresponds to poor local minimum which does not generalize well. By introducing a certain prior on weight, there exists a solution that could generalize. The solution arrived by introducing the prior makes it stable under perturbations of the training data. Recurrent update rules are derived for computing the integrals and hence the solution could be calculated. The authors discuss about the convexity of the effective loss when the variance is large. \n\nThe paper itself is very bad in its presentation. In terms of technical presentation, it is missing a lot of details, which makes reading and understanding the paper very hard.\n1.\tIt does not come with any proper literature review and introduction to the formulation of the problem. \n2.\tThe presentation of the methodology is also missing a lot of the explanation for many of the details used in the method. For example, in section 2, I do not quite understand the reasoning behind setting the probability P(y|x,w) = (1+1/T lnP(y|x,w))^T. Also, why R_t(w)/Q_(t+1)(w) could be approximated by 1. \n3.\tThe theoretical results come in plain words without proper mathematical presentation and the proofs for the statements are not well organized. The correspondence between the proofs and statements are not clear.\n4.\tThere seems to be no experiments conducted to support the practical use of the method proposed in the paper.\n\nOverall, I feel the paper is not ready for publication as a conference paper. The lack of details especially for the technical presentation part make it very hard to read. And the presentation of the results seem to be short of clarity and organization. Further, no experiments showing the practicality of the method are included in the paper.\n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "cdate": 1542234496805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689933, "tmdate": 1552335689933, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyl3G76N27", "original": null, "number": 1, "cdate": 1540834067815, "ddate": null, "tcdate": 1540834067815, "tmdate": 1541534124424, "tddate": null, "forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "content": {"title": "unclear, critically ill-written paper", "review": "Presentation of the work is critically weak and I failed to understand the objective and contributions of the paper (despite a solid knowledge in Bayesian inference). They are many editing problems and the English is problematic, but most importantly the writing fails to properly introduce the problem, the objective and solutions.", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper284/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VARIATIONAL SGD: DROPOUT , GENERALIZATION AND CRITICAL POINT AT THE END OF CONVEXITY", "abstract": "The goal of the paper is to propose an algorithm for learning the most generalizable solution from given training data. It is shown that Bayesian approach leads to a solution that dependent on statistics of training data and not on particular\nsamples. The solution is stable under perturbations of training data because it is defined by an integral contribution of multiple maxima of the likelihood and not by a single global maximum. Specifically, the Bayesian probability distribution\nof parameters (weights) of a probabilistic model given by a neural network is estimated via recurrent variational approximations. Derived recurrent update rules correspond to SGD-type rules for finding a minimum of an effective loss that is an average of an original negative log-likelihood over the Gaussian distributions of weights, which makes it a function of means and variances. The effective loss is convex for large variances and non-convex in the limit of small variances. Among stationary solutions of the update rules there are trivial solutions with zero variances at local minima of the original loss and a single non-trivial solution with finite variances that is a critical point at the end of convexity of the effective loss\nin the mean-variance space. At the critical point both first- and second-order gradients of the effective loss w.r.t. means are zero. The empirical study confirms that the critical point represents the most generalizable solution. While the location of\nthe critical point in the weight space depends on specifics of the used probabilistic model some properties at the critical point are universal and model independent.", "keywords": ["Bayesian inference", "neural networks", "generalization", "critical point solution"], "authorids": ["michael.tetelman@gmail.com"], "authors": ["Michael Tetelman"], "TL;DR": "Proposed method for finding the most generalizable solution that is stable w.r.t. perturbations of trainig data.", "pdf": "/pdf/5c195d40cac7b8a0b4215c8bcc4c27aeb487cfe5.pdf", "paperhash": "tetelman|variational_sgd_dropout_generalization_and_critical_point_at_the_end_of_convexity", "_bibtex": "@misc{\ntetelman2019variational,\ntitle={{VARIATIONAL} {SGD}: {DROPOUT} , {GENERALIZATION} {AND} {CRITICAL} {POINT} {AT} {THE} {END} {OF} {CONVEXITY}},\nauthor={Michael Tetelman},\nyear={2019},\nurl={https://openreview.net/forum?id=r1ztwiCcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper284/Official_Review", "cdate": 1542234496805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1ztwiCcYQ", "replyto": "r1ztwiCcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper284/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335689933, "tmdate": 1552335689933, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper284/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}