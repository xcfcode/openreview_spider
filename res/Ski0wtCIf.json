{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582900760, "tcdate": 1520464280813, "number": 1, "cdate": 1520464280813, "id": "BkZjbl0OM", "invitation": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "signatures": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer3"], "content": {"title": "limited in scope, inconclusive results", "rating": "3: Clear rejection", "review": "The paper describes a method for generating dance moves (in the form of motion capture data) from music audio, using a network architecture that makes use of convolutional layers as well as LSTMs. Two variants of the architecture are evaluated: one with so-called \"delayed skip connections\", and one without. Results are mixed, and which variant performs best seems to depend mostly on the choice of evaluation metric.\n\nThe writing could definitely be improved to be more clear. From reading the paper, it is hard to determine exactly what these delayed skip connections are, and how they modify the architecture. Several sentences in the paper stood out to me as vague or meaningless, or perhaps I simply didn't understand them in the way the authors intended:\n* \"Nevertheless, the dance generation requires higher computational capabilities or larger dataset and is constrained to the training data\"\n* \"For motion generation tasks, CLDNN enhances the mapping between music and motion\"\n* \"For motion generation constrained to music, the decoder may suffer loose connection from the audio features\"\n\nGiven the length constraint, I think it would be best to get rid of such sentences that add no new information, and replace them with more details about what experiments were done exactly, and some motivation for the architectural choices that were made.\n\nThe main contribution of the paper is purported to be a comparison between a model with skip connections and one without, but at the same time, Orhan (2017) is cited as having conducted an extensive study on this topic already. So I'm not sure what kind of new information this work brings to the table.\n\nBesides the presence of skip connections, the rest of the model architecture seems to be fixed for the experiments, but it is quite elaborate and many aspects of it are poorly motivated. This makes the comparison results less impactful as they may be specific to this particular architecture.\n\nFrom tables 1, 2 and 3, my conclusion is that adding skip connections makes the prediction error worse, but improves the F-score. It is not clear from the paper which of these two metrics we should actually care more about. I'm also unsure about the statistical significance of the difference in prediction error in Table 1, given the small size of the dataset. Overall, these results seem very inconclusive and I feel that this is not adequately addressed in the text.\n\nTo summarise, given the limited scope, the poorly motivated architectural choices, the inconclusive results and the lack of novelty, I don't think this work is suitable as a workshop contribution.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DELAYED SKIP CONNECTIONS FOR MUSIC CONTENT DRIVEN MOTION GENERATION", "abstract": "In this study, we employ skip connections into a deep recurrent neural network\nfor modeling basic dance steps using audio as input. Our model consists of two\nblocks, one encodes the audio input sequences, and another generates the motion.\nThe encoder uses a configuration called convolutional, long short-term memory\ndeep neural network (CLDNN) which handle the power features of audio. Furthermore,\nwe implement skip connections between the contexts of music encoder\nand motion decoder (i.e. delayed skip) for consistent motion generation. The\nexperimental results show that the trained model generate predictive basic dance\nsteps from a narrow dataset with low error and maintains similar motion beat fscore\nto the baseline dancer.", "pdf": "/pdf/a28c9b516b95bff98f59b2411041532e8163375c.pdf", "TL;DR": "Employing skip connections into a deep recurrent neural network for modeling basic dance steps using audio as input", "paperhash": "yalta|delayed_skip_connections_for_music_content_driven_motion_generation", "keywords": ["Deep Learning", "Skip Connectios", "CLDRNN"], "authors": ["Nelson Yalta", "Kazuhiro Nakadai", "Tetsuya Ogata"], "authorids": ["nyalta21@gmail.com", "ogata@waseda.jp", "nakadai@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582900530, "id": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper81/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer4"], "reply": {"forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582900530}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582859939, "tcdate": 1520564117048, "number": 2, "cdate": 1520564117048, "id": "ryT9w_yFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "signatures": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer1"], "content": {"title": "minimal contribution", "rating": "3: Clear rejection", "review": "This paper proposes to add skip connections into an encoder-decoder architecture to improve dance steps generation given a piece of music as input. The authors claim that adding the skip connections speed up training and improve the final results.\n\nThe paper is easy to follow, but many details are missing. For example, the network architecture, the size of the hidden layers, the training procedure are not specified. The data sets are also not clear from the description. The numbers in Table 1 also do not match those in Table 2 and 3. I assume they are from different epochs, but it has to be stated clearly.\n\nBesides the missing details, the results are mixed in Table 2 and 3. I would not conclude that the proposed model is better. Some of the numbers actually go down after more epochs. The numbers also does not show what the authors claimed in previous sections.\n\nFinally, adding skip connections is too weak of a contribution.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DELAYED SKIP CONNECTIONS FOR MUSIC CONTENT DRIVEN MOTION GENERATION", "abstract": "In this study, we employ skip connections into a deep recurrent neural network\nfor modeling basic dance steps using audio as input. Our model consists of two\nblocks, one encodes the audio input sequences, and another generates the motion.\nThe encoder uses a configuration called convolutional, long short-term memory\ndeep neural network (CLDNN) which handle the power features of audio. Furthermore,\nwe implement skip connections between the contexts of music encoder\nand motion decoder (i.e. delayed skip) for consistent motion generation. The\nexperimental results show that the trained model generate predictive basic dance\nsteps from a narrow dataset with low error and maintains similar motion beat fscore\nto the baseline dancer.", "pdf": "/pdf/a28c9b516b95bff98f59b2411041532e8163375c.pdf", "TL;DR": "Employing skip connections into a deep recurrent neural network for modeling basic dance steps using audio as input", "paperhash": "yalta|delayed_skip_connections_for_music_content_driven_motion_generation", "keywords": ["Deep Learning", "Skip Connectios", "CLDRNN"], "authors": ["Nelson Yalta", "Kazuhiro Nakadai", "Tetsuya Ogata"], "authorids": ["nyalta21@gmail.com", "ogata@waseda.jp", "nakadai@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582900530, "id": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper81/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer4"], "reply": {"forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582900530}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582667813, "tcdate": 1520748449891, "number": 3, "cdate": 1520748449891, "id": "HkqjDHfKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "signatures": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer4"], "content": {"title": "unclear", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a neural network that predicts dance moves using music features as input. THis is generally an interesting problem, but it would be better to show that the proposed method works on some standard dataset, e.g. speech recognition or other sequence modeling tasks. As is, readers just cannot know if the proposed setup is reasonable, and what performance to expect.\n\nI am confused by Figure 1: this is a sequence-to-sequence model? How so? All the activations are for time t (or t-1 and t+1) - so there is no sequence involved. Which activations get \"concat\"enated at the central layer? Does this maybe act more like a bottle-neck layer? Please clarify. Also, various kinds of skip and highwa connection schemes have been recorded in the litrature, th autors may want to take a look at these and discsus their model in light of this other body of work. \n\nFinally, all the references for the CLDNN model that the authors present point to Google (Sainath's work). Maybe that is a sign that the paper is not as \"widely spread\" as the authors may want us to believe.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DELAYED SKIP CONNECTIONS FOR MUSIC CONTENT DRIVEN MOTION GENERATION", "abstract": "In this study, we employ skip connections into a deep recurrent neural network\nfor modeling basic dance steps using audio as input. Our model consists of two\nblocks, one encodes the audio input sequences, and another generates the motion.\nThe encoder uses a configuration called convolutional, long short-term memory\ndeep neural network (CLDNN) which handle the power features of audio. Furthermore,\nwe implement skip connections between the contexts of music encoder\nand motion decoder (i.e. delayed skip) for consistent motion generation. The\nexperimental results show that the trained model generate predictive basic dance\nsteps from a narrow dataset with low error and maintains similar motion beat fscore\nto the baseline dancer.", "pdf": "/pdf/a28c9b516b95bff98f59b2411041532e8163375c.pdf", "TL;DR": "Employing skip connections into a deep recurrent neural network for modeling basic dance steps using audio as input", "paperhash": "yalta|delayed_skip_connections_for_music_content_driven_motion_generation", "keywords": ["Deep Learning", "Skip Connectios", "CLDRNN"], "authors": ["Nelson Yalta", "Kazuhiro Nakadai", "Tetsuya Ogata"], "authorids": ["nyalta21@gmail.com", "ogata@waseda.jp", "nakadai@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582900530, "id": "ICLR.cc/2018/Workshop/-/Paper81/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper81/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper81/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper81/AnonReviewer4"], "reply": {"forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper81/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582900530}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573605606, "tcdate": 1521573605606, "number": 263, "cdate": 1521573605255, "id": "rk01y1J5M", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Ski0wtCIf", "replyto": "Ski0wtCIf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DELAYED SKIP CONNECTIONS FOR MUSIC CONTENT DRIVEN MOTION GENERATION", "abstract": "In this study, we employ skip connections into a deep recurrent neural network\nfor modeling basic dance steps using audio as input. Our model consists of two\nblocks, one encodes the audio input sequences, and another generates the motion.\nThe encoder uses a configuration called convolutional, long short-term memory\ndeep neural network (CLDNN) which handle the power features of audio. Furthermore,\nwe implement skip connections between the contexts of music encoder\nand motion decoder (i.e. delayed skip) for consistent motion generation. The\nexperimental results show that the trained model generate predictive basic dance\nsteps from a narrow dataset with low error and maintains similar motion beat fscore\nto the baseline dancer.", "pdf": "/pdf/a28c9b516b95bff98f59b2411041532e8163375c.pdf", "TL;DR": "Employing skip connections into a deep recurrent neural network for modeling basic dance steps using audio as input", "paperhash": "yalta|delayed_skip_connections_for_music_content_driven_motion_generation", "keywords": ["Deep Learning", "Skip Connectios", "CLDRNN"], "authors": ["Nelson Yalta", "Kazuhiro Nakadai", "Tetsuya Ogata"], "authorids": ["nyalta21@gmail.com", "ogata@waseda.jp", "nakadai@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518405587003, "tcdate": 1518405587003, "number": 81, "cdate": 1518405587003, "id": "Ski0wtCIf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Ski0wtCIf", "signatures": ["~Nelson_Yalta1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "DELAYED SKIP CONNECTIONS FOR MUSIC CONTENT DRIVEN MOTION GENERATION", "abstract": "In this study, we employ skip connections into a deep recurrent neural network\nfor modeling basic dance steps using audio as input. Our model consists of two\nblocks, one encodes the audio input sequences, and another generates the motion.\nThe encoder uses a configuration called convolutional, long short-term memory\ndeep neural network (CLDNN) which handle the power features of audio. Furthermore,\nwe implement skip connections between the contexts of music encoder\nand motion decoder (i.e. delayed skip) for consistent motion generation. The\nexperimental results show that the trained model generate predictive basic dance\nsteps from a narrow dataset with low error and maintains similar motion beat fscore\nto the baseline dancer.", "pdf": "/pdf/a28c9b516b95bff98f59b2411041532e8163375c.pdf", "TL;DR": "Employing skip connections into a deep recurrent neural network for modeling basic dance steps using audio as input", "paperhash": "yalta|delayed_skip_connections_for_music_content_driven_motion_generation", "keywords": ["Deep Learning", "Skip Connectios", "CLDRNN"], "authors": ["Nelson Yalta", "Kazuhiro Nakadai", "Tetsuya Ogata"], "authorids": ["nyalta21@gmail.com", "ogata@waseda.jp", "nakadai@jp.honda-ri.com"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 5}