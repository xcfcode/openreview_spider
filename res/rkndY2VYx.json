{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028600516, "tcdate": 1490028600516, "number": 1, "id": "BklBuY6ix", "invitation": "ICLR.cc/2017/workshop/-/paper102/acceptance", "forum": "rkndY2VYx", "replyto": "rkndY2VYx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028601084, "id": "ICLR.cc/2017/workshop/-/paper102/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkndY2VYx", "replyto": "rkndY2VYx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028601084}}}, {"tddate": null, "tmdate": 1489172381920, "tcdate": 1489172381920, "number": 2, "id": "BJUswOgig", "invitation": "ICLR.cc/2017/workshop/-/paper102/public/comment", "forum": "rkndY2VYx", "replyto": "ry8W2Bejx", "signatures": ["~antonio_vergari1"], "readers": ["everyone"], "writers": ["~antonio_vergari1"], "content": {"title": "clarifications", "comment": "Dear Reviewer,\n\nthank you for your criticisms and appreciation. Definitely, this workshop version omits more details than the conference one. We will try to answer your questions in the following.\n\n>   The authors should explain the third paragraph of Section 2 more clearly\n\nFollowing the other reviewer comments and given the page length format, we updated the version to include the decode procedure listing in the Appendix.\n\n>    In the 4th paragraph also of Section 2, it is unclear what \\phi_n(u) is\n\nIt stands for the probability distribution encoded by a leaf, as introduced at the beginning of the Section.\n\n>   can the proposed models also outperform the alternatives after fine-grain training (i.e. jointly train the feature extractors with the classifier)?\n\nIf we are allowed to perform \"fine-tuning\", it would be fairer to perform that to SPNs/MPNs as well. We are investigating this kind of hybrid training, which, as far as I know, is unusual (hence interesting) for density estimators.\n\n>    I understood from the paper that for the other networks (RBM, MADE, CAE, DAE) only the activations of the top layer is used. However, because the SPN's embeddings are from all the nodes (or all the inner nodes), have the authors tried using the activations of all the hidden nodes for the other networks?\n\nWe used all node activations for RBMs and MADEs, producing longer embeddings that those from SPNs/MPNs, on different datasets (see the structural statistics reported in the updated version).\nConcerning non-probabilistic autoencoders, we employed only the embeddings from the \"compressed\" mid-representation layer, finding it useful as stated in the literature.\n\nPlease let us know how we could improve this work further."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487354228812, "tcdate": 1487354228812, "id": "ICLR.cc/2017/workshop/-/paper102/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper102/reviewers"], "reply": {"forum": "rkndY2VYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487354228812}}}, {"tddate": null, "tmdate": 1489171991130, "tcdate": 1489171991130, "number": 1, "id": "HkyQI_gjl", "invitation": "ICLR.cc/2017/workshop/-/paper102/public/comment", "forum": "rkndY2VYx", "replyto": "HykWQwJse", "signatures": ["~antonio_vergari1"], "readers": ["everyone"], "writers": ["~antonio_vergari1"], "content": {"title": "updated revision", "comment": "Dear reviewer,\n\nthanks for your time reviewing our work and \"imputing\" the missing parts, we really appreciate it.\n\nWe acknowledge that the current presentation omits several details about the experimental setting. Therefore, we updated the paper by including an appendix comprising the full decoding procedure, some paragraphs about\ntraining the models employed and finally more experimental results. We also refactored the notation following your suggestions.\n\nEven if the time is running out, let us know if other modifications are required."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487354228812, "tcdate": 1487354228812, "id": "ICLR.cc/2017/workshop/-/paper102/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper102/reviewers"], "reply": {"forum": "rkndY2VYx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487354228812}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489171081670, "tcdate": 1487354227809, "number": 102, "id": "rkndY2VYx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rkndY2VYx", "original": "r1Bjj8qge", "signatures": ["~antonio_vergari1"], "readers": ["everyone"], "content": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484167765713, "tcdate": 1478286236941, "number": 321, "id": "r1Bjj8qge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1Bjj8qge", "signatures": ["~antonio_vergari1"], "readers": ["everyone"], "content": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product networks (SPNs) are expressive deep architectures for representing probability distributions, yet allowing exact and efficient inference. SPNs have been successfully applied in several domains, however always as black-box distribution estimators. In this paper, we argue that due to their recursive definition, SPNs can also be naturally employed as hierarchical feature extractors and thus for unsupervised representation learning. Moreover, when converted into Max-Product Networks (MPNs), it is possible to decode such representations back into the original input space. In this way, MPNs can be interpreted as a kind of generative autoencoder, even if they were never trained to reconstruct the input data. We show how these learned representations, if visualized, indeed correspond to \"meaningful parts\" of the training data. They also yield a large improvement when used in structured prediction tasks. As shown in extensive experiments, SPN and MPN encoding and decoding schemes prove very competitive  against the ones employing RBMs and other stacked autoencoder architectures.", "pdf": "/pdf/b47e88100d407f7c6ba87a9d112bc6bc7e949177.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": [], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489161213671, "tcdate": 1489161213671, "number": 2, "id": "ry8W2Bejx", "invitation": "ICLR.cc/2017/workshop/-/paper102/official/review", "forum": "rkndY2VYx", "replyto": "rkndY2VYx", "signatures": ["ICLR.cc/2017/workshop/paper102/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper102/AnonReviewer2"], "content": {"title": "good paper, should be accepted", "rating": "7: Good paper, accept", "review": "* Summary: the paper proposes to use sum-product networks (SPN) for feature extraction. The embedding of an input is the activations of all the nodes in the network or of only the inner nodes. To learn features in an unsupervised manner like in Autoencoder, a decoding method is introduced using the corresponding max-product network. The experimental results on MNIST show that the proposed method outperforms RBM, CAE, DAE in terms of the quality of embeddings for classification. \n\n* Discussion: \nThe idea of the paper is neat, interesting, and innovative. The paper is well written yet quite brief (but understandable given the page limit). The authors should explain the third paragraph of Section 2 more clearly. In the 4th paragraph also of Section 2, it is unclear what \\phi_n(u) is. \n\nThe experiment results are quite strong and convincing. However,\n1. can the proposed models also outperform the alternatives after fine-grain training (i.e. jointly train the feature extractors with the classifier)? \n2. I understood from the paper that for the other networks (RBM, MADE, CAE, DAE) only the activations of the top layer is used. However, because the SPN's embeddings are from all the nodes (or all the inner nodes), have the authors tried using the activations of all the hidden nodes for the other networks? \n\n* pros: \n- the idea is neat, interesting, and innovative\n- experimental results are good and convincing \n\n* cons: \n- the paper is quite brief and unclear at some points (but this shouldn't be considered as a significantly negative point)\n- the experiments can be done better \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489161214410, "id": "ICLR.cc/2017/workshop/-/paper102/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper102/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper102/AnonReviewer1", "ICLR.cc/2017/workshop/paper102/AnonReviewer2"], "reply": {"forum": "rkndY2VYx", "replyto": "rkndY2VYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper102/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper102/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489161214410}}}, {"tddate": null, "tmdate": 1489101559506, "tcdate": 1489101559506, "number": 1, "id": "HykWQwJse", "invitation": "ICLR.cc/2017/workshop/-/paper102/official/review", "forum": "rkndY2VYx", "replyto": "rkndY2VYx", "signatures": ["ICLR.cc/2017/workshop/paper102/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper102/AnonReviewer1"], "content": {"title": "A promising application of SPNs for learning representations and mapping them back into the input space", "rating": "7: Good paper, accept", "review": "This paper describes how to use Sum- and Maximum product networks for unsupervised feature learning and decoding and evaluate it within three different learning scenarios by either directly classifying a binary label set based on the original feature space, or by classifying the labels from generated feature encodings or decoding labels from their embedding. The authors further propose a full pipeline that produces feature embeddings and decodes them into the label space.\n\nThe paper alone is quite hard to comprehend and as a reader without prior knowledge in SPN/MPNs I had to consult a lot of literature, which however was provided sufficiently in the paper.  The authors compare their method to state of the art approaches like RBMs and auto-encoders and show promising results in their framework. Unfortunately the tasks were not described properly and again required to consult further literature. I would recommend putting the evaluations partly into the appendix and to elaborate a little bit on that. \n\nMinor remarks:\n\n- Typo in first sentence of section 3: usupervisedly\n- The change in font size and face on emphasized words makes the general look of the text inconsistent and is quite uncommon", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Encoding and Decoding Representations with Sum- and Max-Product Networks", "abstract": "Sum-Product Networks (SPNs) are deep density estimators allowing exact and tractable inference. While up to now SPNs have been employed as black-box inference machines, we exploit them as feature extractors for unsupervised Representation\nLearning. Representations learned by SPNs are rich probabilistic and hierarchical part-based features. SPNs converted into Max-Product Networks (MPNs) provide a way to decode these representations back to the original input space. In extensive experiments, SPN and MPN encoding and decoding schemes prove highly competitive for Multi-Label Classification tasks.", "pdf": "/pdf/6ee7c114af5548a19021882d8488051a699ad004.pdf", "TL;DR": "Sum-Product Networks can be effectively employed for unsupervised representation learning, when turned into Max-Product Networks, they can also be used as encoder-decoders", "paperhash": "vergari|encoding_and_decoding_representations_with_sum_and_maxproduct_networks", "keywords": ["Unsupervised Learning", "Structured prediction"], "conflicts": ["uniba.it", "tugraz.at", "medunigraz.at", "cs.washington.edu"], "authors": ["Antonio Vergari", "Robert Peharz", "Nicola Di Mauro", "Floriana Esposito"], "authorids": ["antonio.vergari@uniba.it", "robert.peharz@medunigraz.at", "nicola.dimauro@uniba.it", "floriana.esposito@uniba.it"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489161214410, "id": "ICLR.cc/2017/workshop/-/paper102/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper102/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper102/AnonReviewer1", "ICLR.cc/2017/workshop/paper102/AnonReviewer2"], "reply": {"forum": "rkndY2VYx", "replyto": "rkndY2VYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper102/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper102/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489161214410}}}], "count": 6}