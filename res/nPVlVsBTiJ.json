{"notes": [{"id": "nPVlVsBTiJ", "original": "tNtVZUzRW6z", "number": 667, "cdate": 1601308079205, "ddate": null, "tcdate": 1601308079205, "tmdate": 1614985719026, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "TpQ9jgFqicq", "original": null, "number": 1, "cdate": 1610040422856, "ddate": null, "tcdate": 1610040422856, "tmdate": 1610474021754, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Although the connection between randomized smoothing and PDE revealed in this paper is an interesting direction to explore, the method proposed unfortunately is not certified. The method could work as a good empirical defense since the smoothed classifier could be learned more efficiently. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422842, "tmdate": 1610474021737, "id": "ICLR.cc/2021/Conference/Paper667/-/Decision"}}}, {"id": "6I11oU9l_gU", "original": null, "number": 4, "cdate": 1603874471727, "ddate": null, "tcdate": 1603874471727, "tmdate": 1606290593765, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Official_Review", "content": {"title": "Problems with the proposed approach", "review": "This paper proposes to use a deterministic classifier to replace the sampling\nprocess in randomized smoothing based certifiably robust models.  The goal of\ntraining a deterministic robust classifier to avoid the high cost of randomized\nsmoothing is a right direction to look at. \n\nThe reason that we can get certified robustness with Gaussian smoothing is that\nthe smoothed classifier becomes Lipschitz (see Salman et al.). Unfortunately,\nit is a stochastic classifier and typically it is impossible to access the\nsmoothed classifier directly, and that's why in randomized smoothing (e.g.,\nCohen's PREDICT procedure), sampling is necessary.\n\nThe approach in this work is to train a smoothed classifier that essentially\nreturns the same prediction as the mean of the originally stochastic\nclassifier. The authors pointed out the connection between Gaussian smoothed\nclassifier and gradient regularization, so they use gradient regularization to\nobtain the desired deterministic classifier.\n\nUnfortunately, it seems to me that the proposed approach is not sound. With\ngradient regularization, we try to make the learned classifier to be smooth,\nhowever there is no guarantee that such a learned classifier will indeed be\nsmooth and produce the same outcome as the original Gaussian smoothed\nclassifier, so we cannot use the outcome of this classifier to replace Cohen's\nSampleUnderNoise procedure.  More precisely, optimizing the proposed loss\nfunction (3) does not guarantee the learned classifier f^smooth to be Lipschitz,\nwhere the original Gaussian smoothed classifier is guaranteed to be Lipschitz.\nAlthough the loss attempts to do so with gradient regularization, there is no\nguarantee here. So the entire procedure is not certified anymore.\n\nOn the positive side, the proposed method may work as a good empirical defense,\nsince the smoothed classifier can be learned quickly and can be more robust\nthan the original classifier. This may be advantageous for certain applications\nwhere adversarial training is too slow or we don't want to retrain the original\nclassifier.\n\nBecause of the fundamental problem mentioned above, I cannot recommend\nacceptance of this paper. I am willing to discuss with the authors further in\ncase I misunderstand some parts of the paper.\n\n---\n### After rebuttal:\n\nAfter reading the rebuttal, I feel my main concern is still not addressed by the response. The authors agree that they may provide a different kind of guarantee for the certificates as in (Cohen et al., Salman et al.). An empirical comparison between the output of the proposed model and the mean from sampling is not sufficient. We hope the authors can improve on this point and provide formal robustness guarantees like those in (Cohen et al., Salman et al.).\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper667/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137926, "tmdate": 1606915778918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper667/-/Official_Review"}}}, {"id": "I6wz0mnCmJ", "original": null, "number": 2, "cdate": 1606180443761, "ddate": null, "tcdate": 1606180443761, "tmdate": 1606180516044, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Official_Comment", "content": {"title": "Rebuttal to Reviewer Comments and Submission of Rebuttal Revision", "comment": "Thank you reviewers for taking the time to give helpful feedback on our paper. In this work, we aimed to use ideas from PDEs in order to develop a training scheme that results in a Gaussian average of some initial model. In doing so, we found it possible to take any baseline computer vision model and make it certifiably robust to input perturbations. While perhaps not as robust as other certification methods, the main goal of this work was to use PDE theory and calculus of variations to solve the heat equation. Since the solution of the heat equation is in theory equivalent to Gaussian smoothing, we let our main application be computer vision to draw links between our work and that of Cohen et. al. With this in mind, future work must be done to show that we can solve other PDEs using our iterative training method. As pointed out, there were some shortcomings of this work that we wish to address.\n\nIt is indeed a good point that our model may not follow the same certification guarantees as Cohen's stochastically averaged models. In an attempt to show that our model is indeed averaged, we added a section to our revised submission. In section 4.3, we first compare how well our deterministic model's prediction matches predictions of this same model obtained using the stochastic ``voting'' method from Cohen et. al. PREDICT algorithm. We then see how often our model's deterministic single-forward-pass predictions matches the predictions of a model from Cohen et. al. Given that both percentages of agreement in predictions are high, we do indeed believe that our model is worthy of being considered a Gaussian average of some initial model, thus allowing us to implement the algorithms presented in Cohen et al. and Salman et al.\n\nThe point was made that our approach is equivalent to L2 regularization and it therefore not novel. We note that we aren't doing L2 Loss regularization, we are regularizing the vector output of the model. This approach is more aligned to the PDE theory. Furthermore, we implement numerical approximation methods to compute the L2 norm of the Jacobian matrix. To our knowledge, this is not previously done in papers that use regularization as an adversarial defense. Furthermore, a reviewer pointed out that the results in Table 4 are counter-intuitive. The purpose of Table 4 is to show that an adversarially trained model (from Madry et al.) doesn't have a certified radius in terms of the stochastic procedure presented in Cohen et al. By running our ImageNet training procedure, we showed that we can perform Gaussian smoothing on an adversarially trained model, thus giving it a certified radii.\n\nOne reviewer made the very good point that we could have implemented other adversarial attacks and compare our deterministic model's performance against known defense methods such as adversarial training. In fact, we tried this and came across two limitations in doing so. One, we lacked the time and computational resources to implement several state-of-the-art attacks on ImageNet. Secondly, we found that our model under-performed compared to adversarial training. This came as no surprise. In our work we never tried to provide the best adversarial defense. Rather, we demonstrate that using PDE theory, we can obtain models equivalent to the stochastic models in Cohen et al. It is work nothing that in practice, Cohen et al.'s models do not beat adversarial training.\n\nA question was brought up as to whether or not there are PGD adversarially-trained ImageNet models available to download online. There are indeed pretrained ImageNet ResNet-50 models in Aleksander Madry's ``robustness'' GitHub repository. Download links are available towards the end of the following README.md file https://github.com/MadryLab/robustness/blob/master/README.rst.\n\n\nFor references to any papers mentioned in this comment, please see the \"References\" section of our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper667/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nPVlVsBTiJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper667/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper667/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper667/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper667/Authors|ICLR.cc/2021/Conference/Paper667/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868466, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper667/-/Official_Comment"}}}, {"id": "PYL8Z03dmmy", "original": null, "number": 1, "cdate": 1603635898411, "ddate": null, "tcdate": 1603635898411, "tmdate": 1605024634142, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Official_Review", "content": {"title": "Unconvinced results and insufficient discussion on related work", "review": "This paper proposes to finetune a pretrained network with a gradient norm regularizer to mimic the Gaussian noise augmentation during training. I have two main concerns about this paper:\n\n1. As claimed by the authors, they are motivated to solve the stochasticity of previous certified defenses like random smoothing. However,  there are mainstream certified training methods that are deterministic during inference (with a single query) [1][2][3][4], but are not discussed or empirically compared in this paper.\n\n2. The proposed regularizer function in Eq.(3) is actually equivalent to L-2 adversarial training, which can be simply proved by a dual norm trick [5]. This makes the results in Table 4 seem counter-intuitive, where a from-scratch L-2 adversarially trained model performs worse than a model finetuned by a similar mechanism. I suggest the authors better explain these both formally and empirically.\n\nMinor:\nIn Sec.4.2, the authors claim that they use the pretrained ImageNet model from Madry GitHub. But as far as I know, there are only pretrained models on MNIST and CIFAR-10 in their repository, could the authors provide an official link to this baseline? \n\n\n\nReference:\n\n[1] Wong et al. Provable defenses against adversarial examples via the convex outer adversarial polytope. ICML 2018\n\n[2] Wong et al. Scaling provable adversarial defenses. NeurIPS 2018\n\n[3] Dvijotham et al. Training verified learners with learned verifiers. arXiv 2018\n\n[4] Dvijotham et al. A dual approach to scalable verification of deep networks. UAI 2018\n\n[5] Simon-Gabriel et al. First-order adversarial vulnerability of neural networks and input dimension. ICML 2019", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper667/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137926, "tmdate": 1606915778918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper667/-/Official_Review"}}}, {"id": "kas_iz80FKJ", "original": null, "number": 2, "cdate": 1603773941408, "ddate": null, "tcdate": 1603773941408, "tmdate": 1605024634075, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "The paper claims that a (computationally intractable) randomized smoothing of any classifier can be distilled into the (deterministic) classifier itself via fine-tuning it with gradient penalty. This is motivated by a theoretical result that Gaussian smoothing of a classifier is equivalent to solving a certain heat equation, which can be approximated by a regularized loss training. Experimental results use the resulting deterministic classifier to compute the certified radius compared to (stochastic) smoothed classifiers, arguing its efficiency and higher certified radius of the proposed method. \n\nThe relation between randomized smoothing and PDE seems to be an interesting direction to explore. My biggest concern is that, however, whether the proposed deterministic smooth classifier is indeed *certifiably* robust in practice. Apart from its theoretical motivation, I could not find any statistical guarantee that the deterministic smooth classifier is provably close to its stochastic counterpart for every input x, which is essential to claim the certifiable robustness of the proposed model. Otherwise, how one could prove that the deterministic smooth classifier is indeed robust to adversarial examples? - although randomized smoothing may require much more computation time, it at least provides such a statistical yet practical guarantee, namely as the CERTIFY algorithm [1].\n\nAnother possible way to go is to show that the *empirical* robustness of the deterministic smooth classifier is non-trivial. In this context, actually, the message of the paper can be \"very\" surprising (and very unlikely, at the same time): one can robustify any classifier by a single pass of fine-tuning with gradient penalty, even without adversarial training. The paper indeed provide a related result, i.e., empirical test accuracy on adversarial attacks, but the current evaluation is too weak to support the claim: considering a bunch of defense papers that are broken after published [2, 3], it is too hard for me to believe the results as is even it could bear from a specific configuration of PGD and DDN. I would recommend the paper to follow the standard guidelines suggested by [4]. The paper could explore more possible configurations of PGD attacks, or other types of attacks (e.g., gradient-free attacks [5], or black-box attacks), just to name a few.\n\n[1] Cohen et al., Certified Adversarial Robustness via Randomized Smoothing, ICML 2019.\n\n[2] Athalye et al., Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples, ICML 2018.\n\n[3] Tramer et al., On Adaptive Attacks to Adversarial Example Defenses, 2020.\n\n[4] Carlini et al., On Evaluating Adversarial Robustness, 2019.\n\n[5] Uesato et al., Adversarial Risk and the Dangers of Evaluating Against Weak Attacks, ICML 2018.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper667/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137926, "tmdate": 1606915778918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper667/-/Official_Review"}}}, {"id": "xYMg204ey0J", "original": null, "number": 3, "cdate": 1603812304539, "ddate": null, "tcdate": 1603812304539, "tmdate": 1605024633946, "tddate": null, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "invitation": "ICLR.cc/2021/Conference/Paper667/-/Official_Review", "content": {"title": "This paper propose a deterministic method for certified robustness under adversarial attack. Different from the prior random smoothing approach, this paper uses deterministic inference and achieves times of speed up. The idea is novel and interesting.", "review": "Randomized smoothing is the major way to certify the robustness of large scale networks, however, it requires sampling from Gaussian distribution many times, which is not fast enough for real-time inference. This paper uses a regularized loss to get deterministic Gaussian averaged results. This paper points out an interesting direction for certifying robustness, the method is simple and effective.\n\nStrength:\n\n1. The paper is clearly written. \n\n2. It is very interesting to see a method that can certify the robustness without the computational intensive randomized smoothing.\n\n3. The method is simple and effective, does not require much computation resources, which improves the inference speed by around 10 times (Table 2).\n\n4. The speed for randomized smoothing certification is a major concern for the community. This paper address this problem. If this paper is really effective, it can have a broad impact on the research community for robustness certification.\n\nWeakness and Questions:\n\n1. What if the attacker directly attacks the objective function for certification (e.g. equation 2) ? Given that the certification is deterministic, is it possible to fool the certification method?\n\n2. In Figure 2, it seems the deterministic under performs some baselines.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper667/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper667/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Boot Camp: label free certified robustness in one epoch", "authorids": ["~Ryan_Campbell2", "~Chris_Finlay1", "~Adam_M_Oberman1"], "authors": ["Ryan Campbell", "Chris Finlay", "Adam M Oberman"], "keywords": ["machine", "learning", "adversarial", "robustness", "neural", "networks", "image", "classification", "computer", "vision"], "abstract": "Machine learning models are vulnerable to adversarial attacks.  One approach to addressing this vulnerability is certification, which focuses on models that are guaranteed to be robust for a given perturbation size.  A drawback of recent certified models is that they are stochastic: they require multiple computationally expensive model evaluations with random noise added to a given image. In our work, we present a deterministic certification approach which results in a certifiably robust model. This approach is based on an equivalence between training with a particular regularized loss, and the expected values of Gaussian averages. We achieve certified models on ImageNet-1k by retraining a model with this loss for one epoch without the use of label information.", "one-sentence_summary": "Deriving a regularized loss function that leads to certifiably robust computer vision models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "campbell|adversarial_boot_camp_label_free_certified_robustness_in_one_epoch", "supplementary_material": "/attachment/f9f00f5bf7c704656216c8b0362e9cb8a180085c.zip", "pdf": "/pdf/9e1c28b35158fb7c8b2e046c4f74b297821c379f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L9gANh_o80", "_bibtex": "@misc{\ncampbell2021adversarial,\ntitle={Adversarial Boot Camp: label free certified robustness in one epoch},\nauthor={Ryan Campbell and Chris Finlay and Adam M Oberman},\nyear={2021},\nurl={https://openreview.net/forum?id=nPVlVsBTiJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nPVlVsBTiJ", "replyto": "nPVlVsBTiJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper667/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137926, "tmdate": 1606915778918, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper667/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper667/-/Official_Review"}}}], "count": 7}