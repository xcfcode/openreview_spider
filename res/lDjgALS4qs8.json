{"notes": [{"id": "lDjgALS4qs8", "original": "jSTnPYQ7VE", "number": 1513, "cdate": 1601308168075, "ddate": null, "tcdate": 1601308168075, "tmdate": 1614985769045, "tddate": null, "forum": "lDjgALS4qs8", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "cMUa60K-1-8", "original": null, "number": 1, "cdate": 1610040363520, "ddate": null, "tcdate": 1610040363520, "tmdate": 1610473953805, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes to explain the representation for layer-aware neural sequence encoders with multi-order-graph (MoG). Based on the MoG explanation, it further proposes Graph-Transformer as a graph-based self-attention network empowered Transformer. As commented by the authors, a main purpose of Graph-Transformer is to show an example application of the MoG explanation.\n\nDuring the discussion period, after reading the paper and checking the code, the AC had raised a serious concern: There is a big gap between the MoG motivation and the actual implementation. The AC had urged the referees to take a careful look at the implementation details, in particular, Lines 524-561 in the attached code: \"supplement/fairseq-0.6.2_halfdim_gate\u2069 \u25b8 \u2068fairseq\u2069 \u25b8 \u2068models\u2069 \u25b8transformer.py\". The AC had made the following comments to the referees: \"Whether the performance gain of Graph-Transformer over Transformer is due to the MoG explanation is highly unclear. There is no direct evidence, such as appropriate visualization, to support that. In a high-level description, instead of using a usual skip connection that would combine beforex and x, the actual implementation is to 1) define increamental_x = x - beforex, 2) let increamental_x attend on beforex to produce x1, let beforex attend on increamental_x to produce x2, and let increamental_x attend on increamental_x to produce x3, 3) combine beforex, x1, x2, x3 in a certain way to produce the layer output.\"\n\nReviewer 2 responded to the AC's concern: \"After examining the transformer.py and Section 2 & 3, we cannot understand why the output of self-attentions could be regarded as MoG subgraphs? The authors did not explain the connection. In their code, the graph transformer seems to just utilize 3 multi-head attentions (line 539-541) in their encoder. Using MoG to interpret the outputs of three attentions (line 539-541) is not very convincing. The link is weak. We agree with your comments.\"\n\nTo summarize, the link between the actual implementation in the code and all the MoG explanations is quite weak, and the technical novelty of the actual implementation is not strong enough for an ICLR publication. Therefore, the AC recommends Reject.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040363506, "tmdate": 1610473953785, "id": "ICLR.cc/2021/Conference/Paper1513/-/Decision"}}}, {"id": "_hN00kyWYhs", "original": null, "number": 3, "cdate": 1603829528332, "ddate": null, "tcdate": 1603829528332, "tmdate": 1607311177641, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review", "content": {"title": "We carefully review the motivation, approach, and empirical results.", "review": "### Summary\nThe authors propose a new Transformer variant for neural machine translation. Compared with the standard Transformer framework, this work explains the representation generation process of the encoder via a multi-ordered-graph MoG and develops a novel Graph-Transformer method based on MoG, which is capable of capturing diverse relationships within the sequence. Empirical results over benchmark datasets validate the effectiveness of the proposed method. \n\n### Pros\nThis work appears originally in its new explanation of the representation generation process. Specifically, the main pros are summarized below.  \n1.\tIt provides a multi-ordered-graph MoG explanation for the representation generation of Transformer encoder. MoG is capable of capturing diverse relations of the sequence compared with standard simple directly graph explanation.  \n2.\tIt develops a novel method, dubbed Graph-Transformer, by combining MoG and Transformer, which generates layer-wise representation from the previous representation and incremental representation aspects explicitly. \n3.\tIt implements two fusion strategies, i.e., weight-gate and self-gate, for layer-wise information aggregation. Ablation study is also conducted to investigate the effectiveness of them. \n\n### Cons\nMy primary questions/concerns are listed below. \n1.\tThe experiments are limited with only two benchmark datasets. Since Transformer has been demonstrated to be effective in many NLP tasks, it would be better to include more datasets or even other tasks except neural machine translation for a comprehensive comparison. \n2.\tThe selection of the baseline seems unfair. As stated in Section A.5.1, position-encoding can be viewed as a specific MoG because it builds edges between nodes. Therefore, methods that focus on position-encoding should also be included for comparison. For your reference, Transformer-XL[1] and [2] are two tailored solutions for this purpose. \n3.\tIt is unclear about the model\u2019s efficiency, i.e., memory-efficiency and learning efficiency. The idea to split the layer-wise full representation into previous representation and incremental representation is interesting. However, it also inevitably increases the model complexity. The authors should provide more discussions about the model complexity to improve the quality further.\n4.\tIn Figure 3, why position-encoding is needed? MoG is already capable of capturing the sequential information by merging subgraphs. More explanation is welcomed for this configuration. \n\n### Clarify\nThis work is well organized and easy to follow. However, the readability can be improved by addressing the following suggestions. \n1.\tThe layer-level iteration in Figure 2 is difficult to understand; it would be better to give a toy example with a real sentence for a better explanation. \n2.\tIn the second sentence of Section 2.4, r is not defined. In my understanding, each generated edge $e_j$ constructs a unique sub-graph, then what is r stand for?    \n\n[1] Dai, Zihang, et al. \u201cTransformer-XL: Attentive language models beyond a fixed-length context\u201d, ACL, 2019. \n[2] Wang Benyou, et al. \u201cEncoding word order in complex embeddings\u201d, ICLR, 2020. \n \n\n### Response to Rebuttal\nThank authors for taking the time to clarifications and considering my comments.\n\nWe appreciate authors' efforts to add additional experiments results in Table 1 and Table2. However, the performance improvements are marginal (more or less 0.7) and speed of Graph Transformer is slower than transformer.\n\nEven though additional explanations about positional encoding (Appendix 8.1) can resolve our concerns, layer iteration (Figure 6) are not still clear for us, e.g., what are orange blue, yellow nodes? how layer iterations are used in graph transformer? Authors should refine its main context to increase understanding instead of adding lengthy Appendix for us. This such paper presentation and organization are not clear to understand.\n\nConsidering the above points, we still remain our decision.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116913, "tmdate": 1606915762213, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1513/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review"}}}, {"id": "6IXdGoRZ7a", "original": null, "number": 2, "cdate": 1603594357604, "ddate": null, "tcdate": 1603594357604, "tmdate": 1607022132348, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review", "content": {"title": "idea is insightful, but some parts are not clear", "review": "The paper has propose the layer-aware graph transformer to enhance the ability of capturing heterogeneous information of the current models. The experimental results has justified the effectiveness of proposed model.\n\nThe multi-order graph structure is innovative and contribute to the expression of generating subgraphs. Such operations has benefited for questions the proposed in section 2.5, which I believe could bring up insightful idea to the community. However, there are still some parts that should be given an extended clear statement.\n\n1, it is not clear how the \"unified explanation\" is defined and delivered in the graph-transformer model? Does it contribute the the model performance? MoG seems doesn't make a well-defined explainability to reflect the quantitative model explanation.\n\n2, How MoG 'observe' every step of generation of embedding? Is there any model transparency defined?  \n\n3, Could the model be more generalized to other downstream task besides NMT?\n\n4, It is not cleared that in the 2.2, whether it could be overlaps between SN and TN? If so, the number of generation of kinds of subgraph could be very large.\n\n5, The experimental parts lack of detail procedure. How to extract the relations within each sentence? Also the performance of the proposed model may highly be dependent over the quality of MoG construction, which may limit the generalization ability of the model.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116913, "tmdate": 1606915762213, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1513/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review"}}}, {"id": "b8f4NCCbNkW", "original": null, "number": 1, "cdate": 1603379080507, "ddate": null, "tcdate": 1603379080507, "tmdate": 1606742512630, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Summary:\n\nThe paper proposes a new multigraph architecture called Multi-Order-Graph to explain the representation generation process in neural sequence encoders (Self-Attention or SAN based models). The main contribution of this MoG is the introduction of n-order dependency which can model not only relationships between words but also high order relationships such as syntax and semantics between subgraphs. Taking inspiration from MoG explanation, a self-attention powered Graph Transformer is proposed which beats the Transformer baselines on NMT tasks (English-German and German-English).\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPros: \n\n+ The proposed idea of representing the encoding process as generation of a Multi-Order-Graph is novel and is able to provide good insights for SAN-based models.\n\n+ Proposed Graph Transformer uses self-attention and also attends over different order of subgraphs (low-order, middle-order and high-order). Since these subgraphs represent high order relationships (syntax, semantics etc.), the model is able to pay attention to salient subgraphs.\n\n+ This paper could fuel further research in the explainability of neural sequence encoders and SAN based models.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nCons: \n\n- The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. Only two datasets are used for NMT task. There has been a lot of improvement on base Transformer models (cited in the paper), still there is no comparison with them.\n\n- This paper claims that with the proposed MoG explanation it is possible to model high order relationships such as Syntax and Semantics. However, there is no example provided for this. Even some cherry picked examples would have helped in visualising the behaviour.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nOverall, I would like to see the paper at the conference. The idea of modelling the relationships as a Multi-Order-Graph is certainly novel and can fuel further research. However, due to the current state of experimentation I am voting for **rejecting** the paper.  I am willing to increase the score if the concerns are addressed by authors in the rebuttal period. \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nQuestions and Suggestions:\n\n1. I would like to see comparisons with models like Transformer(big), [Shaw et al.](https://arxiv.org/pdf/1803.02155.pdf), [He et al.](https://papers.nips.cc/paper/8019-layer-wise-coordination-between-encoder-and-decoder-for-neural-machine-translation.pdf). I would also encourage usage of some more datasets for the experimentation, for example En-Fr, En-Ro. Datasets can be found in the papers linked above.\n\n2. It would be interesting to see how different components of the Graph Transformer affect perplexity.\n\n3. What is Transformer(small)? I am sorry but I am not aware of this variation. It would be good if you could provide a reference for this particular variation of the model.\n\n4. In Fig 4a, it is evident that self-gate works better at encoding longer sentences. It would be good to see some discussion on this in the paper.\n\n5. Which dataset is used for Fig 4?\n\n6. In fig 4b, we can notice a sudden decrease in subgraph weight at layer 4 for 12 layered model. In figure 5, similar trend can be seen for different layered model except for 9 and 10 layered model. What is the significance of this behaviour?\n\n7. I would encourage the authors to provide the implementation for the model proposed in this paper. \n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nMinor Comments/Typos:\n+ Section 2.4: source node of edge Snj -> Snj, source node of edge Ej\n+ Section 2.4: target node of edge Tnj -> Tnj, target node of edge Ej\n+ Section 3.3: the order of subgraph is in the range of 2^(n-1) to 2^(n) -> the order of subgraph is in range 2^(i-1) to 2^(i)\n   Similar mistake is in Middle-Order and Low-Order section. \n+ Section 7: MoG connects only only words but also subgraphs -> MoG connects not only words but also subgraphs. \n+ A.5.3: Rlationship information generated -> Relationship information generated\n\nPost Rebuttal Comments: Authors have addressed most of my concerns and as a result I have increased the rating from 5 to 6. Thanks!", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538116913, "tmdate": 1606915762213, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1513/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Review"}}}, {"id": "yaUXVX9HVe", "original": null, "number": 8, "cdate": 1606289206889, "ddate": null, "tcdate": 1606289206889, "tmdate": 1606306838993, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "b8f4NCCbNkW", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Response to AnonReviewer3-2", "comment": "Thank you for your review.\n\nAs for your concerned comparisons,\n\nTwo extra language pairs are added in Table 1.\n\nPPL change has been added in Table 2.\n\nMore baselines are from He and Shaw on Transformer-base are added in Table 1.\n\nWe are sorry that with all we get, four nVida RTX GPUs, the experiment of Transformer-big on WMT 16 English-Romanian we conduct has not finished yet. So far, we observe the current training curves of Graph-Transformer and Transformer-big baseline are very close, which indicates that the improvement of Graph-Transformer on Transformer-big will not be so significant as that on Transformer-Base. However, such possible results are not surprising, and consistent with our MoG explanation.\n\nAs we discussed in Section 2.4, the largest order of subgraph is limited by number of layers in SAN-based model. With 8 layers in Transformer-big, the largest order of generated subgraph is 256, which is larger than the length of longest sentence from training set and test set in WMT 16 En-Ro (240 and 140 respectively), which makes model tend to generate subgraph with the largest order. In the meantime, MoG shows that overlap of nodes and edges will happen as shown in Figure 1. Thus too many layers makes Graph-Transformer generate subgraphs with overlap, which may hinder the latent performance improvement."}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "kCtCLftqV0", "original": null, "number": 9, "cdate": 1606295099410, "ddate": null, "tcdate": 1606295099410, "tmdate": 1606306601840, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Revision Summary", "comment": "We thank the reviewers for their time and valuable suggestion. Reviews noted that we propose Multi-Order-Graph to explain the representation generation process, and propose   Graph-Transformer for neural machine translation, and evaluate Graph-Transformer in WMT 14 En-De and IWSLT 14 De-En. However, the reviewers also have some curiosities about explanation in our paper, and concern about our experiments on other tasks and benchmark datasets. To address these concerns, based on the helpful suggestions and constructive questions of the reviewers, we made substantial changes to the paper organization, MoG explanation, useful examples, baseline model, and comparisons to related works, and experiments on Named-entity recognition, part-of-speech tagging, text summarization and machine translation language pairs WMT 14 En-Fr and WMT 16 En-Ro to show the effectiveness of proposed Graph-Transformer. We have uploaded a new version in which more clarification and more experiments reviewers suggested were added.\n\nThe modification can be briefly summarized as follows:\n\n1. Add one section to explain the difference between layer-level iteration and sentence-level iteration (see Appendix Section A.6), and one figure to show the layer-level iteration (see Appendix Figure 6)\n2. Modify and enrich the explanation of position-encoding in the Transformer. (see Appendix Section A.8.1).\n3. Add one section to explain RNN-based model using MoG explanation. (see Appendix  Section A.7).\n4. Add one figure to show an example of modelling syntax as $n$-dependency relationship. (see Appendix Figure 7).\n5. Add results of experiments on Named-entity recognition (see Section 4.2 and Table 3) and part-of-speech tagging (see Section 4.2 and Table 4).\n6. Add results of experiments on two benchmark datasets, WMT 14 En-Fr and WMT 16 En-Ro (see Section 4.1 and Table 1).\n7. Add the comparison of the effects related works, Shaw et al. and He et al. (see Section 4.1 and Table 1).\n8. Add data of the size of model paragraphs, training speed and PPL (see Section 4.1 and Table 2).\n9. Add discussion about Transformer-big in the comments *Response to AnonReviewer3-2*."}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "xdSEW3rIrm_", "original": null, "number": 6, "cdate": 1605809356326, "ddate": null, "tcdate": 1605809356326, "tmdate": 1606296248495, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "b8f4NCCbNkW", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your review.\n\n---\n### About Example of Syntax\nThank you for your valuable suggestions. We give an example about syntax. One syntactic tree can be viewed as one subgraph of MoG. Generating a syntactic tree can be viewed as a process of generating a subgraph.\n\nThis example is updated to pdf in Figure 7.\n\n\n---\n### About experiments on other tasks and datasets\nTo show effectiveness of the proposed MoG and Graph-Transformer, we have conducted several new experiments on NER, POS tagging, text summarization, and  additional machine translation language pairs: WMT14 English-French (En-Fr), and WMT16 English-Romanian (En-Ro). Please refer to General Response. Results of these experiments are updated to Table 1, Table 3 and Table 4 in our pdf.\n\n\n---\n### About Transformer (small)\nTransformer (small) is a specific set of hyperparameters which is designed for the low-resource IWSLT 14 De-En and shown in Table 3. To avoid confusion between it and Transformer (base), we call it Transformer (small).\n\n---\n### About self-gate\n\nSelf-gate is based on self-attention which used in SAN-based model to capture edges and subgraphs. Whether previous or incremental representation is a set of subgraph information. In fact, weighting different representations by self-gate can be viewed as a process to capture edges between subgraphs in previous and incremental representation. It means that self-gate will generate subgraphs of higher order as we discussed in the last paragraph of Section 3.4, which can improve the performance on long sentence.\n\n---\n\n### About dataset in Figure 4\n\nWMT 14 En-De is the dataset used for Figure 4.\n\n---\n\n### About sudden decrease in subgrpah weight shown in Figure 5\n\nWeight reflects importance of representation. 4th-Layer in Graph-Transformer generates subgraphs of order in a range from 4 to 8 as incremental representation and subgraphs of order in a range from 1 to 4 as previous representation. This behaviour reflects that subgraphs of order in a range from 1 to 4 is more important. Considering weights of subgraphs generated in following layers, it means that 1~4-order dependency relationship is more important than other n-order dependency relationship.\n\n\n---\n### About Typos\nThank you for your valuable suggestions. We will revise the next version according to your suggestions and correct these typos.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "TzUNGgmuB__", "original": null, "number": 10, "cdate": 1606296220269, "ddate": null, "tcdate": 1606296220269, "tmdate": 1606296220269, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "b8f4NCCbNkW", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Response to AnonReviewer3-3", "comment": "Thank you for your review.\n\nThe source code has been uploaded as Supplementary Material. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "hMfd59dnRI6", "original": null, "number": 3, "cdate": 1605806384848, "ddate": null, "tcdate": 1605806384848, "tmdate": 1606249376652, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "General Response-1", "comment": "We thank all reviewers so much for the valuable comments on improving the quality of this work. We have updated the paper according to the review and our latest evaluations.\n\n\nThe revision primarily includes\n1. We have conducted several new experiments on Named-entity recognition (NER), part-of-speech tagging (POS tagging), text summarization, and  additional machine translation language pairs.\n2. We added the detailed descriptions about MoG and Graph-Transformer.\n3. We added one new figure to describe the layer-level iteration.\n4. We fixed some typo and writing problems.\n\n---\n\n### About Experiments\nTo show effectiveness of the proposed MoG and Graph-Transformer, we have conducted several new experiments on other tasks, Named-entity recognition (NER), part-of-speech tagging (POS tagging), text summarization, and  additional machine translation language pairs: WMT14 English-French (En-Fr), and WMT16 English-Romanian (En-Ro). We use CONLL2003 dataset, WSJ Corpus and Annotated Gigaword corpus as benchmark of NER task, POS tagging task and text summarization respectively. In these experiments, we use the original Transformer as the baseline.\n\n|Model |  WMT14 En-Fr &nbsp;&nbsp;| WMT16 En-Ro|\n|:----|:----:|:----:|\n|Transformer (base)|40.1 |33.9|\n|Graph-Transformer (halfdim-gate)&nbsp;&nbsp;|40.8 (+0.7)|34.6 (+0.7)|\n\n\n|    Model     | NER (F1)|POS tagging (F1)|\n|:----|:----:|:----:|\n|Transformer |80.2 |96.40|\n|Graph-Transformer (halfdim-gate)&nbsp;&nbsp;|80.3 (+0.1)|96.45 (+0.05)|\n\n|Model|ROUGE-1|\tROUGE-2|\tROUGE-L|\n|:----|:----:|:----:|:----:|\n|Transformer (base)|\t36.84\t|18.01|\t34.31|\n|Graph-Transformer (halfdim-gate)&nbsp;&nbsp;|\t37.38 (+0.54) |\t18.59 (+0.58) |\t34.58 (+0.27)|\n\n\nThis part will be updated to Table 1, Table 3 and Table 4 in our pdf.\n\n\n\n---\n\n### About Layer-level Interation and Sentence-level Interation\n\nAs we discussed in Section 2.4, process of subgraph (edge) generation is an iterative process, in which one subgraph (edge) relies on previous generated subgraphs (edges). Here is one question for this operation,\n\n* **Where did previous generated subgraphs come from?**\n\nwhich can also be transferred to another question, \n\n* **Where will subgraphs generated be used to capture new subgraphs?**\n\nTwo kinds of iteration are answers to the question. Layer-level iteration means that one layer uses subgraph generated by previous layer to capture subgraph, and generated subgraph will be used in next layer. Sentence-level iteration means that one layer uses subgraphs generated in the same layer at previous time step, and generated subgraph will be used in the same layer at next time step.\n\nNote that layer-level iteration is not the opposite of sentence-level iteration. They can be used together in one model, or can exist independently in one model. However, in most of layer-aware model with multiple layers, layer-level iteration is necessary.\n\nThis part is updated to pdf in Section A.6\n\n\n---\n\n### About Contributions\n\nFinally, we want to clarify our contributions.\n\nOur main contribution in this work is providing a unified explanation of representation layer-aware neural sequence encoder, which regards representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. MoG can explain not only  SAN-based model, but also other layer-aware models such as RNN-based model. \n\n\nSpecially, we propose Graph-Transformer based on MoG explanation. It is designed to show an example of application of MoG explanation which is the main purpose of Graph-Transformer. Besides, it can also improve the performance of the Transformer and results of experiments on several NLP tasks show the effectiveness of Graph-Transformer."}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "Cg8uKCA3PtI", "original": null, "number": 5, "cdate": 1605808757447, "ddate": null, "tcdate": 1605808757447, "tmdate": 1606240671679, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "6IXdGoRZ7a", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you for your review.\n\n---\n### About unified explanation in graph-transformer\n\nIn fact, MoG is the unified explanation of representation for layer-aware neural sequence encoders. Please refer to the contribution in our General Response.\n\n\nMoG generated by Graph-Transformer is similar as the MoG from Transformer while MoG from Graph-Transformer changes the weight of edge and enhances ability of capturing subgraph of high order.\n\nFour questions in Section 2.5 are good entries to analyze MoG and characteristics of one model. We can analyze Graph-Transformer in this way.\n\n1. **How to preserve subgraph information?**   Graph-Transformer uses vector to preserve subgraph information same as other neural models.\n2. **How to implement iterative encoding?**  Graph-Transformer uses Layer-level iteration only same as other SAN-based models, which means that (1) Graph-Transformer cannot capture order of sequence directly, (2) maximum order of subgraphs are limited in Graph-Transformer.\n3. **How to capture edges and subgraphs?**  Graph-Transformer uses three self-attentions to capture edges and subgraphs which is the most fundamental difference between Graph-Transformer and the original Transformer. \n4. **Which architecture will be selected?**  Architecture of Graph-Transformer is similar as the Transformer which has feed-forward layer, layer normalization and residual connection while Graph-Transformer have an additional component to combine representation generated by different self-attentions.\n\n\nWith MoG, people can analyze, design and improve models more efficiently for the purpose of optimizing MoG and Graph-Transformer is an example of using MoG to improve the Transformer, which means that MoG explanation can be an effective tool to design and optimize layer-aware neural network model. \n\nAn explanation of RNN-based model is updated to Section A.7 in pdf.\n\n---\n### About How MoG observer every steps of encoding\n\nUsing MoG, we can make a detailed understanding of encoding, and simulate entire encoding process which can be regarded as a process of generating MoG. We can get some information of subgraphs such as when and where subgraph is generated. It helps use to understand the model and the representation more clearly.\n\nAlthough we can simulate the entire encoding process, it is difficult to observer every step of generation of embedding in detail. Number of subgraphs increases at an exponential rate and makes it expensive and difficult to observe the generating of each subgraph in a model. Besides, existed models always preserve multiple subgraph information in one vector which makes it difficult to distinguish,  extract observer one subgraph.\n\n\n---\n### About other downstream tasks\nYes. We have conducted several new experiments on NER, POS tagging, text summarization, and  additional machine translation language pairs: WMT14 English-French (En-Fr), and WMT16 English-Romanian (En-Ro).  Please refer to General Response.\n\nResults of these experiments are updated to Table 1, Table 3 and Table 4 in our pdf.\n\n---\n### About overlaps between $SN$ and $TN$\nYes, you are right. There can be overlaps between $SN$ and $TN$. This status is **Loop** which has been shown in Figure 1 (b). Loop is ubiquitous in MoG. For example, query vector and key vector can point to the same word in the Transformer which generates a loop.\n\n\nAs we discussed in Section 2.4 and showed in Figure 1, the number of generation of kinds of subgraph is very large which makes it impossible to control all subgraphs.\n\n\n---\n### About some detail procedure\nModel can extract relationship between words within sentence during encoding, which is n-order dependency and actually building new edges in MoG. According to Section 3.1, this operation is implemented by self-attention in the original Transformer and Graph-Transformer. \n\n\nIt is true that the performance of model may highly be dependent over the quality of MoG construction. However, from a perspective of MoG, an effective model should generate good MoG to reflect information captured from input as complete and accurate as possible, which means that quality of MoG construction is dependent over the ability of encoding of model basically.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "xGMlLY8qcXn", "original": null, "number": 4, "cdate": 1605807863259, "ddate": null, "tcdate": 1605807863259, "tmdate": 1606240467618, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "_hN00kyWYhs", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Response to  AnonReviewer2", "comment": "Thank you for your review.\n\n---\n### About Graph-Transformer on other NLP tasks and benchmark datasets\nTo show effectiveness of the proposed MoG and Graph-Transformer, we have conducted several new experiments on NER, POS tagging, text summarization, and  additional machine translation language pairs: WMT14 English-French (En-Fr), and WMT16 English-Romanian (En-Ro).  Please refer to General Response-1.\n\nResults of these experiments are also updated to Table 1, Table 3 and Table 4 in our pdf.\n\n\n\n---\n### About position encoding in Graph-Transformer\nPosition-encoding is still the only component for Graph-Transformer to capture position information and necessary. Graph-Transformer enhances ability of capturing subgraphs of high order while ability of capturing position information is same as the original Transformer.\n\n\nWe have rewritten the Section A.8.1 to discuss position encoding from the perspective of MoG.\n\n---\n### About the efficiency of model\nThank you for your reminding. We have recorded the size of model paragraphs and training speed and shown these data in Table 2 of pdf. It shows that it will cost slightly more time to train Graph-Transformer. \n\n---\n### About Layer-level iteration\nLeft part of Figure 2 in paper is a brief schematic of layer-level iteration. Layer-level iteration takes representation generated by previous layer as input only, and new representation generated in one layer can only be used to generate subgraph in next layer which is different from sentence-level iteration. Please refer to General Response-1 for difference between layer-level iteration and sentence-level iteration.\n\nWe update Figure 6 to explain the layer-level iteration in our pdf. \n\nPlease refer to General Response for the difference between layer-level iteration and sentence-level iteration. This part is also updated to Section A.6 in our pdf.\n\n\n---\n### About $r$ in Section 2.4\n$r$ is short for **related subgraph**, and it is used in $sub^G_{j(r)}$ to represent the related subgraphs of edge $e_j$. To make it  clearer, we replace $sub^G_{j(r)}$ by $sub^G_{R(j)}$, where $R(j)$ is a function to get the identifier of related subgraph of $e_j$."}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Paper1513/Reviewers", "ICLR.cc/2021/Conference/Paper1513/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}, {"id": "uooj_jgRAd8", "original": null, "number": 7, "cdate": 1606003219919, "ddate": null, "tcdate": 1606003219919, "tmdate": 1606003219919, "tddate": null, "forum": "lDjgALS4qs8", "replyto": "lDjgALS4qs8", "invitation": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment", "content": {"title": "Time for discussion", "comment": "Dear Reviewers,\n\nThe authors have provided a detailed response and updated their revised manuscript. Would you please take a careful look at their response and update your review accordingly?\n\nThanks,\nAC"}, "signatures": ["ICLR.cc/2021/Conference/Paper1513/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph", "authorids": ["~Sufeng_Duan1", "~hai_zhao1", "~Rui_Wang10"], "authors": ["Sufeng Duan", "hai zhao", "Rui Wang"], "keywords": ["multigraph", "Transformer", "natural language process"], "abstract": "In this paper, we propose a unified explanation of representation for layer-aware neural sequence encoders, which regards the representation as a revisited multigraph called multi-order-graph (MoG), so that model encoding can be viewed as a processing to capture all subgraphs in MoG. The relationship reflected by Multi-order-graph, called $n$-order dependency, can present what existing simple directed graph explanation cannot present. Our proposed MoG explanation allows to precisely observe every step of the generation of representation, put diverse relationship such as syntax into a unifiedly depicted framework. Based on the proposed MoG explanation, we further propose a graph-based self-attention network empowered Graph-Transformer by enhancing the ability of capturing subgraph information over the current models. Graph-Transformer accommodates different subgraphs into different groups, which allows model to focus on salient subgraphs. Result of experiments on neural machine translation tasks show that the MoG-inspired model can yield effective performance improvement.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "duan|to_understand_representation_of_layeraware_sequence_encoders_as_multiordergraph", "one-sentence_summary": "This paper proposes a unified explanation of representation for layer-aware neural sequence encoders.", "pdf": "/pdf/4401daf2fdf9d014c026662a4792629956257651.pdf", "supplementary_material": "/attachment/512f35830a60a85041e917bb959ea105acd697dd.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=WJ4QOicrT1", "_bibtex": "@misc{\nduan2021to,\ntitle={To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph},\nauthor={Sufeng Duan and hai zhao and Rui Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=lDjgALS4qs8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "lDjgALS4qs8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1513/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1513/Authors|ICLR.cc/2021/Conference/Paper1513/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1513/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858840, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1513/-/Official_Comment"}}}], "count": 13}