{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518866615228, "tcdate": 1518866615228, "number": 12, "cdate": 1518866615228, "id": "SkkplqHDM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1PRwFkDG", "replyto": "H1PRwFkDG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper does not follow the formatting guidelines in https://iclr.cc/Conferences/2018/CallForWorkshops and is, thus, rejected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy", "abstract": "Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.", "pdf": "/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf", "TL;DR": "We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.", "paperhash": "hu|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy", "_bibtex": "@misc{\nhu2018anytime,\ntitle={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\nauthor={Hanzhang Hu, Debadeepta Dey, Martial Hebert, J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJa1Nk10b},\n}", "keywords": ["anytime", "neural network", "adaptive prediction", "budgeted prediction"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730188683, "tcdate": 1518471119291, "number": 292, "cdate": 1518471119291, "id": "H1PRwFkDG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1PRwFkDG", "original": "SJa1Nk10b", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy", "abstract": "Anytime predictors first produce crude results quickly, and then continuously refine them until the test-time computational budget is depleted. Such predictors are used in real-time vision systems and streaming-data processing to efficiently utilize varying test-time budgets, and to reduce average prediction cost via early-exits. However, anytime prediction algorithms have difficulties utilizing the accurate predictions of deep neural networks (DNNs), because DNNs are often computationally expensive without competitive intermediate results. \nIn this work, we propose to add auxiliary predictions in DNNs to generate anytime predictions, and optimize these predictions simultaneously by minimizing a carefully constructed weighted sum of losses, where the weights also oscillate during training. The proposed anytime neural networks (ANNs) produce reasonable anytime predictions without sacrificing the final performance or incurring noticeable extra computation. This enables us to assemble a sequence of exponentially deepening ANNs, and it achieves, both theoretically and practically,  near-optimal anytime predictions at every budget after spending a constant fraction of extra cost. The proposed methods are shown to produce anytime predictions at the state-of-the-art level on visual recognition data-sets, including ILSVRC2012.", "pdf": "/pdf/2eeafeb86c4766cddd20be49d2da3c46fa358276.pdf", "TL;DR": "We propose methods to train auxiliary predictors of neural networks to output competitive anytime predictions. We assemble such anytime networks to be near-optimal at any budget.", "paperhash": "hu|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy", "_bibtex": "@misc{\nhu2018anytime,\ntitle={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\nauthor={Hanzhang Hu, Debadeepta Dey, Martial Hebert, J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJa1Nk10b},\n}", "keywords": ["anytime", "neural network", "adaptive prediction", "budgeted prediction"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730188683, "tcdate": 1508991972780, "number": 111, "cdate": 1518730188672, "id": "SJa1Nk10b", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJa1Nk10b", "original": "HyhyEkyC-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy", "abstract": "We present an approach for anytime predictions in deep neural networks (DNNs). For each test sample, an anytime predictor produces a coarse result quickly, and then continues to refine it until the test-time computational budget is depleted. Such predictors can address the growing computational problem of DNNs by automatically adjusting to varying test-time budgets. In this work, we study a \\emph{general} augmentation to feed-forward networks to form anytime neural networks (ANNs) via auxiliary predictions and losses. Specifically, we point out a blind-spot in recent studies in such ANNs: the importance of high final accuracy. In fact, we show on multiple recognition data-sets and architectures that by having near-optimal final predictions in small anytime models, we can effectively double the speed of large ones to reach corresponding accuracy level. We achieve such speed-up with simple weighting of anytime losses that oscillate during training. We also assemble a sequence of exponentially deepening ANNs, to achieve both theoretically and practically near-optimal anytime results at any budget, at the cost of a constant fraction of additional consumed budget.", "pdf": "/pdf/011ca597b2f91c674b738341ac82d2b102c6ac29.pdf", "TL;DR": "By focusing more on the final predictions in anytime predictors (such as the very recent Multi-Scale-DenseNets), we make small anytime models to outperform large ones that don't have such focus. ", "paperhash": "hu|anytime_neural_network_a_versatile_tradeoff_between_computation_and_accuracy", "_bibtex": "@misc{\nhu2018anytime,\ntitle={Anytime Neural Network: a Versatile Trade-off Between Computation and Accuracy},\nauthor={Hanzhang Hu and Debadeepta Dey and Martial Hebert and J. Andrew Bagnell},\nyear={2018},\nurl={https://openreview.net/forum?id=SJa1Nk10b},\n}", "keywords": ["anytime", "neural network", "adaptive prediction", "budgeted prediction"], "authors": ["Hanzhang Hu", "Debadeepta Dey", "Martial Hebert", "J. Andrew Bagnell"], "authorids": ["hanzhang@cs.cmu.edu", "dedey@microsoft.com", "hebert@ri.cmu.edu", "dbagnell@ri.cmu.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}