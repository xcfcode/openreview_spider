{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396675612, "tcdate": 1486396675612, "number": 1, "id": "H1hbpMIux", "invitation": "ICLR.cc/2017/conference/-/paper556/acceptance", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The main idea in this paper is interesting, of considering various forms of adversarial examples in generative models. The paper has been considerably revised since the original submission. The results showing the susceptibility of generative models to attacks are interesting, but the demonstrations need to be more solid and on other datasets to be convincing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396676099, "id": "ICLR.cc/2017/conference/-/paper556/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396676099}}}, {"tddate": null, "tmdate": 1484926361254, "tcdate": 1481912355409, "number": 1, "id": "S1omxn-Vg", "invitation": "ICLR.cc/2017/conference/-/paper556/official/review", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer1"], "content": {"title": "Final review", "rating": "5: Marginally below acceptance threshold", "review": "\nAfter the rebuttal:\n\nThe paper contains an interesting set of results (mainly produced after the initial submission), but novelty is limited, and presentation is suboptimal. \n\nFor me now the biggest problem now is that the title and the content do not correspond. The authors clearly attack deterministic encoder-decoder models (as described in 3.2), which are not at all the same as generative models, even though many generative models make use of this architecture. A small experiment with sampling is interesting, but does not change the overall focus of the paper. This inconsistency in not acceptable. The whole issue could be resolved for example by simply replacing \"generative models\" by \"encoder-decoder networks\" in the title. Then I would tend towards recommending acceptance.\n\n------\nInitial review:\n\nThe paper describes three approaches to generating adversarial examples for deep encoder-decoder generative networks (trained as VAE or VAE-GAN), and shows a comparative analysis of these. While the phenomenon of adversarial examples in discriminative models is widely known and relatively well studied, I am not aware of previous work on adversarial examples for generative networks, so this work is novel (there is a concurrent work by Tabacof et al. which should be cited, though).  The paper has significantly improved since the initial submission; still, I have a number of remarks on presentation and experimental evaluation. I am in the borderline mode, and may change my rating during the discussion phase.\n\nDetailed comments:\n\n1) The paper is 13 pages long - significantly over the recommended page limit of 8 pages. Reviewers have to read multiple papers, multiple versions of each, it is a lot of work. Large portions of the paper should be shortened and/or moved to the appendix. It is job of the authors to make the paper concise and readable. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is a bad excuse - it may be hard, but has to be done.\n\n2) I intentionally avoided term \"generative model\" above because it is not obvious to me if the attacks described by the authors indeed attack generative models. To clarify, the authors train encoder-decoders as generative models (VAE or VAE-GAN), but then remove all stochasticity (sampling) and prior on the latent variables: that is, they treat the models as deterministic encoders-decoders. It is not a big surprise that a deterministic deep network can be easily tricked; it would be much more interesting to see if the probabilistic aspect of generative models makes them more robust to such attacks. Am I missing something? I would like the authors to clarify their view on this and adjust the claims in the paper if necessary.\n\n3) The paper is motivated by possible attacks on a data channel which uses a generative network for compressing information. Description of the attack scenario in 3.1 does not look convincing to me. It takes a huge amount of space and I do not think it adds much to the paper. First, experiments on natural images are necessary to judge if the proposed attack could succeed in a realistic scenario and second, I am not aware of any existing practical applications of VAEs to image compression: attacking JPEG would be much more practical. \n\n4) Experiments are limited to MNIST and, in the latest version, SVHN (which is very nice). While no good generative models of general natural images exist, it is common to evaluate generative models on datasets of faces, so this would be another very natural domain for testing the proposed approach. \n\nSmaller remarks:\n1) Usage of \"Oracle\" in 3.2 does not look appropriate - oracle typically has access to (part of) ground truth, which is not the case here as far as I understand.\n2) Beginning of section 4: \"All three methods work for any generative architecture that relies on a learned latent representation z\" - \"are technically applicable to\" would be more correct than \"work for\". \n3) 4.1: \"confidentally\"\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512543648, "id": "ICLR.cc/2017/conference/-/paper556/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2", "ICLR.cc/2017/conference/paper556/AnonReviewer4"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512543648}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484385866165, "tcdate": 1478352838680, "number": 556, "id": "SJk01vogl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJk01vogl", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "content": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": ["By1eEXVFg"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484385727113, "tcdate": 1484385727113, "number": 6, "id": "ryvTpvv8x", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "S1omxn-Vg", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "writers": ["~Jernej_Kos1"], "content": {"title": "Stochasticity and threat model", "comment": "Thanks for the review!  The new draft is shorter, reducing it to 11 pages while also adding some new results.\n\nIn this draft we have added experimental results using stochastic sampling. They are referenced in the second paragraph of Section 5 with a figure in the appendix. The results show that for most examples, sampling doesn\u2019t meaningfully change the reconstructed adversarial examples, and the attack is similarly successful even when sampling is being used.\n\nWe have also added some citations to clarify where the attack scenario sits in the current literature.  There are a few recent publications exploring using deep networks as compression models, similar to what we describe in Section 3.1, for example: https://arxiv.org/abs/1511.06085, and https://arxiv.org/abs/1608.05148.  We are not attempting to directly attack these networks in this work, however -- we are instead motivating why this type of attack is relevant to current work.\n\nThanks for the suggestion to use faces!  We have added experiments on CelebA in the latest draft.  The results in Section 5.3 show that the attacks are equally effective even in the more complex domain.  We have also addressed your other comments in this draft.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1484383475574, "tcdate": 1484383475574, "number": 5, "id": "rJhxHDPUl", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "H1GtVkvEg", "signatures": ["~Ian_Fischer1"], "readers": ["everyone"], "writers": ["~Ian_Fischer1"], "content": {"title": "Related work, relevance, revisions", "comment": "Thanks for the review, and for pointing out the NIPS workshop paper. That work was not known at the time of our submission to ICLR, as their paper was published online one month after our initial ICLR submission, and a couple of weeks after our first major revision. We have added a paragraph explaining the differences with their paper. It appears that their paper comes to a different conclusion regarding the L_vae attack -- we find it to be effective on MNIST, SVHN and CelebA, while they say that they couldn\u2019t get it to work. We also present a more in-depth study of the topic, which considers more attacks, more generative model architectures, and more datasets (with our addition of CelebA results in the latest draft).\n\nWe agree that ultimately we were able to determine that existing attacks could be modified to apply to the new domain of generative models, and so it may feel self-evident in retrospect.  However, prior to our work, it was unknown in the literature whether the stochasticity of generative models would confer robustness, or what an attack on a generative model would look like, or even under what scenarios an attacker might want to attack a generative model.  With our work, the research community knows that generative models are effectively as vulnerable to adversarial examples as deterministic classifiers, and that there are realistic scenarios where such attacks could matter.  In our opinion, this is an important result, even though it didn\u2019t require discovering substantially different mathematical approaches (and perhaps it is even more important because the attacks transferred so naturally, underscoring the vulnerability of current architectures).\n\nIt\u2019s true that we did a lot of work on this paper after the initial deadline.  ICLR\u2019s unusual submission and review process permits and encourages authors to continue revising their work after submission, which clearly has some advantages and disadvantages for all parties involved.  Presumably norms and expectations around how much papers change after submission will become more settled over time.  Thanks for your patience through the various revisions and additions we have made!  In this version, we have shortened and streamlined the paper, reducing it to 11 pages while also adding some new results.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1484381438116, "tcdate": 1484381438116, "number": 4, "id": "BkIZ68DIx", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "HJ4v6R_Ng", "signatures": ["~Jernej_Kos1"], "readers": ["everyone"], "writers": ["~Jernej_Kos1"], "content": {"title": "Length and title", "comment": "What you say about the telltale noise is correct -- as Goodfellow et al. 2014 show, it is possible to train a network using the adversarial examples and substantially reduce (but not eliminate) the network\u2019s vulnerability to adversarial examples.  Additionally, another ICLR submission this year showed a way to train a separate network to classify images as adversarial or non-adversarial with high accuracy.  All that being said, that paragraph is one of the paragraphs that we\u2019ve removed from the new version of the paper in order to reduce the length.  The paper is now 11 pages plus references and includes some new results.\n\nSince we are the first to study adversarial examples on generative models in depth, we think it is appropriate to use \u201cgenerative models\u201d in the title. The reason for focusing on VAE-like models is that in the paper we propose a plausible scenario how such a model can be used in a compression scheme, which can then be attacked by an adversary. To the best of our knowledge, other types of generative models, such as autoregressive models, do not offer such a clear attack scenario, where adversarial examples may be used by an attacker.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1482382683887, "tcdate": 1482382683887, "number": 3, "id": "HJ4v6R_Ng", "invitation": "ICLR.cc/2017/conference/-/paper556/official/review", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer4"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "Comments: \n\n\"This contrasts to adversarial attacks on classifiers, where any inspection of the inputs will reveal the original bytes the adversary supplied,\nwhich often have telltale noise\"\n\nIs this really true?  If it were the case, wouldn't it imply that training \"against\" adversarial examples should easily make a classifier robust to adversarial examples (if they all have a telltale noise)?  \n\nPros: \n  -The question of whether adversarial examples exist in generative models, and indeed how the definition of \"adversarial example\" carries over is an interesting one.  \n  -Finding that a certain type of generative model *doesn't have* adversarial examples would be a really significant result, finding that generative models have adversarial examples would also be a worth negative result.  \n  -The adversarial examples in figures 5 and 6 seem convincing, though they seem much more overt and noisy than the adversarial examples on MNIST shown in (Szegedy 2014).  Is this because it's actually harder to find adversarial examples in these types of generative models?  \n\nIssues: \n  -Paper is significantly over length at 13 pages.  \n  -The beginning of the paper should more clearly motivate its purpose.  \n  -Paper has \"generative models\" in the title but as far as I can tell the whole paper is concerned with autoencoder-type models.  This is kind of annoying because if someone wanted to consider adversarial attacks on, say, autoregressive models, they might be unreasonably burdened by having to explain how they're distinct from a paper called \"adversarial examples for generative models\".  \n   -I think that the introduction contains too much background information - it could be tightened.  \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512543648, "id": "ICLR.cc/2017/conference/-/paper556/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2", "ICLR.cc/2017/conference/paper556/AnonReviewer4"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512543648}}}, {"tddate": null, "tmdate": 1482253434106, "tcdate": 1482253434106, "number": 2, "id": "H1GtVkvEg", "invitation": "ICLR.cc/2017/conference/-/paper556/official/review", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer2"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper considers different methods of producing adversarial examples for generative models such as VAE and VAEGAN. Specifically, three methods are considered: classification-based adversaries which uses a classifier on top of the hidden code, VAE loss which directly uses the VAE loss and the \"latent attack\" which finds adversarial perturbation in the input so as to match the latent representation of a target input.\n\nI think the problem that this paper considers is potentially useful and interesting to the community. To the best of my knowledge this is the first paper that considers adversarial examples for generative models. As I pointed out in my pre-review comments, there is also a concurrent work of \"Adversarial Images for Variational Autoencoders\" that essentially proposes the same \"latent attack\" idea of this paper with both L2 distance and KL divergence.\n\nNovelty/originality: I didn't find the ideas of this paper very original. All the proposed three attacks are well-known and standard methods that here are applied to a new problem and this paper does not develop *novel* algorithms for attacking specifically *generative models*. However I still find it interesting to see how these standard methods compare in this new problem domain.\n\nThe clarity and presentation of the paper is very unsatisfying. The first version of the paper proposes the \"classification-based adversaries\" and reports only negative results. In the second set of revisions, the core idea of the paper changes and almost an entirely new paper with a new co-author is submitted and the idea of \"latent attack\" is proposed which works much better than the \"classification-based adversaries\". However, the authors try to keep around the materials of the first version, which results in a 13 page long paper, with different claims and unrelated set of experiments. \"in our attempts to be thorough, we have had a hard time keeping the length down\" is not a valid excuse.\n\nIn short, the paper is investigating an interesting problem and apply and compare standard adversarial methods to this domain, but the novelty and the presentation of the paper is limited.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512543648, "id": "ICLR.cc/2017/conference/-/paper556/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2", "ICLR.cc/2017/conference/paper556/AnonReviewer4"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512543648}}}, {"tddate": null, "tmdate": 1481848977000, "tcdate": 1481848976994, "number": 3, "id": "BkKqu2lNl", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "SkCRmryVe", "signatures": ["~Ian_Fischer1"], "readers": ["everyone"], "writers": ["~Ian_Fischer1"], "content": {"title": "SVHN added", "comment": "Thanks for the comments!  Responses are below:\n\n - Yes, the untargeted FGS attacks are not convincing.  We include them because FGS is a common baseline attack in the literature, but we focus on the optimization attacks because those are much more successful.\n - We only learned of this workshop paper at NIPS, so we didn\u2019t have a reference to it in previous drafts.  It is worth noting that our first public draft was uploaded to OpenReview before their first public version (according to arxiv) by almost a month, so this is an example of contemporaneous work.  It appears that the attack they present in their paper is similar to our latent attack, although they use a KL divergence rather than the L2 distance.  Since they only provide one set of examples for the two datasets, it is somewhat hard to compare their attack to our three attacks.  We\u2019ve added a brief reference to that paper in our related work section in the latest draft.\n - We had some minor technical issues that prevented us from including SVHN previously, but those are now resolved, so we have included SVHN in our latest draft.  Please take a look!\n - Yes, in our attempts to be thorough, we have had a hard time keeping the length down.  We may need to rework some of the data presentation to shorten the main body.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1481753558547, "tcdate": 1481753558539, "number": 3, "id": "SkCRmryVe", "invitation": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer2"], "content": {"title": "pre-review questions", "question": "Looking at Figure 8 (first version of paper, untargeted FGS classifier attack), it seems that maybe only 10% of MNIST digits are incorrectly reconstructed. Do the authors think this method is a success in achieving what they defined in the problem statement, i.e., \"reconstruction looks more like something from another class.\"?\n\nHow is this paper different from the \"Adversarial Images for Variational Autoencoders\" paper, in terms of both the ideas and the performance? Specially, I think the idea of \"latent attack\" is very similar in both papers. I think there has to be a section in the paper to explain the difference between these two works.\n\nMNIST dataset is the only dataset that is considered, which is not very convincing.\n\nThe paper is much longer than the standard page-limit, which makes it hard to follow."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481753559164, "id": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer3", "ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481753559164}}}, {"tddate": null, "tmdate": 1480973945424, "tcdate": 1480973945417, "number": 2, "id": "Bk-YRImmg", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "HJzZ800Gl", "signatures": ["~Ian_Fischer1"], "readers": ["everyone"], "writers": ["~Ian_Fischer1"], "content": {"title": "Natural Images, etc.", "comment": "Thanks for the questions! In this work, we\u2019ve chosen to explore the attacks in breadth and depth, which has caused us to remain focused on MNIST for the time being.  However, exploring adversarial attacks on natural image datasets like CIFAR and ImageNet is in our plan for upcoming work (although we don\u2019t mention that in the paper yet).  Using MNIST has some advantages in terms of clarity of results, but, as your questions indicate, the attacks end up having different behavior than they will on natural image datasets, so we can\u2019t claim to be sure that our results will generalize. I would hypothesize that there will be a few major differences once we start on ImageNet:\nThe attacks will be much less noticeable.\nThe generative reconstructions (on clean and adversarial images) will be of much lower quality.\nMean latent vector attacks will not generate as good results with large numbers of images, so we will be limited to a very small number of images for that class of attacks.\n\nIndividual question responses are below:\n\n1. We agree that the attacks are quite visible on MNIST.  That has been the case for most attacks in the literature, however (e.g., with the fast gradient sign, in order to get high attack success rates on MNIST, we\u2019ve found that it\u2019s necessary to have an epsilon of >= 0.3, which is much more noticeable than our L2 optimization attacks).  If we have a generative model that works well with natural images, then we think the various attack scenarios are quite plausible.  The new draft goes into more detail on motivating the attack (Section 3.1) -- let us know if you think it is unclear or problematic in some way.\n\n2. We kept hoping that there would be an interesting defense story in the results, but we haven\u2019t been able to convince ourselves yet that it\u2019s there. Adversarial examples on MNIST are almost always easy to detect, since the dataset is so clean.\n\n3. Not yet, but we would like to.\n\n4. The new version focuses almost entirely on optimization attacks, which do indeed perform much better than FGS.\n\n5. Yes, we should probably include more figures with different inputs in each row and attack details for the different attacks in each column. Thanks for the suggestion!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1480676857753, "tcdate": 1480676857748, "number": 2, "id": "HJzZ800Gl", "invitation": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer1"], "content": {"title": "pre-review questions", "question": "Hello,\n\nAn interesting paper! Below are some questions. They are based on the yesterday version of the paper, and I see some of them are addressed in the new version which just appeared. But unfortunately I do not have time to carefully re-read the paper now, so just let me know if some of these do not apply any more.\n\n1. Do you believe the method would be applicable in the attack scenario you described? The added perturbations are very visible.\n\n2. To me the most interesting aspect of this paper is that the proposed method does _not_ work very well, meaning that generative models are \"difficult to fool\". Do you agree with this intuition? Have you considered this perspective? \n\n3. Have you experimented with other datasets? Ideally natural images, but at least faces?\n\n4. Why are you only using the fast gradient sign method for generating adversarial examples? Wouldn't you get better results with iterative optimization?\n\n5. Don't you think viewing the figures would be more convenient if you grouped the results by samples, not by methods? As in Figure 7. \n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481753559164, "id": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer3", "ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481753559164}}}, {"tddate": null, "tmdate": 1480661298801, "tcdate": 1480661298794, "number": 1, "id": "S1j4FcAGl", "invitation": "ICLR.cc/2017/conference/-/paper556/public/comment", "forum": "SJk01vogl", "replyto": "HJPG-6hMg", "signatures": ["~Ian_Fischer1"], "readers": ["everyone"], "writers": ["~Ian_Fischer1"], "content": {"title": "New draft", "comment": "We have not done the nearest neighbor experiment yet, but it\u2019s an excellent suggestion -- we hope to have that in our next draft.  We have just uploaded a new draft that substantially expands and clarifies our results.  One of the major additions is showing how the reconstructions are classified and using that as the basis for some metrics about how the different approaches perform, which sounds very much like what you are suggesting.  Please take a look!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287522048, "id": "ICLR.cc/2017/conference/-/paper556/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJk01vogl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper556/reviewers", "ICLR.cc/2017/conference/paper556/areachairs"], "cdate": 1485287522048}}}, {"tddate": null, "tmdate": 1480540430994, "tcdate": 1480540430989, "number": 1, "id": "HJPG-6hMg", "invitation": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "forum": "SJk01vogl", "replyto": "SJk01vogl", "signatures": ["ICLR.cc/2017/conference/paper556/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper556/AnonReviewer3"], "content": {"title": "Quantification of the adversarially-perturbed coding", "question": "Figure 7 suggests that your adversarial approach is systematically distorting the representation such that the reconstructed images belong to a different class. Have you compared the nearest neighbor of your perturbed embeddings to the embeddings in your training data set? Additionally, have you quantified the degree to which the reconstructed class might be mis-classified as a different class?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial examples for generative models", "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.", "pdf": "/pdf/503e0fb6d4e69e70b71d90a2a252a3085e7e5eb4.pdf", "TL;DR": "Exploration of ways to attack generative models with adversarial examples and why someone might want to do that.", "paperhash": "kos|adversarial_examples_for_generative_models", "keywords": ["Computer vision", "Unsupervised Learning"], "conflicts": ["berkeley.edu", "google.com"], "authors": ["Jernej Kos", "Ian Fischer", "Dawn Song"], "authorids": ["jernej@kos.mx", "iansf@google.com", "dawnsong.travel@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481753559164, "id": "ICLR.cc/2017/conference/-/paper556/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper556/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper556/AnonReviewer3", "ICLR.cc/2017/conference/paper556/AnonReviewer1", "ICLR.cc/2017/conference/paper556/AnonReviewer2"], "reply": {"forum": "SJk01vogl", "replyto": "SJk01vogl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper556/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481753559164}}}], "count": 14}