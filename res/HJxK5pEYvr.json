{"notes": [{"id": "HJxK5pEYvr", "original": "B1xacgRvwS", "number": 715, "cdate": 1569439121168, "ddate": null, "tcdate": 1569439121168, "tmdate": 1583912028700, "tddate": null, "forum": "HJxK5pEYvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "iwTxnwpbAa", "original": null, "number": 1, "cdate": 1577527975684, "ddate": null, "tcdate": 1577527975684, "tmdate": 1577528101212, "tddate": null, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "invitation": "ICLR.cc/2020/Conference/Paper715/-/Public_Comment", "content": {"title": "About the result on text-classification", "comment": "As far as I know, datasets like IMDB did not provide golden constituency parsing trees of each sentence, so you might use some parser(such as corenlp, etc) to get the parse trees, and there would be many errors. How do Transformers benefit from an error-prone parse tree so much (10% on IMDB)? Some fine-grained analysis or ablation studies would make the result more convincing.\n\nWhat confuses me most is that your result on SST is too low (47.4 on test acc, as a reference, Tree-LSTM can get a 51.0(\u00b10.7) with the help of pre-trained GloVe embeddings and you report 43.9), if it's because you use random-initialized word embeddings, then why your result on IMDB is extremely high (98.2)? as reviewer 1 mentioned, even BERT can only achieve an accuracy of 96.\n\nI've experimented with transformer on tree-like hierarchical structures (https://rlgm.github.io/papers/67.pdf , ICLR-RLGM 2019) before, our model also updates hierarhical features with transformers, the difference is: 1. use naive binary tree or constituency parse tree 2. sparse or dense attention 3. w/ or w/o hierarchical aggregation.\nConsidering the difference, I changed my structure to use constituency parse tree(produced by corenlp) and make the  attention dense, however I did not observe performance gain on IMDB.\n\nI would appreciate it if authors could provide more ablation studies (e.g. what if you use naive binary tree instead of constituency parse trees with your structure, the effect of hierarchical aggregation), thanks."}, "signatures": ["~Zihao_Ye1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Zihao_Ye1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxK5pEYvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504205195, "tmdate": 1576860575040, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper715/Authors", "ICLR.cc/2020/Conference/Paper715/Reviewers", "ICLR.cc/2020/Conference/Paper715/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper715/-/Public_Comment"}}}, {"id": "eWkLLI5xLC", "original": null, "number": 1, "cdate": 1576798704052, "ddate": null, "tcdate": 1576798704052, "tmdate": 1576800932010, "tddate": null, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "invitation": "ICLR.cc/2020/Conference/Paper715/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper incorporates tree-structured information about a sentence into how transformers process it. Results are improved. The paper is clear. Reviewers liked it. Clear accept.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728752, "tmdate": 1576800281220, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper715/-/Decision"}}}, {"id": "H1g_PfRoYS", "original": null, "number": 1, "cdate": 1571705439740, "ddate": null, "tcdate": 1571705439740, "tmdate": 1574239871624, "tddate": null, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "invitation": "ICLR.cc/2020/Conference/Paper715/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper extends transformers by enabling them to incorporate hierarchical structures like constituency trees structured attention with hierarchical accumulation. In particular, they modified the architecture of transformers such that they can learn phrase-level attention scores, and use them in the final assigned task of text classification or language translation. \nThey also explored a couple of variants of their proposed architecture: Hierarchical embeddings and Subnet masking which helped them outperform SOTA methods including Tree-LSTM (similar to this paper in principle, except that it was designed for LSTM-based models and not transformers). \n\nThe paper is well written and well-augmented with supportive figures. Particularly, Figure1 was very helpful in understanding all the complexities of the proposed model. Further, the authors justify the utility of the proposed approach covering different aspects of evaluation including comparative studies with baselines, ablation studies, phrase vs token-level attentions, training-time analysis. \n\nA limitation of this work is high inference cost. As the results indicate, parsing trees from text is the most costly step in the entire framework, and consequently, the inference time of proposed model will still be much higher than transformers. Hence, this work might still not be applicable to low-latency constraint scenarios. \n\n\nOther Comments: \n1) I did not fully understand why it would be better to mask out the non descendants in subnet masking approach. Why shouldn't a phrase node seek attention from tokens outside its scope? Probably the answer lies in the the way these trees are constructed. Nevertheless, it would be useful to provide some intuition with examples to motivate subtree masking. \n\n2) In Equation(5), the subscripts \"i-1\" should be replaced with \"i\"? Otherwise it will be sensitive to ordering of non-terminal nodes in N, and also Figure 1 wouldn't make sense.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652336795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper715/Reviewers"], "noninvitees": [], "tcdate": 1570237748140, "tmdate": 1575652336812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper715/-/Official_Review"}}}, {"id": "BJgV33DTYB", "original": null, "number": 2, "cdate": 1571810475963, "ddate": null, "tcdate": 1571810475963, "tmdate": 1574235134313, "tddate": null, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "invitation": "ICLR.cc/2020/Conference/Paper715/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Summary: This paper describes an attention-based method to encode trees with constant parallel-time complexity maintaining scalability, uses this model to encode constituency parses of input sentences for the tasks of machine translation and text classification, and shows improved accuracy/bleu over models that do not encode the parses. \n\nStrengths:\n\nThe method proposed by the authors is scalable despite encoding tree structures. The models give considerable improvements on various machine translation datasets and the authors also show that the model is more sample efficient. I appreciate the charts showing training and inference time with sentence length, and the tables showing the ablations and attention distributions. \n\nWeaknesses\n\nThe authors do not use pre-trained embeddings for any of the classification models, but using these embeddings boost performance to much more than what they authors have achieved here. My main question is, if we use pre-trained embeddings, do encoding constituency parses add anything over and above them? I would like to see this method improve results over some state of the art classification models and not just over the tree-LSTM. In other words, how much classification performance does this method yield over current SOTA models, because the SST results are quite a bit below the current SOTA.\n\nThe authors achieve an accuracy of 98.2 on the IMDB dataset. Is this actually the case or is this a bug? Even models using BERT barely achieve an accuracy of 96% (http://nlpprogress.com/english/sentiment_analysis.html). Or am I looking at the wrong dataset here? Can the authors clarify?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652336795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper715/Reviewers"], "noninvitees": [], "tcdate": 1570237748140, "tmdate": 1575652336812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper715/-/Official_Review"}}}, {"id": "HklbTMp0YH", "original": null, "number": 3, "cdate": 1571898041008, "ddate": null, "tcdate": 1571898041008, "tmdate": 1572972561341, "tddate": null, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "invitation": "ICLR.cc/2020/Conference/Paper715/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel mechanism to leverage additional tree-structure information into the transformer. The proposed method consist three main component: a hierarchical accumulation strategy to aggregate the leaf node embedding information to the non-terminal nodes; a hierarchical embedding which is akin to positional embedding, however encapsulating the leaf-to-node and relative position information; a sub-tree masking strategy to filter out irrelevant information. Although the formulations seem a little bit overcomplicated/cumbersome to me, I find the figures are really helpful to understand the gists, which are intuitive and straightforward. \n\nStrengths:\n\n* Intuitive and novel ideas. The tree structure information is incorporated into the transformer in a sophisticated and elegant manner, with only constant time-complexity overhead. \n\n* Strong empirical results and well-defined ablation study. Good analysis on the performance vs dataset size and time complexity\n\nCons:\n\n* Heavy notations which could be more concise. \n\n* Implementing such an architecture without source code could be difficult.\n\nQuestions:\n\n* Comparing with the naive transformer architecture (e.g. base model), how many additional parameters are there in your model? I see that for fair comparison, the authors use the same base transformer architecture, however it would still be very helpful if they can provide the statistics of the number of parameters for the proposed model and the compared baselines. \n\n* Are all the baselines using the same parser (CoreNLP)? If not, would the difference of parsing trees be a confounding factor?\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper715/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tree-Structured Attention with Hierarchical Accumulation", "authors": ["Xuan-Phi Nguyen", "Shafiq Joty", "Steven Hoi", "Richard Socher"], "authorids": ["nxphi47@gmail.com", "sjoty@salesforce.com"], "keywords": ["Tree", "Constituency Tree", "Hierarchical Accumulation", "Machine Translation", "NMT", "WMT", "IWSLT", "Text Classification", "Sentiment Analysis"], "abstract": "Incorporating hierarchical structures like constituency trees has been shown to be effective for various natural language processing (NLP) tasks. However, it is evident that state-of-the-art (SOTA) sequence-based models like the Transformer struggle to encode such structures inherently. On the other hand, dedicated models like the Tree-LSTM, while explicitly modeling hierarchical structures, do not perform as efficiently as the Transformer. In this paper, we attempt to bridge this gap with Hierarchical Accumulation to encode parse tree structures into self-attention at constant time complexity. Our approach outperforms SOTA methods in four IWSLT translation tasks and the WMT'14 English-German task. It also yields improvements over Transformer and Tree-LSTM on three text classification tasks. We further demonstrate that using hierarchical priors can compensate for data shortage, and that our model prefers phrase-level attentions over token-level attentions.", "pdf": "/pdf/55798e850eb51021c8c65ef727968718b41a91d4.pdf", "paperhash": "nguyen|treestructured_attention_with_hierarchical_accumulation", "_bibtex": "@inproceedings{\nNguyen2020Tree-Structured,\ntitle={Tree-Structured Attention with Hierarchical Accumulation},\nauthor={Xuan-Phi Nguyen and Shafiq Joty and Steven Hoi and Richard Socher},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxK5pEYvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/43bd3d496c9cad1f6793096e87c97dd193318116.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxK5pEYvr", "replyto": "HJxK5pEYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper715/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652336795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper715/Reviewers"], "noninvitees": [], "tcdate": 1570237748140, "tmdate": 1575652336812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper715/-/Official_Review"}}}], "count": 6}