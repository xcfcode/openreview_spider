{"notes": [{"id": "Hkg313AcFX", "original": "ByltmgAqFm", "number": 1019, "cdate": 1538087907618, "ddate": null, "tcdate": 1538087907618, "tmdate": 1545355415879, "tddate": null, "forum": "Hkg313AcFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1ghUnQpyE", "original": null, "number": 1, "cdate": 1544531027703, "ddate": null, "tcdate": 1544531027703, "tmdate": 1545354498994, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Meta_Review", "content": {"metareview": "This paper provides a good finding that maximizing a lower bound of the M-H acceptance rate is equivalent to minimizing the symmetric KL divergence between target the proposal. This lower bound is then used to learn sampler for both density and sample-based settings. It also nicely connects GAN with MCMC by providing a novel loss function to train the discriminator. Experiment on MNIST dataset in Sec 4.2 shows training the proposal with the symmetric KL is better than variational inference that optimizes KL(q||p).\n\nHowever, there are a few concerns raised in both the reviews and other comments that should be further clarified.\n1. Training an independent proposal may reduce the rate of convergence. \n2. In the density-based setting experiments, the learnt independent proposal is only used to provide an initial point and a random-walk kernel is actually used for sampling. This is different from what is proposed algorithm in Section 3.\n3. The proposed algorithm is only compared with VI in density-based setting, and there are no comparison with other baselines in the sample-based setting, despite the close connections of the proposed method with other models. Stochastic gradient MCMC methods, A-NICE-MC, GAN will be good baselines for empirical comparisons. Also, the dataset in Sec 4.2 is a subset of the standard MNIST, which makes comparison with other literatures difficult.\n\nFor the first concern, the authors provided new experiments for low-dimensional synthetic distributions. It is very helpful to show the comparable performance with A-NICE-MC in this case, but the real challenge in high-dimensional distributions remains unexamined. For the second concern, the authors consider the use of random-walk as a heuristic that allows to obtain better samples from the posterior, but that significantly changes the proposed transition kernel in Alg. 1.\n\nThis paper would be significantly stronger and make a very good contribution to this area by addressing the problems above.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Nice theoretic finding and connection of GAN with MCMC, but some concerns are to be addressed for a strong publication"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1019/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352997519, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352997519}}}, {"id": "S1xgAXoWJ4", "original": null, "number": 8, "cdate": 1543775176468, "ddate": null, "tcdate": 1543775176468, "tmdate": 1543775176468, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Sylt4u5nRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for your interest!", "comment": "\nWe are very pleased with your interest in our paper!\nWe also were enjoyed by reading yours! :)\n\nIt is very important for us to provide the reader with a comprehensive overview of different sampling methods, so we will glad to cite both papers in the camera-ready version of our paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "H1xaz7obJN", "original": null, "number": 7, "cdate": 1543774997441, "ddate": null, "tcdate": 1543774997441, "tmdate": 1543774997441, "tddate": null, "forum": "Hkg313AcFX", "replyto": "BJllVttk1N", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for your interest!", "comment": "\nThank you for reading our paper!\nLet us make things more clear.\n\n> MH is notoriously slow in large scale datasets because it requires computing the likelihood over the entire dataset. In the MNIST classification experiments, how is Metropolis-Hastings performed? Is this the reason why the dataset is delibrately limited to 20,000 examples? How does this method compare with other stochastic gradient MCMC methods, such as SGLD?\n\nIn section 3.1 we show that recently proposed techniques [1,2] can be adapted to use only minibatches of data while sampling via the Metropolis-Hastings algorithm. It allows us to estimate gradients of the proposed lower bound on the acceptance rate using only minibatches of data. We use only 20 000 of train images in order to make the MNIST classification problem more challenging since it is known that a large number of algorithms are able to obtain almost perfect test classification using the full trainset. For this problem, we compare our approach only with the variational inference as the most popular way of deriving the approximation for posterior that is further used for estimation of predictive distribution.\n\n> https://arxiv.org/abs/1505.05424 , a VI method, claims significantly better test accuracy than what is claimed here, but it uses a larger dataset, so the results are not directly comparable. Why not use the 50,000-sample MNIST dataset in this paper?\n\nIn our paper, we use a reduced LeNet-5 architecture with 8550 parameters. In the paper [3] authors use the network from the paper [4] that consists of two fully-connected layers (4000 and 2000 hidden units) with 11136000 parameters in total. We use only 20 000 of train images in order to make the MNIST classification problem more challenging since it is known that a large number of algorithms are able to obtain almost perfect test classification using the full trainset.\n\n> Moreover, the proposed method for sample based has appeared in Song's A-NICE-MC paper (section 3). In fact, it is a special case of Song's method, where m = 1 and x ~ p(x) and converging from random noise is not concerned.\n\nOur algorithm proposed in the sample-based setting is motivated by the maximization of the acceptance rate and entirely derived from the desired objective. The proposed algorithm differs from A-NICE-MC [5] in some key points:\n- We use different loss for the discriminator and the different structure of the discriminator. The discriminator in A-NICE-MC tries to distinguish between (real, real) pairs and (fake, real) pairs, while our discriminator tries to distinguish between (fake, real) pairs and (real, fake) pairs. That implies different loss function and results in the estimation of density ratio that is needed for the acceptance rate estimation. \n- We use different loss for the generator (both in case of maximization of the acceptance rate and its lower bound). The generator loss in A-NICE-MC is motivated by [6]. In our paper, we propose to learn the generator to maximize the acceptance rate (or its lower bound).\n- We propose to use the discriminator while sampling for the approximate density ratio estimation and performing the MH algorithm.\nHence, our algorithm can\u2019t be considered as a special case of A-NICE-MC.\n\n1. Korattikara, Anoop, Yutian Chen, and Max Welling. \"Austerity in MCMC land: Cutting the Metropolis-Hastings budget.\" In International Conference on Machine Learning, pp. 181-189. 2014.\n2. Chen, Haoyu, Daniel Seita, Xinlei Pan, and John Canny. \"An Efficient Minibatch Acceptance Test for Metropolis-Hastings.\" arXiv preprint arXiv:1610.06848 (2016).\n3. Blundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. \"Weight uncertainty in neural networks.\" arXiv preprint arXiv:1505.05424 (2015).\n4. Nair, Vinod, and Geoffrey E. Hinton. \"Rectified linear units improve restricted boltzmann machines.\" In Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807-814. 2010.\n5. Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. In Advances in Neural Information Processing Systems, pp. 5140\u20135150, 2017.\n6. Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "BJllVttk1N", "original": null, "number": 3, "cdate": 1543637288426, "ddate": null, "tcdate": 1543637288426, "tmdate": 1543637288426, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "content": {"comment": "I find the density based formulation interesting and potentially useful for training amortized approximations to energy based models. However, I have some concerns about the experiments and the sample based method.\n\nMH is notoriously slow in large scale datasets because it requires computing the likelihood over the entire dataset. In the MNIST classification experiments, how is Metropolis-Hastings performed? Is this the reason why the dataset is delibrately limited to 20,000 examples? How does this method compare with other stochastic gradient MCMC methods, such as SGLD? \n\nhttps://arxiv.org/abs/1505.05424 , a VI method, claims significantly better test accuracy than what is claimed here, but it uses a larger dataset, so the results are not directly comparable. Why not use the 50,000-sample MNIST dataset in this paper?\n\nMoreover, the proposed method for sample based has appeared in Song's A-NICE-MC paper (section 3). In fact, it is a special case of Song's method, where m = 1 and x ~ p(x) and converging from random noise is not concerned.\n", "title": "Some questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311697964, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hkg313AcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311697964}}}, {"id": "BklWnDOk14", "original": null, "number": 2, "cdate": 1543632809033, "ddate": null, "tcdate": 1543632809033, "tmdate": 1543632809033, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Sylt4u5nRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "content": {"comment": "If not, then I think it is not very relevant, since the Markov chain does not have a discrete accept step in VW.", "title": "Does variational walk back consider Metropolis Hastings?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311697964, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hkg313AcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311697964}}}, {"id": "SJgcTjApRQ", "original": null, "number": 6, "cdate": 1543527361578, "ddate": null, "tcdate": 1543527361578, "tmdate": 1543527361578, "tddate": null, "forum": "Hkg313AcFX", "replyto": "SJx-1SnF3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for the update!", "comment": "\nThank you again for your efforts on reviewing our paper!\n\n> First, the artificial examples are still rather low-dimensional where independent MH is expected to perform well. \n\nIndeed, scaling for a large number of dimensions is important to verify. In order to do that, one needs to learn a sophisticated proposal. Since it seems infeasible to sample from the true posterior distribution using a gaussian as proposal, if we somehow measured the quality of samples obtained via a simple proposal it would be poor. For now, we can say that our model behaves reasonably well in high-dimensions as it allows to obtain better test log-likelihood than variational inference.\n\n> Second, ESS does not help to assess the biasedness of the sample; maybe [1] can help with this.\n> Finally, to show the correctness of the method, I would suggest to 1) let alpha converge to zero such that \\phi will be fixed at some point, and 2) ensure that the proposal has full support under the target for any value of \\phi. In this case, the sample drawn from the target will be unbiased for large enough n (same arguments as for adaptive MCMC should apply). \n\nThanks for this paper, we will definitely check it out. \nHowever, it\u2019s worth mentioning that in the updated version we have added the following paragraph in Section 3.1 to motivate the usage of normalizing flows.\n\u201cIn this paper, we consider two types of explicit proposals: simple parametric family (Section 4.2) and normalizing flows (Rezende & Mohamed, 2015; Dinh et al., 2016) (Section 4.1). Rich family of normalizing flows allows to learn expressive proposal and evaluate its density in any point of target distribution space. Moreover, an invertible model (such as normalizing flow) is a natural choice for the independent proposal due to its ergodicity. Indeed, choosing the arbitrary point in the target distribution space, we can obtain the corresponding point in the latent space using the inverse function. Since every point in the latent space has positive density, then every point in the target space also has positive density.\u201d\nHence, the usage of an ergodic proposal combined with the Metropolis-Hastings algorithm guarantees unbiased sampling from the target distribution after convergence to the stationary distribution. However, the number of steps to obtain a representative set of samples from the target distribution can differ significantly, and ESS allows to estimate such number. Moreover, in Fig. 1 we provide histograms for different proposals and show that qualitative difference of histograms is successfully captured by the quantitative difference in ESS (see Table 1).\n\n> Third, NUTS might be a better baseline than standard HMC which is know to be sensitive to the stepsize/number of leapfrog steps. An effective sample size of 1 suggests that the method did not even start to sample - likely because of a bad choice of the stepsize and/or mass matrix. I would suggest using PyMC3's NUTS implementation. \n\nOur main competitor in terms of ESS is A-NICE-MC method [1], which also learns the neural network as proposal. In this paper they designed such synthetic distributions that are hard for HMC to sample from. For such synthetic distributions HMC fails to jump across modes because they are too distant with low variance, hence gradients around the one of modes have little information about the other modes. These distributions and performance of HMC are reported in [1], maybe it\u2019s better to remove the comparison with HMC from Table 1 so as not to confuse the reader.\n\n1. Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. In Advances in Neural Information Processing Systems, pp. 5140\u20135150, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "SJx-1SnF3Q", "original": null, "number": 2, "cdate": 1541158104715, "ddate": null, "tcdate": 1541158104715, "tmdate": 1543487297403, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "content": {"title": "The theoretic finding that maximizing the Metropolis Hastings acceptance rate is equivalent to minimize the symmetric KL divergence between target and proposal is great. Though, I have doubts about the correctness of the (main) proposed algorithm leveraging those insights and would expect, at least, an empirical verification of correctness.", "review": "One of the main contributions of the paper is showing how maximizing the acceptance rate in Metropolis Hastings (MH) translates in minimizing the symmetric KL divergence of the target and the proposal distribution. The result aligns with the intuition that if this variance is 0, detailed balanced is satisfied and, hence, we can always accept the proposal. Also Equation 11 nicely fits the intuition that a good (independent) proposal should minimize the KL divergence between proposal and target (as in VI) under the constraint that the proposal has full support compared to the target distribution, which is enforced by the last term. Theorem 1 and its proof are great.\n\nHowever, the proposed algorithms leveraging these findings are problematic. Algorithm 1 suggest independent Metropolis-Hastings in order to avoid the proposal to collapse to a point distribution, that is, a Dirac delta function centered at the current position. However, in the experiments, the authors study a \"modified\" version using a random walk proposal parameterized with the current (diagonal) covariance estimate. This is surprising as the authors explicitly avoided this proposal when motivating MH acceptance rate maximization.\n\nIn any case, the correctness of the algorithm is neither shown for an independent proposal nor a Markov Chain proposal. Indeed, I would argue that Algorithm 1 generally does not find a sample distributed according to the target distribution. The algorithm assumes that we can create a sample from the target p(x) that can be used to approximate the loss (a bound on the expected acceptance rate). However, if we could find an unbiased sample in the very first iteration of the algorithm, we could immediately stop and there wouldn't be a need for the algorithm at all. Hence, we must assume that the sample drawn from the target is biased (in the beginning); which is indeed a realistic assumption as neither independent MH nor random walk MH will yield an unbiased sample in any reasonable time (for any practical problem). However, this would bias the loss and, consequently, the parameter update for the proposal distribution. In particular, for random walk MH, I would expect the covariance to contract such that the final proposal indeed collapses to a point distribution. This is because the proposal is only required to have support around the observed samples and this area will become smaller over time. I would expect a proof that the sample at iteration k is \"better\" than a sample drawn at iteration k-1, to show that the bias vanishes over time. Though, I assume that this is hard to show as the proposal parameters form a Markov Chain itself. So at least a rigor empirical study is needed.\n\nTherefore, I would expect a metric measuring the quality of the final sample. The log-likelihood is not such a measure. While the marginalized log-likelihood could measure the quality of the sample, we cannot compute it for any real data/model (which is why we use sampling in the first place). So we need some artificial settings. However, the 1-dimensional toy example is insufficient as MH primarily suffers in high-dimensional spaces. It would be interesting to also report the acceptance rate depending on the number of dimensions of the target distribution. I would assume an exponential decay; even with learning, which might be the reason why the authors only report random walk MH in Section 4.2.\n\nAlgorithm 2 does not require to sample from some target distribution but can leverage the observed sample. While the algorithm nicely connects GANs and sampling, the actual value of the algorithm is not fully clear to me. Learning an independent proposal reduces the problem to learning a GAN; and learning a Markov Chain seems only relevant for sampling-based inference; however, we already have a sample from the target distribution, and we can sample more data using a trained GAN.\n\nMinor comments:\n- The prefix/in-fix notation of integrals is mixed, e.g. in Eq 19, \"dx\" appears before the integrand, but \"du\" appears after the integrand of the inner integral.\n\n\nUPDATE:\n\nThe revised version is much better in empirically demonstrating the value of the method; though, there is still some work needed. First, the artificial examples are still rather low-dimensional where independent MH is expected to perform well. Second, ESS does not help to assess the biasedness of the sample; maybe [1] can help with this. Third, NUTS might be a better baseline than standard HMC which is know to be sensitive to the stepsize/number of leapfrog steps. An effective sample size of 1 suggests that the method did not even start to sample - likely because of a bad choice of the stepsize and/or mass matrix. I would suggest using PyMC3's NUTS implementation. Finally, to show the correctness of the method, I would suggest to 1) let alpha converge to zero such that \\phi will be fixed at some point, and 2) ensure that the proposal has full support under the target for any value of \\phi. In this case, the sample drawn from the target will be unbiased for large enough n (same arguments as for adaptive MCMC should apply).\n\nThe idea of reusing the samples from previous iterations for approximating the loss is interesting and worth exploring.\n\n[1] Jackson Gorham, Lester Mackey. \"Measuring Sample Quality with Kernels\", https://arxiv.org/abs/1703.01717\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "cdate": 1542234324700, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335854910, "tmdate": 1552335854910, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Sylt4u5nRm", "original": null, "number": 1, "cdate": 1543444529058, "ddate": null, "tcdate": 1543444529058, "tmdate": 1543444541926, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "content": {"comment": "Hello, \n\nI enjoyed reading your paper. \n\nI would like to point our paper where we learn a transition operator of MCMC chain by directly parameterizing it with a neural network. (Variational Walkback, https://arxiv.org/abs/1711.02282). We feel like our paper should be cited.\n\nIf you indeed decide to cite my paper, you should also reference this paper.\n\n[1] https://arxiv.org/abs/1503.03585, Deep Unsupervised Learning using Non equilibrium thermodynamics.\n\nThanks for your time! :)", "title": "You can (also) directly parameterize the transition operator of MCMC chain."}, "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anirudh_Goyal1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311697964, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Hkg313AcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311697964}}}, {"id": "HJggW5cFhQ", "original": null, "number": 1, "cdate": 1541151224071, "ddate": null, "tcdate": 1541151224071, "tmdate": 1543432908062, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "content": {"title": "Some points must clarified", "review": "I think the paper could be published, however I have some concerns.\n\nMayor comments:\n\n- My main concern is that I do not understand why do not directly apply the  KL divergence with respect to p(x) and q(x) instead of considering p(x)\\times q(x') and  p(x')\\times q(x). More specifically, I have understood that your approach is motivated by Theorem 1 (nice result, by the way) but I am not sure it is better than just applying the KL divergence with respect to p(x) and q(x), directly.\n\n- The state-of-the-art discussion for MCMC schemes in the introduction must be completed at least including the Multiple Try Metropolis algorithms, \n\nJ. S. Liu, F. Liang, W. H. Wong, The multiple-try method and local optimization in metropolis sampling, Journal of the American Statistical Association 95 (449) (2000) 121\u2013134.\n\nL. Martino, \"A Review of Multiple Try MCMC algorithms for Signal Processing\", Digital Signal Processing, Volume 75, Pages: 134-152, 2018. \n\nThe sentence about adaptive MCMC's should be also completed.\n\nMinor comments:\n\n- Why do you say that \"MCMC is non-parametric\" in the introduction? in which sense? MCMC methods are sampling algorithms. Please, clarify.\n\n- In my opinion, Eq. (5)-(6)-(8)-(9)-(11)-(12)-(13) are not proper mathematically written  (maybe the same \"wrong\" way of written that, is repeated  in other parts of the text).\n\n- The results in the Figures in the simulations should be averaged more. Specially, Figure 3.\n\n\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "cdate": 1542234324700, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335854910, "tmdate": 1552335854910, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklgioCFCX", "original": null, "number": 3, "cdate": 1543265176167, "ddate": null, "tcdate": 1543265176167, "tmdate": 1543275001120, "tddate": null, "forum": "Hkg313AcFX", "replyto": "SJx-1SnF3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "\nThank you for providing such detailed feedback!\n\n> However, the proposed algorithms leveraging these findings are problematic...\n\nWe use random walk proposal only to obtain the samples from the target distribution during the training. Thus, we don\u2019t learn the Markov chain proposal, but use it to estimate the loss for the independent proposal. Test performance is reported for samples obtained from the independent proposal. Using the random walk proposal can be considered as heuristic that allows to obtain better samples from the posterior during the training.\n\n> In any case, the correctness of the algorithm is neither shown for an independent proposal nor a Markov Chain proposal...\n\nFirst of all, we\u2019ve provided new experiments (see Section 4.1) that show how the proposed algorithm performs for synthetic distributions. We also demonstrate learned proposals and samples from them (see Appendix C.6) for distributions with distant modes. Our results show that proposals learned by Algorithm 1 cover all the modes of the target distribution while variational inference fails to cover them.\n\nThe problem you\u2019ve described can be partially addressed by the choice of the initial variance of the proposal. Making the proposal wide enough allows us to cover all the target distribution and obtain diverse set of samples (most likely, with low values of densities). However, we can reuse these samples on the following iterations, thus preventing our proposal from collapsing to the single mode. Moreover, in Section 4.2 we do not aim to accurately cover full posterior with the single Gaussian (which seems to be infeasible). Our intuition was that in such situation KL(p||q)-term allows us to spread the proposal on several modes, thus describing the posterior better than variational inference does.\n\n> Therefore, I would expect a metric measuring the quality of the final sample...\n\nWe provide such artificial setting in the new experiments (Section 4.1). We simulate the situation of distant modes by taking the mixture of six distant Gaussians and show that using the RealNVP model [1] as the independent proposal allows to efficiently sample from this distribution. Using the RealNVP model as the proposal allows to deal with high dimensional distributions - experiments in [1] show that RealNVP can efficiently sample images from ImageNet that has 64x64 resolution. Our approach allows to efficiently train such models in case when the unnormalized density of the target distribution is given.\n\n> Algorithm 2 does not require to sample from some target distribution but can leverage the observed sample...\n\nSimilar to GANs, the main purpose of Algorithm 2 is to learn sampler. Besides the connection of GANs and optimization of the acceptance rate of the MH algorithm, Algorithm 2 proposes the extensions to the GANs framework. The minor extension is the Markov chain proposal and formulation of the loss function for it. The major extension to the GANs framework is the usage of the discriminator while sampling to approximately perform the MH correction. In Fig. 4 we demonstrate that such approximate correction allows to obtain samples of better quality.\n\n1. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.  Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "BklQxnRYCX", "original": null, "number": 4, "cdate": 1543265259085, "ddate": null, "tcdate": 1543265259085, "tmdate": 1543266027883, "tddate": null, "forum": "Hkg313AcFX", "replyto": "HJggW5cFhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "\nThank you for your valuable feedback!\n\n> My main concern is that I do not understand why do not directly apply the KL divergence with respect to p(x) and q(x) instead of considering p(x)\\times q(x') and p(x')\\times q(x).\n\nAs you pointed out, this objective is motivated by maximization of the acceptance rate, since our final goal is to obtain efficient proposal for the Metropolis-Hastings algorithm. \nWe added new experiments in the paper (Section 4.1) where we compared the performance of proposals learned using different objectives. We additionally performed the comparison of proposed objective with KL(p||q) on synthetic data in terms of Effective Sample Size:\nRing:                                       820 for KL(p||q), 850 for KL(q||p) + KL(p||q)\nMixture of 2 gaussians:      466 for KL(p||q), 604 for KL(q||p) + KL(p||q)\nMixture of 6 gaussians:      336 for KL(p||q), 367 for KL(q||p) + KL(p||q)\nMixture of 5 distinct rings: 291 for KL(p||q), 255 for KL(q||p) + KL(p||q)\nSince the sampling from proposal distribution is much easier than sampling from the target distribution additional optimization of KL(q||p) does not seem to be very expensive. Moreover, the Monte Carlo estimation of KL(q||p) has much less variance during the first iterations of the algorithm and allows to improve the initial proposal faster.\n\n> The state-of-the-art discussion for MCMC schemes in the introduction must be completed... \n\nThanks for these papers. We have cited them in the new version of our paper.\n\n> Why do you say that \"MCMC is non-parametric\" in the introduction? in which sense?\n\nYou are right, this formulation is confusing. We have reformulated this sentence in the new version.\n \n> In my opinion, Eq. (5)-(6)-(8)-(9)-(11)-(12)-(13) are not proper mathematically written.\n\nWe have improved the notation in the new version.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "BJeZxjRFAQ", "original": null, "number": 2, "cdate": 1543265000640, "ddate": null, "tcdate": 1543265000640, "tmdate": 1543265000640, "tddate": null, "forum": "Hkg313AcFX", "replyto": "BkgWy94p2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "Thanks for the review!", "comment": "\nThank you for pointing out this flaw.\nTo make things more clear, we added new experiments (see Section 4.1), where we demonstrate how to adapt the expressive family of  RealNVP [1] to learn independent proposals. In these experiments we demonstrate comparable performance with A-NICE-MC [2] model and outperform the proposal learned by variational inference.\nUnfortunately, we have not found the exact parameters of synthetic distributions of L2HMC [3] to compare against it. However, the reduction of autocorrelation is the inherent property of both models. L2HMC reduces autocorrelation by the maximization of the expected square jump distance, while our approach reduces autocorrelation by maximization of the acceptance rate of an independent proposal. Moreover, the combination of two losses (the acceptance rate and expected square jump distance) seems to be the great point of interest for the future research, since the maximization of jump distance won\u2019t allow Markov chain proposal to collapse to the delta-function.\n\n1. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.  Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.\n2. Jiaming Song, Shengjia Zhao, and Stefano Ermon.  A-nice-mc:  Adversarial training for mcmc.  In Advances in Neural Information Processing Systems, pp. 5140\u20135150, 2017.\n3. Levy, Daniel, Matthew D Hoffman, and Jascha Sohl-Dickstein. 2017. \u201cGeneralizing Hamiltonian Monte Carlo with Neural Networks.\u201d ArXiv Preprint ArXiv:1711.09268.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "rkgCV90K0X", "original": null, "number": 1, "cdate": 1543264822444, "ddate": null, "tcdate": 1543264822444, "tmdate": 1543264844057, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "content": {"title": "We have added the requested evaluation", "comment": "\nAs reviewers requested, we\u2019ve added additional experiments (Section 4.1). \nThe new experiments demonstrate performance of our approach on synthetic data for independent proposals. For independent proposal we\u2019ve chosen RealNVP[1] model and have shown that it can be easily adapted for maximization of the acceptance rate. We demonstrate that for all target distributions obtained samplers outperform the proposal with the same architecture but learned via variational inference. We also demonstrate that obtained samplers obtain the comparable performance in terms of Effective Sample Size (ESS) and ESS per second.\n\nMoreover, in Appendix C.5 we provide additional empirical evidence that maximization of the proposed lower bound results in the maximization of the acceptance rate. For that purpose we evaluate the acceptance rate lower bound (ARLB) and the acceptance rate (AR) at each iteration during the optimization of (ARLB). Then we estimate how the ARLB correlates with the logarithm of AR during training. Correlation coefficients for different distributions are: \n1. 0.914 for ring distribution, \n2. 0.905 for mixture of two gaussians, \n3. 0.956 for mixture of six gaussians, \n4. 0.982 for mixture of 5 distinct rings.\n\n1. Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.  Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610091, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hkg313AcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1019/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1019/Authors|ICLR.cc/2019/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers", "ICLR.cc/2019/Conference/Paper1019/Authors", "ICLR.cc/2019/Conference/Paper1019/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610091}}}, {"id": "BkgWy94p2m", "original": null, "number": 3, "cdate": 1541388761402, "ddate": null, "tcdate": 1541388761402, "tmdate": 1541533491363, "tddate": null, "forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "content": {"title": "Empirical evaluation lacking to test the validity of the proposed objective", "review": "The paper proposes to learn transition kernels for MCMC by optimizing the acceptance rate (or its lower bound) of Metropolis-Hastings.\n\nMy main reason for worry is the use of independent proposals for this particular learning objective. While I can buy the argument in the Appendix on how this avoids the collapse problem of Markov proposals, I think the rate of convergence of the chain would greatly reduce in practice because of this assumption. \n\nUnfortunately, the empirical evaluation in this work is lacking to formally confirm or reject my hypothesis. In particular, it is absolutely crucial to compare the performance of this method with Song et al., 2017 (which does not make this assumption) using standard metrics such as Effective Sample Size. Another recent work [1] optimizes for the expected square jump distance and should also have been compared against.\n\n[1]: Levy, Daniel, Matthew D Hoffman, and Jascha Sohl-Dickstein. 2017. \u201cGeneralizing Hamiltonian Monte Carlo with Neural Networks.\u201d\u00a0ArXiv Preprint ArXiv:1711.09268.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1019/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Metropolis-Hastings view on variational inference and adversarial training", "abstract": "In this paper we propose to view the acceptance rate of the Metropolis-Hastings algorithm as a universal objective for learning to sample from target distribution -- given either as a set of samples or in the form of unnormalized density. This point of view unifies the goals of such approaches as Markov Chain Monte Carlo (MCMC), Generative Adversarial Networks (GANs), variational inference. To reveal the connection we derive the lower bound on the acceptance rate and treat it as the objective for learning explicit and implicit samplers. The form of the lower bound allows for doubly stochastic gradient optimization in case the target distribution factorizes (i.e. over data points). We empirically validate our approach on Bayesian inference for neural networks and generative models for images.", "keywords": ["MCMC", "GANs", "Variational Inference"], "authorids": ["k.necludov@gmail.com", "vetrodim@gmail.com"], "authors": ["Kirill Neklyudov", "Dmitry Vetrov"], "TL;DR": "Learning to sample via lower bounding the acceptance rate of the Metropolis-Hastings algorithm", "pdf": "/pdf/a8d3b96a9cd3fe5265667d660bdece339323d74d.pdf", "paperhash": "neklyudov|metropolishastings_view_on_variational_inference_and_adversarial_training", "_bibtex": "@misc{\nneklyudov2019metropolishastings,\ntitle={Metropolis-Hastings view on variational inference and adversarial training},\nauthor={Kirill Neklyudov and Dmitry Vetrov},\nyear={2019},\nurl={https://openreview.net/forum?id=Hkg313AcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1019/Official_Review", "cdate": 1542234324700, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hkg313AcFX", "replyto": "Hkg313AcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1019/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335854910, "tmdate": 1552335854910, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1019/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}