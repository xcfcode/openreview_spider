{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458842754345, "tcdate": 1458842754345, "id": "NLokZ1m68u0VOPA8ixVA", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "p8jOo5YAKcnQVOGWfpk7", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "Response 2/2", "comment": "Reviewer: For example, the experiment on the bottom right seems misleading. The first evaluation by CMA-ES reports a lower error than other approaches are able to ever attain, or attain just prior to convergence. However, this evaluation was done completely at random, so it is not indicative of the performance of this method in general, just of the initialization strategy.\n\nAuthors: We empathized the role of priors in our original text: \"This might be because of a bias towards the middle of the range\". \nThe results of TPE with gaussian priors suggest that priors are useful (please see the update version of the paper) . However, Figure 2 in Supplementary Material clearly shows that the best solutions most of the time do not lie in the middle of the search range (see, e.g., $x_3, x_6, x_9, x_{12}, x_{13}, x_{18}$). Please note that the results shown in Figure 1 top are from 8 different problems of a MNIST-based class. Figure 5 shows some results on CIFAR 10. \n  \nReviewer: In addition, while we expect CMA-ES to perform well for a large number of observations, GP-based approaches cannot scale to this regime. As such, this approach must be compared to an appropriate baseline (for example, Snoek 2015).\n\nAuthors: The updated version of the paper also involves two variants of TPE and SMAC. We agree that the work of Snoek 2015 (please note that we mention it in our paper) should also be  considered. However, our workshop paper does not attempt to make a final conclusion but rather introduces a new tool to the field."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458842730422, "tcdate": 1458842730422, "id": "gZWJBXDkPiAPowrRUAKL", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "p8jOo5YAKcnQVOGWfpk7", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "Response 1/2", "comment": "Thanks for your comments and suggestions! \nIn the remaining responses, we'll refer to an updated version of the paper available at \nhttps://sites.google.com/site/cmaesfordnn/iclr2016___hyperparameters.pdf?attredirects=0&d=1 (anonymously for the visitors) \n\nReviewer: One aspect that is not clear to me is the tradeoff between \"perfect parallelism\" and observation efficiency. That is, random search also features perfect parallelism, but past observations don't meaningfully inform future evaluations. \n\nAuthors: As stated in the paper, at each iteration, CMA-ES generates lambda solutions to be evaluated. They can be evaluated in parallel or sequentially, whether the former or the latter is used, the behavior of the algorithm is the same. In fact, the most common scenario is sequential evaluation. Parallel evaluations is a way to speed up the wall clock time by a factor of lambda by running on e.g. lambda cpu/gpu without any loss in performance, i.e., performance versus number of function evaluations. The default lambda in CMA-ES is in order of 10-20. Much greater lambda may lead to a loss in performance on uni-modal problems but was shown to be useful on multimodal and noisy problems. Please check http://loshchilov.com/src/allBBOB_20D.png to see how CMA-ES (top curves) works compared to random search (bottom green curve) on 20-dimensional BBOB problems of different properties.\n\nReviewer: CMA-ES is claimed to perform well for larger function budgets, but this seems to be in contrast to the usual (and necessary) assumption of expensive function evaluations. The experiments presented report results for evaluation times of 5-30 minutes, but this is one to two orders of magnitude less than realistic neural network training times.\n\nAuthors: Please have a look at our updated version of the paper anonymously available at https://sites.google.com/site/cmaesfordnn . We include some preliminary experiments on CIFAR10 (without data augmentation) where CMA-ES can find validation errors below 10% for 2 hours long training.  It is important to note that if the search range is broad enough to enable global search, then best solutions found after very few evaluations are not good enough (i.e., manually selected baselines might be better) compared to what can be found later. This is why practitioners often narrow the search range a lot to get some good results quickly (unfortunately, the search ranges are hard to guess and the best solutions are often do not lie in the middle, please see details in our updated version of the paper) . However, this also leads to the fact that random search works quite well in these settings (Bergstra & Bengio, 2012).  We envision that massively parallel evaluations is the only way to optimize (rather than fine-tune) hyperparameters of realistic neural networks, in this case the wall clock time controlled by the number of iterations might become tractable. We benchmark only the original CMA-ES while there are several extensions to noisy (Hansen et al., 2009) and expensive optimization (Loshchilov et al., 2012).    \n\nReviewer: Apart from a more comprehensive experiment coverage, all existing experiments require multiple evaluations and corresponding error bars. \n\nAuthors: \nWe agree that it is useful to run algorithms several times to get error bars. We\u2019ll do that in the future. For now, we used our small computational budget to also study multiple problems, running CMA-ES on multiple different problems (see Figure 1 top) to quantify its variation across problems as well. One can see that the results for Adam and Adadelta are quite similar when the same budgets are used. This is also the case for the runs with different time budgets.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458842164513, "tcdate": 1458842164513, "id": "ZYE6lW1Aki5Pk8ELfENW", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "MwVMZzRBDSqxwkg1t71M", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "Dear Reviewer", "comment": "Dear Reviewer, just in case you missed our reply due to the lack of notifications in OpenReview, we addressed your questions and comments here: http://beta.openreview.net/forum?id=xnrA4qzmPu1m7RyVi38Z Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458842074879, "tcdate": 1458842074879, "id": "vl62XZ46AH7OYLG5in8k", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "MwVMBoZJzfqxwkg1t71j", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "Dear Reviewer", "comment": "Dear Reviewer, just in case you missed our reply due to the lack of notifications in OpenReview, we addressed your questions and comments here: http://beta.openreview.net/forum?id=xnrA4qzmPu1m7RyVi38Z Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458251695755, "tcdate": 1458251695755, "id": "p8jOo5YAKcnQVOGWfpk7", "invitation": "ICLR.cc/2016/workshop/-/paper/126/review/12", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["ICLR.cc/2016/workshop/paper/126/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/126/reviewer/12"], "content": {"title": "An very interesting idea worth exploring further. However, additional experiments are required to evaluate its empirical performance.", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes using CMA-ES for hyperparameter optimization. An advantage of employing this model is its clear parallelism strategy, which is difficult to achieve in existing approaches. This is an interesting direction of research, as it introduces a new alternative to popular hyperparameter tuning techniques. I am not familiar with CMA-ES, so cannot comment on the novelty of this idea. However, additional experiments are required to validate its empirical success \u2014 especially once this contribution evolves into a conference track submission.\n\nOne aspect that is not clear to me is the tradeoff between \"perfect parallelism\" and observation efficiency. That is, random search also features perfect parallelism, but past observations don't meaningfully inform future evaluations. \n\nCMA-ES is claimed to perform well for larger function budgets, but this seems to be in contrast to the usual (and necessary) assumption of expensive function evaluations. The experiments presented report results for evaluation times of 5-30 minutes, but this is one to two orders of magnitude less than realistic neural network training times.\n\nApart from a more comprehensive experiment coverage, all existing experiments require multiple evaluations and corresponding error bars. For example, the experiment on the bottom right seems misleading. The first evaluation by CMA-ES reports a lower error than other approaches are able to ever attain, or attain just prior to convergence. However, this evaluation was done completely at random, so it is not indicative of the performance of this method in general, just of the initialization strategy.\n\nIn addition, while we expect CMA-ES to perform well for a large number of observations, GP-based approaches cannot scale to this regime. As such, this approach must be compared to an appropriate baseline (for example, Snoek 2015).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580010032, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580010032, "id": "ICLR.cc/2016/workshop/-/paper/126/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/126/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458164271298, "tcdate": 1458164271298, "id": "6XAk3KEykUrVp0EvsEBg", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "This is a reply to the reviews by the authors. Part 3/3.", "comment": "Reviewer 2: \nThe suggestion of using priors over the search space within Bayesian optimization seems very sensible.  Note that, Scalable Bayesian Optimization using Deep Networks does exactly this (using a prior mean function as a quadratic bowl centered in the middle of the space).  That is in a way analogous to the setup for CMA-ES here (starting with a Gaussian spray of points centered in the middle of the space).\n\nThe initialization seems like a major possible source of bias.  One might worry that the bounds are setup with the optimum near the center, which would favor the approach that starts with random points at the center.  It would be useful to experimentally validate this by starting the Bayesian optimization approaches at the center as well.\n\nAuthors: \nWe agree that the role of priors is important (in fact, we emphasized that in our original text \"This might be because of a bias towards the middle of the range\"). We have also added results for TPE, with the same priors, and the priors certainly help. However, Figure 3 in the supplementary material clearly shows that the best solutions most of the time do not lie in the middle of the search range (see, e.g., $x_3, x_6, x_9, x_{12}, x_{13}, x_{18}$). \n\n\nWe thank the reviewers for considering this very different approach for hyperparameter optimization. While we clearly do not believe it to be the answer to all problems, its strengths appear to nicely complement those of existing methods."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458164110273, "tcdate": 1458164110273, "id": "yovRVMVJMur682gwszPD", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "This is a reply to the reviews by the authors. Part 2/3.", "comment": "Reviewer 1: It would also be really helpful to see what the best hyperparameters are, particularly if they are near the center of the search space.\n\nAuthors: Thank you for the suggestion. For many dimensions, they are not near the middle; please see the new Figure 3 in the supplementary material. \n\n\nReviewer 2\n\nReviewer 2: \nAlso, the reported results on the CIFAR-10 validation set seem too good to be true, which makes one worry about the experimental setup. \nValidation errors below 0.3\\% sounds extremely low for CIFAR-10.  Typical values currently reported (i.e. state-of-the-art) are around 6\\% to 8\\% depending on the type of data augmentation performed.\n\nAuthors: \nPlease note that we never talk about CIFAR-10 in this paper and all given results are for MNIST where 0.3\\% is close to state-of-the-art. However, in the meantime, we performed some experiments on CIFAR-10. They are preliminary and are not discussed in the main paper. Please find them in supplementary material Figure 5. \n\nReviewer 2: In the introduction, I don't think 'perfect parallelization' seems like a fair statement at all.  Random search and grid search offer 'perfect parallelization' but that doesn't not imply that these are better approaches.  I highly doubt that CMA-ES uses the parallel experiments more efficiently than other approaches.  In fact, one might view (as I do) that the need to distribute a random sample of points in CMA-ES is a major disadvantage.  It *has* to parallelize, which seems terribly inefficient.\n\nAuthors: We use the term \u201cperfect parallelization\u201d to mean N-fold speedups given N compute units. Another related term is linear speedups, but a speedup of 0.5N is also linear, that\u2019s why we avoided that term. \n\nReviewer 2: The authors run a single optimization on just one problem and the experimental setup may have some issues.\n\nAuthors: In fact, as described in the paper, we ran 8 different hyperparameter optimization problems. But the reviewer is correct that all of them were based on MNIST, so the supplementary material (Figure 5) now also shows some preliminary results on CIFAR-10. \n\nReviewer 2: Wow, the bounds for selection pressure seem very broad...  Does this hyperparameter really vary the objective in an interesting way over a range of 100 orders of magnitude?  One might imagine that this could really confound model-based optimization approaches, unless the objective varies smoothly across this space.\n\nAuthors: This really just comes down to how you parameterize the system. In our parameterization, the best values for selection pressure were in the order of 10^8. This motivated our mapping to be 10^{\u22122+10^{2x1}} -> [10^{\u22122}, 10^{98}]. One can see that 0.5 in the original space maps to 10^8. Please see Figure 3 in the supplementary material where the final solutions are illustrated. The suggested best results for x_1 and x_2 (selection pressure values in the beginning and in the end, respectively) found by TPE and CMA-ES cover pretty much the whole range. \n\n\nReviewer 2: The CMA curve never seems to sample close to the optimum (i.e. the best values are always extreme outliers).  That seems strange.  Has it just not converged to the optimum?\n\nAuthors: For any optimization algorithm, the best result is always an outlier and especially in a noisy scenario like the one considered here. This is not any different for Spearmint, TPE and SMAC. \nIt was precisely this noisy property of the function that motivated us to plot all evaluated points and not only the best result. It can be seen that the best result at, e.g., evaluation 500 is a quite common result by the time of evaluation 1000, and this is very natural for noisy optimization. Still, running it longer would probably find better results. To investigate this, we used greater budgets for our experiments on CIFAR-10 that we now added to the paper (Figure 5, supplementary material). Interestingly, the best solutions generated by TPE are even more serious outliers compared to the ones of CMA-ES. \n\nReviewer 2: In particular, the comparison is likely conflated by discontinuities in the optimization surface. It seems reasonable to compare to approaches that take this into account and for which implementations are provided in the same package as the authors ran (e.g. PESC). \nOne concern is discontinuities in the objective function, which could be caused by having the neural net being trained diverge.  Looking at the hyperparameter bounds, it seems reasonable to expect this to happen (e.g. high momentum and high learning rate).  Various papers (Gelbart et al., Gardner et al., PESC, Snoek, Gramacy) developed constraints to deal with this issue.  Did the model diverge during training and if so, did you consider using the constrained alternatives? \n\nAuthors: \nWe found that divergence of the network is very rare, so no special measures were taken. The detailed distribution of evaluation qualities is given in Figure 4 of the supplementary material.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458163949599, "tcdate": 1458163949599, "id": "4Qyg1WpYnFBYD9yOFqjP", "invitation": "ICLR.cc/2016/workshop/-/paper/126/comment", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"title": "This is a reply to the reviews by the authors. Part 1/3.", "comment": "This is a reply to the reviews by the authors.\nThanks for the reviews! We first point out 2 misunderstandings and then reply to the reviewers\u2019 questions. \n\nReviewer 2 got confused, saying that the reported results of 0.3% on the CIFAR-10 validation set seem too good to be true (state of the art is 6% to 8%), which makes one worry about the experimental setup. \nThat would be true, except that we never mentioned CIFAR-10 in the paper; everything is on MNIST, for which 0.3% is exactly the performance we should be getting.\nReviewer 2 said CMA-ES \u201chas to parallelize\u201d, which is not true: of course, one can do function evaluations sequentially. Indeed, the bottom left figure is for the sequential setting.\n\nIn the remaining responses, we\u2019ll refer to an updated version of the paper available at \nhttps://sites.google.com/site/cmaesfordnn/iclr2016___hyperparameters.pdf?attredirects=0&d=1 (anonymously for the visitors). \nThe webpage with this pdf is https://sites.google.com/site/cmaesfordnn/ \n\nResponse to Reviewer 1\n\n\nReviewer 1: As far as I know CMA-ES searches locally, which could be a cause for concern if the function is multi-modal. I think that running CMA-ES with a few different initial distributions would be helpful to show whether it is robust to this effect.\n\nAuthors: CMA-ES is designed to find local optima but often works well on multi-modal problems when i) large populations are used, and/or ii) restarts with each time a larger population size are employed. The original description of the effects of large population sizes is given in \"Evaluating the CMA evolution strategy on multimodal test functions\" by Nikolaus Hansen and Stefan Kern, PPSN 2004. Please consider the Black-box Optimization Benchmarking (BBOB) Workshop available at http://coco.gforge.inria.fr/doku.php?id=start. More specifically,  http://coco.lri.fr/downloads/download15.02/bbobdocfunctions.pdf where BBOB benchmark problems are described and illustrated. Some of them are highly multi-modal, e.g., on page 78, 86, 94, 98, etc. These functions are difficult for all algorithms but CMA-ES in the so-called BIPOP and IPOP extensions showed the best results on these problems.\nIn our settings, the budgets of function evaluations are too small to restart the algorithm, however, our larger than default population size 30 should help to deal with noisy and/or multi-modal problems. \nWe agree that it is useful to run algorithms several times to get error bars. We\u2019ll do that in the future. For now, we used our small computational budget to also study multiple problems, running CMA-ES on multiple different problems (see Figure 1 top) to quantify its variation across problems as well. One can see that the results for Adam and Adadelta are quite similar when the same budgets are used. This is also the case for the runs with different time budgets.\n\nReviewer 1: The paper is well written overall, but there is very little information on the CMA-ES algorithm used in the experiments. I recommend adding an algorithm box outlining the CMA-ES approach used in the paper. There are also some non-standard hyperparameters, such as the batch selection, that should be briefly explained.\n\nAuthors: This comment is very fair. While the 3-page format didn\u2019t allow for a detailed description of these approaches, we now more explicitly point to the literature where they are described in detail. \n\nReviewer 1: The idea of applying CMA-ES for hyperparameter optimization is not necessarily novel (CMA-ES was used to tune speech recognition models in [1], for example), but the idea is simple enough and potentially practical enough that it is worth investigating for deep learning. A reference to [1] should be added.\n\nAuthors: Thank you for the reference, we were not aware of it and added it.\n\nReviewer 1: I'm not sure if the claim that there is no way to truly parallelize SMBO is true. For example, there is the q-EI acquisition function in [2].\n\nAuthors: Our original intention with the phrase \"perfect parallelization appears unattainable since the decisions in each step depend on all data points gathered so far\" was to say that a certain loss of performance of parallel versions of sequential model-based optimization w.r.t. their original versions should be expected. We clarified the sentence as \"appears difficult to achieve\". The work on q-EI is great, but it does not achieve N-fold speedups given N cores, so it\u2019s not perfect parallelization.\n\nReviewer 1: From the experiments, there is a table of transformations that were applied to the hyperparameters. Were these transformations used for all of the methods? Hopefully yes since that would otherwise have a drastic effect on the results.\n\nAuthors: Yes, of course. All algorithms were provided with the same information and all search in [0,1]^19. (We agree that anything else would lead to completely misleading results.)\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455824236959, "ddate": null, "super": null, "final": null, "tcdate": 1455824236959, "id": "ICLR.cc/2016/workshop/-/paper/126/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/126/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457916673782, "tcdate": 1457916673782, "id": "MwVMBoZJzfqxwkg1t71j", "invitation": "ICLR.cc/2016/workshop/-/paper/126/review/11", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["ICLR.cc/2016/workshop/paper/126/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/126/reviewer/11"], "content": {"title": "Potentially very useful algorithm proposed for hyperparameter tuning, although it has been proposed before. Promising results but requires more thorough experiments.", "rating": "6: Marginally above acceptance threshold", "review": "Summary:\nThis paper investigates the use of the CMA-ES algorithm for embarrassingly parallel hyperparameter optimization of continuous (or integer) hyperparameters in deep learning models, specifically convolutional networks. The experiments show that this method can potentially outperform GP-based hyperparameter optimization, although more experiments are needed to draw any solid conclusions. As one example, I think it would be worth investigating how well random search does on this problem as a baseline. For the smaller 5-minute problem at least, the methods should be run multiple times to get error bars.\n\nAs far as I know CMA-ES searches locally, which could be a cause for concern if the function is multi-modal. I think that running CMA-ES with a few different initial distributions would be helpful to show whether it is robust to this effect.\n\nNovelty:\nThe idea of applying CMA-ES for hyperparameter optimization is not necessarily novel (CMA-ES was used to tune speech recognition models in [1], for example), but the idea is simple enough and potentially practical enough that it is worth investigating for deep learning. A reference to [1] should be added.\n\nClarity:\nThe paper is well written overall, but there is very little information on the CMA-ES algorithm used in the experiments. I recommend adding an algorithm box outlining the CMA-ES approach used in the paper. There are also some non-standard hyperparameters, such as the batch selection, that should be briefly explained.\n\nI\u2019m not sure if the claim that there is no way to truly parallelize SMBO is true. For example, there is the q-EI acquisition function in [2].\n\nFrom the experiments, there is a table of transformations that were applied to the hyperparameters. Were these transformations used for all of the methods? Hopefully yes since that would otherwise have a drastic effect on the results.\n\nIt would also be really helpful to see what the best hyperparameters are, particularly if they are near the center of the search space.\n\nSignificance:\nThe nice thing about this paper is that it could result in a very simple and practical methodology. At the moment there are still several open questions, but if the conclusions hold up to more intense scrutiny then it could be very significant.\n\nQuality:\nThis paper is a straightforward application of a simple algorithm to a difficult problem and is of sufficient quality for a workshop paper.\n\n\nPros:\n- Simple application of a well known algorithm to a practical problem\n- Results show a lot of promise and merit further investigation\n\nCons:\n- Needs more experiments before any conclusions can be drawn\n- The paper is light on details of the design choices in the experiments\n- CMA-ES should be more thoroughly described\n\nReferences:\n[1] Watanabe, S. and Le Roux, J. Black box optimization for automatic speech recognition. MITSUBISHI ELECTRIC RESEARCH LABORATORIES TR2014-021, May 2014\n\n[2] M. Schonlau. Computer Experiments and global optimization. PhD Thesis. University of Waterloo, 1997\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580010235, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580010235, "id": "ICLR.cc/2016/workshop/-/paper/126/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/126/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457880707142, "tcdate": 1457880707142, "id": "MwVMZzRBDSqxwkg1t71M", "invitation": "ICLR.cc/2016/workshop/-/paper/126/review/10", "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["ICLR.cc/2016/workshop/paper/126/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/126/reviewer/10"], "content": {"title": "Comparing CMA-ES to Bayesian Optimization is a really great idea, but this needs more careful empirical work to be valuable to the community.", "rating": "4: Ok but not good enough - rejection", "review": "This paper explores the use of an algorithm from the evolutionary optimization literature as an alternative approach to Bayesian optimization for hyperparameters.  In particular, the authors propose the use of CMA-ES for the parallelized hyperparameter optimization of a deep neural network.  On one problem, they demonstrate that CMA-ES appears to reach better validation performance than a popular Bayesian optimization method.\n\nThis is a well written paper that is easy to follow and offers an interesting datapoint for Bayesian optimization and hyperparameter optimization researchers.  One concern, however, is that the empirical evaluation is too light.  The authors run a single optimization on just one problem and the experimental setup may have some issues.  In particular, the comparison is likely conflated by discontinuities in the optimization surface.  It seems reasonable to compare to approaches that take this into account and for which implementations are provided in the same package as the authors ran (e.g. PESC).  Also, the reported results on the CIFAR-10 validation set seem too good to be true, which makes one worry about the experimental setup.\n\nIn Figure 1, it looks like the GP-based approaches (EI and PES) experience major model fitting issues.  This would be suggested by the observation that they don't seem to improve at all after the first few function evaluations.  One concern is discontinuities in the objective function, which could be caused by having the neural net being trained diverge.  Looking at the hyperparameter bounds, it seems reasonable to expect this to happen (e.g. high momentum and high learning rate).  Various papers (Gelbart et al., Gardner et al., PESC, Snoek, Gramacy) developed constraints to deal with this issue.  Did the model diverge during training and if so, did you consider using the constrained alternatives?\n\nThe CMA curve never seems to sample close to the optimum (i.e. the best values are always extreme outliers).  That seems strange.  Has it just not converged to the optimum?\n\nValidation errors below 0.3% sounds extremely low for CIFAR-10.  Typical values currently reported (i.e. state-of-the-art) are around 6% to 8% depending on the type of data augmentation performed.\n\nThe suggestion of using priors over the search space within Bayesian optimization seems very sensible.  Note that, Scalable Bayesian Optimization using Deep Networks does exactly this (using a prior mean function as a quadratic bowl centered in the middle of the space).  That is in a way analagous to the setup for CMA-ES here (starting with a Gaussian spray of points centered in the middle of the space).\n\nThe initialization seems like a major possible source of bias.  One might worry that the bounds are setup with the optimum near the center, which would favor the approach that starts with random points at the center.  It would be useful to experimentally validate this by starting the Bayesian optimization approaches at the center as well.\n\nWow, the bounds for selection pressure seem very broad...  Does this hyperparameter really vary the objective in an interesting way over a range of 100 orders of magnitude?  One might imagine that this could really confound model-based optimization approaches, unless the objective varies smoothly accross this space.\n\nIn the introduction, I don't think 'perfect parallelization' seems like a fair statement at all.  Random search and grid search offer 'perfect parallelization' but that doesn't not imply that these are better approaches.  I highly doubt that CMA-ES uses the parallel experiments more efficiently than other approaches.  In fact, one might view (as I do) that the need to distribute a random sample of points in CMA-ES is a major disadvantage.  It *has* to parallelize, which seems terribly inefficient.\n\nOverall, the idea of comparing to CMA-ES seems like a really great idea, since it is the champion algorithm from the evolutionary optimization field.  I think this is a good start, but I am concerned as an empirical study it needs more rigor before it should be accepted.  Perhaps the authors can address the above concerns in their next manuscript.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580010820, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580010820, "id": "ICLR.cc/2016/workshop/-/paper/126/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "xnrA4qzmPu1m7RyVi38Z", "replyto": "xnrA4qzmPu1m7RyVi38Z", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/126/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455824235973, "tcdate": 1455824235973, "id": "xnrA4qzmPu1m7RyVi38Z", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "xnrA4qzmPu1m7RyVi38Z", "signatures": ["~Ilya_Loshchilov1"], "readers": ["everyone"], "writers": ["~Ilya_Loshchilov1"], "content": {"CMT_id": "", "title": "CMA-ES for Hyperparameter Optimization of Deep Neural Networks", "abstract": "Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. \nAs an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known for its state-of-the-art performance in derivative-free optimization. CMA-ES has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy usage example using CMA-ES to tune hyperparameters of a convolutional neural network for the MNIST dataset on 30 GPUs in parallel. ", "pdf": "/pdf/xnrA4qzmPu1m7RyVi38Z.pdf", "paperhash": "loshchilov|cmaes_for_hyperparameter_optimization_of_deep_neural_networks", "conflicts": ["uni-freiburg.de", "inria.fr", "epfl.ch"], "authors": ["Ilya Loshchilov", "Frank Hutter"], "authorids": ["ilya@cs.uni-freiburg.de", "fh@cs.uni-freiburg.de"]}, "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 11}