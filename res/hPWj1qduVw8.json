{"notes": [{"id": "hPWj1qduVw8", "original": "mC_AvzP8_cB", "number": 2558, "cdate": 1601308282980, "ddate": null, "tcdate": 1601308282980, "tmdate": 1614582101285, "tddate": null, "forum": "hPWj1qduVw8", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "d-Z2rILZohQ", "original": null, "number": 1, "cdate": 1610040414124, "ddate": null, "tcdate": 1610040414124, "tmdate": 1610474011961, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper studies the problem of visual question answering in multi-turn dialogues.\nThe proposed method is to identify relevant dialog turns as a path in a semantic graph that connects the dialogue turns. Empirical performance of the proposed method is strong. Reviewers concerns have been compressively addressed. Overall, the paper has novelty, and explores an interesting direction in this line of work."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040414110, "tmdate": 1610474011945, "id": "ICLR.cc/2021/Conference/Paper2558/-/Decision"}}}, {"id": "NUphSR-bqni", "original": null, "number": 1, "cdate": 1603804067870, "ddate": null, "tcdate": 1603804067870, "tmdate": 1606211072694, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review", "content": {"title": "Interesting engineering contribution, but the underlying principle seems not really new and lack of discussion with relevant related works", "review": "Summary:\n\nThis paper addresses the visual question answering in a multi-turn or conversational setting. Given a video (series of frames or images), a model has to reason across space and time to arrive at a correct answer for a given question. This task involves understanding the content and context of dialogue turns, i.e., given a question and N dialogue turns, only M<<N of the dialogue turns are strongly related to the question posed. This paper proposes to simulate the dependencies between dialogue turns, forming a reasoning path, to answer a given question. In a way, the proposed approach selects relevant dialogue turns that are useful to answer the question.  \n\nThere are two steps to make the reasoning path:\n(1) At each dialogue turn, a graph network is constructed at the turn level. Any two turns are connected if they have an overlapping lexical span or if their lexical spans are semantically similar.\n(2) Secondly, a path generator is trained to predict a path from the current dialogue turn to past dialogue turns that provide additional and relevant cues to answer the current question. \n\nUltimately, the main idea to create a reasoning path is based on compositional semantic similarities.\n\n\n\nComments (Technical, Major Flaws of this paper): \n\n(A) I am not sure whether the author(s) is aware, but from the NLP perspective, the current method (step 1) is trying to simulate the discourse structure of dialogues. I believe that this is an important direction, and the uniqueness of this works lies in the multi-modality of the input, i.e., possibility of the interplay between texts and images (using the information in both modalities). \n- The claimed novelty in this paper is in the construction or usage of reasoning graph, i.e., to construct a graph structure to connect turn-level representations in dialogue. However, in Step 1, the use of entity and/or compositional similarity to create a graph structure out of a text is not new at all, and the paper fails to cite related works, as if it is the first one to propose this. In fact, the idea has been used in NLP for a long time (albeit mostly in the monologue). \n- I am not sure whether combining entity with action phrases (called \"lexical spans\" in the paper) is new. Can you confirm whether the proposed \"lexical spans\" is indeed new to construct/simulate the discourse structure?\n- Regarding step 1, perhaps the main contribution of this paper is applying the idea to dialogues, instead of monologues? Another possible contribution is \"filtering out\" unimportant semantic relations. In normal discourse structure, all parts of texts are connected in a single structure. However, in the context of this paper, only edges that are relevant to the posed question are used. \n- Unless the paper can discuss the related NLP works for step 1, I can only treat this paper as the extension of the corresponding NLP method in a multi-modal setting. There is an engineering contribution, but not from the methodological (theoretical) perspective.\n- I think the author(s) will benefit much by surveying papers on discourse structures (or the ``shallow\" construction of them), instead of machine reading comprehension. Many studies tried to establish discourse structure (albeit in a monologue) using entities that are mentioned and their semantic representations. A few of such works are:\n  R. Barzilay and M. Lapatta. 2008. Modelling Local Coherence: An Entity-based Approach. https://www.aclweb.org/anthology/J08-1001.pdf\n  C. Guinaudeau and M. Strube. 2013. Graph-based Local Coherence Modeling. https://www.aclweb.org/anthology/P13-1010/\n  J.W.G. Putra and T. Tokunaga. 2017. http://www.aclweb.org/anthology/W/W17/W17-2410.pdf \n- The currently proposed method step 1 seems to be the combination of entity graph + semantic similarity graph in these related works, but the current paper \"filters\" only edges relevant to the posed question. \n- A related work to construct the discourse structure in dialogues:\n+ G. Morio and K. Fujita. 2018. End-to-end Argument Mining for Discussion Threads Based on Parallel Constrained Pointer Architecture. https://www.aclweb.org/anthology/W18-5202.pdf\n\n(B) The reasoning model, which is a combination of GCN + transformer can be interesting. However, the idea of cross-modality representation refinement is somewhat similar to what has been studied in VQA.\n  Le, T. M., Le, V., Venkatesh, S., & Tran, T. (2020). Dynamic Language Binding in Relational Visual Reasoning. In IJCAI 2020.\n  Gao, P., Jiang, Z., You, H., Lu, P., Hoi, S. C., Wang, X., & Li, H. (2019). Dynamic fusion with intra-and inter-modality attention flow for visual question answering. In CVPR 2019.\n\n(C) After constructing the reasoning path (in response to the given question), the next step is to decode such representation to generate the answer. This paper proposes to use the transformer model to do that. I believe the use of the transformer model to generate text is not new. In fact, the author(s) mentions this in the paper (the last paragraph of Section 3.4). \n\n(D) In overall, if we look at the pipeline (system) level, the proposed pipeline is new (the whole process). However, I seriously concern about the step (1) of the proposed method (page 1). My main concern about this paper is its lack of awareness of related works in text processing (step 1 of their method). In fact, it fails to cite relevant works (that are very similar to this work). I might appreciate this paper in terms of engineering contribution (in a multi-modal setting), but I cannot acknowledge that step 1 is novel. Having that said, I think the authors need to provide a comparison to related works, proving the novelty of the current method. I am willing to increase the rating if the authors can properly address my concerns during the rebuttal phase.\n\n(E) The content from 3.3 to 3.4 is very hard to follow. \nCorrection of terms:\n\t- linguistic dependency parsers --> \"syntactical\" dependency parsers (this is the correct term)\n\t- linguistically, the term \"lexical span\" is weird. A span is a series of continuous lexicons (in the text surface). I suggest using a better term, as the \"lexical span\" in this paper might be discontinuous (do I misunderstand?).", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093640, "tmdate": 1606915776560, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review"}}}, {"id": "9yHVxmtquDC", "original": null, "number": 4, "cdate": 1606204333965, "ddate": null, "tcdate": 1606204333965, "tmdate": 1606204333965, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "NUphSR-bqni", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We want to thank R2 for the valuable and detailed feedback to improve the paper. Please find below our response. We also revised and updated the paper itself. \n\n* Concern #1:\n\u201cin Step 1, the use of entity and/or compositional similarity to create a graph structure out of a text is not new at all, and the paper fails to cite related works\u201d:\n\nThank you for your comments about the relevance of our work to this line of research. We carefully review prior work in this research domain and have improved our paper with citation of these papers (Please refer to Section 2). \n\nThere has been related work in discourse structures in dialogues. However, they focus on very different types of dialogues (e.g. online discussion forums) for argument mining tasks with very different problem settings from our paper. In our work, the types of dialogues are closer to casual open-domain dialogues with less presence of argument components. Moreover, the presence of video as a common grounding information among dialogue turns also necessitates cross-modality interaction. For more details, we reviewed research of discourses in monologues and dialogues and provided comparison to our approach in Section 2. \n\nCompared to prior work in discourse structures, our modification of lexical spans which combine both entities and action phrases, are designed for more comprehensive cross-modality interaction between text and visual features. In a video, its visual features are not limited to just entity-based information but often contain action-based signals as well. Including action phrases supports the cross-modality interaction process to obtain relevant visual information. \n\n* Concern #2:\n\u201cThe idea of cross-modality representation refinement is somewhat similar to what has been studied in VQA.\u201d:\n\nIndeed, cross-modality representation learning is not new and has been studied in VQA problems. Instead, in this paper, we address a different challenge when the problem is positioned in a multi-turn setting. Our approach directly tackles how cross-modality features are passed temporally from turn to turn in a response generation task. Please see Section 2 for our explanation.  \n\n* Concern #3:\n\u201cI believe the use of the transformer model to generate text is not new.\u201d:\n\nIndeed, this method has been used before. However, this is not the central contribution of the paper and we described the technical details for completeness of our method. The main argument of our paper focuses on how a dialogue model learns to extract contextual cues turn by turn. \n\n* Concern #4:\nTechnical term definitions:\n\nThank you for your suggestion. We have corrected some of the technical terms in the current manuscript, e.g. changing \u2018lexical spans\u2019 to \u2018sub-nodes\u2019, and will incorporate all of the feedback in the final version. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hPWj1qduVw8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2558/Authors|ICLR.cc/2021/Conference/Paper2558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846968, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment"}}}, {"id": "gdqT6nmZ-a1", "original": null, "number": 3, "cdate": 1606203968596, "ddate": null, "tcdate": 1606203968596, "tmdate": 1606203968596, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "f-ovP7fU1O7", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment", "content": {"title": "Response to Reviewer 4 ", "comment": "We want to thank R4 for the valuable and detailed feedback. We are glad that R4 found our method beneficial as \u201cthe generated reasoning path can serve as extra explanations for the answer.\u201d Please find below our response:\n\n* Concern #1:\n\u201cThe model is graph-based thus is restricted to scenarios with a small number of turns, and becomes computationally expensive for long-conversation scenarios\u201d:\n\nWe agree that the approach involving graph structures requires additional computational effort. To construct graph structures, we recommend to preprocess dialogue utterances cumulatively, i.e. the semantic graph of the current turn is built on top of the graph from the previous turn. In this way, the computation expense is linear as the dialogue length increases. To have an overview of the graph data, in the revised paper, we provided statistical details and analysis of the constructed semantic graphs (Please refer to Appendix D).\n\n* Concern #2:\n\u201cIt would be more convincing if it gives an analysis of failure cases.\u201d\n\nThank you for your suggestion! We added an analysis of scenarios where our models are susceptible to in the Appendix C. There are two main areas we think the current approach can be improved, including cases with long complex utterances and contextualized semantic similarity. \n\n* Concern #3:\n\u201cThe reasoning path generator uses $C_t$ as input, does it include $A_t$ during inference?\u201d:\n\nNo, it does not use $A_t$ as input during inference. Given a current turn $t$, $C_t$ is limited to questions and answers of past dialogue turns only, i.e. turn $1$ to turn $t-1$. \n\n* Concern #4:\nQuestions regarding section 3.3 and 3.4\n\nIn section 3.3, Eq (4), the initial $Z_0$ is the embedding of the current turn index $t$. We added this description in the current revision. In section 3.4, we added an explanation of model training. \n\n* Concern #5:\n\u201c Does the frequency influence the similarity?\u201d:\n\nCurrently we do not factor in the frequency of identical lexical spans in an utterance. It is a good observation and we will try to incorporate it in our final version. \n\n* Concern #6:\n Math symbol definition and typos:\n\nThank you! We have improved the paper and will thoroughly check again in the final version. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hPWj1qduVw8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2558/Authors|ICLR.cc/2021/Conference/Paper2558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846968, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment"}}}, {"id": "Y85cIKTG7Qc", "original": null, "number": 2, "cdate": 1606203491399, "ddate": null, "tcdate": 1606203491399, "tmdate": 1606203491399, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "ChZf8L9MBS_", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment", "content": {"title": "Response to Reviewer 3 ", "comment": "We want to thank R3 for the valuable and detailed feedback. We are glad that R3 found our method novel with demonstrated benefit of using the selected nodes from graphs. Please find below our response: \n\n* Concern #1:\n\u201cwhether there is any benefit to learning that path is unclear from the ablation study present in Table 5 where there is no noticeable difference in the results between learned v/s fixed paths\u201d:\n\nIn Table 5, we demonstrate that the performance of the learned path outperforms most baseline models exploiting fixed temporal paths. While the performance using \u2018Path through last 7 turns\u2019 might be close (except for CIDEr score), our learned path approach does not require a finetuning process to explore which fixed path configuration is optimal in the current dataset (e.g. last 7 turns in the AVSD benchmark).  Our approach enables a model to learn to extract paths instead. Beyond numerical results, our model also improves system transparency as it can explicitly output a clear reasoning path through past temporal steps in dialogue context (Please refer to Figure 3 and 4). One important application that our approach can be used is to assess the complexity of dialogues when creating new benchmarks involving multi-turn reasoning. \n\n* Concern #2:\n\u201cResults in Table 4 seems within noise error from each other, hence making the arguments \"component lexical overlap is better than global lexical overlap\" weak\u201d:\n\nAbout the experiment results in Table 4, even though the results between graphs built upon component lexical overlap and global lexical overlap are closed in some metrics (e.g. METEOR and ROUGE-L), the former approach is more robust in cases of long dialogues which involve more entities and action phrases. This method also provides a clearer way to extract automatic supervision of reasoning paths by selecting the best paths with the most number of component lexical overlap (Please refer to Section 3.3, Data Augmentation). \n\n* Concern #3:\n\u201cIt is unclear if this approach of creating the graph (by finding pairwise semantic similarity) is scalable to real world datasets containing several turns with potentially large number of tokens\u201d:\n\nAbout the scalability of our approach, we experimented with the AVSD benchmark which contains dialogues designed to be close to real world dialogues. Each dialogue was created by human annotators and contains up to 10 dialogue turns, and each question/answer in each turn has close to 10 tokens in average (See Table 2).  To have an overview of the graph data, in the revised paper, we provided statistical details and analysis of the constructed semantic graphs (Please refer to Appendix D). \n\n* Concern #4:\n\u201cIt is unclear what makes this method specific to video-grounded Q/A. Can this methodology be applied to other problems involving dialogues?\u201d:\n\nOur method can be applied to other problems involving dialogues that often contain multiple dependencies among dialogue turns. We chose to investigate video-grounded dialogue in which each dialogue turn is explicitly grounded in a common source, i.e. video, and hence, dialogue turns are deemed related via this grounding information. In traditional chit-chat or open domain dialogue problems, there might be less number of cases with clear semantic dependencies among turns. An interesting extension of our method to open-domain dialogues is in scenarios in which the dialogues are situated in a multi-modal environment with topical constraints."}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "hPWj1qduVw8", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2558/Authors|ICLR.cc/2021/Conference/Paper2558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846968, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Comment"}}}, {"id": "f-ovP7fU1O7", "original": null, "number": 2, "cdate": 1603837050964, "ddate": null, "tcdate": 1603837050964, "tmdate": 1605024182864, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review", "content": {"title": "Moderate contribution but lacking some details concerning the implementation", "review": "The paper studies the problem of video-grounded multi-turn QA and adopts reasoning paths to exploit dialogue information.\n\nSequential: fail to exploit long turn dependencies\nGraphical: fixed structure, fail to factor temporal dependencies\nThe proposed reasoning path method: balanced between sequential and graphical\n\nIt first constructs a turn-level semantic graph based on overlapping lexical span:\n- Extract lexical spans from each turn (<Q, A> pair) using a (Stanford) parser\n- Two turns are connected if one of their corresponding lexical spans are similar (in terms of word2vec embedding).\n\nThen it trains a path generator to predict paths from each turn to its preceding turns:\n- It starts from the current turn and auto-regressively finds the most dependent preceding turn with Transformers\n- The turn-level semantic graph is used to mask the dependencies.\n- It is trained with supervised loss where the target paths are constructed by running BFS on the semantic graph\n\nFinally, the proposed paths are used to employ multimodal reasoning:\n- Visual features are combined with turn level attention\n- Multi-model turn-level embeddings are propagated using GCN\n- Then use SOTA decoder to generate language response\n\nThe author conducts experiments on a benchmark, and the proposed method achieves better QA performance than SOTA without a pre-trained language model and achieves comparable performance when the pre-trained language model is involved.\n\nThe author further studies different variations of graph structures and show that using graphs constructed based on lexical spans is better than fully connected graphs or graphs based on whole sentence embedding. And it also shows that including bidirectional edges does not necessarily improve the performance.\n\nA nice feature of the method is that the generated reasoning path can serve as extra explanations for the answer.\n\nSome concerns:\n\n- The model is graph-based thus is restricted to scenarios with a small number of turns, and becomes computationally expensive for long-conversation scenarios.\n- Need a more detailed explanation of how the message passing part (section 3.4) is trained.\n- Each pair of turns may share multiple pairs of lexical spans that are identical, e.g. Figure 3-A, \u201cshe\u201d in turn 10, but there are 2 \u201cshe\u201ds in turn 9. Does the frequency influence the similarity?\n- It would be more convincing if it gives an analysis of failure cases.\n- Section 3.3: Eq.(4), what is the initial $D_0$ correspond to $Z_0$?\n- The reasoning path generator uses $C_t$ as input, does it include $A_t$ during inference?\n\nMinor concerns:\n\n- Many symbols are used before their definition:\n    > Explanation of $\\mathcal{V}$ is first given in Algorithm 1 (section 3.2) but first used in Eq.1 (section 3.1).\n    > Section 3.3, 2nd paragraph, 4th line: undefined symbols $\\hat{r}_1,\\dots,\\hat{r}_{m-1}$. They are later mentioned as turn indices in section 3.4, last line of page 5.\n- Page 5, line 2: \u201cincorporate\u201d \u2014> \u201cincorporates\u201d.\n- Index m is used as a word position in Eq.(1) but becomes a decoding step from section 3.3.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093640, "tmdate": 1606915776560, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review"}}}, {"id": "ChZf8L9MBS_", "original": null, "number": 3, "cdate": 1603848632772, "ddate": null, "tcdate": 1603848632772, "tmdate": 1605024182782, "tddate": null, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "invitation": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review", "content": {"title": "Learned reasoning paths are very interesting but ablation studies show limited benefits of proposed components", "review": "This paper proposes creating a semantic graph connecting the multiple turns in a dialogue and subsequently learning reasoning paths in that graph to find the most relevant nodes for answering a given question in a dialogue context.\n\nStrengths:\n* This paper proposes to learn non-linear information flows in a sequential data which is a well-motivated problem.\n* The proposed method is novel. Training a transformer decoder to learn reasoning paths and using BFS supervision to find the ground-truth paths is very interesting (however empirical support that this is effective is lacking, see Weaknesses below).\n* The proposed approach significantly and consistently outperforms existing benchmarks on the AVSD dataset.\n* The illustration in Fig 3 demonstrates the benefit of using the selected nodes from the graph.\n\nWeaknesses:\n* From the results in Table 1, 3, it is clear that multimodal reasoning over \"relevant\" nodes in the semantic graph (termed as reasoning path in the paper) helps the model by reducing the noise. However, whether there is any benefit to learning that path is unclear from the ablation study present in Table 5 where there is no noticeable difference in the results between learned v/s fixed paths. In fact, \"Path through last 7 turns\" performs comparable to the \"Learned Path\" which raises question about the usefulness of transformer decoder based path learner.\n* Similarly results in Table 4 seem within noise error from each other, hence making the arguments \"component lexical overlap is better than global lexical overlap\" weak.\n* It is unclear if this approach of creating the graph (by finding pairwise semantic similarity) is scalable to real world datasets containing several turns with potentially large number of tokens. There is no analysis of information redundancy among the nodes of the graph which can help prune the graph.\n* The results are only presented on one dataset. The transformer decoder learning seems to be tuned to this particular dataset (\"greedy approach works better due to small size of V\"). This questions the generality of the proposed approach.\n\nQuestions / Suggestions\n* It is unclear what (if) makes this method specific to video-grounded Q/A. Can this methodology be applied to other problems involving dialogues? Can this be applied to any problem containing time series data?\n* There needs to be some discussion as to why RLM performs better/comparable (Table 1) when using pretraining.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2558/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2558/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues", "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "keywords": ["video-grounded dialogues", "reasoning paths", "semantic graphs"], "abstract": "Compared to traditional visual question answering, video-grounded dialogues require additional reasoning over dialogue context to answer questions in a multi-turn setting. Previous approaches to video-grounded dialogues mostly use dialogue context as a simple text input without modelling the inherent information flows at the turn level. In this paper, we propose a novel framework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers information flows among dialogue turns through a semantic graph constructed based on lexical components in each question and answer. PDC model then learns to predict reasoning paths over this semantic graph. Our path prediction model predicts a path from the current turn through past dialogue turns that contain additional visual cues to answer the current question. Our reasoning model sequentially processes both visual and textual information through this reasoning path and the propagated features are used to generate the answer. Our experimental results demonstrate the effectiveness of our method and provide additional insights on how models use semantic dependencies in a dialogue context to retrieve visual cues.", "one-sentence_summary": "We introduce PDC, a novel approach to learn reasoning paths over semantic graphs which are built upon dialogue context at each turn, for video-grounded dialogues. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "le|learning_reasoning_paths_over_semantic_graphs_for_videogrounded_dialogues", "pdf": "/pdf/c7f4e978ac75833ccb55c20a8c0c0e1e3f25c2f0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nle2021learning,\ntitle={Learning Reasoning Paths over Semantic Graphs for Video-grounded Dialogues},\nauthor={Hung Le and Nancy F. Chen and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=hPWj1qduVw8}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "hPWj1qduVw8", "replyto": "hPWj1qduVw8", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538093640, "tmdate": 1606915776560, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2558/-/Official_Review"}}}], "count": 8}