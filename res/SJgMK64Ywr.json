{"notes": [{"id": "SJgMK64Ywr", "original": "SkgWVn2PDS", "number": 661, "cdate": 1569439098024, "ddate": null, "tcdate": 1569439098024, "tmdate": 1583912044310, "tddate": null, "forum": "SJgMK64Ywr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "LWKPZfGnOU", "original": null, "number": 6, "cdate": 1576976093123, "ddate": null, "tcdate": 1576976093123, "tmdate": 1576976093123, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "pperiADu3", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment", "content": {"title": "We thank the reviewers and the AC for reviewing", "comment": "We are very pleased that our paper is accepted to ICLR 2020. \n\nHowever, we find the AC's short comment \"The work is not terribly innovative\" provided without any explanation confusing and misleading, as none of the reviewers expressed any concern regarding the novelty of our paper. One reviewer explicitly stated the novelty as the strength of the paper, and we are not aware of any previous neural architecture search works or video understanding works exploring multi-stream connectivity learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgMK64Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper661/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper661/Authors|ICLR.cc/2020/Conference/Paper661/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168146, "tmdate": 1576860553360, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment"}}}, {"id": "pperiADu3", "original": null, "number": 1, "cdate": 1576798702621, "ddate": null, "tcdate": 1576798702621, "tmdate": 1576800933380, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The submission applies architecture search to find effective architectures for video classification. The work is not terribly innovative, but the results are good. All reviewers recommend accepting the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728940, "tmdate": 1576800281449, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper661/-/Decision"}}}, {"id": "Hkln1LQnoB", "original": null, "number": 4, "cdate": 1573823971829, "ddate": null, "tcdate": 1573823971829, "tmdate": 1573823971829, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "rkx9gQk_iH", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment", "content": {"title": "acknowledged", "comment": "Thanks for the reply. Nevertheless, I think the NAS-baseline I proposed will further evidence the contribution but does not take away your contribution. "}, "signatures": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgMK64Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper661/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper661/Authors|ICLR.cc/2020/Conference/Paper661/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168146, "tmdate": 1576860553360, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment"}}}, {"id": "r1xft7kdiH", "original": null, "number": 3, "cdate": 1573544825648, "ddate": null, "tcdate": 1573544825648, "tmdate": 1573544825648, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "S1lGGDPUtB", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment", "content": {"title": "We thank the reviewer for the comments and questions.", "comment": "We thank the reviewer for the comments and questions. Please find our answers to the comments below:\n\n1. \"Most work in video action recognition tend not to use optical flow. Many people believe that if 3D conv or (2+1)D conv can be trained well, there is no point in using optical flow. What is the motivation of using flow in this work? I'm interested to know.\"\n\nOur motivation is that the explicit iterative optimization process in the optical flow extraction could allow the model to more easily capture motion information. This is also motivated by the observations of previous papers (e.g., I3D) suggesting the use of two-stream RGB+OpticalFlow models helps video recognition in practice. Training a pure 3D or (2+1)D conv layers to replicate the same behavior would likely require more layers and therefore more training data, and we wanted to overcome it given a fixed amount of data. \n\n2. \"As shown in Figure 5 of the appendix, the search space is quite large. For each block, it seems that authors search with r=1/2/4/8. However, the best searched network seems to only has r=1, 2 or 4. This is kind of counter-intuitive because longer sequences should in general give better results. Is there an explanation or insight that why r=8 does not show up?\" \n\nThis is because, while r=8 is allowed, it corresponds to a very large temporal stride which is more useful for videos on the longer side, which are not often seen in the dataset we used for the evolution (i.e., Moments-in-Time which has 3 second videos).\n\n3. \"The learning rate for both datasets are very high, one is 3.2, the other is 25.6. This is quite unusual. Although many NAS literature show that large learning rate can help to achieve better performance, but 25.6 is really high. Did the authors do learning rate search as well? And what is the search space for learning rate?\"\n\nWe checked multiple learning rates, starting with a base of 0.8 and increasing by a multiple of the batch size. While these learning rates are larger than for other tasks, this is likely due to a large batch size. We found a larger learning rate to be quite helpful for learning, and did not cause instability or other adverse effects, also thanks to the cosine decay.\n\n4. \"I understand that this paper focuses on learning the connectivity pattern from different inputs, like rgb and flow. Have the authors tried using RGB alone and searching the architecture?\"\n\nYes, we also tried the search while only using two RGB input stems. Our evolved RGB-only multi-stream model obtains 30.3% accuracy on the MiT dataset (with Table 3 setting) which is slightly lower than its RGB+Optical flow counterpart (31.4%). The hand-designed two-stream (late fusion) RGB-only model obtains 28.3%."}, "signatures": ["ICLR.cc/2020/Conference/Paper661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgMK64Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper661/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper661/Authors|ICLR.cc/2020/Conference/Paper661/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168146, "tmdate": 1576860553360, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment"}}}, {"id": "rked7QJ_ir", "original": null, "number": 2, "cdate": 1573544735725, "ddate": null, "tcdate": 1573544735725, "tmdate": 1573544735725, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "r1l1KJOAFH", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment", "content": {"title": "We thank the reviewer for the comments and for this suggestion.", "comment": "\"You mention at the start of section 4.2 that your models have the equivalent number of parameters to ResNet-50. This is good, but you should probably emphasize it more/earlier, since I'd been worrying that you were only winning due to size....\"\n\nWe thank the reviewer for the comments and for this suggestion. We will emphasize more that AssembleNet models use the equivalent number of parameters to previous models, introducing them early in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgMK64Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper661/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper661/Authors|ICLR.cc/2020/Conference/Paper661/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168146, "tmdate": 1576860553360, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment"}}}, {"id": "rkx9gQk_iH", "original": null, "number": 1, "cdate": 1573544690495, "ddate": null, "tcdate": 1573544690495, "tmdate": 1573544690495, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "HkgiqJWDqB", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment", "content": {"title": "We thank the reviewer for the comments, questions and suggestions.", "comment": "We thank the reviewer for the comments, questions and suggestions. The main contribution of this paper is in the introduction of the multi-stream (e.g., four-stream) architecture with evolved connectivity for videos. Previous NAS works focused on discovering modules to be used within a single-stream architecture (due to being designed mostly for images), while our search focuses on newly exploring multi-stream networks for videos and their stream connectivity, which we believe are two complementary directions. In the future we could combine our connectivity search with the previous-NAS-like module-level search suggested by the reviewer, jointly obtaining more sophisticated networks with evolved connectivity between complex modules, which will likely bring performance gains but will require a larger search space. Also note that all our models and hand-designed baselines used in the experiments are composed of the exact same convolutional modules and only their connectivity differ."}, "signatures": ["ICLR.cc/2020/Conference/Paper661/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJgMK64Ywr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper661/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper661/Authors|ICLR.cc/2020/Conference/Paper661/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168146, "tmdate": 1576860553360, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper661/Authors", "ICLR.cc/2020/Conference/Paper661/Reviewers", "ICLR.cc/2020/Conference/Paper661/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Comment"}}}, {"id": "S1lGGDPUtB", "original": null, "number": 1, "cdate": 1571350281832, "ddate": null, "tcdate": 1571350281832, "tmdate": 1572972567837, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission proposes a way to do multi-stream neural architecture search for video classification. I give an initial rating of accept because (1) there are not many work on video architecture search yet (2) the paper is well written (3) experiments are complete and results are strong. I have a few comments as below. \n\n1. Most work in video action recognition tend not to use optical flow. Many people believe that if 3D conv or (2+1)D conv can be trained well, there is no point in using optical flow. What is the motivation of using flow in this work? I'm interested to know. \n\n2. As shown in Figure 5 of the appendix, the search space is quite large. For each block, it seems that authors search with r=1/2/4/8. However, the best searched network seems to only has r=1, 2 or 4. This is kind of counter-intuitive because longer sequences should in general give better results. Is there an explanation or insight that why r=8 does not show up? \n\n3. The learning rate for both datasets are very high, one is 3.2, the other is 25.6. This is quite unusual. Although many NAS literature show that large learning rate can help to achieve better performance, but 25.6 is really high. Did the authors do learning rate search as well? And what is the search space for learning rate? \n\n4. I understand that this paper focus on learning the connectivity pattern from difference inputs, like rgb and flow. Have the authors tried using RGB alone and searching the architecture? \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244968027, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper661/Reviewers"], "noninvitees": [], "tcdate": 1570237748913, "tmdate": 1575244968042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Review"}}}, {"id": "r1l1KJOAFH", "original": null, "number": 2, "cdate": 1571876727140, "ddate": null, "tcdate": 1571876727140, "tmdate": 1572972567799, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper is a neural architecture search paper. In particular, it applies this to finding better neural architectures for video understanding, emphasizing exploring the video temporal resolutions needed and how to combine intermediate representations capturing appearance and motion. It introduces a somewhat new algorithm for connection-strength-weighted architecture evolution focused on this high-level information fusion problem of video understanding.\n\nI am no expert in the problem domain of this paper (video understanding) but I fond the paper very clear and well-written and easy to understand. The techniques and thinking used seemed good (e.g., using dilated convolutions rather than manual preparing videos with different temporal resolutions!). \n\nThe evolutionary search algorithm was not wildly original or a huge breakthough in the general context of previous work, but seems appropriate, well thought out and works well. \n\nThe results reported are very strong. They get state-of-the-art results on two datasets. I particularly appreciated the evident care in producing strong baselines and ablations (their Charades 2-stream baseline also outperforms all previous work; they show the general strength of four-stream architectures and a random architecture with connection strength learning).\n\nYou mention at the start of section 4.2 that your models have the equivalent number of parameters to ResNet-50. This is good, but you should probably emphasize it more/earlier, since I'd been worrying that you were only winning due to size...."}, "signatures": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244968027, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper661/Reviewers"], "noninvitees": [], "tcdate": 1570237748913, "tmdate": 1575244968042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Review"}}}, {"id": "HkgiqJWDqB", "original": null, "number": 3, "cdate": 1572437907004, "ddate": null, "tcdate": 1572437907004, "tmdate": 1572972567751, "tddate": null, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "invitation": "ICLR.cc/2020/Conference/Paper661/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "\nSummary: \n\nThis paper aims to adapt the standard neural architecture search scheme to search a two-input convolutional neural network for video representations. To this end, the paper formulates a direct acyclic graph with two input nodes (for RGB image and optical flow), where each node represents some pre-composed layers and edge represents the data flow with a trainable weight. The searching policy is a modified evolutionary algorithm, which is guided by the trainable weights on the edge, and a set of graph limitations are in-place to avoid over-complicated graphs. The best-selected model outperforms previous baselines and achieves a new state-of-the-art on two video datasets.\n\nOverall, this paper presents a concrete application of neural architecture search for video CNN with interesting results. Edge-weight guided evolutionary algorithm also demonstrates a small improvement in the ablation study. My concern, as detailed later, is if the comparison only with previously human-designed models is necessary. Nevertheless, this paper presents an interesting application of NAS and discover a feasible way to conduct an evolutionary algorithm within a reasonable cost (less than 100 sampled architectures). \n\n\nStrength:\n+ Writing in good shape, easy to follow and understand.\n+ Motivation is clear and timely, reformulate neural architecture search for video representation is novel.\n+ Clear experimental settings and reasonable convincing results.\n\n\nWeakness:\n- Lack of comparison with previous neural architecture search algorithms\nAlthough the results yield that the proposed new search space is meaningful, considering each model has a similar dimension comparing to ResNet-50, it is still unknown if only comparing to human-designed model is a truly fair baseline. In my perspective, since this paper is built on top of NAS strategy with minor adaptation, could the author add one comparison experiment that, the proposed new search space is superior to those previous NAS spaces? For example, one could based on the earlier two-stream ResNet-50 with RGB+F modality, switch the backbone model into a NAS-based one and search with the same evolutionary algorithm (removing the edge weights adaptation). Otherwise, the improvement shown in the paper is not that surprising.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper661/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures", "authors": ["Michael S. Ryoo", "AJ Piergiovanni", "Mingxing Tan", "Anelia Angelova"], "authorids": ["mryoo@google.com", "ajpiergi@indiana.edu", "tanmingxing@google.com", "anelia@google.com"], "keywords": ["video representation learning", "video understanding", "activity recognition", "neural architecture search"], "TL;DR": "We search for multi-stream neural architectures with better connectivity and spatio-temporal interactions for video understanding.", "abstract": "Learning to represent videos is a very challenging task both algorithmically and computationally. Standard video CNN architectures have been designed by directly extending architectures devised for image understanding to include the time dimension, using modules such as 3D convolutions, or by using two-stream design to capture both appearance and motion in videos. We interpret a video CNN as a collection of multi-stream convolutional blocks connected to each other, and propose the approach of automatically finding neural architectures with better connectivity and spatio-temporal interactions for video understanding. This is done by evolving a population of overly-connected architectures guided by connection weight learning. \nArchitectures combining representations that abstract different input types (i.e., RGB and optical flow) at multiple temporal resolutions are searched for, allowing different types or sources of information to interact with each other. Our method, referred to as AssembleNet, outperforms prior approaches on public video datasets, in some cases by a great margin. We obtain 58.6% mAP on Charades and 34.27% accuracy on Moments-in-Time.", "pdf": "/pdf/47259a18219d759a9fe8ebefbe0829933b4edcaa.pdf", "paperhash": "ryoo|assemblenet_searching_for_multistream_neural_connectivity_in_video_architectures", "_bibtex": "@inproceedings{\nRyoo2020AssembleNet:,\ntitle={AssembleNet: Searching for Multi-Stream Neural Connectivity in Video Architectures},\nauthor={Michael S. Ryoo and AJ Piergiovanni and Mingxing Tan and Anelia Angelova},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJgMK64Ywr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/520bd449b94e5b6fb8da9a2e0aa9119bb83572bc.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJgMK64Ywr", "replyto": "SJgMK64Ywr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper661/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575244968027, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper661/Reviewers"], "noninvitees": [], "tcdate": 1570237748913, "tmdate": 1575244968042, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper661/-/Official_Review"}}}], "count": 10}