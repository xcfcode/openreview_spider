{"notes": [{"id": "HyloPnEKPr", "original": "SJg0ajDqHr", "number": 17, "cdate": 1569438819241, "ddate": null, "tcdate": 1569438819241, "tmdate": 1577168214058, "tddate": null, "forum": "HyloPnEKPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Context-aware Attention Model for Coreference Resolution", "authors": ["Yufei Li", "Xiangyu Zhou", "Jie Ma", "Yu Long", "Xuan Wang", "Chen Li"], "authorids": ["vermouthtarot@gmail.com", "zxy951005@stu.xjtu.edu.cn", "majack@stu.xjtu.edu.cn", "longyu95@stu.xjtu.edu.cn", "wangxuan8888@stu.xjtu.edu.cn", "cli@xjtu.edu.cn"], "keywords": ["Coreference resolution", "Feature Attention"], "TL;DR": "We demonstrate an attention model reweighing features around different contexts to reduce the wrongful predictions between similar or identical texts units", "abstract": "Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "pdf": "/pdf/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "paperhash": "li|contextaware_attention_model_for_coreference_resolution", "original_pdf": "/attachment/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "_bibtex": "@misc{\nli2020contextaware,\ntitle={Context-aware Attention Model for Coreference Resolution},\nauthor={Yufei Li and Xiangyu Zhou and Jie Ma and Yu Long and Xuan Wang and Chen Li},\nyear={2020},\nurl={https://openreview.net/forum?id=HyloPnEKPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hBs16wkOeP", "original": null, "number": 1, "cdate": 1576798685057, "ddate": null, "tcdate": 1576798685057, "tmdate": 1576800949826, "tddate": null, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "invitation": "ICLR.cc/2020/Conference/Paper17/-/Decision", "content": {"decision": "Reject", "comment": "Main content:\n\nBlind review #2 summarizes it well:\n\nThis paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.\n\n--\n\nDiscussion:\n\nAll reviewers rejected.\n\n--\n\nRecommendation and justification:\n\nThe paper must be rejected due to its violation of blind submission (the authors reveal themselves in the Acknowledgments).\n\nFor information, blind review #2 also summarized well the following justifications for rejection:\n\nI recommend rejection for this paper due to the following reasons:\n- The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). \n- The experiment results aren't strong enough. And the experiments are done on only one dataset.\n- I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-aware Attention Model for Coreference Resolution", "authors": ["Yufei Li", "Xiangyu Zhou", "Jie Ma", "Yu Long", "Xuan Wang", "Chen Li"], "authorids": ["vermouthtarot@gmail.com", "zxy951005@stu.xjtu.edu.cn", "majack@stu.xjtu.edu.cn", "longyu95@stu.xjtu.edu.cn", "wangxuan8888@stu.xjtu.edu.cn", "cli@xjtu.edu.cn"], "keywords": ["Coreference resolution", "Feature Attention"], "TL;DR": "We demonstrate an attention model reweighing features around different contexts to reduce the wrongful predictions between similar or identical texts units", "abstract": "Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "pdf": "/pdf/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "paperhash": "li|contextaware_attention_model_for_coreference_resolution", "original_pdf": "/attachment/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "_bibtex": "@misc{\nli2020contextaware,\ntitle={Context-aware Attention Model for Coreference Resolution},\nauthor={Yufei Li and Xiangyu Zhou and Jie Ma and Yu Long and Xuan Wang and Chen Li},\nyear={2020},\nurl={https://openreview.net/forum?id=HyloPnEKPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730702, "tmdate": 1576800283547, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper17/-/Decision"}}}, {"id": "rkgHD5LntS", "original": null, "number": 1, "cdate": 1571740252841, "ddate": null, "tcdate": 1571740252841, "tmdate": 1572972649152, "tddate": null, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "invitation": "ICLR.cc/2020/Conference/Paper17/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. \n\nThere is not much in the paper to review. I don't think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn't we just learn the weights using another layer? It could be context dependent if desired.\n\nIt is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable."}, "signatures": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-aware Attention Model for Coreference Resolution", "authors": ["Yufei Li", "Xiangyu Zhou", "Jie Ma", "Yu Long", "Xuan Wang", "Chen Li"], "authorids": ["vermouthtarot@gmail.com", "zxy951005@stu.xjtu.edu.cn", "majack@stu.xjtu.edu.cn", "longyu95@stu.xjtu.edu.cn", "wangxuan8888@stu.xjtu.edu.cn", "cli@xjtu.edu.cn"], "keywords": ["Coreference resolution", "Feature Attention"], "TL;DR": "We demonstrate an attention model reweighing features around different contexts to reduce the wrongful predictions between similar or identical texts units", "abstract": "Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "pdf": "/pdf/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "paperhash": "li|contextaware_attention_model_for_coreference_resolution", "original_pdf": "/attachment/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "_bibtex": "@misc{\nli2020contextaware,\ntitle={Context-aware Attention Model for Coreference Resolution},\nauthor={Yufei Li and Xiangyu Zhou and Jie Ma and Yu Long and Xuan Wang and Chen Li},\nyear={2020},\nurl={https://openreview.net/forum?id=HyloPnEKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575883867739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper17/Reviewers"], "noninvitees": [], "tcdate": 1570237758341, "tmdate": 1575883867754, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper17/-/Official_Review"}}}, {"id": "BJgdQY86KH", "original": null, "number": 2, "cdate": 1571805471951, "ddate": null, "tcdate": 1571805471951, "tmdate": 1572972649118, "tddate": null, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "invitation": "ICLR.cc/2020/Conference/Paper17/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.\n\nI recommend rejection for this paper due to the following reasons:\n- The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). \n- The experiment results aren't strong enough. And the experiments are done on only one dataset.\n- I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware.\n\nOther suggestions:\n- The citation format seems weird through out the paper. \n- Table 1 and 3 look like ablation results. It might be less confusing if it's presented as \"Full system: xx%; without pairwise FA: yy%; without grammatical numbers zz% ...\".\n- Equation 8 - 10 are quite confusing. What is f(x)? How large is V? What is u? etc.\n- Please define/explain the \"grammatical numbers\" feature when it's introduced in Section 2.2."}, "signatures": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-aware Attention Model for Coreference Resolution", "authors": ["Yufei Li", "Xiangyu Zhou", "Jie Ma", "Yu Long", "Xuan Wang", "Chen Li"], "authorids": ["vermouthtarot@gmail.com", "zxy951005@stu.xjtu.edu.cn", "majack@stu.xjtu.edu.cn", "longyu95@stu.xjtu.edu.cn", "wangxuan8888@stu.xjtu.edu.cn", "cli@xjtu.edu.cn"], "keywords": ["Coreference resolution", "Feature Attention"], "TL;DR": "We demonstrate an attention model reweighing features around different contexts to reduce the wrongful predictions between similar or identical texts units", "abstract": "Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "pdf": "/pdf/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "paperhash": "li|contextaware_attention_model_for_coreference_resolution", "original_pdf": "/attachment/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "_bibtex": "@misc{\nli2020contextaware,\ntitle={Context-aware Attention Model for Coreference Resolution},\nauthor={Yufei Li and Xiangyu Zhou and Jie Ma and Yu Long and Xuan Wang and Chen Li},\nyear={2020},\nurl={https://openreview.net/forum?id=HyloPnEKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575883867739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper17/Reviewers"], "noninvitees": [], "tcdate": 1570237758341, "tmdate": 1575883867754, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper17/-/Official_Review"}}}, {"id": "Skgu_zsaKH", "original": null, "number": 3, "cdate": 1571824240078, "ddate": null, "tcdate": 1571824240078, "tmdate": 1572972649075, "tddate": null, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "invitation": "ICLR.cc/2020/Conference/Paper17/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper unfortunately violates the blind-review policy: its acknowledgement exposes the authors. I thus support desk rejection. \n                                                                                                                                                                                                                                                                                                                                                                                "}, "signatures": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper17/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Context-aware Attention Model for Coreference Resolution", "authors": ["Yufei Li", "Xiangyu Zhou", "Jie Ma", "Yu Long", "Xuan Wang", "Chen Li"], "authorids": ["vermouthtarot@gmail.com", "zxy951005@stu.xjtu.edu.cn", "majack@stu.xjtu.edu.cn", "longyu95@stu.xjtu.edu.cn", "wangxuan8888@stu.xjtu.edu.cn", "cli@xjtu.edu.cn"], "keywords": ["Coreference resolution", "Feature Attention"], "TL;DR": "We demonstrate an attention model reweighing features around different contexts to reduce the wrongful predictions between similar or identical texts units", "abstract": "Coreference resolution is an important task for gaining more complete understanding about texts by artificial intelligence. The state-of-the-art end-to-end neural coreference model considers all spans in a document as potential mentions and learns to link an antecedent with each possible mention. However, for the verbatim same mentions, the model tends to get similar or even identical representations based on the features, and this leads to wrongful predictions. In this paper, we propose to improve the end-to-end system by building an attention model to reweigh features around different contexts. The proposed model substantially outperforms the state-of-the-art on the English dataset of the CoNLL 2012 Shared Task with 73.45% F1 score on development data and 72.84% F1 score on test data.", "pdf": "/pdf/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "paperhash": "li|contextaware_attention_model_for_coreference_resolution", "original_pdf": "/attachment/165ccbb4e4af9a98b694db9a13d96043f227340f.pdf", "_bibtex": "@misc{\nli2020contextaware,\ntitle={Context-aware Attention Model for Coreference Resolution},\nauthor={Yufei Li and Xiangyu Zhou and Jie Ma and Yu Long and Xuan Wang and Chen Li},\nyear={2020},\nurl={https://openreview.net/forum?id=HyloPnEKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyloPnEKPr", "replyto": "HyloPnEKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper17/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575883867739, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper17/Reviewers"], "noninvitees": [], "tcdate": 1570237758341, "tmdate": 1575883867754, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper17/-/Official_Review"}}}], "count": 5}