{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124455851, "tcdate": 1518467137715, "number": 248, "cdate": 1518467137715, "id": "S1qru_kDf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1qru_kDf", "signatures": ["~Chun-Min_Chang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity", "abstract": "We propose a novel convolutional neural networks (CNNs) training procedure to allow dynamically trade-offs between different resource and performance requirements. Our approach prioritizes the channels to enable structured sparsity and multi-fidelity approximations at inference time. We train the VGG network with our method on various benchmark datasets. The experiment results show that, on the CIFAR-10 dataset, a 63x parameters reduction and a 11x FLOPs reduction can be achieved, with only a 2% accuracy drop.", "paperhash": "chang|channelprioritized_convolutional_neural_networks_for_sparsity_and_multifidelity", "_bibtex": "@misc{\n  chang2018channel-prioritized,\n  title={Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity},\n  author={Chun-Min Chang and Hung-Yi Ou Yang and Chia-Ching Lin and Chin-Laung Lei and Kuan-Ta Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qru_kDf}\n}", "authorids": ["d05921027@ntu.edu.tw", "frank840925@gmail.com", "d05921018@ntu.edu.tw", "cllei@ntu.edu.tw", "swc@iis.sinica.edu.tw"], "authors": ["Chun-Min Chang", "Hung-Yi Ou Yang", "Chia-Ching Lin", "Chin-Laung Lei", "Kuan-Ta Chen"], "keywords": ["filter/channel prioritization", "network pruning", "compression", "computational efficiency", "multi-fidelity inference"], "pdf": "/pdf/65552da8d147545aae5aa6efb2eb9c08fef39233.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582984178, "tcdate": 1519729305457, "number": 1, "cdate": 1519729305457, "id": "r1bsq3MOG", "invitation": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "forum": "S1qru_kDf", "replyto": "S1qru_kDf", "signatures": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer3"], "content": {"title": "Interesting idea but baselines are very poor", "rating": "4: Ok but not good enough - rejection", "review": "The paper suggests an interesting approach to rank the importance of different channels by monotonically decreasing initialization of scaling factors in batch-norm layers. This potentially allows to order the channels wrt their importance and hence to establish a trade-off between evaluation time vs accuracy.\n\nAlthough the idea is interesting I am not convinced it really works since all the VGG-16 baselines are very poor. On both CIFAR-10 and CIFAR-100 those architecture can be trained to much higher level of accuracy (93% for CIFAR-10 and about 79% for CIFAR-100). Because of this it is not clear whether further sparsification/switching to different fidelity levels will not lead to unacceptably bad quality. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity", "abstract": "We propose a novel convolutional neural networks (CNNs) training procedure to allow dynamically trade-offs between different resource and performance requirements. Our approach prioritizes the channels to enable structured sparsity and multi-fidelity approximations at inference time. We train the VGG network with our method on various benchmark datasets. The experiment results show that, on the CIFAR-10 dataset, a 63x parameters reduction and a 11x FLOPs reduction can be achieved, with only a 2% accuracy drop.", "paperhash": "chang|channelprioritized_convolutional_neural_networks_for_sparsity_and_multifidelity", "_bibtex": "@misc{\n  chang2018channel-prioritized,\n  title={Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity},\n  author={Chun-Min Chang and Hung-Yi Ou Yang and Chia-Ching Lin and Chin-Laung Lei and Kuan-Ta Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qru_kDf}\n}", "authorids": ["d05921027@ntu.edu.tw", "frank840925@gmail.com", "d05921018@ntu.edu.tw", "cllei@ntu.edu.tw", "swc@iis.sinica.edu.tw"], "authors": ["Chun-Min Chang", "Hung-Yi Ou Yang", "Chia-Ching Lin", "Chin-Laung Lei", "Kuan-Ta Chen"], "keywords": ["filter/channel prioritization", "network pruning", "compression", "computational efficiency", "multi-fidelity inference"], "pdf": "/pdf/65552da8d147545aae5aa6efb2eb9c08fef39233.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582983963, "id": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper248/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer1"], "reply": {"forum": "S1qru_kDf", "replyto": "S1qru_kDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582983963}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582960259, "tcdate": 1520138746923, "number": 2, "cdate": 1520138746923, "id": "B1XZceFdz", "invitation": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "forum": "S1qru_kDf", "replyto": "S1qru_kDf", "signatures": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer2"], "content": {"title": "Interesting approach to obtain efficient inference based on pruning the batch norm", "rating": "5: Marginally below acceptance threshold", "review": "Pros:\n-Somehow i like the idea of dynamically modify the computational cost depending on the inference resources available. This paper aims at that by using the so called 'fidelity'.\n\nCons:\n- How do we select that fidelity? At random? And when it comes to multiple fidelities... how do we select that?\n- The paper is introduced in a generic manner, however, this is based on a pre-trained effort. \n\nGiven said that, I do not see how this term is directly related to the computational resources available, and, it it is not, then probably it is better to just compress with different parameters. As far as I understand, everything needs retraining (and actually experiments are based on a pre-trained VGG16 and transferred to cifarX)\n\nThe idea of structured pruning using the batch norm layer is not strictly new and while seems to be benefits, I guess needs better explanations. For instance, the multi-fidelity seems interesting but is not really explained. How do I link inference benefits to having multiple fidelity?\n\nI also wonder the impact in compression of the linear layers. Tables show the total number of parameters while figures are about convolutional layers. I understand compressing the last layer has a significant impact directly on the first linear layer, and therefore those numbers are a bit 'overselling'.\n\nAs minor comments, I also think the paper needs serious readability improvements. For instance, figures and tables are hard to read. Things are not defined properly. Took me a while to understand that, in Table 1, there is a direct comparison in each row between the proposed approach and the IDP (from the literature) and I still do not understand the differences between the rows when comes to 'ours'\n\n\nIn table 1, how can I compare the numbers for the model at 25% fidelity (Achieving 75.9 vs 65)? I can not see that in the table.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity", "abstract": "We propose a novel convolutional neural networks (CNNs) training procedure to allow dynamically trade-offs between different resource and performance requirements. Our approach prioritizes the channels to enable structured sparsity and multi-fidelity approximations at inference time. We train the VGG network with our method on various benchmark datasets. The experiment results show that, on the CIFAR-10 dataset, a 63x parameters reduction and a 11x FLOPs reduction can be achieved, with only a 2% accuracy drop.", "paperhash": "chang|channelprioritized_convolutional_neural_networks_for_sparsity_and_multifidelity", "_bibtex": "@misc{\n  chang2018channel-prioritized,\n  title={Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity},\n  author={Chun-Min Chang and Hung-Yi Ou Yang and Chia-Ching Lin and Chin-Laung Lei and Kuan-Ta Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qru_kDf}\n}", "authorids": ["d05921027@ntu.edu.tw", "frank840925@gmail.com", "d05921018@ntu.edu.tw", "cllei@ntu.edu.tw", "swc@iis.sinica.edu.tw"], "authors": ["Chun-Min Chang", "Hung-Yi Ou Yang", "Chia-Ching Lin", "Chin-Laung Lei", "Kuan-Ta Chen"], "keywords": ["filter/channel prioritization", "network pruning", "compression", "computational efficiency", "multi-fidelity inference"], "pdf": "/pdf/65552da8d147545aae5aa6efb2eb9c08fef39233.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582983963, "id": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper248/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer1"], "reply": {"forum": "S1qru_kDf", "replyto": "S1qru_kDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582983963}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582705763, "tcdate": 1520695545200, "number": 3, "cdate": 1520695545200, "id": "H1bbtdZtG", "invitation": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "forum": "S1qru_kDf", "replyto": "S1qru_kDf", "signatures": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer1"], "content": {"title": "simple effective approach for pruning cnns", "rating": "7: Good paper, accept", "review": "This paper introduces a method of inducing sparsity over cnn channels by imposing sparsity loss and and a monotonicity inducing penalty on the batchnorm scaling factors for each layer. The end result is a network with many scaling factors close to zero, so these channels can be pruned and then the network is subsequently finetuned. Channel-wise sparsity is not a new approach, but the novelty in this method comes from the dynamics scaling idea. In particular in the finetuning phase they optimize for multiple fidelity level simultaneously, and then the model can be adapted to different resource requirements. The approach appears to outperform other recent methods of pruning cnns.\n\nSpecific questions:\n- could you say something about other methods that prune chanel-wise? what would be the outcome of using another approach, and then pruning with different fidelity levels and then finetuning jointly for each level? more specifically I would like to get at the contributions of the particular sparsity loss used during training (sparsity + monotonicity), and how this relates to other chanel-wise sparsity penalties, and the multi-fidelity fine tuning.\n- if you optimize for a single fidelity, can you achieve better performance (for that level) versus when you optimize for may levels simultaneously?\n- could you say more about the relation between desenets (Huang et al., 2017)? this approach also learns networks of different capacities, albeit in a different manner by classifying an image earlier or later along a cascade of networks depending on the confidence. How does the test time efficacy of this approach compare to your method?\n\nOverall this paper is clear, easy to read and presents clear improvements over existing pruning methods. Additional discussions to related models would be helpful to put this work in the context of previous pruning methods and make clear the specific contributions of this work. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity", "abstract": "We propose a novel convolutional neural networks (CNNs) training procedure to allow dynamically trade-offs between different resource and performance requirements. Our approach prioritizes the channels to enable structured sparsity and multi-fidelity approximations at inference time. We train the VGG network with our method on various benchmark datasets. The experiment results show that, on the CIFAR-10 dataset, a 63x parameters reduction and a 11x FLOPs reduction can be achieved, with only a 2% accuracy drop.", "paperhash": "chang|channelprioritized_convolutional_neural_networks_for_sparsity_and_multifidelity", "_bibtex": "@misc{\n  chang2018channel-prioritized,\n  title={Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity},\n  author={Chun-Min Chang and Hung-Yi Ou Yang and Chia-Ching Lin and Chin-Laung Lei and Kuan-Ta Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qru_kDf}\n}", "authorids": ["d05921027@ntu.edu.tw", "frank840925@gmail.com", "d05921018@ntu.edu.tw", "cllei@ntu.edu.tw", "swc@iis.sinica.edu.tw"], "authors": ["Chun-Min Chang", "Hung-Yi Ou Yang", "Chia-Ching Lin", "Chin-Laung Lei", "Kuan-Ta Chen"], "keywords": ["filter/channel prioritization", "network pruning", "compression", "computational efficiency", "multi-fidelity inference"], "pdf": "/pdf/65552da8d147545aae5aa6efb2eb9c08fef39233.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582983963, "id": "ICLR.cc/2018/Workshop/-/Paper248/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper248/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper248/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper248/AnonReviewer1"], "reply": {"forum": "S1qru_kDf", "replyto": "S1qru_kDf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper248/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582983963}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573583114, "tcdate": 1521573583114, "number": 171, "cdate": 1521573582766, "id": "BJP0CACtz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1qru_kDf", "replyto": "S1qru_kDf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity", "abstract": "We propose a novel convolutional neural networks (CNNs) training procedure to allow dynamically trade-offs between different resource and performance requirements. Our approach prioritizes the channels to enable structured sparsity and multi-fidelity approximations at inference time. We train the VGG network with our method on various benchmark datasets. The experiment results show that, on the CIFAR-10 dataset, a 63x parameters reduction and a 11x FLOPs reduction can be achieved, with only a 2% accuracy drop.", "paperhash": "chang|channelprioritized_convolutional_neural_networks_for_sparsity_and_multifidelity", "_bibtex": "@misc{\n  chang2018channel-prioritized,\n  title={Channel-Prioritized Convolutional Neural Networks for Sparsity and Multi-fidelity},\n  author={Chun-Min Chang and Hung-Yi Ou Yang and Chia-Ching Lin and Chin-Laung Lei and Kuan-Ta Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=S1qru_kDf}\n}", "authorids": ["d05921027@ntu.edu.tw", "frank840925@gmail.com", "d05921018@ntu.edu.tw", "cllei@ntu.edu.tw", "swc@iis.sinica.edu.tw"], "authors": ["Chun-Min Chang", "Hung-Yi Ou Yang", "Chia-Ching Lin", "Chin-Laung Lei", "Kuan-Ta Chen"], "keywords": ["filter/channel prioritization", "network pruning", "compression", "computational efficiency", "multi-fidelity inference"], "pdf": "/pdf/65552da8d147545aae5aa6efb2eb9c08fef39233.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}