{"notes": [{"id": "BkecJjCEuN", "original": "HkeBcRafuN", "number": 37, "cdate": 1553423074058, "ddate": null, "tcdate": 1553423074058, "tmdate": 1562082117392, "tddate": null, "forum": "BkecJjCEuN", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "LABEL-EFFICIENT AUDIO CLASSIFICATION THROUGH MULTITASK LEARNING AND SELF-SUPERVISION", "authors": ["Tyler Lee", "Ting Gong", "Suchismita Padhy", "Andrew Rouditchenko", "Anthony Ndirango"], "authorids": ["tyler.p.lee@intel.com", "ting.gong@intel.com", "suchismita.padhy@intel.com", "roudi@mit.edu", "anthony.ndirango@intel.com"], "keywords": ["multitask learning", "self-supervised learning", "end-to-end audio classification"], "TL;DR": "Label-efficient audio classification via multi-task learning and self-supervision", "abstract": "While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6% through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.", "pdf": "/pdf/5ca5b4c41d074f84de1225a326afe54b6dc289c5.pdf", "paperhash": "lee|labelefficient_audio_classification_through_multitask_learning_and_selfsupervision"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "SJg86aKA_E", "original": null, "number": 1, "cdate": 1554058685995, "ddate": null, "tcdate": 1554058685995, "tmdate": 1555512028974, "tddate": null, "forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Official_Review", "content": {"title": "Accept", "review": "Summary:\nThe paper proposes a method for using multi-task learning with a variety of supervised and unsupervised audio datasets to train a convolutional network model that operates on the raw waveform. The results show that their technique improves results on three datasets compared to training on those datasets alone, and the results scale with the amount of training data.\n\nPros:\nThe paper is well written and a pleasure to read. The experiments are rigorous and well explained.\n\nI think this is the first time I've seen a multi-task architecture for raw audio.\n\nCons:\nThese datasets are not what I'd call \"limited labeled data\"---Google Speech Commands, for example, has thousands of examples of each word. The datasets are smaller than LibriSpeech, which you use as the big unsupervised dataset, but ironically LibriSpeech probably counts more as \"limited labeled data\": consider how many times the word \"xylophone\" might turn up (probably not often). Perhaps a better experiment would be to recognize these rare words, given just a few examples, and see how well multi-task learning + unsupervised learning helps with that.\n\nThe paper \"Listening to the World Improves Speech Command Recognition\" (https://arxiv.org/abs/1710.08377v1, accepted to AAAI 2017) is about a related approach: transfer learning, as opposed to multi-task learning, for audio tasks. You should cite this paper and compare your method with theirs. \n\n93% accuracy on Google Speech Commands seems weirdly low. I myself have trained a model that operates on the raw waveform, without any data augmentation/multi-task learning, and gets 95% accuracy for the full 30 commands, not just the 12 labels you picked. It might be because you don't use any recurrent layers and just use convolutional layers. It's not that important given the other results, but maybe it indicates a bug. I hope you release your code!", "rating": "3: Marginally above acceptance threshold", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LABEL-EFFICIENT AUDIO CLASSIFICATION THROUGH MULTITASK LEARNING AND SELF-SUPERVISION", "authors": ["Tyler Lee", "Ting Gong", "Suchismita Padhy", "Andrew Rouditchenko", "Anthony Ndirango"], "authorids": ["tyler.p.lee@intel.com", "ting.gong@intel.com", "suchismita.padhy@intel.com", "roudi@mit.edu", "anthony.ndirango@intel.com"], "keywords": ["multitask learning", "self-supervised learning", "end-to-end audio classification"], "TL;DR": "Label-efficient audio classification via multi-task learning and self-supervision", "abstract": "While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6% through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.", "pdf": "/pdf/5ca5b4c41d074f84de1225a326afe54b6dc289c5.pdf", "paperhash": "lee|labelefficient_audio_classification_through_multitask_learning_and_selfsupervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Official_Review", "cdate": 1553713415760, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415760, "tmdate": 1555511825995, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper37/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Sye52SE_FN", "original": null, "number": 2, "cdate": 1554691505689, "ddate": null, "tcdate": 1554691505689, "tmdate": 1555511885643, "tddate": null, "forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Official_Review", "content": {"title": "Intriguing approach, but unclear relationship to transfer learning", "review": "The authors propose a technique for improving the performance of audio classification neural networks by simultaneously training them to solve various auxiliary self-supervised tasks. This is achieved by sharing a common \"trunk\" network with multiple \"head\" networks, each tailored to solve a specific task. Typically, a network will be optimized with four head networks, one to solve a main task and the rest to solve three auxiliary tasks. Since the trunk is shared for the different tasks, the idea is that the auxiliary tasks will result in an improved trunk network compared to only optimizing the main objective. The authors present experimental results on three main tasks: audio tagging, speaker identification, and speech command classification. These were coupled with three auxiliary, self-supervised tasks: next-step prediction, denoising, and upsampling. Since training data for these auxiliary tasks could be created from unlabeled data, it represents a virtually inexhaustible supply of potential training data. Experimental results for this architecture showed a significant improvement was obtained by joint training with these auxiliary tasks. However, the authors also present results for a transfer learning experiment, where the network was first trained on the auxiliary tasks and then fine-tuned for the main task. This yielded an even greater improvement, which calls into question the utility of the proposed method. Unfortunately, no analysis of explanation of this fact is presented.\n\nThe description of the method is quite comprehensive, but at times vague and lacking some details. First, while the paper describes three main tasks (audio tagging, speaker identification, and speech command classification), much of the description of the architecture only refers to audio tagging. The method for the joint training is also only mentioned towards the end, in the appendix. It would be clearer if some of these details were included in the main text. It is also not completely clear whether the trunk network is jointly trained between all three main tasks and the auxiliary tasks, or between a main task and the auxiliary tasks. While it is likely the latter, it is never clearly spelled out. There is also little motivation for the auxiliary tasks. Why were these chosen as opposed to others? For example, what do the authors expect the next-step prediction task to add? If it is simply a matter of predicting the next sample in a time series, this shouldn't necessary require a very high-level knowledge of the audio signal structure, since continuity already provides a very strong prior. Similar questions can be posed for the other auxiliary tasks. Furthermore, some auxiliary tasks are trained with an L^2 loss, while others are trained with a smoothed L^1 loss. Why were these different choices made? Finally, the method of choosing hyperparameters is also not clear. The authors state that they were chosen \"heuristically favoring performance on the main task\". What does this mean?\n\nThe experiments provide interesting results, but are not as complete as they could be. For example, there are several popular datasets for audio tagging. why were the particular datasets chosen? More importantly, why are no results for state-of-the-art methods presented? It is not necessary that the proposed architecture perform better than the current state of the art, but it is important to provide a context for the results. The authors also add a significant amount of training data to the problem through the auxiliary training tasks, but neglect to discuss any impact on training time. While this may not always be an issue, it is a relevant trade-off to be made when considering when to adopt the proposed architecture. Finally, the authors make a claim about additivity (or complementarity) of their proposed approach when coupled with data augmentation. It would be interesting to see to what extent the changed labels overlap between the two methods. If they are indeed complementary, there would be little overlap between the set of labels changed by adding one and then the other.\n\nFinally, the transfer learning results (as mentioned above), pose a significant problem with respect to the utility of the proposed algorithm. If we can simply train the network trunk for the auxiliary tasks separately, and then add a new head network and train that for the main task, why should we consider the whole apparatus proposed in the current manuscript? This holds doubly true considering that the transfer learning approach significantly outperforms the proposed approach for the considered tasks. There may be some properties like reduced training time, fewer hyperparameters to select, and so on, but no discussion is provided. Since this is a potentially very important alternative, analysis and discussion of the merits of the two approaches is strictly necessary.\n\nSome more minor comments follow:\n- With respect to general-purpose audio representations, the authors may want to mention the audio scattering transforms developed by Mallat and collaborators.\n- In Section 3, the authors mentioned experiments being made with \"0 to 3\" auxiliary tasks. The subsequent experiments only present results for 0 _and_ 3 auxiliary tasks, with no results for 1 or 2 tasks. This should be corrected.\n- One dataset is described as containing \"uncompressed PCM\", another as \"WAVE format files\", while the format of the third is not specified. If the authors insist on including this information, they should be consistent in their descriptions. What is the format of the third dataset? Are the WAVE files also stored as PCM? Or are they stored in \u00b5-law or some other format?\n- The authors make the claim that \"spectral/cepstral representations of audio ... significantly restrict the range of audio processing tasks which they can perform\". A finely sampled spectral representation contains enough information for synthesizing a new signal which sounds virtually identical to the original. Where they may fall short is in sample-by-sample reconstructions, since they do not include the phase. One could argue that this sample-by-sample reconstruction is rarely what's desired in audio classification tasks. Indeed, the type of tasks for which they are necessary mostly includes low-level processing tasks such as the auxiliary tasks introduced in this paper. The fact that that there is such an \"impedance mismatch\" between the main and auxiliary tasks should be cause for concern.\n- The MAP@3, Top-1, and Top-5 metrics, although well-known, should be defined completeness.\n- The difference between the \"baseline\" and \"none (0)\" rows in Table 1 is a bit subtle. While the second does not include any unlabeled data, it still performs joint training on the main and auxiliary tasks but on the main task's training data. This is not obvious from a first glance and should be clarified.\n- In Table 2, it would be useful to provide results for \"NI + PS\", \"NI + PS + MTL100\" and the same for \"MTL500\". In the interest of space, however, it may be better to simply sketch these results in the text if they do not add too much more information.\n- The authors state that \"Interestingly, the performance gains from augmenting with noisy data are similar to those obtaining by training the main task jointly with a self-supervised noise-reduction task.\" Why is this interesting? Why could this be the case? Is there a similarity in label assignment as well? I suggest the authors finish this train of though.\n- In Table 3, please write out the full names of the tasks as in Table 1.\n- There are several capitalization errors in the bibliography. In particular, several uppercase characters have been converted to lowercase: \"English\", \"Mandarin\", \"ChiME\", \"PyTorch\".\n- Having figures in gray make them hard to read, especially when printed. Unless there is some compelling reason not to, I suggest they be regenerated in black.\n- Please provide a definition for dilated convolution.\n- The sequence in Section 6.2.1 should be delimited by parentheses, not curly braces (which delimit sets).\n- The \"smoothed L^1\" norm is also known as the Huber loss in statistics and other fields.\n- \"Python\" has the first letter capitalized.\n- Please define SNR.\n\nFinally, I strongly suggest the authors make their code available to the general public.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LABEL-EFFICIENT AUDIO CLASSIFICATION THROUGH MULTITASK LEARNING AND SELF-SUPERVISION", "authors": ["Tyler Lee", "Ting Gong", "Suchismita Padhy", "Andrew Rouditchenko", "Anthony Ndirango"], "authorids": ["tyler.p.lee@intel.com", "ting.gong@intel.com", "suchismita.padhy@intel.com", "roudi@mit.edu", "anthony.ndirango@intel.com"], "keywords": ["multitask learning", "self-supervised learning", "end-to-end audio classification"], "TL;DR": "Label-efficient audio classification via multi-task learning and self-supervision", "abstract": "While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6% through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.", "pdf": "/pdf/5ca5b4c41d074f84de1225a326afe54b6dc289c5.pdf", "paperhash": "lee|labelefficient_audio_classification_through_multitask_learning_and_selfsupervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Official_Review", "cdate": 1553713415760, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper37/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713415760, "tmdate": 1555511825995, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper37/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "Hyxg_gpGqE", "original": null, "number": 1, "cdate": 1555382376059, "ddate": null, "tcdate": 1555382376059, "tmdate": 1555510974258, "tddate": null, "forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LABEL-EFFICIENT AUDIO CLASSIFICATION THROUGH MULTITASK LEARNING AND SELF-SUPERVISION", "authors": ["Tyler Lee", "Ting Gong", "Suchismita Padhy", "Andrew Rouditchenko", "Anthony Ndirango"], "authorids": ["tyler.p.lee@intel.com", "ting.gong@intel.com", "suchismita.padhy@intel.com", "roudi@mit.edu", "anthony.ndirango@intel.com"], "keywords": ["multitask learning", "self-supervised learning", "end-to-end audio classification"], "TL;DR": "Label-efficient audio classification via multi-task learning and self-supervision", "abstract": "While deep learning has been incredibly successful in modeling tasks with large, carefully curated labeled datasets, its application to problems with limited labeled data remains a challenge. The aim of the present work is to improve the label efficiency of large neural networks operating on audio data through a combination of multitask learning and self-supervised learning on unlabeled data. We trained an end-to-end audio feature extractor based on WaveNet that feeds into simple, yet versatile task-specific neural networks. We describe several easily implemented self-supervised learning tasks that can operate on any large, unlabeled audio corpus. We demonstrate that, in scenarios with limited labeled training data, one can significantly improve the performance of three different supervised classification tasks individually by up to 6% through simultaneous training with these additional self-supervised tasks. We also show that incorporating data augmentation into our multitask setting leads to even further gains in performance.", "pdf": "/pdf/5ca5b4c41d074f84de1225a326afe54b6dc289c5.pdf", "paperhash": "lee|labelefficient_audio_classification_through_multitask_learning_and_selfsupervision"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper37/Decision", "cdate": 1554736077230, "reply": {"forum": "BkecJjCEuN", "replyto": "BkecJjCEuN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736077230, "tmdate": 1555510961636, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}