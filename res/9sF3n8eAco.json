{"notes": [{"id": "9sF3n8eAco", "original": "i_Ak6SOz4bU", "number": 1646, "cdate": 1601308182403, "ddate": null, "tcdate": 1601308182403, "tmdate": 1614985728915, "tddate": null, "forum": "9sF3n8eAco", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "oL09qUDBJwM", "original": null, "number": 1, "cdate": 1610040410848, "ddate": null, "tcdate": 1610040410848, "tmdate": 1610474008192, "tddate": null, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "After reading the paper, reviews and authors\u2019 feedback. The meta-reviewer agrees with the reviewers that the paper has limited novelty as there are already previous studies on setting floating point configurations. Additionally, the particular hardware setting that the authors provide seems to rely on a fp32 FMA, which defeats the purpose of a low bit floating point(where smaller FMA could have been used).Therefore this paper is rejected.\n\nThank you for submitting the paper to ICLR. \n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040410833, "tmdate": 1610474008175, "id": "ICLR.cc/2021/Conference/Paper1646/-/Decision"}}}, {"id": "aKRg2Cwqzph", "original": null, "number": 12, "cdate": 1605626410422, "ddate": null, "tcdate": 1605626410422, "tmdate": 1605658540250, "tddate": null, "forum": "9sF3n8eAco", "replyto": "BuPDcYXImy", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 1 Part 1", "comment": "Q1:\nIt is not clear to me the objective of this paper. This paper introduces an 8-bit quantized inference framework. However, it is well-known that, 8-bit precision can be applied to popular DNN models to accelerate inference while maintaining model accuracies and many such systems have already been put into products (e.g. TPU). The state-of-the-art inference quantization work are primarily in the precision of 4 bit or less. But this paper did not compare their work to any of the latest inference works. Instead, the three references this paper compared to, i.e. (Wang & Choi, 2018; Cambier et al., 2020; Sun et al., 2019), are all for the acceleration of DNN TRAINING, where the challenging can be very different, e.g. quantization of gradient tensors.\n\nA:\nThank you for the comment. Let us elaborate more on the reviewer\u2019s concerns as follows.\n\n1)\nThe objective of this paper is clear: developing a better 8-bit floating-point format that achieves both accurate and memory-efficient inference.\nThe reviewer mentioned about Google\u2019s TPU as an example of 8-bit inference. However, please note that TPU only supports 8-bit INTERGER computations but not 8-bit floating-point computations. In most cases, the combination of 8-bit floating-point inputs plus 32-bit floating-point fused-multiply-add (FMA) units achieves higher accuracy than its integer counterpart.\nMeanwhile, the meaning of \u201cwhile maintaining model accuracies\u201d could be ambiguous since some papers consider the model accuracy is \u201cmaintained\u201d even if an accuracy loss of 1\\% to 2\\% is actually presented. Nevertheless, in our paper, the accuracy loss is around 0.1% to 0.3% for VGG-16 and ResNet18/34/50. In our opinion, floating-point operations inherently outperform integer operations in terms of model accuracy.\n\n2)\nAs the reviewer mentioned, there are many quantization techniques are in 4-bit precision or even less (e.g., binarized models). Once again, these techniques still have to pay the price: a notable lower model accuracy. Recently, a NIPS-2019 paper, [ Ron Banner, Yury Nahshan, Daniel Soudry, Post training 4-bit quantization of convolutional networks for rapid-deployment ] \n [ https://papers.nips.cc/paper/2019/file/c0a62e133894cdce435bcb4a5df1db2d-Paper.pdf ], has already investigated this issue in detail.\n\n3)\nAs the reviewer also mentioned, three previous studies (Wang & Choi, 2018; Cambier et al., 2020; Sun et al., 2019) are about 8-bit TRAINING. We are fully aware that these three studies are about training. As well, there is no doubt that training in 8-bit floating-point numbers is very challenging and the above three papers are all excellent in that specific topic.\nIndeed, this paper has nothing to do with 8-bit training. However, the OBJECTIVE of this paper is to achieve a more accurate INFERENCE result, and the proposed FFP8-based technique DOES provide more accurate inference outcomes than those three studies.\n\n<< Highlights of the Response >>\n\nA. The objective of this paper is clear: developing a better and highly flexible 8-bit floating-point format that achieves both accurate and memory-efficient inference.\n\nB. It is inherent that FP computations achieves higher model accuracy than integer computations. Hence, models in 8-bit floating-point are required in additional to those models in INT8 if a higher model accuracy is desired.\n\nC. A model in 4-bit precision or less suffers a notable accuracy loss, which is a price cannot be avoided. Hence, it is absolutely not the target of this paper.\n\nD. (Wang & Choi, 2018; Cambier et al., 2020; Sun et al., 2019) are three excellent papers about DNN training in 8-bit FP numbers; however, the model using the proposed FFP8 format achieves more accurate inference outcomes than the models trained by those three studies.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "HNKYeYVoIan", "original": null, "number": 11, "cdate": 1605626259708, "ddate": null, "tcdate": 1605626259708, "tmdate": 1605626259708, "tddate": null, "forum": "9sF3n8eAco", "replyto": "BuPDcYXImy", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 1 Part 2", "comment": "Q2:\nOn the same note, this paper claimed advantages of using FP32 computation engine, however, the three training papers, by definition, can also be used in FP32 computations through the same high precision converting described in this paper. In addition, it is not clear to me how much one can benefit from quantization while keeping computation in 32bit? For example, one may save some memories space, but the whole inference will be limited by the FP32 computations in terms of throughput, latency, power and cost. Can the authors provide an application case where only quantizing data is beneficial?\n\n\nA:\nWe would like to thank the reviewer for bringing up these issues. We elaborate more on the following two issues: \u201chow much one can benefit from quantization while keeping computation in 32bit\u201d and \u201cthe whole inference will be limited by the FP32 computations in terms of throughput, latency, power and cost\u201d.\n\n1)\nUsing a shorter format (8-bit or 16-bit) for loading/storing data from/to the external memory while still keeping the internal computation in 32-bit precision is exactly the current technology trend. The primary reason is that the actual performance bottleneck in today\u2019s GPU servers is the limited external memory bandwidth, not the computation capacity. Therefore, using shorter data for external memory accesses greatly helps relieve the performance bottleneck while using FP32 in the internal computation cores helps maintain high computation accuracy.\n\nHere we list three documents for the reviewer\u2019s reference.\nDocument 1: Nvidia Tesla V100 GPU Architecture, officially released by Nvidia,\n[ https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf ]\n\nFigure 8 on Page 15: V100 supports \u201cTensor Core 4x4 Matrix Multiply and Accumulate\u201d, i.e., D = A x B + C, where A~D are all 4x4 matrices; C and D are in FP32, A and B are in FP16. This kind of computation core is also referred as fused-multiply-accumulate (FMA) unit.\n\nFigure 9 on Page 16 explicitly points out that input data from external memory are in FP16 and the internal MAC computations are in FP32.\n\nDocument 2: Nvidia A100 Tensor Core GPU Architecture, officially released by Nvidia,\n[ https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf ]\nTable 3 on Page 27 explicitly points out that up to 16x performance gain can be achieved if FP16 instead of FP32 is in use for input data on A100.\n\nDocument 3: [ https://techdecoded.intel.io/resources/leadership-performance-with-2nd-generation-intel-xeon-scalable-processors/#gs.l4lz4j ]  from Intel\n\nNot only Nvidia, Intel also adopts the similar approach to boost the AI inference performance of the 2nd generation Xeon scalable processor with AVX-512 VNNI (Vector Neural Net Instructions) instruction set: one instruction can do 64 MAC operations, where inputs are in INT8 and the accumulation result is in INT32.\n\nThe above three documents are from two industry giants: Nvidia and Intel. Hence, it should be conclusive that the combination of shorter external data + FP32 internal computation is both efficient and accurate. In this paper, what we would like to achieve is to further shrink the memory data from FP32 to FFP8 while maintaining the model accuracy.\n\n2)\nPer the issue of \u201cthe whole inference will be limited by the FP32 computations in terms of throughput, latency, power and cost\u201d, we have already clarified in the above item: both Nvidia and Intel point out that the performance bottleneck is the limited memory bandwidth instead of FP32 computation. Hence, an even shorter memory data format (e.g., FP32/FP16 to FFP8) can further boost the system performance and throughput significantly. Per the power issue, the external memory access is one of the major power consumption sources. Hence, the proposed 8-bit FFP8 format can also help reduce the energy consumption per a data access. Per the cost issue, the required external memory capacity can be reduced to just a quarter.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "NksSRZdzwvy", "original": null, "number": 10, "cdate": 1605626146095, "ddate": null, "tcdate": 1605626146095, "tmdate": 1605626222716, "tddate": null, "forum": "9sF3n8eAco", "replyto": "BuPDcYXImy", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 1 Part 3", "comment": "Q3:\nThe main contribution claimed in the paper are to use exponent bias to cover tensors with different range of distribution. However, this technique has be introduced and discussed in (Sun et al., 2019) paper and has also been investigated in detail in (Cambier et al., 2020) paper. The authors of this paper did not provide any new insight although using in a much simpler case, i.e. only forward paths.\n\nA:\nThank you for the comment. We respectfully disagree with the reviewer, however. Please allow us to elaborate more on this issue.\n\n1)\nIn (Sun et al., 2019), the authors use FP(1,4,3) for forward inference and FP(1,5,2) for backward propagation. For forward inference, a nontrivial exponent bias value 11 instead of 7 is used. However, the exponent bias is still a fixed value in the entire study. That is, the exponent bias is never dynamically adjusted to best fit the given input distributions of different layers of various models. In addition, only two formats (1,4,3) and (1,5,2) are adopted throughout the study. There are no discussions about exploring different combinations of the field size of exponent and fraction. Furthermore, the paper explicitly mentions that a notable accuracy loss is observed unless FP16 is used for the first layer and the last layer while performing inference of ResNet-18.\nNevertheless, as explicitly mentioned in our paper, the proposed FFP8 format possesses three key features:\n\nA. The exponent bias can be ANY integer.\n\nB. The field size of exponent/fraction can also be adjusted.\n\nC. The sign bit can be presented or removed.\n\nThese three features together achieve a remarkably low model accuracy loss (0.1\\%~0.3\\%) for tested models even the external weights/activations are only in 8-bit floating-point precision. In our opinion, the ideas proposed and the results achieved in this paper are far more than those introduced in (Sun et al., 2019).\n\nLast but not least, we have proposed a systematic transformation procedure to convert weights of a pre-trained FP32 model to a best-fit FFP8 model with extremely low accuracy loss. Yes, it is simple and needs no complicated TRAINING methodology. In our opinion, doing simple, doing less, but getting better results should be considered as an advantage, not a shortcoming, shouldn\u2019t it?\n\n2)\nIn (Cambier et al., 2020), only FP8(1,5,2) format with a fixed bias b=15 is used. The authors proposed a \u201cshift\u201d operation and a \u201csqueeze\u201d operation to redistribute the weights and activations so that the value range after redistribution can be fit into the representable range window of FP8(1,5,2). The exact equation to do so is expressed as Equation-(1) on Page 4 of their paper. Note that every \u201cshift operation\u201d actually requires a floating-point multiplication. Moreover, a \u201csqueeze\u201d operation needs to transform an input value X to the output value Y = X^\u03b1, where Y, X, \u03b1 are all floating-point numbers. There are obviously no low-cost means, neither hardware-based nor software-based, to carry out such \u201csqueeze\u201d operation, i.e., \u201csqueeze\u201d is an expensive (either hardware-hungry or runtime-consuming) operation.\nAlternatively, to support our proposed FFP8 format, only a simple low-cost FFP8 to FP32 hardware converter, illustrated in new Figure 6(c) of the revised manuscript, is required. As well, there is virtually no performance loss in a pipelined hardware design. In short, it is much easier to support FFP8 in a hardware perspective as compared to S2FP8 in (Cambier et al., 2020).\n\n<< Summary >>\nAdvantages over (Sun et al., 2019):\n\nA. The proposed FFP8 format is much more flexible: the presence of the sign bit, flexible field size for exponent and fraction, and ANY integer can be set as the exponent bias. It is the key to achieve a higher model accuracy. None of them was mentioned in (Sun et al., 2019).\n\nB. More accurate inference results.\n\nAdvantages over (Cambier et al., 2020):\n\nA. Only FP8(1,5,2) is used in (Cambier et al., 2020), no flexibility on the format selection.\n\nB. No low-cost HW/SW solutions to the implementation of the \u201csqueeze\u201d operation in S2FP8, whereas FFP8 can be easily supported in HW with minimal extra logic.\n\nOnce again, there is no doubt that (Sun et al., 2019) and (Cambier et al., 2020) are two excellent papers. However, our study still adds several new technical advances that have never been presented and discussed, which consequently leads to a more accurate inference outcome. Moreover, we have also presented a low-cost hardware solution to carry out all of the proposed ideas to show that our ideas are practical and implementable.\n\n====================\n\nAt last, we sincerely hope that the reviewer could kindly take a second look and re-evaluate the value of this paper. Thank you very much.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "aQF-FVrRYJ", "original": null, "number": 7, "cdate": 1605539295489, "ddate": null, "tcdate": 1605539295489, "tmdate": 1605611851174, "tddate": null, "forum": "9sF3n8eAco", "replyto": "aYkeJ-w9ILV", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 2 Part 3", "comment": "<< Summary Section>>\n\nIn the comments, the reviewer has raised four major concerns and requested for more information and explanation:\n\n(1) The hardware support details for FFP8  =>  Q1\n\n(2) The frequency of setting format registers  =>  Q2\n\n(3) The compiler/framework support for FFP8 and how the programmers can adopt FFP8 easily in a framework  =>  Q3\n\n(4) More evaluation results on other NNs  =>  Q4\n\nIn the above response, we have addressed all the reviewer\u2019s concerns in detail and provided the corresponding answers each by each. We sincerely hope the reviewer can re-evaluate the contribution of this paper after examining our answers to all the raised questions here. Thank you very much.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "ZLJnyMKbgA", "original": null, "number": 8, "cdate": 1605539339929, "ddate": null, "tcdate": 1605539339929, "tmdate": 1605587193985, "tddate": null, "forum": "9sF3n8eAco", "replyto": "aYkeJ-w9ILV", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 2 Part 2", "comment": "\nQ3:\nCompiler support for FFP8. The authors have not described how FFP 8 would be emitted, especially when different settings are used for different layers. Do you expect the programmer to hand code this? Or will there be compiler support? If so, what would the compiler support look like? What would the compiler optimization algorithm look like to produce FFP 8 code? These details are not answered by the paper.\n\nA:\nThank you very much for bringing up the issue of compiler support. We elaborate more on this topic as follows.\n\nWe certainly do not expect the programmers to deal with the details about how to correctly convert an FFP8 number to/from a typical FP32 number. If we do, the proposed method would be programmer-unfriendly. Instead, in order to be programmer-friendly, those details must be hidden behind the supports of compilers and deep learning frameworks (e.g., TensorFlow, PyTorch, ...).\n\nThere may be a lot of feasible approaches to accomplish this goal and we just show one of the possible ways. The following shows a hypothetical code segment, which demonstrates what would look like if the proposed FFP8 format is supported in PyTorch:\n\n**********\n\\# CNN Layer 1 of VGG-16\n\nself.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n\nconv1_1.ffp8(w_config = (1,2,3), in_config = (1,3,6), out_config = (0,4,11))\n\nself.relu1_1 = nn.ReLU(inplace=True)\n\n\\# CNN Layer 2 of VGG-16\n\nself.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n\nconv1_2.ffp8(w_config = (1,2,4), in_config = (0,4,11), out_config = (0,4,10))\n\nself.relu1_2 = nn.ReLU(inplace=True)\n\nself.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=False)\n\n...\n\n\\# CNN Layers 3~12 of VGG-16 here\n\n...\n\n\\# CNN Layer 13 of VGG-16\n\nself.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n\nconv5_3.ffp8(w_config = (1,2,6), in_config = (0,4,8), out_config = (0,4,8))\n\nself.relu5_3 = nn.ReLU(inplace=True)\n\nself.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=False)\n\n...\n**********\n\nIn the above code segment, you can find three statements including a substring of \u201cffp8\u201d. Each of them explicitly specifies the FFP8 format parameters (x, y, b) of weights, input activations, and output activations for the corresponding CNN layer, respectively. That is, each of such function call is designated to properly set the hardware \u201cformat registers\u201d with the help of compilers and/or learning frameworks. Note that those function calls do nothing but set format registers only. If those ffp8-related function calls are removed, the above code segment looks just like a regular code segment for running VGG-16 in the current PyTorch environment.\n\nIn this way, the compiler/framework developers can easily provide the FFP8-related supports, while the compiler/framework users can easily adopt the FFP8 data format in their deep learning applications with minimal extra effort.\n\n====================\n\n\nQ4:\nMore evaluation on other commonly used NNs. The authors present results for VGG-16. I would like to see more results on different networks. The authors can cut down on some of the graphs or make them smaller to get more space (especially Figure 5). Also, in general I feel like rather than giving out all the results that bring us to the same conclusion, the authors can summarize the results or graphs to make the message / conclusion of the paper more clearer.\n\nA:\nThank you very much for the valuable comment.\nIn our first submission, we reported the results for not only VGG-16 but also ResNet-18/34/50. Currently, in addition to the models mentioned above, we have more results about FCN32s for semantic segmentation, and an LSTM model for ECG check.\n\nFCN32s is a popular network model for semantic segmentation. For FCN32s in default FP32, the mIOU is 63.63% using the VOC2011 dataset. Alternatively, if all the weights are in FFP8(1,4,3,14) and all the activations are in FFP8(1,4,3,2), the resultant mIOU is 63.45%, a slight drop of 0.18%.\n\nBesides, an LSTM model for ECG examination, [ https://physionet.org/content/challenge-2017/1.0.0/ ], has also been evaluated. For the LSTM model in default FP32, the check accuracy is 81.12%. If all the weights are in FFP8(1,3,4,6), the first-layer activations are in FFP8(1,3,4,5), and activations in the other layer are in FFP8(1,4,3,16), the resultant accuracy is 81.53%, an ACCURACY GAIN of 0.41%.\n\nCurrently, we keep on evaluating more neural network models in various application domains in additional to image classification. For example, one of the above two models is a CNN for semantic segmentation while the other is an LSTM for ECG examination. The experimental results have demonstrated that FFP8 also work very well in these two categories.\n\nWe will include these new results into Section 4 of the revised manuscript to demonstrate that the proposed FFP8 format can be applied to a broad range of deep learning applications.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "FvR3xWLkg7X", "original": null, "number": 9, "cdate": 1605539377708, "ddate": null, "tcdate": 1605539377708, "tmdate": 1605539377708, "tddate": null, "forum": "9sF3n8eAco", "replyto": "aYkeJ-w9ILV", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 2 Part 1", "comment": "Q1:\nLimited details on how hardware support for FFP8 look like. The authors claim in section 5 that the extra hardware support required for FFP8 would be minimal. However, no data or simulations are there to back this claim.\n\nA:\nThank you for your valuable comment.\nDue to the page limitation, we only put a very simplified hardware block diagram in Figure 6(c) of our initial manuscript. According to your suggestion, more hardware details of the proposed converter is revealed as shown in [ https://imgur.com/a/CQam38l ], and the manuscript will be further revised to include this new figure accordingly.\n\nAs illustrated above, the extra hardware merely consists of few register bits, few logic gates, a mux-based data-bit selector, a subtractor, and an adder. An example of conversion from FFP8 to FP32 is also included. It should be clear enough that the size of the proposed converter is indeed much smaller than that of a 32-bit floating-point fused-multiply-add (FMA) unit, and thus the extra hardware support is considered minimal.\n\n====================\n\nQ2:\nIn a possible hardware implementation of FFP8, each floating point instruction should be preceded by an instruction which sets (x,y,b) registers for FFP8 to F32 conversion. The frequency of setting these registers may vary based on the application, but nevertheless is an overhead compared to the current FP 8 implementations. The authors should quantify this overhead with a hardware simulation. Alternatively, the authors can describe a different and an efficient implementation of FP 8. This is an important detail that is missing from the paper.\n\nA:\nThank you very much for the comment. Please allow us to elaborate more on this issue.\n\nWe would like to point out that the statement \u201ceach floating point instruction should be preceded by an instruction which sets (x,y,b) registers for FFP8 to F32 conversion\u201d is not correct. In fact, the \u201cformat registers\u201d shown in [ https://imgur.com/a/CQam38l ] are updated only when a new FFP8 format is selected for the incoming activations or weights. That is, the format registers are set \nONLY ONCE if a deep learning application only adopts one format for activation and one format for weight throughout the entire application time. If the proposed layer-wise optimization (LWO) technique is in use, those \u201cformat registers\u201d are updated ONLY at the beginning of each layer. Take VGG-16 as an example, there are more than 1.8 billon MAC operations in Layer 2. It suggests once the format registers are properly set, those settings remain unchanged and valid for the succeeding computational operations for a long while. Moreover, there are at least 90 million MACs in any layer of VGG-16. In summary, the update of format registers is indeed infrequent and the runtime overhead for setting the format registers is negligible.\n\nLet us emphasize once again: the update of the format register is infrequent. Once set, all succeeding activation and weight values from the external memory are simply 8-bit FFP8 data transfers, as the figure demonstrates.\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "KI1RYzbIOe", "original": null, "number": 4, "cdate": 1605457157448, "ddate": null, "tcdate": 1605457157448, "tmdate": 1605457211333, "tddate": null, "forum": "9sF3n8eAco", "replyto": "lNbu8u5AsKO", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 4", "comment": "Q1: Although there is a long line of research examining the data format for training and inference in deep networks, the paper provides evidence of the usefulness of some flexibility in the 8-bit format. The proposed would not require hardware changes except for simple translations between FP32 and FFP8.\n\n\nA:\nThank you very much for your encouraging comment. As you have precisely pointed out: the proposed FFP8 format can bring a set of advantages for inference in deep networks, and the only extra hardware cost is merely a simple converter between FP32 and FFP8. For your information, we will include a more detailed hardware block diagram in the revised manuscript to show how an FFP8 number is converted to its FP32 counterpart, as illustrated in [ https://imgur.com/a/CQam38l ]. Just like what you have mentioned, the hardware for the translation is simple.\n\n====================\n\n\nMinor comments / typos:\n\n\nA: Thank you very much for pointing out these issues, which greatly helps us improve our manuscript further.\n1) The max values of the vertical axes in Figure 4 and in Figure 5 are now identical.\n2) Two grammatical errors on Page 2 and Page 3 have been corrected according to your suggestions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "_VOdSrQTkWA", "original": null, "number": 3, "cdate": 1605457005866, "ddate": null, "tcdate": 1605457005866, "tmdate": 1605457100361, "tddate": null, "forum": "9sF3n8eAco", "replyto": "wNn5UIC_-29", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment", "content": {"title": "Reply to AnonReviewer 3", "comment": "Q1:\nNo full system implementation to demonstrate the performance/memory gains. I doubt that the updates to the form register will be infrequent and cheap.\n\nA:\nThank you for the comment about the performance/memory gains as well as the cost and the updating frequency of the \u201cformat register\u201d. We address your concerns as follows.\n\n1)\nWhen deep learning applications run on current CPU/GPU-based servers, the real performance bottleneck in most cases is actually the limited memory bandwidth, not the lack of computing resources. That is why Nvidia has developed NVLink, NVSwitch, and GPU Direct technologies to boost the external memory bandwidth and shorten the external memory latency. Therefore, storing activations and weights in a shorter format (32-bit to 8-bit) obviously reduces the memory bandwidth requirement significantly, which helps relieve the memory bottleneck and thus improves the overall system performance. One more evidence, recent Nvidia GPU cores start to support two new floating-point formats, 16-bit BFloat16(BF16 or BFP16) and 19-bit TensorFloat32(TF32). Again, it is a very clear sign that a shorter data format does help achieve better system performance. An official article released by Nvidia, \n[ https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/ ], which explicitly indicates that a shorter data format (FP32 to FP16) can accelerate the BERT training by 3x on V100.\n\nIn summary, it is conclusive that an 8-bit floating-point format can undoubtedly deliver a significant performance and memory gains as compared to its 16-bit and 32-bit counterparts in today\u2019s state-of-the-art GPU computing platforms.\n\n2)\nThe \u201cformat registers\u201d are updated only when a new FFP8 format is selected for the incoming activations or weights. That is, the format registers are set ONLY ONCE if a deep learning application only adopts one format for activation and one format for weight throughout the entire application time. If the proposed layer-wise optimization (LWO) technique is in use, those \u201cformat registers\u201d are updated ONLY at the beginning of each layer if necessary. Take VGG-16 as an example, there are more than 1.8 billion MAC operations in Layer 2. It suggests once the format registers are properly set, those settings remain unchanged and valid for the succeeding computational operations for a long while. Moreover, there are at least 90 million MACs in any layer of VGG-16. In summary, the update of format registers is indeed infrequent and the runtime overhead for setting the format registers is negligible.\n\nBesides, the update of format registers is achieved through setting new values to hardware register bits, which is considered not an expensive operation.\n\nQ2: \nThe hardware details are very sketchy. Figure 6 seems to be only showing the high-level components.\n\nA:\nThank you for your valuable comment.\nDue to the page limitation, we only put a very simplified block diagram in Figure 6(c) of our initial manuscript. According to your suggestion, more hardware details of the proposed converter are revealed as shown in [ https://imgur.com/a/CQam38l ], and the manuscript will be further revised to include this new figure accordingly.\n\nAs illustrated above, the extra hardware merely consists of few register bits, few logic gates, a mux-based data-bit selector, a subtractor, and an adder. An example of conversion from FFP8 to FP32 is also included. It should be clear enough that the size of the proposed converter is indeed much smaller than that of a 32-bit floating-point fused-multiply-add (FMA) unit.\n\nLet us emphasize once again that the update of the format register is infrequent. Once set, all succeeding activation and weight values from the external memory are simply 8-bit FFP8 data transfers, as the figure demonstrates.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "9sF3n8eAco", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1646/Authors|ICLR.cc/2021/Conference/Paper1646/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Comment"}}}, {"id": "aYkeJ-w9ILV", "original": null, "number": 1, "cdate": 1603890915962, "ddate": null, "tcdate": 1603890915962, "tmdate": 1605024392690, "tddate": null, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review", "content": {"title": "More information about HW and Compiler support needed", "review": "## Summary:\n\nThe paper introduces a new floating point format FFP8 that can adaptively choose the exponent bias as well as the existence of the sign bit. FFP8 is more flexible than other 8-bit floating point formats with fixed exponent biases. The authors show how FF8 can be used to cover the dynamic range of NN weights and activations while still achieving higher precision than commonly used FP 8 formats.\n\n## Strengths:\n\n* Experiments on how various settings of FFP8 can be used to represent weights and activations of different layers and comparison with existing FP 8 formats.\n* Varying exponent bias is a good insight to shift the dynamic range of FP formats.\n\n## Weaknesses and Questions for the Authors\n\n* Limited details on how hardware support for FFP8 look like.\nThe authors claim in section 5 that the extra hardware support required for FFP8 would be minimal. However, no data or simulations are there to back this claim. In a possible hardware implementation of FFP8, each floating point instruction should be preceded by an instruction which sets (x,y,b) registers for FFP8 to F32 conversion. The frequency of setting these registers may vary based on the application, but nevertheless is an overhead compared to the current FP 8 implementations. The authors should quantify this overhead with a hardware simulation. Alternatively, the authors can describe a different and an efficient implementation of FP 8. This is an important detail that is missing from the paper.\n* Compiler support for FFP8\n The authors have not described how FFP 8 would be emitted, specially when different settings are used for different layers. Do you expect the programmer to hand code this? Or will there be compiler support? If so, what would the compiler support look like? What would the compiler optimization algorithm look like to produce FFP 8 code? These details are not answered by the paper.\n* More evaluation on other commonly used NNs\nThe authors present results for VGG-16. I would like to see more results on different networks. The authors can cut down on some of the graphs or make them smaller to get more space (specially Figure 5). Also, in general I feel like rather than giving out all the results that bring us to the same conclusion, the authors can summarize the results or graphs to make the message / conclusion of the paper more clearer.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113916, "tmdate": 1606915775633, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1646/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review"}}}, {"id": "BuPDcYXImy", "original": null, "number": 2, "cdate": 1603901629894, "ddate": null, "tcdate": 1603901629894, "tmdate": 1605024392626, "tddate": null, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review", "content": {"title": "paper lacks clear objective and novelty", "review": "This paper explores 8-bit floating point formats for the inference of deep neural networks. The quantization is applied on the weight and activation tensors, but computation engine remains in FP32.  To cover the different ranges of weight and activation tensors, the authors propose to use exponent bias. The authors did ablation studies on the impact of numerical formats, e.g. bit-width and exponent biases, on model accuracies. The experiments are performed on vision models on ImageNet, including VGG and ResNet 50/34/18. \n\nThe paper is relatively clear written, and experiments seem sound, however, I have some major concerns on the objective and novelty of this paper. \n\n1). It is not clear to me the objective of this paper. This paper introduces an 8-bit quantized inference framework. However, it is well-known that, 8-bit precision can be applied to popular DNN models to accelerate inference while maintaining model accuracies and many such systems have already been put into products (e.g. TPU). The state-of-the-art inference quantization work are primarily in the precision of 4 bit or less. But this paper did not compare their work to any of the latest inference works. Instead, the three references this paper compared to, i.e. (Wang & Choi, 2018; Cambier et al., 2020; Sun et al., 2019), are all for the acceleration of DNN TRAINING, where the challenging can be very different, e.g. quantization of gradient tensors.\n\n2). On the same note, this paper claimed advantages of using FP32 computation engine, however, the three training papers, by definition, can also be used in FP32 computations through the same high precision converting described in this paper. In addition, it is not clear to me how much one can benefit from quantization while keeping computation in 32bit? For example, one may save some memories space, but the whole inference will be limited by the FP32 computations in terms of throughput, latency, power and cost. Can the authors provide an application case where only quantizing data is beneficial?\n\n3). The main contribution claimed in the paper are to use exponent bias to cover tensors with different range of distribution. However, this technique has be introduced and discussed in (Sun et at., 2019) paper and has also been investigated in detail in (Cambier et al., 2020) paper. The authors of this paper did not provide any new insight although using in a much simpler case, i.e. only forward paths.\n\nI think this paper lacks clear objective, novelty and insightful analysis, therefore not good enough for ICLR.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113916, "tmdate": 1606915775633, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1646/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review"}}}, {"id": "lNbu8u5AsKO", "original": null, "number": 3, "cdate": 1603934049988, "ddate": null, "tcdate": 1603934049988, "tmdate": 1605024392546, "tddate": null, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review", "content": {"title": "New flexible floating point format - review", "review": "The paper proposes a new flexible floating point format (FFP8) on 8 bits, to help alleviate the high memory demand of deep networks inference, while preserving high accuracy. There is a large body of literature on reducing the data format, typically from 32 bits to 16, 8 and even below. There is previous work on using an 8-bit floating point FP(8), usually (1,4,3) or (1,5,2) where 1 bit is used for sign, 5 or 4 bits are used for the exponent and 3 or 2 bits are used for the fraction. \n\nThe new FFP8 proposed in the paper offers more configurable parameters: (1) the bit width of exponent/fraction, (2) the exponent bias (usually this is implicit in FP8 but it can be changed in FFP8), (3) the presence of the sign bit (this can be removed and the bit can be used for exponent or fraction). By allowing flexible control of the dynamic range of the 8-bit FFP8, the accuracy loss is minimized. The authors observe that both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNNs. Therefore, the variable exponent size and exponent bias are better suited. Some activation functions always produce non-negative results (e.g., ReLU), therefore the sign bit can be reclaimed. \n\nThe paper includes experimental evaluation where a best fit format selection shows a minimal accuracy loss for the VGG-16 network. Further optimization can be achieved over each layer, because distributions across different layers are dissimilar for both weights and activations, and the distribution of an individual layer is typically narrower than the one of the entire model.\n\nI advocate for the acceptance of the paper. Although there is a long line of research examining the data format for training and inference in deep networks, the paper provides evidence of the usefulness of some flexibility in the 8-bit format. The proposed would not require hardware changes except for simple translations between FP32 and FFP8.\n\nMinor comments / typos:\n\nFig. 4 and 5 - the vertical axis might look better with the same max values (range).\n\npg.2 : Moreover, recent studies proposed several training frameworks that __producing__ weights only in 8-bit floating-point formats\n\npg.3 : Note that there is actually only one parameter, the bit width of the exponent (y), __that__ can be freely chosen when defining", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113916, "tmdate": 1606915775633, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1646/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review"}}}, {"id": "wNn5UIC_-29", "original": null, "number": 4, "cdate": 1604292836409, "ddate": null, "tcdate": 1604292836409, "tmdate": 1605024392484, "tddate": null, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "invitation": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review", "content": {"title": "Promising idea but no end-to-end experiment ", "review": "This paper presents a flexible floating-point format that can save storage space by being highly configurable in terms of the bit width of the exponent/fraction field, the exponent bias, and the presence of the sign bit. The experiments in the paper demonstrate that the proposed format achieves a very low accuracy loss of < 0.3% compared to the regular float32 format for several popular image classification models. \n\nStrengths\n---\n- The concept of flexible floating-point format makes sense and can potentially result in significant space savings for large models. \n- The loss drop in the models compared to the standard float32 format seems to be minimal\n- The authors evaluated the proposed format over multiple models: VGG 16, ResNet-50, ResNet-34, and ResNet-18 \n\nWeaknesses\n---\n- No full system implementation to demonstrate the performance/memory gains. I that the updates to the form registers will be infrequent and cheap \n- The hardware details are very sketchy. Figure 6 seems to be only showing the high-level components.\n\n\nOverall, the paper\u2019s ideas are promising but my score reflects the fact that the authors presented end-to-end experiments demonstrating the performance/storage gains.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1646/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1646/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks", "authorids": ["~Juinn-Dar_Huang1", "~Cheng-Wei_Huang1", "~Tim-Wei_Chen1"], "authors": ["Juinn-Dar Huang", "Cheng-Wei Huang", "Tim-Wei Chen"], "keywords": ["8-bit floating-point format", "accuracy loss minimization", "numerics", "memory-efficient inference", "deep learning"], "abstract": "Modern deep neural network (DNN) models generally require a huge amount of weight and activation values to achieve good inference outcomes. Those data inevitably demand a massive off-chip memory capacity/bandwidth, and the situation gets even worse if they are represented in high-precision floating-point formats. Effort has been made for representing those data in different 8-bit floating-point formats, nevertheless, a notable accuracy loss is still unavoidable. In this paper we introduce an extremely flexible 8-bit floating-point (FFP8) format whose defining factors \u2013 the bit width of exponent/fraction field, the exponent bias, and even the presence of the sign bit \u2013 are all configurable. We also present a methodology to properly determine those factors so that the accuracy of model inference can be maximized. The foundation of this methodology is based on a key observation \u2013 both the maximum magnitude and the value distribution are quite dissimilar between weights and activations in most DNN models. Experimental results demonstrate that the proposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim 0.3\\%$ for several representative image classification models even without the need of model retraining. Besides, it is easy to turn a classical floating-point processing unit into an FFP8-compliant one, and the extra hardware cost is minor.", "one-sentence_summary": "An extremely flexible 8-bit floating-point format, where all parameters (bit width of sign/exponent/fraction field and exponent bias) are configurable, is proposed to achieve more accurate inference even without the need of model retraining.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|allyoucanfit_8bit_flexible_floatingpoint_format_for_accurate_and_memoryefficient_inference_of_deep_neural_networks", "pdf": "/pdf/93c33281fea918bdbcd3edc58fc0a0bbdd16909f.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=iBX2TI_9m", "_bibtex": "@misc{\nhuang2021allyoucanfit,\ntitle={All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and Memory-Efficient Inference of Deep Neural Networks},\nauthor={Juinn-Dar Huang and Cheng-Wei Huang and Tim-Wei Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=9sF3n8eAco}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "9sF3n8eAco", "replyto": "9sF3n8eAco", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1646/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113916, "tmdate": 1606915775633, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1646/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1646/-/Official_Review"}}}], "count": 14}