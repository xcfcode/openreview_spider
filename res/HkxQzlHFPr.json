{"notes": [{"id": "HkxQzlHFPr", "original": "Hye-0WgFDH", "number": 2167, "cdate": 1569439754938, "ddate": null, "tcdate": 1569439754938, "tmdate": 1577168270504, "tddate": null, "forum": "HkxQzlHFPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "weiwang1@fudan.edu.cn"], "title": "Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out", "authors": ["Wanyun Cui", "Guangyu Zheng", "Wei Wang"], "pdf": "/pdf/637863d5dc750bb3be69581016870fe1895ee851.pdf", "TL;DR": "We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic.", "abstract": "In natural language inference, the semantics of some words do not affect the inference. Such information is considered superficial and brings overfitting. How can we represent and discard such superficial information? In this paper, we use first order logic (FOL) - a classic technique from meaning representation language \u2013 to explain what information is superficial for a given sentence pair. Such explanation also suggests two inductive biases according to its properties. We proposed a neural network-based approach that utilizes the two inductive biases. We obtain substantial improvements over extensive experiments.", "keywords": ["natural language inference", "first order logic"], "paperhash": "cui|robust_natural_language_representation_learning_for_natural_language_inference_by_projecting_superficial_words_out", "original_pdf": "/attachment/637863d5dc750bb3be69581016870fe1895ee851.pdf", "_bibtex": "@misc{\ncui2020robust,\ntitle={Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out},\nauthor={Wanyun Cui and Guangyu Zheng and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQzlHFPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mQm0s8ccJo", "original": null, "number": 1, "cdate": 1576798742242, "ddate": null, "tcdate": 1576798742242, "tmdate": 1576800893976, "tddate": null, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "invitation": "ICLR.cc/2020/Conference/Paper2167/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes using first order logic to rule out superficial information for improved natural language inference. While the topic is of interest, reviewers find that the paper misses much of the previous literature on semantics which is highly relevant. \n\nI thank the authors for submitting this paper to ICLR. Please take the reviewers' comments, especially recommended references, to improve the paper for future submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "weiwang1@fudan.edu.cn"], "title": "Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out", "authors": ["Wanyun Cui", "Guangyu Zheng", "Wei Wang"], "pdf": "/pdf/637863d5dc750bb3be69581016870fe1895ee851.pdf", "TL;DR": "We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic.", "abstract": "In natural language inference, the semantics of some words do not affect the inference. Such information is considered superficial and brings overfitting. How can we represent and discard such superficial information? In this paper, we use first order logic (FOL) - a classic technique from meaning representation language \u2013 to explain what information is superficial for a given sentence pair. Such explanation also suggests two inductive biases according to its properties. We proposed a neural network-based approach that utilizes the two inductive biases. We obtain substantial improvements over extensive experiments.", "keywords": ["natural language inference", "first order logic"], "paperhash": "cui|robust_natural_language_representation_learning_for_natural_language_inference_by_projecting_superficial_words_out", "original_pdf": "/attachment/637863d5dc750bb3be69581016870fe1895ee851.pdf", "_bibtex": "@misc{\ncui2020robust,\ntitle={Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out},\nauthor={Wanyun Cui and Guangyu Zheng and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQzlHFPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725414, "tmdate": 1576800277300, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2167/-/Decision"}}}, {"id": "r1lxoVPRKB", "original": null, "number": 1, "cdate": 1571873943586, "ddate": null, "tcdate": 1571873943586, "tmdate": 1572972374406, "tddate": null, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "invitation": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper uses first order logic (FOL) to help reduce so-called \u201csuperficial\u201d information/semantics that is less relevant to the judgement of natural language inference relations. The submission misses the major literature of and comparison to previous work that uses FOL for natural language inference (aka. RTE), for example, [Bos and Markert \u201805], [Beltagy et al. \u201813], [Abzianidze \u201817], among others, as well as work based on natural logic, e.g., [MacCartney \u201809], which operates directly on parsed sentences. The submission contains little contribution with regard to the exiting work. Key concepts such as \u201csuperficial semantics\u201d is vague and not well defined. I do not recommend it for the conference. \n\nBos and Markert \u201805, Recognising Textual Entailment with Robust Logical Inference. \nBeltagy et al. \u201813, Montague Meets Markov: Deep Semantics with Probabilistic Logical Form.\nAbzianidze \u201917, A Natural Proof System for Natural Language.\nMacCartney \u201909, natural language inference (PhD thesis)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "weiwang1@fudan.edu.cn"], "title": "Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out", "authors": ["Wanyun Cui", "Guangyu Zheng", "Wei Wang"], "pdf": "/pdf/637863d5dc750bb3be69581016870fe1895ee851.pdf", "TL;DR": "We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic.", "abstract": "In natural language inference, the semantics of some words do not affect the inference. Such information is considered superficial and brings overfitting. How can we represent and discard such superficial information? In this paper, we use first order logic (FOL) - a classic technique from meaning representation language \u2013 to explain what information is superficial for a given sentence pair. Such explanation also suggests two inductive biases according to its properties. We proposed a neural network-based approach that utilizes the two inductive biases. We obtain substantial improvements over extensive experiments.", "keywords": ["natural language inference", "first order logic"], "paperhash": "cui|robust_natural_language_representation_learning_for_natural_language_inference_by_projecting_superficial_words_out", "original_pdf": "/attachment/637863d5dc750bb3be69581016870fe1895ee851.pdf", "_bibtex": "@misc{\ncui2020robust,\ntitle={Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out},\nauthor={Wanyun Cui and Guangyu Zheng and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQzlHFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575603661062, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2167/Reviewers"], "noninvitees": [], "tcdate": 1570237726739, "tmdate": 1575603661075, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review"}}}, {"id": "rke9JLOAYS", "original": null, "number": 2, "cdate": 1571878370186, "ddate": null, "tcdate": 1571878370186, "tmdate": 1572972374359, "tddate": null, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "invitation": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tried to reduce superficial information in natural language inference (NLI) to prevent overfitting. It utilized the first order logic to explain what is superficial information.\nThen, it introduced a superficial factors in the existing neural networks. Furthermore, they introduce a graph neural network (GNN) to model relation between premise and hypothesis.\nIt was evaluated on a bunch of NLI benchmarks including SNLI, SciTail, MNLI etc, showing the effectiveness of the proposed model. \n\nThis paper is well motivated and the ideas are interesting. However, there are a few concerns detailed as follows: \n\n1. Do these methods only work on small tasks? For example, the big improvement only appears in small tasks such as MRPC and RTE. However, the proposed method experiences performance decreases on large tasks such as SNLI and MNLI. E.g., in CAFE settings, the proposed approach got  75.2/74.7 (matched/mismatched) vs 76.3/76 (baselines). The similar observations are found in MwAN settings on MNLI and SNLI. I\u2019d like to see some discussion in the paper on this.\n\n2. What common logic patterns did the model learn when removing all the superficials? For different relations, e.g., entailment vs contradiction, are these patterns different? This may help the reader understand whether the model really filtered these information. \n\n3. It requires more discussions on Table 5. E.g., in NLI (2 classes), the random guess should be 50%. But the model performance was 41.4% on CAFE when transferring from RTE to SciTail, which is even worse than random guess. In contrary, when transferring from SciTail to RTE, the model performance was 56.1%, which seems reasonable. I believe more analysis and discussions are required to understand this model.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "weiwang1@fudan.edu.cn"], "title": "Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out", "authors": ["Wanyun Cui", "Guangyu Zheng", "Wei Wang"], "pdf": "/pdf/637863d5dc750bb3be69581016870fe1895ee851.pdf", "TL;DR": "We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic.", "abstract": "In natural language inference, the semantics of some words do not affect the inference. Such information is considered superficial and brings overfitting. How can we represent and discard such superficial information? In this paper, we use first order logic (FOL) - a classic technique from meaning representation language \u2013 to explain what information is superficial for a given sentence pair. Such explanation also suggests two inductive biases according to its properties. We proposed a neural network-based approach that utilizes the two inductive biases. We obtain substantial improvements over extensive experiments.", "keywords": ["natural language inference", "first order logic"], "paperhash": "cui|robust_natural_language_representation_learning_for_natural_language_inference_by_projecting_superficial_words_out", "original_pdf": "/attachment/637863d5dc750bb3be69581016870fe1895ee851.pdf", "_bibtex": "@misc{\ncui2020robust,\ntitle={Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out},\nauthor={Wanyun Cui and Guangyu Zheng and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQzlHFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575603661062, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2167/Reviewers"], "noninvitees": [], "tcdate": 1570237726739, "tmdate": 1575603661075, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review"}}}, {"id": "Byx4K0PJqH", "original": null, "number": 3, "cdate": 1571942012100, "ddate": null, "tcdate": 1571942012100, "tmdate": 1572972374315, "tddate": null, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "invitation": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presents an approach to treat natural language inference using first-order logic, and to infuse neural NLI models with logical information to be more robust at inference. However, the paper does not contain a single reference to the computational semantics literature, where logical approaches towards semantics were the dominant trend for many years (see e.g. [1, 2]). Indeed, 'neuralising' first order logic has been an active area of recent research ([3] or indeed much of the recent work coming from Sebastian Riedel's group). This is a glaring oversight.\n\nThe paper starts by introducing background on first-order logic, and then gives a definition of a 'superficial' predicate, namely one whose extension is not necessary to prove an implication for any collection of background facts. However, by extension, this makes s_1 -> s_2 a tautology, which is the 'true' notion that the authors are looking for. Indeed, if |- (s_1 -> s_2), then for any collection of formulae \\Delta then \\Delta |- (s_1 -> s_2) (by monotonicity of entailment) and clearly if for any \\Delta we have \\Delta |- (s_1 -> s_2), we can take \\Delta to be the empty set. Finally, the authors show that tautologies are still tautologies under change of predicates (i.e. if we only require logical rules to prove one statement from another, then the extensions of predicates in those statements do not matter).\n\nThe authors then use this to motivate two extensions to inference models. One is to 'drop out' word information, and the other is to treat different occurrences of the same word as reflecting the same underlying predicate. The first somewhat transparently forces the model to care less about the exact meaning (i.e. extension in the logical world) of words (indeed, word vectors have been shown to capture extensional information [4, 5]), and so may force the inference model to learn more 'logical' inference rules. Further, the word dropout calculation includes whether the word is in both sentences, which is a strong signal that its extension may not be necessary. However, the second only forces the intuition that different mentions of the same word are likely to be coreferent, which is a weak assumption that models may already pick up. Indeed, it is noticeable that this component seems to be less necessary in the authors' ablation study.\n\nIn summary, while I am sympathetic to the aim of grounding neural models in explicit notions of semantics, this paper shows such a lack of awareness of previous literature that I cannot recommend acceptance. \n\n[1] The Meaning Factory: Formal Semantics for Recognizing Textual Entailment and Determining Semantic Similarity, Bjerva et al. 2014\n[2] Natural Logic for Textual Inference, MacCartney and Manning 2009\n[3] End-to-end Differentiable Proving, Rocktaschel and Riedel 2017\n[4] Building a shared world: mapping distributional to model-theoretic semantic spaces, Vecchi and Herbelot 2015\n[5] Deriving Boolean structures from distributional vectors, Kreuzewski et al 2015"}, "signatures": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2167/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cui.wanyun@sufe.edu.cn", "simonzgy@outlook.com", "weiwang1@fudan.edu.cn"], "title": "Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out", "authors": ["Wanyun Cui", "Guangyu Zheng", "Wei Wang"], "pdf": "/pdf/637863d5dc750bb3be69581016870fe1895ee851.pdf", "TL;DR": "We use neural networks to project superficial information out for natural language inference by defining and identifying the superficial information from the perspective of first-order logic.", "abstract": "In natural language inference, the semantics of some words do not affect the inference. Such information is considered superficial and brings overfitting. How can we represent and discard such superficial information? In this paper, we use first order logic (FOL) - a classic technique from meaning representation language \u2013 to explain what information is superficial for a given sentence pair. Such explanation also suggests two inductive biases according to its properties. We proposed a neural network-based approach that utilizes the two inductive biases. We obtain substantial improvements over extensive experiments.", "keywords": ["natural language inference", "first order logic"], "paperhash": "cui|robust_natural_language_representation_learning_for_natural_language_inference_by_projecting_superficial_words_out", "original_pdf": "/attachment/637863d5dc750bb3be69581016870fe1895ee851.pdf", "_bibtex": "@misc{\ncui2020robust,\ntitle={Robust Natural Language Representation Learning for Natural Language Inference by Projecting Superficial Words out},\nauthor={Wanyun Cui and Guangyu Zheng and Wei Wang},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxQzlHFPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxQzlHFPr", "replyto": "HkxQzlHFPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2167/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575603661062, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2167/Reviewers"], "noninvitees": [], "tcdate": 1570237726739, "tmdate": 1575603661075, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2167/-/Official_Review"}}}], "count": 5}