{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1492480319953, "tcdate": 1492448838966, "number": 27, "id": "SyyI8_GCx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Training set for SVHN domain?", "comment": "In section 5.1, you said that you employ the extra training split of SVHN for two purposes: learning the function f and as a unsupervised training set s. So can I say that, in Table 2, your results are based on SVHN 'extra' training split as s domain traning set? But it is different from other methods since SVHN dataset includes train set, test set and extra training split. As far as I know, other methods are using train set as the s domain unsupervised tranining data. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489324280172, "tcdate": 1478300500845, "number": 533, "id": "Sk2Im59ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sk2Im59ex", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 33, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396662289, "tcdate": 1486396662289, "number": 1, "id": "SJCeTGU_g", "invitation": "ICLR.cc/2017/conference/-/paper533/acceptance", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors propose a application of GANs to map images to new domains with no labels. E.g., an MNIST 3 is used to generate a SVHN 3. Ablation analysis is given to help understand the model. The results are (subjectively) impressive and the approach could be used for cross-domain transfer, an important problem. All in all, a strong paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396662793, "id": "ICLR.cc/2017/conference/-/paper533/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396662793}}}, {"tddate": null, "tmdate": 1485193174980, "tcdate": 1482182967109, "number": 1, "id": "B1JrbRHVe", "invitation": "ICLR.cc/2017/conference/-/paper533/official/review", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/conference/paper533/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper533/AnonReviewer2"], "content": {"title": "Final review", "rating": "7: Good paper, accept", "review": "Update: After reading the rebuttal comments and the revised paper, I'm leaving the rating as it was before.\n\nThis paper proposes an unsupervised algorithm for transferring samples from one domain to another (related) domain under the constraint that some predefined f returns same result for the input and the result.\n\nPros:\n1. The paper presents an interesting idea of comparing samples from different domains using a fixed perceptual function f.\n\n2. The proposed method produces visually appealing results on several datasets\n\n3. The authors demonstrate how their approach can be used for domain adaptation and obtain improved results on the SVHN->MNIST task\n\n4. The paper is well-written and easy to read\n\nCons:\n1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution)\n\n2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain  2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f.\n\n3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches.\n\nI would also like to point out that using super-resolved outputs as opposed to the actual model\u2019s outputs can produce a false impression of the visual quality of the transferred samples. I\u2019d suggest moving original outputs from the appendix into the main part.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483103337049, "id": "ICLR.cc/2017/conference/-/paper533/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper533/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper533/AnonReviewer2", "ICLR.cc/2017/conference/paper533/AnonReviewer1", "ICLR.cc/2017/conference/paper533/AnonReviewer3"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483103337049}}}, {"tddate": null, "tmdate": 1485161339716, "tcdate": 1485161339716, "number": 26, "id": "HyNYXHXvg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~yunjey_choi1"], "readers": ["everyone"], "writers": ["~yunjey_choi1"], "content": {"title": "Interesting Work", "comment": "This paper presents an unsupervised domain transfer from the image of domain S to the image of domain T. \nIt was really refreshing that this conversion was possible without any mapping data. \nFor example, in the paper, the model can transfer the SVHN image '3' to the MNIST image '3' without the mapping data.\nThe model can be roughly divided into GAN and Content Extractor (f in the paper).\n\n1. GAN\nDuring training, the discriminator sees the mnist image and learns to determine it as a real image. \nAnd with GAN loss, the generator learns to get the mnist image as output when it receives an svhn image as input to deceive the discriminator.\n\n2. Content Extractor\nIf the model use only GAN loss, the content in the image may not be retained even if the domain is changed.\nFor example, the generator may convert the svhn image '3' to the mnist image '2' to deceive the discriminator.\nIn this paper, authors introduce a new function called 'f' to maintain the content.\nThe generator includes f and generates a fake mnist image when it receives an svhn image as input.\nThe original svhn image and the generated fake mnist image are put back into f.\nThen additional loss function is set so that the resulting values \u200b\u200bare the same.\nHere, f is learning to extract content regardless of domain.\n\n\nI felt very fresh in this paper so i implemented this paper myself.\nHere is the code I implemented.\n\nhttps://github.com/yunjey/dtn-tensorflow"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1484914636909, "tcdate": 1484914636909, "number": 25, "id": "B1H0yKyDl", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "rJFsDvIVx", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "Re: Updated Review", "comment": "Dear reviewer,\n\nThank you for updating the review!\n\nThe authors."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1484580784193, "tcdate": 1484580784193, "number": 24, "id": "Hk_hDPcLx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "Authors' summary of discussion", "comment": "We thank the reviewers for their time and insights. All 3 reviewers seem to agree that the work is interesting, well-written and presents extensive experiments. R1 & R2 both note the f-constancy as a novelty of this work. Also, the fact that the method does not require training pairs for the two domains is noted by R3 as a major contribution, which \u201ccould be impactful in broad problem context\u201d. R2 & R3 both agree that the output generations are visually appealing. We have no factual dispute with the reviewers and replied to each individually below.\n\nThe open review discussion has been extremely beneficial to us and the paper has been revised in order to address all actionable items raised throughout the review period. We thank the reviewers for their thoughtful and constructive reviews and all other community members who shared their comments. During the review period, the work has already been cited several times, reimplemented on github, and drawn considerable attention.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1484344854827, "tcdate": 1482221472916, "number": 2, "id": "rJFsDvIVx", "invitation": "ICLR.cc/2017/conference/-/paper533/official/review", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/conference/paper533/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper533/AnonReviewer1"], "content": {"title": "Interesting work, needs more explanations ", "rating": "6: Marginally above acceptance threshold", "review": "Update: thank you for running more experiments, and add more explanations in the manuscript. They addressed most of my concerns, so I updated the score accordingly. \n\n\nThe work aims at learning a generative function G that can maps input from source domain to the target domain, such that a given representation function f remain unchanged accepting inputs from either domain. The criteria is termed f-constancy. The proposed method is evaluated on two visual domain adaptation tasks. \n\nThe paper is relatively easy to follow, and the authors provided quite extensive experimental results on the two datasets. f-constancy is the main novelty of the work. \n\nIt seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset.  As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. \n\nDo the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly?\n\nFigure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483103337049, "id": "ICLR.cc/2017/conference/-/paper533/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper533/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper533/AnonReviewer2", "ICLR.cc/2017/conference/paper533/AnonReviewer1", "ICLR.cc/2017/conference/paper533/AnonReviewer3"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483103337049}}}, {"tddate": null, "tmdate": 1483446361867, "tcdate": 1483446361867, "number": 23, "id": "BJGDuzYHx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "rJFsDvIVx", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"title": "Re: Review", "comment": "Dear reviewer, \n\nIn addition to information provided below (\"More explanations\") we would like to note that we just performed the expression transfer experiment you mentioned in your review. As it turns out, the same f can be used for both identity and expression!\n\nWe believe that the additional information we provide at openreview.net as well as the manuscript revision could justify a second view and would be very much interested in an open discussion in order to find out if there are any remaining unfavorable factors.\n\nThank you,\n\nThe authors\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1483446227367, "tcdate": 1483446227367, "number": 22, "id": "B1jCDMKBl", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "B1JrbRHVe", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"title": "Re: Review", "comment": "Thank you for your positive and supporting review. Regarding the few observed disadvantages:\n\n[AnonReviewer2:] 1. The novelty of the method is relatively minor (I consider f-constancy term as the main contribution) \n\n[Authors:] There are a few very recent and concurrent publications that do images analogies. All require paired training samples. Our work is unique in its unsupervised nature, which we think is noteworthy. f-constancy is the means of obtaining this.\n\n[AnonReviewer2:] 2. It feels like the proposed approach would break for more dissimilar domains. The method relies on a fixed f which is trained on the source domain. This f can potentially drop information important for obtaining 1) better reconstructions in the target domain 2) more tightly related x and g(f(x)). I think the authors should consider either training all the modules in the model end-to-end or incorporating target samples into the training of f. \n\n[Authors:]  In our experiments, f is demonstratively more powerful in S than in T (Tab. 1 last row). Still we observe what we call \u201cthe magic of analogies\u201d \u2014 an f that is rather weak on T as a classifier is good enough in order to create analogies using nothing more than unlabeled examples from both domains. Based on these analogies we can transfer labels from S and create a good classifier in T (Tab. 2). \n\nAs the experiment in the second revision show, the f trained on photos can maintain not only identity information in T but also expression information. This further supports that even when f is trained separately on S, f-constancy assures that x and g(f(x)) are tightly related along many dimensions, including dimensions that f is supposedly invariant to.\n\n[AnonReviewer2:] 3. A single domain adaptation experiment is definitely not enough to consider the proposed method as a universal alternative to the existing DA approaches. \n\n[Authors:] We agree. The purpose of the DA experiment is to show that the reduction of the DA problem to the domain transfer problem (Sec. 2) is viable. While the experiment is very successful, it is indeed only a single experiment and is not meant to shift the focus on the paper from the new cross domain transfer problem to the well explored problem of DA.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1483383400875, "tcdate": 1483383400875, "number": 21, "id": "B1-uGX_Sg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "HyxO2R7Be", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "An interesting work", "comment": "We thank the reviewer for the comments. Indeed, the main invention is learning to map without the supervision of matching pairs. \n\nThe reviewer pointed to two shortcomings of the experiments.\nReviewer: -It will be more interesting to show results in other domains such as texts and images. \n\nAuthors: Indeed, the options are endless. Analogies are a very powerful tool, and making these in an unsupervised manner has a lot of applications that are not explored in this manuscript. However, we are afraid that this is beyond the scope of the current submission.\n\nReviewer: -In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain.\n \nAuthors: Following the request of both AnonReviewer1 and AnonReviewer3, we added new experiments and revised our manuscript. As it turns out the face identification network does contain enough expression information to allow cross-domain transfer of expression. All that was needed is to provide unlabeled smiling emoji so that the discriminator would not identify smiling emoji as fake. Please see our subsequent comment regarding the revised version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1483383132212, "tcdate": 1483383132212, "number": 20, "id": "B1NvWQuBx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "Revision #2", "comment": "Following the request of the reviewing team, we have just uploaded a new version of our manuscript, which includes expression preserving experiments. \nThe new experiments are in Appendix B. \n\nIn order to provide a quick way to track the changes from the original submission, we color all modifications in red. \n\nThank you for the extremely useful feedback.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1483103336244, "tcdate": 1483103336244, "number": 3, "id": "HyxO2R7Be", "invitation": "ICLR.cc/2017/conference/-/paper533/official/review", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/conference/paper533/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper533/AnonReviewer3"], "content": {"title": "An interesting work", "rating": "7: Good paper, accept", "review": "This paper presents an unsupervised image transformation method that maps a sample from source domain to target domain. The major contribution lies in that it does not require aligned training pairs from two domains. The model is based on GANs. To make it work in the unsupervised setting, this paper decomposes the generation function into two modules: an encoder that identify a common feature space between two domains and an decoder that generates samples in the target domain. To avoid trivial solutions, this paper proposed two additional losses that penalize 1) the feature difference between a source sample and its transformed sample and 2) the pixel difference between a target sample and its re-generated sample. This paper presents extensive experiments on transferring SVHN digit images to MNIST style and transferring face images to emoji style. \n+The proposed learning method enables unsupervised domain transfer that could be impactful in broad problem contexts. \n+This paper presents careful ablation studies to analyze the effects of different components of the system, which is helpful for understanding the paper. \n+The transferred images are visually impressive and quantitative results also show the image identities are preserved across domains to some degree. \n-It will be more interesting to show results in other domains such as texts and images. \n-In addition to the face identities, it is also of great interest to analyze how well the facial attributes are preserved when mapping to target domain. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483103337049, "id": "ICLR.cc/2017/conference/-/paper533/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper533/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper533/AnonReviewer2", "ICLR.cc/2017/conference/paper533/AnonReviewer1", "ICLR.cc/2017/conference/paper533/AnonReviewer3"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483103337049}}}, {"tddate": null, "tmdate": 1482265616731, "tcdate": 1482265616731, "number": 19, "id": "H1KGNfvNx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "rJFsDvIVx", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "More explanations", "comment": "Thank you for your review. \n\n**1.  AnonReviewer1: f-constancy is the main novelty of the work. **\n\nAuthors: Thank you for noting the novelty. A few prominent recent contributions present image to image mapping using supervised (input image,output image) pairs. We are unique in that we do not require such pairs. This is quite remarkable and is achieved using f-constancy among other contributions. \n\n**2.  AnonReviewer1: It seems counter-intuitive to force the function G to be of g o f, i.e., starting from a restricted function f which might have already lost information. **\n\nAuthors:  Since L_CONST requires the preservation of identity, the information in f is exactly the relevant information. Experimentally, we show a very strong advantage over the baseline method \"DTN G does not contain f\" in Table 1.\n\n**3.  AnonReviewer1: As in the face dataset, f is learned to optimize the performance of certain task on some external dataset. It is not clear if an input from the source or target domain can be recovered from applying G as in equation (5) and (6). **\n\nAuthors: We respectfully disagree. The fact that the generated caricatures are remarkably identifiable (median rank of 16 out of 100,000 for retrieving among real images) clearly shows that identity information is extremely well preserved.\n\n**4.  AnonReviewer1: Also, the f function is learned with a particular task in mind. As in the two experiments, the representation function f is learned to identify the digits in the source SVHN dataset or the identity of some face dataset. As a result, the procedure has to be repeated if we were to perform domain adaptation for the same domains but for different tasks, such as recognizing expressions instead of identity. **\n\nAuthors: Yes, but this is desirable. For example, in the face experiments we wish to maintain identity and not other factors such as expression, illumination, head pose, etc.\n\n**5.  AnonReviewer1: Do the authors have insight on why the baseline method proposed in equation (1) and (2) perform so poorly? **\n\nAuthors: As stated above (2), incorporating f in G allows the network to focus on the most relevant aspects of this mapping. In the unsupervised setting this seems crucial.\n\n**6.  AnonReviewer1: Figure 5 shows some visual comparison between style transfer and the proposed method. It is not clear though which method is better. **\n\nAuthors: Figure 5 aims to clarify the major difference between the tasks of style-transfer and domain transfer:  \"the output image [in 5(c)] is perhaps visually appealing; However, it does not belong to the space t of emoji\". Therefore, style transfer does not perform cross-domain generation and solves a different problem. \n\n**7.  AnonReviewer1: Will it be possible to apply style transfer to generate emojis from photos and repeat the experiments shown in table 4?**\n\nAuthors: Style transfer is not relevant to Table 4 since, as stated above, it does not create an emoji image. It is therefore not comparable to the other methods in the table.\n\n***\n\nSince AnonReviewer1 asked for additional explanations, we respectfully ask that the reviewer consider updating the review based on our clarifications or let us know if these are not satisfactory.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1482168373172, "tcdate": 1482168373172, "number": 18, "id": "Bkp4_qHVl", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "Revision", "comment": "Following the ongoing discussion with the reviewing team as well as with other readers, we have just uploaded a new version of our manuscript, which is aimed at improving clarity. \nIn order to provide a quick way to track the changes, we color all modifications in red. \nIn addition, as mentioned below, we will soon share our open implementation in Torch. \nThank you all for the extremely useful feedback."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1482150206127, "tcdate": 1482150206127, "number": 17, "id": "ByLSb8BEg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "HywugTNVe", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "Accuracy without reconstructing images in target domain", "comment": "Thank you for your question. Reconstructing images in the target domain (loss L_{TID} modulated by \\beta) helps to obtain better performance, but seem the least crucial amongst the generator's L_{G} losses, as we further discuss in Section 5.1 & Table 1 (where we also omit this loss)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1482113164624, "tcdate": 1482113134961, "number": 16, "id": "HywugTNVe", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~Xun_Huang1"], "readers": ["everyone"], "writers": ["~Xun_Huang1"], "content": {"title": "Accuracy without reconstructing images in target domain", "comment": "Given the goal of transferring samples from source domain to target domain, I am wondering whether it is necessary to simultaneously train the model to reconstruct images in the target domain. Any ablation studies on this?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1481447386813, "tcdate": 1481447386805, "number": 15, "id": "r1mJO5qXl", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "HkWTACumg", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"title": "Re: Explanations", "comment": "Thank you for your questions.\n\n1.  Equation 1 is the GAN risk for a binary GAN that compares \u201cfake\u201d (i.e, generated) samples crated by mapping samples in S to \u201creal\u201d samples from T. It evolves to both Equation 3 and Equation 4. The differences between 1 and 3+4 are that we moved from the language of risk, i.e., using expectations, to the language of loss, using sums (the writing as expectations and not as sums is a typo). Second, we moved from a binary D to a ternary D and distinguish between three populations, labeled respectively as classes 1,2,3: (1) G mapping of samples from s, (2) G mapping of samples from t, (3) the actual samples from t.\n\nEquations 3 and 4 can no longer be specified as one equation, since we want to be specific that G maps the first two classes to class 3. If G were to simply maximize 3, there would be other options, e.g., confusing classes 1 and 2.\n\n2.  Equations 3 and 4 make sure that the generated analogy, i.e., the output of G,  is in the target space T. However, they do not enforce any similarity between the source sample x and the generated G(x). This is done by Equations 5 and 6. Equation 5 enforces f-constancy for x \\in S. Equation 6 enforces that for samples x\\in T, which are already in the target space, G is the identity mapping. This is the desirable behavior, e.g., given an emoji, one would like it to remain the same emoji under the mapping of G. It can also be seen as an autoencoder type of  loss, applied only to samples from T. This example and motivation will be added to the revised manuscript.\n\nWhile Equation 5 is sufficient (see Tab. 1, DTN w/o L_{TID}), Equation 6 helps obtain better performance. Interestingly, Equation 6, even without Equation 5, is a strong enough constraint to ensure a meaningful mapping (see Tab 1, DTN w/o L_{CONST}). Removing both Equation 5 and Equation 6, (Tab1, DTN w/o L_{CONST} & L_{TID}) leads to a mapping that does not preserve identity, as expected. These observations appear in Sec. 5.1, and we will make sure to clarify the role of Equation 6 further.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1481334457319, "tcdate": 1481334457314, "number": 2, "id": "HkWTACumg", "invitation": "ICLR.cc/2017/conference/-/paper533/pre-review/question", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/conference/paper533/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper533/AnonReviewer1"], "content": {"title": "Explanations", "question": "1. Could you please explain equation (3)? I assume R_GAN in equation (1) translate to L_GANG in equation (4), what is the motivation behind (3)? \n2. Why do you want to enforce identify matrix on G as in equation (6)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481334457802, "id": "ICLR.cc/2017/conference/-/paper533/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper533/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper533/AnonReviewer2", "ICLR.cc/2017/conference/paper533/AnonReviewer1"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481334457802}}}, {"tddate": null, "tmdate": 1480838373538, "tcdate": 1480838373530, "number": 14, "id": "BJpyTHW7x", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "r1h8YLkmg", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"title": "Re: Questions", "comment": "Thank you for your questions!\n\n1. f-constancy is the mechanism that ensures that the analogy is valid, i.e., that the mapping of a sample from S to T maintains a similarity as defined by f. For example, the DeepFace representation ensures that the created emoji maintains the identity information of the original image. We will make sure to clarify this point in the abstract and introduction.\n\n2.  (i) As mentioned in the manuscript, our GAN follows DCGAN (Radford et al. 2015). No special tricks were applied. \n(ii) Stopping was performed after 3 epochs. \n(iii) As mentioned in Sec. 5.2, the upscaling to 152x152 was done in order to employ a pretrained $f$. We failed to mention the exact upscaling method \u2014 it was done using a simple bilinear upscaling. The superresolution method was not applied during training and is only employed to create print quality images. The appendix shows the results without it. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1480730209621, "tcdate": 1480710484340, "number": 1, "id": "r1h8YLkmg", "invitation": "ICLR.cc/2017/conference/-/paper533/pre-review/question", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["ICLR.cc/2017/conference/paper533/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper533/AnonReviewer2"], "content": {"title": "Questions", "question": "1. From the introduction and abstract it's not clear what do the authors mean by f. Why would anyone care about f and f-constancy? It would be nice if that could be explained in the very beginning.\n\n2. In the emoji experiment, did you apply any tricks to make GAN training stable? What was the stopping criterion for the training procedure? Is superresolution applied during training too (to upscale model outputs to 152x152 and match the resolution of the real data)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481334457802, "id": "ICLR.cc/2017/conference/-/paper533/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper533/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper533/AnonReviewer2", "ICLR.cc/2017/conference/paper533/AnonReviewer1"], "reply": {"forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper533/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481334457802}}}, {"tddate": null, "tmdate": 1479115179958, "tcdate": 1479115179951, "number": 13, "id": "ry42-WPWg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "HkqzxaS-x", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "About the configuration of the generator", "comment": "Very glad to hear that. In my re-implementation, the architecture of D is 1x32x32 -> 128x16x16 -> 256x8x8 -> 512x4x4 -> 3(reshape and fully connected), all the conv kernels are 5x5. The architecture of G is 128x1 -> 512x4x4 -> 256x8x8 -> 128x16x16 -> 1x32x32 as you suggested, all the deconv kernels are 5x5. The solver uses AdamOptimizer, beta1 = 0.5, learning rate is set to 0.0002, alpha=beta=15. Before sending the target MNIST images and generated source images into f, they are replicated three times. The D_loss and G_loss are computed using softmax_with_cross_entropy_loss with different labels. According to this setting, I can generate MNIST images very well but fail to adapt the SVHN images into MNIST-like images. Very disappointed, I can not find reasons. Could you tell me the reasons? Sorry to trouble you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1479032849713, "tcdate": 1479032849707, "number": 12, "id": "HkqzxaS-x", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "SyOcVLHZx", "signatures": ["~Adam_Polyak1"], "readers": ["everyone"], "writers": ["~Adam_Polyak1"], "content": {"title": "About the configuration of the generator", "comment": "There is no discrepancy.  $g$ employs four blocks, and map vector representations to 32x32 grayscale images. The first layer can be seen as either a convolution or a fully connected layer (please see Radford et al.). The two are equivalent and in fact all blocks employ the Torch layer nn.SpatialFullConvolution. For the last layer, the paper clearly states \"In order to apply $f$ on MNIST images, we replicate the grayscale image three times.\" (see also reply to the thread \u201cOne puzzle\u201d below).  You might be happy to learn  that we plan to post our code online."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1479005328409, "tcdate": 1479005328404, "number": 11, "id": "SyOcVLHZx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "By89tlHZl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "About the configuration of the generator", "comment": "Thank you, but this is not consistent with what you stated in the paper. In the paper, for the digits, 'g employs four blocks of deconvolution' and 'map the 128D representations to 32*32 grayscale images',  but actually you use three blocks and generate three channel images, so I am confused which one is true. Finally, I am wondering if the architecture of the discriminator is the inverse of the generator. Thanks for your kind help."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478982029757, "tcdate": 1478982029750, "number": 10, "id": "By89tlHZl", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "rkqFFrN-l", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "About the configuration of the generator", "comment": "Thank you for your question. Generator $g$'s first layer, which takes 128d vectors in the case of digits (256 with faces) could be called fully-connected as it is just a matrix multiplication, but the result is reshaped into a 3-dimensional tensor. As discussed in Section 5.1, we then apply 'blocks' of: convolution, followed by batch-normalization, followed by ReLU. \n\nFor faces, as we mention in section 5.2, \"adding 1x1  convolution to each block resulted in lower $L_{\\text{CONST}}$ training errors, and made $g$ 9-layers deep\", hence we added such to each block. So overall, for digits we used: 128x1 -> 512x4x4 -> 256x8x8 -> 128x16x16 -> 3x32x32 , for faces: 256x1 -> 512x4x4 -> 512x4x4 -> 256x8x8 -> 256x8x8 -> 128x16x16 -> 128x16x16 -> 64x32x32 -> 16x32x32 -> 3x64x64. Both were terminated with tanh. These architectures were based on Radford et al., and it's possible that better ones exist as we did not search exhaustively for the best one."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478936962480, "tcdate": 1478936962474, "number": 9, "id": "rkqFFrN-l", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "About the configuration of the generator ", "comment": "Hi, it is really interesting and I want to re-implement this work. But the paper doesn't provide much information about the configuration of the generator. For example, do you use a fully connected layer as the first layer? If true, what is the dimension?  How many filters of each deconvolution layer do you use ?  Thank you very much."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478874068994, "tcdate": 1478874068989, "number": 8, "id": "HkpRm8QZg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "SJcGTBm-l", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"comment": "Yes.", "title": "One puzzle"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478872338261, "tcdate": 1478872338256, "number": 7, "id": "SJcGTBm-l", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "BkG8kSmZg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "comment", "comment": "Thank you, so before feeding to f to compute L_const, the generated source images, i.e., g(f(x)) for x from source domain, are replicated three times ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478868810071, "tcdate": 1478868810065, "number": 6, "id": "BkG8kSmZg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "HJbhsNQWg", "signatures": ["~Lior_Wolf1"], "readers": ["everyone"], "writers": ["~Lior_Wolf1"], "content": {"title": "One puzzle", "comment": "Thank you for your comment. Your second suggestion is exactly what we do. Quoting from the paper: \"In order to apply $f$ on MNIST images, we replicate the grayscale image three times.\""}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478867880818, "tcdate": 1478867880812, "number": 5, "id": "HJbhsNQWg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "One puzzle", "comment": "In section 5.1, you map SVHN-trained representation to 32*32 grayscale images, but the encoder f's input is three-channel, how do you solve this ? Why don't you transform RGB SVHN images into grayscale images or replicate the grayscale MNIST images three times ? I mean why don't you use the same image format for training f and the adversarial network? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478598711954, "tcdate": 1478598711948, "number": 4, "id": "B1lrlX1Wg", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "H1tNe0Cge", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "How is this work different from style transfer?", "comment": "Thank you for your question. The links between our work and style transfer are discussed in Sec. 2. In addition, we directly compare our work to style transfer in Sec. 5.3.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478578225239, "tcdate": 1478578225231, "number": 3, "id": "H1tNe0Cge", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "How is this work different from style transfer?", "comment": "Can the authors comment on what they are doing in terms of end-use/goal in addition to what we know from style transfer already?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478463525600, "tcdate": 1478463525533, "number": 2, "id": "S1aXlMalx", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "BJjU2w2ge", "signatures": ["~Yaniv_Taigman1"], "readers": ["everyone"], "writers": ["~Yaniv_Taigman1"], "content": {"title": "About the reference and one minor error", "comment": "Thank you very much for the very quick and important feedback.\n\nThe work by Dosoviskiy and Brox \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\", arXiv preprint arXiv:1602.02644, 2016  is indeed relevant, and we are sorry to have missed it. The components used in their work have 1:1 mapping to most components of our loss, as you correctly point out.\n\nHowever, there are important differences:  First, Dosoviskiy and Brox (2016) deal with learning a supervised mapping from a set of input-target pairs, while we solve the unsupervised domain transfer network.  Second, while the terms are similar, they are applied differently: L_{CONST} is applied in the source domain S and L_{TID} is applied at T. There are indeed two pipelines in our system \u2014 one for samples that arise from S and one for samples that arise from T. The pipeline for samples from S is unsupervised and uses L_{CONST}. The pipeline for samples that arise from T is supervised and the part of it that begins after the extraction of the representation by f bares similarity to the pipeline of Dosoviskiy and Brox (2016). Third, in our work, the discriminator D is ternary and not binary.\n\nExperimentally, Dosoviskiy and Brox (2016) is particularly interested in reconstructing input image I, hence it makes use of the activations of lower layers (e.g. Conv5, see Fig. 5 there) of its encoder (AlexNet) in order to have a better reconstruction. In contrast, our work focuses on the most semantic layers, since domain transfer is ultimately a semantic task. We have noticed that using activations from lower layers has a clear detrimental effect on the  transfer results, due to the differences between S & T.\n\nTo summarize: we are sorry for overlooking Dosoviskiy and Brox (2016) despite the mentioned similarities in constraining the output of the generator (namely, the loss in feature and pixel spaces). However, both the task that we solve and the method are crucially different than the ones presented in that paper. We will revise Sec. 2 to reflect the above discussion and provide proper credit. We are also grateful to the reviewer for pointing out the missing \\log in equation #3.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}, {"tddate": null, "tmdate": 1478421587299, "tcdate": 1478421587289, "number": 1, "id": "BJjU2w2ge", "invitation": "ICLR.cc/2017/conference/-/paper533/public/comment", "forum": "Sk2Im59ex", "replyto": "Sk2Im59ex", "signatures": ["~WANG_Zongwei1"], "readers": ["everyone"], "writers": ["~WANG_Zongwei1"], "content": {"title": "About the reference and one minor error", "comment": "This work is really interesting, but I find the network architecture and loss functions in this work are extremely similar to that in the work \"Generating Images with Perceptual Similarity Metrics based on Deep Networks\" by Alexey Dosoviskiy and Thomas Brox.  \n\nThe loss function in this work vs that of the latter: \nL_const is equal to L_feat;\nL_tid is equall to L_img;\nL_GANG is equal to L_adv.\n\nHowever, I do not see the later's name in your reference paper list.  Besides, have you forgotten to print a 'log' before D_3(x) in your L_D loss  on page 4?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Unsupervised Cross-Domain Image Generation", "abstract": "We study the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.", "pdf": "/pdf/2b3b65044995627de4b782921d5b17dbe9086eb0.pdf", "paperhash": "taigman|unsupervised_crossdomain_image_generation", "conflicts": ["fb.com", "tau.ac.il"], "keywords": ["Computer vision", "Deep learning", "Unsupervised Learning", "Transfer Learning"], "authors": ["Yaniv Taigman", "Adam Polyak", "Lior Wolf"], "authorids": ["yaniv@fb.com", "adampolyak@fb.com", "wolf@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287533192, "id": "ICLR.cc/2017/conference/-/paper533/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk2Im59ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper533/reviewers", "ICLR.cc/2017/conference/paper533/areachairs"], "cdate": 1485287533192}}}], "count": 34}