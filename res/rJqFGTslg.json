{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488577564259, "tcdate": 1478378114276, "number": 590, "id": "rJqFGTslg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJqFGTslg", "signatures": ["~Hao_Li1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396694341, "tcdate": 1486396694341, "number": 1, "id": "ryCz6ML_g", "invitation": "ICLR.cc/2017/conference/-/paper590/acceptance", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper presents a simple but effective approach for pruning ConvNet filters with extensive evaluation using several architectures on ImageNet and CIFAR-10.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396694846, "id": "ICLR.cc/2017/conference/-/paper590/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJqFGTslg", "replyto": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396694846}}}, {"tddate": null, "tmdate": 1485168158366, "tcdate": 1482163976308, "number": 2, "id": "B1xzDtrVg", "invitation": "ICLR.cc/2017/conference/-/paper590/official/review", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["ICLR.cc/2017/conference/paper590/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper590/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a simple method for pruning filters in two types of architecture to decrease the time for execution.\n\nPros:\n- Impressively retains accuracy on popular models on ImageNet and Cifar10\n\nCons:\n- There is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.\n- There is no direct comparison to the multitude of other pruning and speedup methods.\n- While FLOPs are reported, it is not clear what empirical speedup this method gives, which is what people interested in these methods care about. Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483444142282, "id": "ICLR.cc/2017/conference/-/paper590/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper590/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper590/AnonReviewer3", "ICLR.cc/2017/conference/paper590/AnonReviewer2", "ICLR.cc/2017/conference/paper590/AnonReviewer1", "ICLR.cc/2017/conference/paper590/AnonReviewer4"], "reply": {"forum": "rJqFGTslg", "replyto": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483444142282}}}, {"tddate": null, "tmdate": 1485168021558, "tcdate": 1485167856957, "number": 3, "id": "B1Kxp8QPx", "invitation": "ICLR.cc/2017/conference/-/paper590/official/comment", "forum": "rJqFGTslg", "replyto": "BkoY59y8l", "signatures": ["ICLR.cc/2017/conference/paper590/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper590/AnonReviewer2"], "content": {"title": "Response", "comment": "Thanks for the timings and additional experiments which do indeed shed more light on the efficacy of L1 pruning. Very interesting to see the mean-std criterion being effective -- it makes sense that low variance filter responses are disposable as they act more as biases, and I agree that it seems L1 norm is a good heuristic then for data-free selection. I think the comparisons to all these other selection criteria should be part of the main paper as it is empirical justification for your method, and then this paper becomes a good resource for pruning criteria comparisons."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508790, "id": "ICLR.cc/2017/conference/-/paper590/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper590/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper590/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508790}}}, {"tddate": null, "tmdate": 1485104076399, "tcdate": 1485104076399, "number": 7, "id": "S1NCmDzvl", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "Bye08wZvg", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "Low gradient values to detect stuck filters while low weights to prune filters", "comment": "Low gradient filters often indicate stuck/no learning while low weight filters within a layer indicate poor contribution to that layer's output.\n\nI read your paper and it appears that the idea you are proposing in the paper is to re-initialize the filters with low gradients i.e. to revive filters that have low learning due to poor initialization. This seems like a neat idea to explore the parameter space and I will be very interested in knowing if you can train smaller networks on a real world dataset (even MNIST) on established baselines (e.g. CIFAR-10 over Resnet etc.). For your problem, the gradient gives a good information about filters that may be stuck due to poor initialization. \n\nIn our paper, we propose pruning away filters with low weights with the ultimate goal of reducing FLOPs without using any sparse libraries. From our experiments over Imagenet/CIFAR-10 using Resnet/VGG networks, we find that this is simple, effective and requires lower re-training time than related work, and chops down even the very efficient Resnets. We agree that simply finding with filters with low weights across the network may not be a good idea. Hence, we prune away filters with small weights on a layer-wise basis, such that there is a small effect on the overall expectation of the magnitude of the output feature map (Section 3.1). \n\nHence, while your paper allows more filters to be explored for a given network size (fixed FLOPS), we explore a larger parameter space and then trim down the filters to cutdown the FLOPS with structured sparsity. More importantly, we prune away a fully trained network rather than during the training process. Reading your paper, I got the sense that the biggest gains in final accuracy happen when one limits reviving filters to initial phases of the training process to revive stuck filters. For a trained network, low gradient values can be fairly common and I am not convinced comparing low gradient values even at low thresholds can be used to solve a different problem i.e. to prune low activation filters in well-trained networks. Here, pruning away filters with low weight values seems like the more obvious thing to do.\n\nFinally, I would like to thank you for taking time to read our paper and commenting on it! \n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1485039304159, "tcdate": 1485039304159, "number": 6, "id": "Bye08wZvg", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["~Joseph_Paul_Cohen1"], "readers": ["everyone"], "writers": ["~Joseph_Paul_Cohen1"], "content": {"title": "Pruning criterion", "comment": "Just some feedback on your idea:\n\nThe criteria for determining a filter that is not important was the focus of my ICLR16 paper: RandomOut: Using a convolutional gradient norm to win The Filter Lottery https://arxiv.org/abs/1602.05931 \n\nUsing the L1-norm of the gradients in a filter would provide information on how the inputs and weights impact the loss. We used the gradients because small weights could still have a large impact on the loss and this would be reflected by the gradients. Maybe this criterion would be useful in your application.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1484915742036, "tcdate": 1483444141644, "number": 4, "id": "ByL3yfYrx", "invitation": "ICLR.cc/2017/conference/-/paper590/official/review", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["ICLR.cc/2017/conference/paper590/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper590/AnonReviewer4"], "content": {"title": "Good idea, well thought through and decently tested", "rating": "7: Good paper, accept", "review": "The idea of \"pruning where it matters\" is great. The authors do a very good job of thinking it through, and taking to the next level by studying pruning across different layers too.\n\nExtra points for clarity of the description and good pictures. Even more extra points for actually specifying what spaces are which layers are mapping into which (\\mathbb symbol - two thumbs up!).\n\nThe experiments are well done and the results are encouraging. Of course, more experiments would be even nicer, but is it ever not the case?\n\nMy question/issue - is the proposed pruning criterion proposed? Yes, pruning on the filter level is what in my opinion is the way to go, but I would be curious how the \"min sum of weights\" criterion compares to other approaches.\nHow does it compare to other pruning criteria? Is it better than \"pruning at random\"?\n\nOverall, I liked the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483444142282, "id": "ICLR.cc/2017/conference/-/paper590/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper590/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper590/AnonReviewer3", "ICLR.cc/2017/conference/paper590/AnonReviewer2", "ICLR.cc/2017/conference/paper590/AnonReviewer1", "ICLR.cc/2017/conference/paper590/AnonReviewer4"], "reply": {"forum": "rJqFGTslg", "replyto": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483444142282}}}, {"tddate": null, "tmdate": 1484785871236, "tcdate": 1483872898838, "number": 3, "id": "BkoY59y8l", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "B1xzDtrVg", "signatures": ["~Hao_Li1"], "readers": ["everyone"], "writers": ["~Hao_Li1"], "content": {"title": "More results in appendix: comparison with random filter pruning, feature map pruning with low activation patterns and wall-clock time ", "comment": "We thank the reviewer for the kind suggestions to improve the paper. We have updated the paper with more results added in the appendix.\n\n1. \u201cThere is no justification for for low L1 or L2 norm being a good selection criteria. There are two easy critical missing baselines of 1) randomly pruning filters, 2) pruning filters with low activation pattern norms on training set.\u201d\n\nIn Appendix 6.1, we compared our approach with pruning random filters and largest filters for VGG-16 on CIFAR-10. We find that pruning smallest filters works better than random pruning for most of the layers at different pruning ratios, e.g., all layers have better performance at the pruning ratio of 90%. It also clearly outperforms pruning filters with largest L1-norm, indicating the importance of large filters for certain layers.\n\nUpdates on Jan.18: In Appendix 6.2, we compared our approach with activation-based feature map pruning, which needs sample data as input to determine which feature maps to prune. We compute 5 statistics over the feature map with sampled training data, including: 1) the mean value. 2) the standard deviation 3) L1-norm 4)  L2-norm and 5) the contribution variance (Polyak & Wolf (2015)).  We find that L1-norm filter pruning has clear advantage over criterion 1, 3, 4 and 5. The mean-std criterion has better or similar performance by up to pruning ratio of 60% but drops quickly after 60% for certain layers.  We think L1-norm is good heuristic for filter selection considering that it is data free.\n\n2. \u201cThere is no direct comparison to the multitude of other pruning and speedup methods.\u201d\n\nWe had some discussions about the relationship with recent work in the related work and section 2.1. Past work on weights pruning (Han et al 2016) and recent group sparse based methods (Zhou et al 2016, Wen et al 2016) need to specify layer-wise thresholds or parameters, which is in the continues space and could be hard to set especially for CNNs with many layers. Filter-based pruning can be used in addition to other speedup methods such as low-rank approximation, quantisation and binary networks. \n\nWe choose to report results on ResNet as it is the state-of-the-art model for tasks such as image/video recognition rather than AlexNet or VGGNet. It is also a quite thin network and does not have large free parameters in the FC layer that can be pruned away as in AlexNet/VGGNet, which is challenging to make further speedup. We are not aware of other pruning work done for deep ResNets at the time of submission. \n\n3. \"Wall-clock speedup is trivial to report, so the lack of wall-clock speedup is suspect.\u201d\n\nIn Appendix 6.4, we report the wall-clock inference time on the test set of CIFAR-10 and the validation set of ImageNet 2012 with the latest GPU (Titan X Pascal) and library (cuDNN 5.1). As can be seen in Table 3, the saved wall-clock time is close to the FLOP reduction.  \n\nSince we physically prune the filters by creating a smaller model and then copy the weights, there are no masks or sparsity introduced to the original dense BLAS operations. The wall-clock time of the pruned model is the same as creating a model with less number of filters from scratch, which is dependent on the underlying hardware and software implementations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1484785379686, "tcdate": 1484785379686, "number": 5, "id": "Hy3kDFpLg", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "ByL3yfYrx", "signatures": ["~Hao_Li1"], "readers": ["everyone"], "writers": ["~Hao_Li1"], "content": {"title": "Comparison with other pruning criteria", "comment": "We thank the reviewer for the kind feedback and comments. We have updated the paper with comparisons with other criteria in the appendix. \n\n1. \u201cis the proposed pruning criterion proposed?\u201d\n\nIn the level of filter or feature map pruning, we are not aware of similar approaches. Past work on feature map pruning used to evaluate the importance of a feature map based on the variance of activation-norm [1] or the loss of validation accuracy if gets pruned [2, 3], but not based on the norm of the filter.\n\n2. \"Is it better than \"pruning at random\u201d?\n\nYes, it is better than \u201cpruning at random\u201d. In Appendix 6.1, we compared with pruning random filters and largest filters. Pruning smallest filters outperforms both approaches.\n\n3. How does it compare to other pruning criteria?\n\nIn Appendix 6.2, we compared with feature map pruning based on activation patterns. We compute 5 criteria on the feature map over all training data, including: 1) the mean value. 2) the standard deviation 3) L1-norm 4)  L2-norm and 5) the contribution variance (Polyak & Wolf (2015)).  We find that L1-norm based filter pruning has clear advantage over criterion 1, 3, 4 and 5. The mean-std criterion has better or similar performance by up to pruning ratio of 60% but drops quickly after that for certain layers. We think the L1-norm is a good heuristic for filter selection considering its data-free property.\n\nIn Appendix 6.3, we also compared with L2-norm for filter selection and observe a slightly better performance over L2-norm.\n\n[1] Polyak & Wolf, Channel-Level Acceleration of Deep Face Representations, IEEE Access, 2015\n[2] Zheng et al, Compact Deep Neural Networks for Device Based Image Classification, ICME workshop, 2015\n[3] Anwar et al, Structured Pruning of Deep Convolutional Neural Networks, arXiv, 2015"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1484252949684, "tcdate": 1484252949684, "number": 4, "id": "SJAMPDSIx", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "Thanks", "comment": "We would like to thank all reviewers for their comments and suggestions. We have added new experiments based on reviewer suggestions (and will continue to add more as necessary). The new experiments from Hao in Appendix A report fprop times for the pruned networks and random pruning results that address Reviewer 4's question about random pruning and Reviewer 2's concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1482935893779, "tcdate": 1482935893779, "number": 3, "id": "HyAUArZSl", "invitation": "ICLR.cc/2017/conference/-/paper590/official/review", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["ICLR.cc/2017/conference/paper590/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper590/AnonReviewer1"], "content": {"title": "Pruning Filters for Efficient ConvNets", "rating": "7: Good paper, accept", "review": "This paper prunes entire groups of filters in CNN so that they reduce computational cost and at the same time do not result in sparse connectivity. This result is important to speed up and compress neural networks while being able to use standard fully-connected linear algebra routines. \nThe results are a 10% improvements in ResNet-like and ImageNet, which may be also achieved with better design of networks. New networks should have been also compared, but this we know it is time-consuming.\nA good paper with some useful results.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483444142282, "id": "ICLR.cc/2017/conference/-/paper590/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper590/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper590/AnonReviewer3", "ICLR.cc/2017/conference/paper590/AnonReviewer2", "ICLR.cc/2017/conference/paper590/AnonReviewer1", "ICLR.cc/2017/conference/paper590/AnonReviewer4"], "reply": {"forum": "rJqFGTslg", "replyto": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483444142282}}}, {"tddate": null, "tmdate": 1482460237256, "tcdate": 1482442065032, "number": 2, "id": "BkYUraKEl", "invitation": "ICLR.cc/2017/conference/-/paper590/public/comment", "forum": "rJqFGTslg", "replyto": "HJu2_ZGNe", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "transfer learning looks good for case, more experiments needed", "comment": "Thanks for your comments. We demonstrate how to apply the structured sparsity to the most commonly used Resnets (which was non-trivial). Most prior work demonstrate their results on Alexnet or VGG which have lots of free parameters. However, these networks are considered largely inefficient and are no longer state of the art. Hence, we hope our methods and experience to prune Resnets can be helpful to the community.\n\nWe agree that transfer learning results are important since no one uses to test on Imagenet in real world problems. However, transfer learning can be pretty robust. With transfer learning, new classes often get mapped to some other class from the original dataset and the discriminative power is often maintained. Recent work has shown that transfer learning is robust to fewer classes and examples[1]. Training on fewer classes may even improve performance in some cases[1]. Furthermore, they find that removing target classes from Imagenet leads to small reduction in performance (Removing PASCAL classes from ImageNet leads to an insignificant reduction in performance for the PASCAL TL task\" [Table 3]). However, from our experiments it is not clear if filters are removed at the class/concept level.\n\nWe performed a simple transfer learning experiment using our pre-trained and pruned resnet-34-b (from Table 1). We use the network to perform action recognition on the UCF-11 dataset. We use a simple CNN+LSTM model and connect the second last (pooling) layer as in input to a two layer LSTM. We achieve 82.7% using Resnet-34 and 83.6% using the pruned resnet-34b on the UCF-11 dataset. Additionally, during feature generation we have proportional time savings. In our simple transfer learning model, the feature generation is a one-time task. However, when using optical flow or spatial attention based models where the features are extracted from the convolution layer, the CNN is re-trained along with the LSTM. In these cases, the time savings can be significant over multiple iterations. While this is just one experiment where the TL is not affected and we get a slightly better accuracy, we can expand our experiments to other datasets to understand how the transfer learning performance is affected.\n\n\n[1] What makes ImageNet good for transfer learning? https://arxiv.org/abs/1608.08614"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287508913, "id": "ICLR.cc/2017/conference/-/paper590/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJqFGTslg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper590/reviewers", "ICLR.cc/2017/conference/paper590/areachairs"], "cdate": 1485287508913}}}, {"tddate": null, "tmdate": 1481935024428, "tcdate": 1481935024428, "number": 1, "id": "HJu2_ZGNe", "invitation": "ICLR.cc/2017/conference/-/paper590/official/review", "forum": "rJqFGTslg", "replyto": "rJqFGTslg", "signatures": ["ICLR.cc/2017/conference/paper590/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper590/AnonReviewer3"], "content": {"title": "Simple idea with good experiments; transfer learning results would improve it", "rating": "7: Good paper, accept", "review": "This paper proposes a very simple idea (prune low-weight filters from ConvNets) in order to reduce FLOPs and memory consumption. The proposed method is experimented on with VGG-16 and ResNets on CIFAR10 and ImageNet.\n\nPros:\n- Creates *structured* sparsity, which automatically improves performance without changing the underlying convolution implementation\n- Very simple to implement\n\nCons:\n- No evaluation of how pruning impacts transfer learning\n\nI'm generally positive about this work. While the main idea is almost trivial, I am not aware of any other papers that propose exactly the same idea and show a good set of experimental results. Therefore I'm inclined to accept it. The only major downside is that the paper does not evaluate the impact of filter pruning on transfer learning. For example, there is not much interest in the tasks of CIFAR10 or even ImageNet. Instead, the main interest in both academia and industry is the value of the learned representation for transferring to other tasks. One might expect filter pruning (or any other kind of pruning) to harm transfer learning. It's possible that the while the main task has about the same performance, transfer learning is strongly hurt. This paper has missed an opportunity to explore that direction.\n\nNit: Fig 2 title says VGG-16 in (b) and VGG_BN in (c). Are these the same models?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Pruning Filters for Efficient ConvNets", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy.  However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.  ", "pdf": "/pdf/425c9fb25fd63057e3caf7b821d9876c982bc227.pdf", "paperhash": "li|pruning_filters_for_efficient_convnets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "keywords": ["Computer vision", "Deep learning"], "conflicts": ["umd.edu", "nec-labs.com", "nervanasys.com"], "authorids": ["haoli@cs.umd.edu", "asim@nec-labs.com", "igord@nec-labs.com", "hjs@cs.umd.edu", "hpg@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483444142282, "id": "ICLR.cc/2017/conference/-/paper590/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper590/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper590/AnonReviewer3", "ICLR.cc/2017/conference/paper590/AnonReviewer2", "ICLR.cc/2017/conference/paper590/AnonReviewer1", "ICLR.cc/2017/conference/paper590/AnonReviewer4"], "reply": {"forum": "rJqFGTslg", "replyto": "rJqFGTslg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper590/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483444142282}}}], "count": 13}