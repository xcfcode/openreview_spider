{"notes": [{"id": "BJlSHsAcK7", "original": "HJgQms2tdm", "number": 86, "cdate": 1538087741316, "ddate": null, "tcdate": 1538087741316, "tmdate": 1545355405718, "tddate": null, "forum": "BJlSHsAcK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJg8w-elxV", "original": null, "number": 1, "cdate": 1544712542221, "ddate": null, "tcdate": 1544712542221, "tmdate": 1545354507798, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Meta_Review", "content": {"metareview": "The authors propose an approach for continual learning of a sequence of tasks which augments the network with task-specific neurons which encode 'adversarial subspaces' and prevent interference and forgetting when new tasks are being learnt. The approach is novel and seems to work relatively well on a simple sequence of MNIST or CIFAR10 classes, and has certain advantages, such as not requiring any stored data. However, the reviewers agreed that the presentation of the method is quite confusing and that the paper does not provide adequate intuition, visualisation, or explanation of the claim that they are preventing forgetting through the intersection of adversarial subspaces. Moreover, there was a concern that the baselines were not strong enough to validate the approach.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper86/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper86/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353341927, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper86/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper86/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper86/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353341927}}}, {"id": "ByxNJHr52m", "original": null, "number": 3, "cdate": 1541194972002, "ddate": null, "tcdate": 1541194972002, "tmdate": 1541534296221, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "content": {"title": "Paper with Interesting novel ideas, but it needs major presentation improvements", "review": "The paper is about a new method for training neural networks in the continual learning setting, where tasks are presented in a sequential manner (and data from the previous task cannot be revisited). The method proposes a new architecture that adds task-parameters parameters to prevent catastrophic forgetting.\n\nTo my understanding, the paper proposes a modification to EWC in which the capacity of the network is augmented after a new task is added. Unlike similar methods (like Progressive networks, see bellow), this augmentation is input agnostic. It acts as a correction of the model parameters such that the new task can be easier to train while still maintaining the 'normal parameters' close to the ones of the initial task (as in EWC). I find this idea interesting and certainly worth publishing. In my view, the paper cannot be published in its current state. With the current presentation it is very difficult to understand what is being proposed. Please correct me if I misunderstood the work. \n\nThe writing of the manuscript needs significant improvement. I read it carefully several times and I am still not sure of how exactly the model is trained. I had to read the paper by Elsayed et al 2018, to have an idea of what could have been proposed here. As I mentioned, the paper has novel and interesting ideas, but it would be greatly improved with some important re-writing. Please find bellow some questions.\n\n- In the second to last paragraph of page two, the authors say that: instead of adding a perturbation that would force the network to perform a misclassification, tune it using \"the input's own correct class to assist correct classification\". If the gradients are computed with respect to the correct class of a given input, why is this called an adversarial perturbation? \n\n- Elsayed maintain the parameters of the first task fixed and train a fixed input-agnostic correction that can be added to the input such that a second task can be trained (with a re-mapping the outputs). Applying Elsayed et al 2018 to the continual learning setting, the model should only learn correction for task 2 (and 3). How do the authors compute the corrections for task 1? Computing a correction requires having access to the training data.\n\n- The authors use the FGSM method to compute \"adversarial perturbations\". This method was proposed as a proxy for performing gradient descent to minimize the computational load required for finding adversarial examples. In this application, unlike the case of adversarial perturbations, the memories don't need to be constrained to be smaller than a given epsilon. What is the motivation of using this method? How do you explain the difference in the results.\n\n- Having mentioned this, both W_task and M_task are updated by minimizing the same loss function (ignoring the difference of using FGSD or not). In that case, why is it needed to have a factorized form W_task * M_task instead of a single bias?\n\n- Throughout the paper the authors say that the long term memory lies on the \"intersection of adversarial subspaces\". It is not clear at all why this should be the case. The authors do not explain adversarial subspaces corresponding to which model.\n\n- The authors should cite the Progressive networks as this is a very related work. Unlike progressive networks, this work proposes and augmentation that is input agnostic which is interesting. https://arxiv.org/abs/1606.04671\n\n- With EWC, once the model is trained, one does not need to know the task being evaluated at test time. This is not the case in the proposed model. This should be clarified. Also, when having many tasks mapping to the same input, the fair way of comparing to EWC would to have a different head per task. This baselines should be included.\n\n- What are the task specific functions g_taskA and g_taskB?\n\n- Adding an explicit algorithm, the exact loss functions used should help clarifying the proposed method.\n\n- The paper would be stronger if more complex tasks would be added.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper86/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "cdate": 1542234541015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper86/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335646087, "tmdate": 1552335646087, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper86/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxrL3Jc3m", "original": null, "number": 2, "cdate": 1541172301163, "ddate": null, "tcdate": 1541172301163, "tmdate": 1541534295962, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "content": {"title": "Insufficient Experimental Validation", "review": "This paper proposes a novel continual learning method that stores intersection of adversarial subspaces into long-term memory units for each task, which is used used to characterize the given task at future tasks. This adversarial memory network requires supposedly less number of parameters for each task to store, compared to methods that stores explicit examples. The authors validated the proposed model on three datasets for continual learning, on which it obtains good performance when networks trained with plain gradient descent and elastic weight consolidation suffers from catastrophic forgetting.\n\nPros\n- The idea of using adversarial subspaces to characterize a task is a novel idea which seems to work to some degree.\n\nCons\n\nExperimental validation is lacking in many aspects. \n\n- First, while the proposed method requires additional memory storage and parameters, it is not comparing against any of the existing work that increases network capacity or storing a small subset of the original dataset. To list a few that seems relevant, [Yoon et al. 18] proposes a network that can dynamically expand its capacity with minimal number of units per layer, and [Nguyen et al. 18] proposes to store a small subset called CoreSet that well-represent the task-specific dataset. To show that the proposed method is indeed effective in terms of accuracy over number of parameters, the authors should compare against such baselines with additional parameters. The increase in the network capacity reported in the paper seems quite large (over 60% for AD+EWC) and thus its effectiveness is questionable without such comparative study. \n\n- Their implementation of EWC seems suboptimal as it is only applied to fully connected layers, and thus the EWC baseline performs much poorly than what are reported in many of the previous work, and performs comparable to PGD. Since EWC baseline is crippled the only message that is remaining is that the proposed method works better than simple PGD. \n\n- The reported results using the proposed method shows some performance degradation on earlier tasks, which seems large considering the difficulty of the tasks. Again, the authors should compare against recent methods on continual learning so that the readers can understand how good these reported performances are.\n\n- It is difficult to understand why storing adversarial subspaces helps, since there is no visualization or illustrations that provide intuitive explanations. \n\nIn sum, while the proposed model seems novel, its motivation is unclear and it is difficult to assess the effectiveness of the proposed method due to lack of experimental validation. Thus I recommend the rating of reject for this paper, until the authors provide additional experimental results for proper assessment of the method's effectiveness.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper86/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "cdate": 1542234541015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper86/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335646087, "tmdate": 1552335646087, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper86/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkx1mVZEn7", "original": null, "number": 1, "cdate": 1540785175387, "ddate": null, "tcdate": 1540785175387, "tmdate": 1541534295757, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "content": {"title": "Counter-intuitive adversarial memory units lacking persuading theoretical or empirical explanations", "review": "This paper proposes a new approach to sequential learning by introducing an adversarial memory unit for each new task and uses EWC as a regularizer for training other parts of the network on the new task.\nThe memory units are trained with Fast Gradient Sign Method to increase the loss, and they are connected to the next layer with weights trained to decrease the loss. \nIt shows superior performance than EWC and the plain gradient descent baseline on disjoint MNIST/CIFAR10 and EMNIST. The authors also share their experience with EWC, which provides useful feedbacks to the community.\n\nThe proposed adversarial memory unit is novel to the best of my knowledge. However, its motivation is not quite intuitive to me, and the authors fail to provide persuading explanations. My major concern is whether it is better to take the adversarial direction rather than the direction that decrease the loss for the memory units.\n\nTo support their ideas, the authors mentioned the paper \"Adversarial Reprogramming of Neural Networks\" and said this paper's \"adversarial program\" is formed by choosing the \"intersection of adversarial subspaces\" as in their paper. However, they (Elsayed et al. 2018) are actually finding such adversarial programs in the direction of decreasing the loss, which is contrary to finding the \"intersection of adversarial subspaces\". \nThe authors also want to support the pros of adversarial memory units by comparing against \"Gradient\" memory units that are trained to decrease the loss with the experiment shown in Figure 2. However, Figure 2a seems problematic to me, so I am not sure whether the authors are doing their experiments correctly. I think the experimental conditions for FGSD and Gradient are different, which makes the comparison meaningless. We can see that the network's accuracy with Adversarial memory unit on task 1 is a constant when the network is trained on task 2 and 3, because the network's weights (except memory units and their weights for task 2 and 3) and task 1's memory units are fixed, as described in the experimental setting for \"AD\". The accuracy on task 1 with Gradient memory units is changing when the network is trained on task 2 and 3, which means either the network's weights are changing or the memory unit is changing. \n\nAs a result, I don't think this paper will be accepted until the authors provide further explanations and results to support the adversarial memory unit, or clarify my misunderstandings in the comments above.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper86/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Official_Review", "cdate": 1542234541015, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper86/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335646087, "tmdate": 1552335646087, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper86/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkg95oqhtm", "original": null, "number": 1, "cdate": 1538202514269, "ddate": null, "tcdate": 1538202514269, "tmdate": 1538202514269, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "rJxz28U2FX", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Official_Comment", "content": {"title": "EWC needs task ID during testing in the shared outputs case", "comment": "Thank you for your comment. We will add some discussion in the paper about this issue.\n\nIn the permuted MINST dataset, as you note, EWC does not have to know the task boundaries.\n\nHowever, consider disjoint MINST tasks, where you want to sequentially train task1 with handwritten digits 0,1,2, then task2 with 4,5,6, and then task3 with 7,8,9.\n\nFor EWC, in the shared outputs case (the network only has 3 outputs), if you do not know the task ID during testing, then the network's output is ambiguous. For example, if, after softmax, you see that output 1 is the most probable one, you cannot tell whether this test sample is digit 0, or digit 4, or digit 7 (since these three digits share output 1). So you have to know the task ID during testing in the shared outputs case, to disambiguate which digit output 1 corresponds to. As you can see from our figure 3 a) and b), the EWC algorithm, with the knowledge of task ID, still fails to chance level after 20 epochs. So this is a fair comparison in our opinion.\n\nFor the disjoint outputs case, you now have explicit unique representations for digits 0,1,2,4,5,6,7,8,9. Thus you do not need explicit task ID during test. But EWC's accuracy for task1 drops to 0% during testing after task2 and task3 have been trained, as it fails to map task1 test samples to the corresponding outputs. For example, after finishing training of task3 the network always outputs 7,8 or 9 for any test sample and has completely forgotten about 0,1,2,4,5,6; if you present a sample from the task1 test set, the output still is 7,8, or 9 only. Thus, in this case, we agree that EWC does not need to know the task ID while we do, but EWC does not work at all (0% correct on task1 and task2) while our method performs very well on all 3 tasks."}, "signatures": ["ICLR.cc/2019/Conference/Paper86/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper86/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper86/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622096, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJlSHsAcK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper86/Authors", "ICLR.cc/2019/Conference/Paper86/Reviewers", "ICLR.cc/2019/Conference/Paper86/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper86/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper86/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper86/Authors|ICLR.cc/2019/Conference/Paper86/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper86/Reviewers", "ICLR.cc/2019/Conference/Paper86/Authors", "ICLR.cc/2019/Conference/Paper86/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622096}}}, {"id": "rJxz28U2FX", "original": null, "number": 1, "cdate": 1538184874060, "ddate": null, "tcdate": 1538184874060, "tmdate": 1538186766764, "tddate": null, "forum": "BJlSHsAcK7", "replyto": "BJlSHsAcK7", "invitation": "ICLR.cc/2019/Conference/-/Paper86/Public_Comment", "content": {"comment": "The experiments comparing with EWC could be not very fair in my opinion.\nIn literature, there are many different assumptions while performing continual learning, \nThe most difficult task setting would be assuming no knowledge of task boundary both during training and testing.\nAs a compromise, one can assume the boundary is known during training but unknown during testing, like in EWC.\nSome of the works assume the task boundary is known during both training and testing, like in this paper, since there are task neurons, I assume during test the task id is required.\n\nIn my opinion, comparing two methods with different assumptions on the knowledge of task boundary is not fair.", "title": "Comments on the Experiments"}, "signatures": ["~Min_Lin1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper86/Reviewers/Unsubmitted"], "writers": ["~Min_Lin1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Overcoming catastrophic forgetting through weight consolidation and long-term memory", "abstract": "Sequential learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby previously learned knowledge is erased during learning of new, disjoint knowledge. Here, we propose a new approach to sequential learning which leverages the recent discovery of adversarial examples. We use adversarial subspaces from previous tasks to enable learning of new tasks with less interference. We apply our method to sequentially learning to classify digits 0, 1, 2 (task 1), 4, 5, 6, (task 2), and 7, 8, 9 (task 3) in MNIST (disjoint MNIST task). We compare and combine our Adversarial Direction (AD) method with the recently proposed Elastic Weight Consolidation (EWC) method for sequential learning. We train each task for 20 epochs, which yields good initial performance (99.24% correct task 1 performance). After training task 2, and then task 3, both plain gradient descent (PGD) and EWC largely forget task 1 (task 1 accuracy 32.95% for PGD and 41.02% for EWC), while our combined approach (AD+EWC) still achieves 94.53% correct on task 1. We obtain similar results with a much more difficult disjoint CIFAR10 task (70.10% initial task 1 performance, 67.73% after learning tasks 2 and 3 for AD+EWC, while PGD and EWC both fall to chance level). We confirm qualitatively similar results for EMNIST with 5 tasks and under 3 variants of our approach. Our results suggest that AD+EWC can provide better sequential learning performance than either PGD or EWC.", "paperhash": "wen|overcoming_catastrophic_forgetting_through_weight_consolidation_and_longterm_memory", "TL;DR": "We enable sequential learning of multiple tasks by adding task-dependent memory units to avoid interference between tasks", "authorids": ["shixianwen1993@gmail.com", "itti@usc.edu"], "authors": ["Shixian Wen", "Laurent Itti"], "keywords": ["Catastrophic Forgetting", "Life-Long Learning", "adversarial examples"], "pdf": "/pdf/d416b27ccc89fe33c2a5d954a1277f2bb407f0bb.pdf", "_bibtex": "@misc{\nwen2019overcoming,\ntitle={Overcoming catastrophic forgetting through weight consolidation and long-term memory},\nauthor={Shixian Wen and Laurent Itti},\nyear={2019},\nurl={https://openreview.net/forum?id=BJlSHsAcK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper86/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311921989, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJlSHsAcK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper86/Authors", "ICLR.cc/2019/Conference/Paper86/Reviewers", "ICLR.cc/2019/Conference/Paper86/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper86/Authors", "ICLR.cc/2019/Conference/Paper86/Reviewers", "ICLR.cc/2019/Conference/Paper86/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311921989}}}], "count": 7}