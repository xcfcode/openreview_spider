{"notes": [{"id": "Syx7WyBtwB", "original": "Hyxy4Yi_wr", "number": 1536, "cdate": 1569439482535, "ddate": null, "tcdate": 1569439482535, "tmdate": 1577168255213, "tddate": null, "forum": "Syx7WyBtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "feJeNRVyd", "original": null, "number": 1, "cdate": 1576798725904, "ddate": null, "tcdate": 1576798725904, "tmdate": 1576800910593, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Decision", "content": {"decision": "Reject", "comment": "The paper contains interesting ideas for giving simple explanations to a NN; however, the reviewers do not feel the contribution is sufficiently novel to merit acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715288, "tmdate": 1576800265170, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Decision"}}}, {"id": "rkeCgD1Gir", "original": null, "number": 7, "cdate": 1573152502142, "ddate": null, "tcdate": 1573152502142, "tmdate": 1573152502142, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"title": "Thank you to reviewers - updates to manuscript", "comment": "We would like to thank all reviewers for their time and effort. We have responded to their concerns below, and made the following changes to the manuscript as a result:\n\n- We have added references to Zaidan 2007 and Strout 2019\n\n- Per the comment from Joseph Janizek (author of the expected gradients paper), we updated the computational and accuracy results for expected gradients. While improved, it still fails to beat a random baseline\n\n- Improved ColorMNIST results: Previous results on ColorMNIST were non-deterministic  (despite a set random seed) due to a strange cuDNN setting. While rerunning those experiments we discovered that increasing the regularization parameter improves the mean accuracy using CDEP to 31% (previously 25.5%).  We have updated the manuscript accordingly. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "HyeYjLkMor", "original": null, "number": 6, "cdate": 1573152417280, "ddate": null, "tcdate": 1573152417280, "tmdate": 1573152417280, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "rJgWqUyGsB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"title": "Author's response continued", "comment": "\u201cFigure 3 is nice but not terribly surprising\u201d\n\nWe agree - we included Figure 3 not as a shocking finding, but to visually explain what our method does (in addition to the text/equation descriptions elsewhere), as well as provide a sanity check. It should also be noted that we obtain the explanations with a different method (GradCAM) than the one used for the optimization (CD). This gave us some indication that we were not overfitting to a particular explanation algorithm."}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "rJgWqUyGsB", "original": null, "number": 5, "cdate": 1573152393435, "ddate": null, "tcdate": 1573152393435, "tmdate": 1573152393435, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Hkgqk1x6Fr", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"title": "Author's response", "comment": "We would like to thank the reviewer for their thoughtful review. We address your concerns below.\n\n\u201cThe main advantage of this effort compared to work that directly penalizes the gradients (as in Ross et al.) is that the method does not rely on second gradients (gradients of gradients), which is computationally problematic\u201d\n\nWhile our approach has computational benefits, we would also note that empirically CDEP produces significantly better results. On color MNIST Ross et al. provides no benefit (accuracy the same as a random baseline - 10%), while CDEP achieves 31%. Similarly, for the skin cancer dataset, Ross et al. actually hurt accuracy. We attribute this to CDEP allowing the penalization of features, including interactions of features, rather than just feature-level gradients.\n\n\u201cI am not sure I agree with the premise as stated here. Namely, the authors write \"For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective\" -- I would argue that an explanation may be useful in and of itself by highlighting how a model came to a prediction. I am not convinced that it need necessarily lead to, e.g., improving model performance. I think the authors are perhaps arguing that explanations might be used to interactively improve the underlying model, which is an interesting and sensible direction.\u201d\n\nWe agree that our abstract is strongly worded - this is by design. We feel that explainable deep learning research is currently overwhelmed with different explanation algorithms, yet has very few (arguably no) success stories of researchers actually using these algorithms to accomplish something of interest to the broader community.\n\nExplainable DL techniques can certainly be used to \u201chighlight how a model came to a prediction\u201d, but we feel that this is only an intermediate objective, not an end in itself. Ultimately, users want to do things like improve model performance, build trust in a model, identify flaws, or verify that model is being fair with respect to attributes like race, gender, etc. \n\nAs a community, we do not currently know how to use our explanation algorithms to accomplish these things, or whether our explanation algorithms are well suited to do so. In fact, we suspect that many published explanation algorithms would fail when evaluated on real end tasks - as we saw with gradients and integrated gradients in this paper. \n\nFiguring out how to use explainable DL techniques for anything real is essentially being neglected by current researchers, so we framed our paper to try to shed light on this, and nudge things in the right direction.\n\nIf you still disagree with our premise, we\u2019d be happy to tamp things down, and adjust our abstract to motivate things through the vein of \u201cexplanations could be useful to improve predictions\u201d.\n\n\u201cThis work, which aims to harness user supervision on explanations to improve model performance, seems closely related to work on \"annotator rationales\" (Zaidan 2007 being the first work on this), but no mention is made of this. \"Do Human Rationales Improve Machine Explanations?\" by Strout et al. (2019) also seems relevant as a more recent instance in this line of work\u201c\n\nThanks for bringing Zaidan 2007 and Strout 2019 to our attention, they are indeed useful prior work in this field. We will include both references in an updated version.\n\n\u201cThe authors compare their approach to Ross and colleagues in Table 1 but see quite poor results for the latter approach. Is this a result of the smaller batch size / learning rate adjustment? It seems that some tuning of this approach is warranted.\u201d\n\nAs you noted, the approach by Ross penalizes gradients of gradients, preventing learning of those weights. This works quite nicely for tasks where the feature to be ignored is always in the same location as we see in the results on DecoyMNIST.  In contrast, for the ISIC dataset, the patches are distributed roughly uniformly over the image.  By penalizing gradients for the patches, the gradient updates are \u2018dampened\u2019 over the entire input for a large part of the training data (patches are present in 45% of samples) and learning is prevented. This issue may be further amplified by the low learning rate and batch size necessary for this approach and dataset. \n\nWe can assure you we tried our best to tune Ross\u2019 approach in order to achieve a fair baseline (despite the fact that their approach is roughly 80 times slower than CDEP, making extensive tuning difficult). "}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "S1l8xL1GjB", "original": null, "number": 4, "cdate": 1573152238355, "ddate": null, "tcdate": 1573152238355, "tmdate": 1573152238355, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "rklgSCRptr", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"title": "Author's response", "comment": "We would like to thank the reviewer for their time and thoughtful comments. We address their concerns below.\n\n\u201cI like the high-level idea of this work and agree that there is not much work on using prediction explanations to help improve model performance. However, there are two major concerns of the model and experiment design. \n\nFirst, it seems like the proposed method requires whoever use it already know what the problem is.\u201d\n\nWe agree - after a practitioner has found a flaw in their model or limitations in their training data (using any existing interpretation technique), our technique is designed to help rectify that flaw by altering the model.\n\n\u201cMy question is that if we already know the bias or the mismatch, why not directly use this information in the regularization to penalize some features? Is it necessary to resort to some explanation generation methods?\u201d\n\nIt is not clear to us how, exactly, we could \u201cdirectly use this information in the regularization to penalize features\u201d without resorting to explanations. Consider the skin cancer (ISIC) example shown in Figure 2. The image patches that we want the model to ignore occur at different places in different images, and the model used is a CNN. To us, it is unclear how to compute (let alone regularize) the contribution of a feature to a model/prediction without the use of explanations. Beyond the methods we compare against, we are not aware of any other way to do so.\n\nIf there is relevant prior work that we are missing that describes such a technique, we would love to take a look.\n\n\u201cMy second concern is more like a personal opinion. In the experiment of section 4.2, if the colors are good indicators of these digits in the training set, I don't it is wrong for a model to capture these important features. However, the way of altering examples in the same class with different colors in training and test sets seems questionable, because now, the distributions of training and test images are different. On the other hand, if we already know color is the issue, why not simply convert the images into black-and-white? A similar argument can also be applied to the experiment in section 4.3\u201d\n\nThis is a great thought, which merits some discussion. At first blush, we agree that these are simple problems, which could be solved in simpler ways.\n\nHowever, we feel these simulations studies are actually very useful in developing and evaluating algorithms (including, but not limited to explanation algorithms). The reason for this is that they present a \u201cbare minimum\u201d for prospective methods to clear, and provide a clear metric of success. Put simply, if a prospective method cannot solve something so simple, it is unlikely to be of use on any \u201creal\u201d, i.e. not simulated, datasets.\n\nIn the color MNIST example of section 4.2, for instance, we have a clearly defined, spurious correlation (color), which is easy to check if a method has successfully removed from a model. In this idealized setting, our method was able to partially remove the confounding, while other techniques fail completely (underperforming a random benchmark).\n\nOf course, passing the \u201cbare minimum\u201d is not sufficient to fully validate a method, which is why we included a very real and consequential example in section 4.1 on skin cancer detection. However, we do think that CDEP\u2019s performance on simulations (both absolute and relative to baselines) provides additional, meaningful, evidence of its effectiveness. \n\nAs an aside, we are far from the first ones to use simulation studies like this to validate our methods. Color MNIST was in CVPR last year [1], and was also discussed in a keynote at ICLR 2019 (this is what led us to use it). The Decoy-MNIST dataset was introduced in [2], a fairly successful (90+ citation in 2 years) paper. \n\n[1] https://arxiv.org/pdf/1904.07911.pdf\n[2] https://arxiv.org/pdf/1703.03717.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "SyxRnHJMiH", "original": null, "number": 3, "cdate": 1573152181537, "ddate": null, "tcdate": 1573152181537, "tmdate": 1573152181537, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "BJxDW5Jl5B", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"title": "Author's response", "comment": "We would like to thank the reviewer for their thoughtful points. We have addressed their concerns below.\n\n\u201cHowever, I am a bit worried that the proposed approach is somewhat ad hoc. I can imagine there are various explanations that can be generated for the same model. There can also be different prior knowledge available for a particular problems. Which prior knowledge and explanations to use seem to affect a lot about the learned model. But there is no principled approaches for making the selection.\u201d\n\nThis is an excellent point. In short, this is, in some sense of the word, an ad hoc method - but we don\u2019t think that is a bad thing. CDEP\u2019s ability to incorporate different forms of prior knowledge is a necessary feature to enable practitioners to use it in a wide variety of settings. While CDEP lacks a formal, mathematical derivation, it produces strong empirical results.\n\n\u201cI can imagine there are various explanations that can be generated for the same model.\u201d\n\nWhen it comes to the mechanical details of our algorithm, CDEP is certainly ad hoc. In particular, we have no proof that CDEP is mathematically optimal/unique, and it is possible that there could be some other version of CDEP, which could produce better results. Such a version may use a different explanation algorithm, or a different approach for penalizing the explanations. \n\nHowever, for the methodological choices we made, we are able to show meaningful empirical improvements across a number of different datasets. While a uniqueness proof would be nice, we feel that our empirical results are sufficient to demonstrate the effectiveness of our method.\n\n\u201cThere can also be different prior knowledge available for a particular problem\u201d\n\nWe should be clear - CDEP is not a plug and play tool that can be blindly applied without any knowledge of the underlying data. Rather, CDEP requires a practitioner to carefully examine their model, and dataset. Subsequently, CDEP enables them to use their best judgement in determining what patterns are likely to generalize, and should be used by the model. \n\nThis type of \u201cad hoc\u201d analysis is critical for real-world uses of machine learning, and CDEP provides a useful tool for doing so. In our skin cancer example, without properly analyzing the model and data, and using CDEP, a practitioner would construct a model that learns to predict whether a patient has a band-aid. Using that band-aid predictor to help diagnose skin cancer would be problematic, to say the least.\n\n\u201cBut there is no principled approaches for making the selection.\u201d\n\nPractitioners can optimize their selections for predictive accuracy on an appropriate dataset.\n\n\n\u201cFor instance, consider the example in Figure 2 about the presence of patches. Isn't that a too specific knowledge about the dataset, which in turn makes the proposed approach not general? I have doubts on how useful a method is if it relies on such specific prior knowledge about the data.\u201d\n\nAs we discussed above, Figure 2 is one example of the type of prior knowledge that CDEP can use. However, the general theme of models learning spurious correlations is a fairly common problem that should not require much motivation.\n\nFor other examples, within our paper, we\u2019d point to our other results in 4.2 and 4.3, as well as prior work on penalizing explanations [1]. As noted by another reviewer, there is also a line of work on non-deep learning models in NLP surrounding annotator rationales [2] [3]. CDEP could also certainly be used in improving the fairness of a model (ensuring that a model does not discriminate based on sensitive attributes like gender, race, etc.). There have also been other failures in medical machine learning that could benefit from CDEP [4]. In the month since we completed this work, we\u2019ve also come in touch with some biologists who will be using CDEP in their research. \n\n[1] https://arxiv.org/pdf/1703.03717.pdf\n[2] https://www.aclweb.org/anthology/N07-1033.pdf\n[3] https://arxiv.org/abs/1905.13714\n[4] Slide 30: http://theory.stanford.edu/~ataly/Talks/berkeley_ig_talk_feb_2019.pdf (talk from co-creator of integrated gradients)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "Hkgqk1x6Fr", "original": null, "number": 1, "cdate": 1571778274168, "ddate": null, "tcdate": 1571778274168, "tmdate": 1572972455712, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method intended to allow practitioners to *use* explanations provided by various methods. Concretely, the authors propose contextual decomposition explanation penalization (CDEP), which aims to use explanation methods to allow users to dissuade the model from learning unwanted correlations. \n\nThe proposed method is somewhat similar to prior work by Ross et al., in that the idea is to include an explicit term in the objective that encourages the model to align with prior knowledge. In particular, the authors assume supervision --- effectively labeled features, from what I gather --- provided by users and define an objective that penalizes divergence from this. The object that is penalized is $\\Beta(x_i, s)$, which is the importance score for feature s in instance $i$; for this they use a decontextualized representation of the feature (this is the contextual decomposition aspect). Although the authors highlight that any differentiable scoring function could be used, I think the use of this decontextualized variant as is done here is nice because it avoids issues with feature interactions in the hidden space that might result in misleading 'attribution' w.r.t. the original inputs.\n\nThe main advantage of this effort compared to work that directly penalizes the gradients (as in Ross et al.) is that the method does not rely on second gradients (gradients of gradients), which is computationally problematic. Overall, this is a nice contribution that offers a new mechanism for exploiting human provided annotations. I do have some specific comments below.\n\n- I am not sure I agree with the premise as stated here. Namely, the authors write \"For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective\" -- I would argue that an explanation may be useful in and of itself by highlighting how a model came to a prediction. I am not convinced that it need necessarily lead to, e.g., improving model performance. I think the authors are perhaps arguing that explanations might be used to interactively improve the underlying model, which is an interesting and sensible direction.\n\n- This work, which aims to harness user supervision on explanations to improve model performance, seems closely related to work on \"annotator rationales\" (Zaidan 2007 being the first work on this), but no mention is made of this. \"Do Human Rationales Improve Machine Explanations?\" by Strout et al. (2019) also seems relevant as a more recent instance in this line of work. I do not think such approaches are necessarily directly comparable, but some discussion of how this effort is situatied with respect to this line of work would be appreciated.\n\n- The experiment with MNIST colors was neat. \n\n- The authors compare their approach to Ross and colleagues in Table 1 but see quite poor results for the latter approach. Is this a result of the smaller batch size / learning rate adjustment? It seems that some tuning of this approach is warranted. \n\n- Figure 3 is nice but not terribly surprising: The image shows that the objective indeed works as expected; but if this were not the case, then it would suggest basically a failure of optimization (i.e., the objective dictates that the image should look like this *by construction*). Still, it's a good sanity check.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500275749, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Reviewers"], "noninvitees": [], "tcdate": 1570237735951, "tmdate": 1575500275763, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review"}}}, {"id": "rklgSCRptr", "original": null, "number": 2, "cdate": 1571839543783, "ddate": null, "tcdate": 1571839543783, "tmdate": 1572972455668, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper presents a way of using generated explanations of model predictions to help prevent a model from learning \"unwanted\" relationships between features and class labels. This idea was implemented with a particular explanation generation method from prior work, called contextual decomposition (CD). For a given feature, the corresponding CD can be used to measure its importance. The proposed learning objective in this work optimizes not only the cross entropy loss, but also the difference between the CD score of a given feature and its explanation target value. Experiments show that this new learning algorithm can largely improve the classification performance.\n\nI like the high-level idea of this work and agree that there is not much work on using prediction explanations to help improve model performance. However, there are two major concerns of the model and experiment design. \n\nFirst, it seems like the proposed method requires whoever use it already know what the problem is. For example, \n\n- in section 3.3, the model inputs include a collection of features and the corresponding explanation target values.\n- in section 4.1, it is already known that some colorful patches only appear in some non-cancerous images but not in cancerous images. \n- it is even more obvious in section 4.2 and 4.3, because in both experiments, the training and test examples were altered on purpose to create some mismatch. \n\nMy question is that if we already know the bias or the mismatch, why not directly use this information in the regularization to penalize some features? Is it necessary to resort to some explanation generation methods?\n\nMy second concern is more like a personal opinion. In the experiment of section 4.2, if the colors are good indicators of these digits in the training set, I don't it is wrong for a model to capture these important features. However, the way of altering examples in the same class with different colors in training and test sets seems questionable, because now, the distributions of training and test images are different. On the other hand, if we already know color is the issue, why not simply convert the images into black-and-white? A similar argument can also be applied to the experiment in section 4.3\n\nOverall, I like the idea of using explanations to help build a better classifier. However, I am concerned about the value of this work. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500275749, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Reviewers"], "noninvitees": [], "tcdate": 1570237735951, "tmdate": 1575500275763, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review"}}}, {"id": "BJxDW5Jl5B", "original": null, "number": 3, "cdate": 1571973631449, "ddate": null, "tcdate": 1571973631449, "tmdate": 1572972455625, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose to add a regularizer to the loss function when training a prediction model. In particular, the regularizer considers explanations during the model training; if the explanations are not consistent with some prior knowledge, then explanation errors will be introduced. \n\nThe motivation for the proposed research is interesting and has some merit. However, I am a bit worried that the proposed approach is somewhat ad hoc. I can imagine there are various explanations that can be generated for the same model. There can also be different prior knowledge available for a particular problems. Which prior knowledge and explanations to use seem to affect a lot about the learned model. But there is no principled approaches for making the selection.\n\nIn some sense, standard regularizers such as L1 or L2 are are intrinsic regularizers, while the proposed regularizer is extrinsic regularizer. I think the extrinsic regularizer certainly has some merit, but it is also hard to regulate.\n\nFor instance, consider the example in Figure 2 about the presence of patches. Isn't that a too specific knowledge about the dataset, which in turn makes the proposed approach not general? I have doubts on how useful a method is if it relies on such specific prior knowledge about the data."}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575500275749, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Reviewers"], "noninvitees": [], "tcdate": 1570237735951, "tmdate": 1575500275763, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Review"}}}, {"id": "Bkg-Tf8j_B", "original": null, "number": 2, "cdate": 1570624184953, "ddate": null, "tcdate": 1570624184953, "tmdate": 1570624184953, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "SkgINhwquH", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"comment": "Thanks for your feedback on how to better customize your method to the color MNIST task. Based on your suggestion, we ran an experiment which penalized the variance between attribution of different color channels, yielding a new accuracy of 10.3%. Seeing as baseline (random) accuracy is 10% on this dataset, 10.3% is not a meaningful gain, especially relative to the 25.2% accuracy our method achieves. In fact, this solution occurs only at a high enough penalty rate that the training accuracy goes down to near random.\n\nWe will report these numbers, including the computational comparison, in an updated manuscript, when allowed to do so (after reviews have been returned).\n", "title": "Penalizing variance in attributions for color channels  does not improve performance meaningfully"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}, {"id": "SkgINhwquH", "original": null, "number": 2, "cdate": 1570565166182, "ddate": null, "tcdate": 1570565166182, "tmdate": 1570565166182, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "Syx7WyBtwB", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Public_Comment", "content": {"comment": "Hi, I\u2019m one of the authors of the Attribution Priors (Expected Gradients) method. Thank you for the citation \u2014 it\u2019s always exciting to see more work on this relatively new research area!\n\nWe noticed that you said EG has high runtime and memory requirements because we recommend 200 samples per example - but our paper actually recommends exactly the opposite! In fact, all of our image experiments (MNIST and ImageNet in the supplement), use exactly 1 sample per example during training, which corresponds to no additional memory requirements and roughly the same training speed as Ross et al. (2017). This works because a single sample is an unbiased estimator for the true value of EG! Thus, this process regularizes the true value in expectation over many training steps. \n\nWe also notice that for the color MNIST problem, you choose to penalize the magnitude of the individual EG attributions (using an L2 penalty). One benefit of our attribution priors framework is that many human-intuitive priors (such as \u201cattributions should be similar across color channels\u201d) can be directly encoded as a penalty on the EG attributions. We believe such task-specific priors can lead to greatly improved performance.\n\nWe would be eager to see further comparisons with our method, and hope this insight allows for a more computationally-manageable workload.", "title": "Computational Efficiency of Expected Gradients"}, "signatures": ["~Joseph_David_Janizek1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Joseph_David_Janizek1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504193347, "tmdate": 1576860578613, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Public_Comment"}}}, {"id": "B1gBfcu2wS", "original": null, "number": 1, "cdate": 1569651213150, "ddate": null, "tcdate": 1569651213150, "tmdate": 1569651213150, "tddate": null, "forum": "Syx7WyBtwB", "replyto": "B1epMEznwr", "invitation": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment", "content": {"comment": "Hi Pankaj,\n\nThanks for your comment. By my count, we cited 18 papers proposing different interpretation techniques, as yours does. Given the number of papers in this space, we simply can't cite everything, particularly given that the relevance here is indirect - your paper focuses on RNNs, which is only 1 of our 4 examples, and we focus on uses of interpretation techniques, not developing them.\n\nI see that you have made similar comments on many other submissions this year, many with the exact same text. I do not think this is behavior that is good for our community.", "title": "We will not add reference"}, "signatures": ["ICLR.cc/2020/Conference/Paper1536/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "authors": ["Laura Rieger", "Chandan Singh", "W. James Murdoch", "Bin Yu"], "authorids": ["lauri@dtu.dk", "c_singh@berkeley.edu", "jmurdoch@berkeley.edu", "binyu@berkeley.edu"], "keywords": ["explainability", "deep learning", "interpretability", "computer vision"], "TL;DR": "Explanations are useful now! We introduce CDEP, a technique for penalizing explanations in order to improve predictive accuracy.", "abstract": "For an explanation of a deep learning model to be effective, it must provide both insight into a model and suggest a corresponding action in order to achieve some objective.  Too often, the litany of proposed explainable deep learning methods stop  at  the  first  step,  providing  practitioners  with  insight  into  a  model,  but  no way to act on it.  In this paper, we propose contextual decomposition explanation penalization (CDEP), a method which enables practitioners to leverage existing explanation methods in order to increase the predictive accuracy of deep learning models.  In particular, when shown that a model has incorrectly assigned importance to some features, CDEP enables practitioners to correct these errors by directly regularizing the provided explanations.  Using explanations provided by contextual decomposition (CD) (Murdoch et al., 2018), we demonstrate the ability of our method to increase performance on an array of toy and real datasets.", "pdf": "/pdf/787a0ce5d582f627112b00a2143507f809127388.pdf", "code": "https://drive.google.com/drive/folders/16XHi-Onen2gjOvRx3qIUP1Z-3SvrAY4P?usp=sharing", "paperhash": "rieger|interpretations_are_useful_penalizing_explanations_to_align_neural_networks_with_prior_knowledge", "original_pdf": "/attachment/19c34622821e249be2b3fe5b142244b1480de3d3.pdf", "_bibtex": "@misc{\nrieger2020interpretations,\ntitle={Interpretations are useful: penalizing explanations to align neural networks with prior knowledge},\nauthor={Laura Rieger and Chandan Singh and W. James Murdoch and Bin Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=Syx7WyBtwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Syx7WyBtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1536/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1536/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1536/Authors|ICLR.cc/2020/Conference/Paper1536/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154561, "tmdate": 1576860545242, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1536/Authors", "ICLR.cc/2020/Conference/Paper1536/Reviewers", "ICLR.cc/2020/Conference/Paper1536/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1536/-/Official_Comment"}}}], "count": 13}