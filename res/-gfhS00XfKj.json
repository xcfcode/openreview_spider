{"notes": [{"id": "-gfhS00XfKj", "original": "XgLrrmdek8kQ", "number": 841, "cdate": 1601308097066, "ddate": null, "tcdate": 1601308097066, "tmdate": 1615899343650, "tddate": null, "forum": "-gfhS00XfKj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "GdrpX14XKFj", "original": null, "number": 1, "cdate": 1610040389621, "ddate": null, "tcdate": 1610040389621, "tmdate": 1610473983727, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper shows that transformer models can be used to learn certain advanced mathematical concepts such as the local stability of differential equations. Reviewers found this surprising and useful for engineers, and the evaluation was adequate. They also felt that it opens the doors to similar studies on other aspects of mathematics."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040389606, "tmdate": 1610473983708, "id": "ICLR.cc/2021/Conference/Paper841/-/Decision"}}}, {"id": "HQW-C05114", "original": null, "number": 3, "cdate": 1603978655531, "ddate": null, "tcdate": 1603978655531, "tmdate": 1606745767276, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Review", "content": {"title": "Really neat new dataset and application ", "review": "=Quality=\nHigh: well executed and motivated\n\n=Clarity=\nWell-situated wrt to related work.  Baseline needs to be explained more (see below).\n\n=Originality=\nThe ML method is standard. The novelty is in setting up the datasets and evaluation metrics. Doing was technically complex. The datasets very valuable and will hopefully be open-sourced.\n\n=Significance=\nThis paper demonstrates that neural networks are surpisingly good at the task of predicting certain properties of differential systems, such as their stability. This is a neat result and is sufficient for publication. However, the paper would have more impact if there was a concrete proof of concept of how such a tool could be used to improve the lives of practioners.\n\n=Motivation/Introduction=\nI agree with you that it is very impressive that neural networks can nearly solve these tasks. It would be helpful if you devoted more of the exposition to explaining how your classifiers could be used by practitioners to improve their workflows. Are there engineering applications, for example,  where the classifier could be used to quickly screen proposed systems?\n\nAlso, I'd like to better understand what the novel capability of your tool is. Is it faster than numerical methods for verifying these properties? More accurate? Applicable in situations where symbolic manipulation is impossible?\n\n=Predicting Control feedback matrices=\nI found this section very cool and encouraging for future work! I'd mention it in the intro more.\n\n\n=Baseline classifier=\nMy primary hesitation with the paper is that there is not enough justification for why your baseline is sufficient. \n\nYou need to provide far more details about the FastText baseline. Describing the name of the software package is insufficient. What is the actual model? \n\n\"Such high performances over difficult mathematical tasks may come as a surprise, and one\nmight wonder whether the model is exploiting some defect in the dataset, or some trivial\nproperty of the problems that would allow an easy way to correct solutions. We believe\nthis is very unlikely...because a trivial solution would be found by the text classification tool we use as a baseline.\"\n\nThis argument is weak without an explanation of what the baseline model is and what its inductive biases are.\n\n\nAre there no heuristics from the application community that would serve as baselines? \n\n\nIn the discussion, you mention in passing that \"providing at train time intermediate results that would help a human calculator (frequencies for PDE, or Jacobians for stability) does not improve accuracy.\" It seems to me that a baseline method would be a simple machine learning model on top of some hand-crafted features of these intermediate results.\n\n\n=Discussion section=\n\"in some of our problems, even a model with one layer and 64 dimensions\nobtains a high accuracy, and such a small model would never be able to memorize that\nmany examples.\"\nEither make this precise or remove it. At first glance, it seems to be that 64 dimensions has a lot of capacity. For example, 2^64 is much bigger than the size of your dataset.\n\n\n=Open Sourcing=\nWill you be able to open-source the datasets? Doing so would considerably increase the future impact of your work, as it would provide a benchmark for future ML methods.\n\n=Evaluation set=\nIt would be cool if you could have a couple of anecdotes of applying your classifier to famous equations from papers, particularly ones where the derivations to prove stability, for example, were quite tedious. You argue that the test set is representative, since it is uniformly sampled, but are the equations of interest to the community in some corner of this space?\n\n=Analysis/interpretation=\n\nDo the attention patterns of the transformer reveal anything interesting?\n\nWhat seems to characterize the equations that the model makes mistakes on?\n\nCan you use your model to get  per-equation embeddings? Do they reveal interesting cluster structure in the data?\n\n*** After reading the authors' responses ***\nI have raised my score to a 7. I felt that some of the key questions, e.g. regarding generalization to mathematical expressions that are qualitatively different than the training data,  were answered well. This paper should not be reviewed as being methods-driven. It's about demonstrating a new way that deep learning could be transformative for engineering, by allowing engineers to screen proposed designs for stability, etc.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133822, "tmdate": 1606915769489, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper841/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Review"}}}, {"id": "CAB_JfObxMO", "original": null, "number": 19, "cdate": 1606256891036, "ddate": null, "tcdate": 1606256891036, "tmdate": 1606256891036, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "e_xDJWqhZLX", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions 2/2", "comment": "Here is the table of the results \n\n\\begin{array}{l|c|cccc}\n        \\\\hline\n                    & \\\\text{Overall} &  \\\\text{Degree 2}  & \\\\text{Degree 3}   & \\\\text{Degree 4}   & \\\\text{Degree 5         }\\\\\\\\\n        \\\\hline                                                                                  \n        \\text{Baseline: training distribution}  & 96.4 & 98.4 & 97.3 & 95.9 & 94.1 \\\\\\\\\n        \\\\hline                                                                                  \n        \\\\text{Unary operators: no trigs}  & {95.7} & {98.8} & {97.3} & {95.5} & {91.2} \\\\\\\\\n        \\\\text{Unary operators: no logs } & {95.3} & {98.2} & {97.1} & {95.2} & {90.8} \\\\\\\\\n        \\\\text{Unary operators: no logs and trigs } & {95.7} & {98.8} & {97.7} & {95.2} & {91.0} \\\\\\\\\n        \\\\text{Unary operators: less logs and trigs}  & {95.9} & {98.8} & {96.8} & {95.0} & {93.1} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Variables and integers: 10\\\\% integers  }& {96.1} & {98.6} & {97.3} & {94.7} & {93.8} \\\\\\\\\n        \\\\text{Variables and integers: 50\\\\% integers } & {95.6} & {97.8} & {96.7} & {94.3} & {93.1} \\\\\\\\\n        \\\\text{Variables and integers: 70\\\\% integers}  & {95.7} & {95.7} & {95.9} & {95.7} & {95.5} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Expression lengths: $n+3$ to $3n+3$ } & {89.5} & {96.5} & {92.6} & {90.0} & {77.9} \\\\\\\\\n        \\\\text{Expression lengths: $2n+3$ to $4n+3$ } & {79.3} & {93.3} & {88.3} & {73.4} & {58.2} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{System degree: degree 6} & 78.7 \\\\\\\\\n\\\\hline                                                                                  \n    \\end{array}\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "e_xDJWqhZLX", "original": null, "number": 18, "cdate": 1606256822591, "ddate": null, "tcdate": 1606256822591, "tmdate": 1606256822591, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "SgmkrXTT4Cc", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions 1/2", "comment": "Following your relevant remarks and other reviewers suggestions we tested the trained model on datasets with different distributions for operators, variables, constants, expression length and system degrees. The results (given in details below) suggest that changes in the mathematical structure of the systems have little impact on accuracy and generalization. On the other hand, the distribution of the length of sequences input into the transformer impact generalization. The latter could have been expected, as it is a known weakness of transformers, but resilience to change in the mathematical structure is a nice property.  \nSurprisingly, the model is able to generalize to systems that were not in the problem space of the training set. A model trained on systems of 2 to 5 equations manages to achieve high accuracy ($78\\%$) on systems of 6 equations, even though it has never seen a system with 6 equations at train time. We detail below the way we constructed the test sets and provide detailed results and interpretations.\n\nWe modified the data generator to produce new test datasets for end to end stability prediction. Four modifications were considered:\n\n - Unary operators: varying the distribution of operators in the system. In the training data, unary operators are selected at random from a set of nine, three trigonometric functions, three inverse trigonometric functions, logarithm and exponential, and square root (the four basic operations are always present). In this set of experiments, we generated four test sets, without trigonometric functions, without logs and exponentials, only with square roots, and with a different balance of operators (mostly square roots).\n- Variables and integers: varying the distribution of variables in the system. In the training data, $30\\%$ of the leaves are numbers, the rest variables. We changed this probability to $0.1$, $0.5$ and $0.7$. This has no impact on expression length, but higher probabilities make the Jacobians more sparse.\n- Expression lengths: making expressions longer than in the train set. In the training data, for a system of $n$ equations, we generate functions with $3$ to $2n+3$ operators. In this experiments, we tried functions between $n+3$ and $3n+3$ and $2n+3$ and $4n+3$. This means that the test sequences are, on average, much longer that those seen at training, a known weakness of sequence to sequence models.\n- Larger degree: our models were trained on systems with $2$ to $5$ equations, we tried to test it on systems with $6$ equations. Again, this usually proves difficult for transformers.\n\nNote that the two first sets of experiments feature out-of-distribution tests, exploring different distributions over the same problem space as the training data. The two last sets, on the other hand, explore a different problem space, featuring longer sequences.\nA table below presents the results of these experiments. Changing the distribution of operators, variables and integers has little impact on accuracy, up to two limiting cases. First, over systems of degree five (the largest in our set, and more difficult for the transformers) change in operator distribution has a small adverse impact on performance (but not change in variable distribution). Second, which the proportion of integers become very large, and therefore Jacobians become very sparse, the degree of the systems has less impact on performance. But overall results remain over $95\\%$, and the model proves to be very resistant to changes in distribution over the same problem space.\n\nOver systems with longer expressions, overall accuracy tends to decreases. Yet, systems of two or three equations are not affected by a doubling of the number of operators (and sequence length), compared to the training data. Most of the loss in performance concentrates on larger degrees, which suggests that it results from the fact that the transformer is presented at test time with much longer sequences that what it saw at training. In any case, all results but one are well above the fastText baseline ($60.5\\%$).\n\nWhen tested on systems with six equations, the trained model predicts stability in $78.7\\%$ of cases. This is a very interesting result, where the model is extrapolating out of the problem space (i.e. no system of six equations have been seen during training) with an accuracy well above chance level, and the fastText baseline. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "HMAMFxLLc5r", "original": null, "number": 17, "cdate": 1606256653628, "ddate": null, "tcdate": 1606256653628, "tmdate": 1606256653628, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "xbyGu5jsqqF", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions and generalizability 2/2", "comment": "Here is the table of the results \n\n\\begin{array}{l|c|cccc}\n        \\\\hline\n                    & \\\\text{Overall} &  \\\\text{Degree 2}  & \\\\text{Degree 3}   & \\\\text{Degree 4}   & \\\\text{Degree 5         }\\\\\\\\\n        \\\\hline                                                                                  \n        \\text{Baseline: training distribution}  & 96.4 & 98.4 & 97.3 & 95.9 & 94.1 \\\\\\\\\n        \\\\hline                                                                                  \n        \\\\text{Unary operators: no trigs}  & {95.7} & {98.8} & {97.3} & {95.5} & {91.2} \\\\\\\\\n        \\\\text{Unary operators: no logs } & {95.3} & {98.2} & {97.1} & {95.2} & {90.8} \\\\\\\\\n        \\\\text{Unary operators: no logs and trigs } & {95.7} & {98.8} & {97.7} & {95.2} & {91.0} \\\\\\\\\n        \\\\text{Unary operators: less logs and trigs}  & {95.9} & {98.8} & {96.8} & {95.0} & {93.1} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Variables and integers: 10\\\\% integers  }& {96.1} & {98.6} & {97.3} & {94.7} & {93.8} \\\\\\\\\n        \\\\text{Variables and integers: 50\\\\% integers } & {95.6} & {97.8} & {96.7} & {94.3} & {93.1} \\\\\\\\\n        \\\\text{Variables and integers: 70\\\\% integers}  & {95.7} & {95.7} & {95.9} & {95.7} & {95.5} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Expression lengths: $n+3$ to $3n+3$ } & {89.5} & {96.5} & {92.6} & {90.0} & {77.9} \\\\\\\\\n        \\\\text{Expression lengths: $2n+3$ to $4n+3$ } & {79.3} & {93.3} & {88.3} & {73.4} & {58.2} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{System degree: degree 6} & 78.7 \\\\\\\\\n\\\\hline                                                                                  \n    \\end{array}\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "QfW9LQdSE1", "original": null, "number": 16, "cdate": 1606256606354, "ddate": null, "tcdate": 1606256606354, "tmdate": 1606256606354, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "WE-n-UFFtV", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions 2/2", "comment": "Here is the table of the results \n\n\\begin{array}{l|c|cccc}\n        \\\\hline\n                    & \\\\text{Overall} &  \\\\text{Degree 2}  & \\\\text{Degree 3}   & \\\\text{Degree 4}   & \\\\text{Degree 5         }\\\\\\\\\n        \\\\hline                                                                                  \n        \\text{Baseline: training distribution}  & 96.4 & 98.4 & 97.3 & 95.9 & 94.1 \\\\\\\\\n        \\\\hline                                                                                  \n        \\\\text{Unary operators: no trigs}  & {95.7} & {98.8} & {97.3} & {95.5} & {91.2} \\\\\\\\\n        \\\\text{Unary operators: no logs } & {95.3} & {98.2} & {97.1} & {95.2} & {90.8} \\\\\\\\\n        \\\\text{Unary operators: no logs and trigs } & {95.7} & {98.8} & {97.7} & {95.2} & {91.0} \\\\\\\\\n        \\\\text{Unary operators: less logs and trigs}  & {95.9} & {98.8} & {96.8} & {95.0} & {93.1} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Variables and integers: 10\\\\% integers  }& {96.1} & {98.6} & {97.3} & {94.7} & {93.8} \\\\\\\\\n        \\\\text{Variables and integers: 50\\\\% integers } & {95.6} & {97.8} & {96.7} & {94.3} & {93.1} \\\\\\\\\n        \\\\text{Variables and integers: 70\\\\% integers}  & {95.7} & {95.7} & {95.9} & {95.7} & {95.5} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Expression lengths: $n+3$ to $3n+3$ } & {89.5} & {96.5} & {92.6} & {90.0} & {77.9} \\\\\\\\\n        \\\\text{Expression lengths: $2n+3$ to $4n+3$ } & {79.3} & {93.3} & {88.3} & {73.4} & {58.2} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{System degree: degree 6} & 78.7 \\\\\\\\\n\\\\hline                                                                                  \n    \\end{array}\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "wv2aUMotznU", "original": null, "number": 15, "cdate": 1606256570543, "ddate": null, "tcdate": 1606256570543, "tmdate": 1606256570543, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "B3e2HgghTd6", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions 2/2", "comment": "Here is the table of the results \n\n\\begin{array}{l|c|cccc}\n        \\\\hline\n                    & \\\\text{Overall} &  \\\\text{Degree 2}  & \\\\text{Degree 3}   & \\\\text{Degree 4}   & \\\\text{Degree 5         }\\\\\\\\\n        \\\\hline                                                                                  \n        \\text{Baseline: training distribution}  & 96.4 & 98.4 & 97.3 & 95.9 & 94.1 \\\\\\\\\n        \\\\hline                                                                                  \n        \\\\text{Unary operators: no trigs}  & {95.7} & {98.8} & {97.3} & {95.5} & {91.2} \\\\\\\\\n        \\\\text{Unary operators: no logs } & {95.3} & {98.2} & {97.1} & {95.2} & {90.8} \\\\\\\\\n        \\\\text{Unary operators: no logs and trigs } & {95.7} & {98.8} & {97.7} & {95.2} & {91.0} \\\\\\\\\n        \\\\text{Unary operators: less logs and trigs}  & {95.9} & {98.8} & {96.8} & {95.0} & {93.1} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Variables and integers: 10\\\\% integers  }& {96.1} & {98.6} & {97.3} & {94.7} & {93.8} \\\\\\\\\n        \\\\text{Variables and integers: 50\\\\% integers } & {95.6} & {97.8} & {96.7} & {94.3} & {93.1} \\\\\\\\\n        \\\\text{Variables and integers: 70\\\\% integers}  & {95.7} & {95.7} & {95.9} & {95.7} & {95.5} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{Expression lengths: $n+3$ to $3n+3$ } & {89.5} & {96.5} & {92.6} & {90.0} & {77.9} \\\\\\\\\n        \\\\text{Expression lengths: $2n+3$ to $4n+3$ } & {79.3} & {93.3} & {88.3} & {73.4} & {58.2} \\\\\\\\\n\\\\hline                                                                                  \n        \\\\text{System degree: degree 6} & 78.7 \\\\\\\\\n\\\\hline                                                                                  \n    \\end{array}\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "xbyGu5jsqqF", "original": null, "number": 14, "cdate": 1606256449034, "ddate": null, "tcdate": 1606256449034, "tmdate": 1606256449034, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "kZjWyov4f_3", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions and generalizability 1/2", "comment": "Following your suggestion we tested the trained model on datasets with different distributions for operators, variables, constants, expression length and system degrees. The results (given in details below) suggest that changes in the mathematical structure of the systems have little impact on accuracy and generalization. On the other hand, the distribution of the length of sequences input into the transformer impact generalization. The latter could have been expected, as it is a known weakness of transformers, but resilience to change in the mathematical structure is a nice property.  \nSurprisingly, the model is able to generalize to systems that were not in the problem space of the training set. A model trained on systems of 2 to 5 equations manages to achieve high accuracy ($78\\%$) on systems of 6 equations, even though it has never seen a system with 6 equations at train time. We detail below the way we constructed the test sets and provide detailed results and interpretations.\n\nWe modified the data generator to produce new test datasets for end to end stability prediction. Four modifications were considered:\n\n - Unary operators: varying the distribution of operators in the system. In the training data, unary operators are selected at random from a set of nine, three trigonometric functions, three inverse trigonometric functions, logarithm and exponential, and square root (the four basic operations are always present). In this set of experiments, we generated four test sets, without trigonometric functions, without logs and exponentials, only with square roots, and with a different balance of operators (mostly square roots).\n- Variables and integers: varying the distribution of variables in the system. In the training data, $30\\%$ of the leaves are numbers, the rest variables. We changed this probability to $0.1$, $0.5$ and $0.7$. This has no impact on expression length, but higher probabilities make the Jacobians more sparse.\n- Expression lengths: making expressions longer than in the train set. In the training data, for a system of $n$ equations, we generate functions with $3$ to $2n+3$ operators. In this experiments, we tried functions between $n+3$ and $3n+3$ and $2n+3$ and $4n+3$. This means that the test sequences are, on average, much longer that those seen at training, a known weakness of sequence to sequence models.\n- Larger degree: our models were trained on systems with $2$ to $5$ equations, we tried to test it on systems with $6$ equations. Again, this usually proves difficult for transformers.\n\nNote that the two first sets of experiments feature out-of-distribution tests, exploring different distributions over the same problem space as the training data. The two last sets, on the other hand, explore a different problem space, featuring longer sequences.\nA table below presents the results of these experiments. Changing the distribution of operators, variables and integers has little impact on accuracy, up to two limiting cases. First, over systems of degree five (the largest in our set, and more difficult for the transformers) change in operator distribution has a small adverse impact on performance (but not change in variable distribution). Second, which the proportion of integers become very large, and therefore Jacobians become very sparse, the degree of the systems has less impact on performance. But overall results remain over $95\\%$, and the model proves to be very resistant to changes in distribution over the same problem space.\n\nOver systems with longer expressions, overall accuracy tends to decreases. Yet, systems of two or three equations are not affected by a doubling of the number of operators (and sequence length), compared to the training data. Most of the loss in performance concentrates on larger degrees, which suggests that it results from the fact that the transformer is presented at test time with much longer sequences that what it saw at training. In any case, all results but one are well above the fastText baseline ($60.5\\%$).\n\nWhen tested on systems with six equations, the trained model predicts stability in $78.7\\%$ of cases. This is a very interesting result, where the model is extrapolating out of the problem space (i.e. no system of six equations have been seen during training) with an accuracy well above chance level, and the fastText baseline. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "WE-n-UFFtV", "original": null, "number": 13, "cdate": 1606256395963, "ddate": null, "tcdate": 1606256395963, "tmdate": 1606256395963, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "uPjRDw0FG0J", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing on different distributions 1/2", "comment": "Following your suggestion we tested the trained model on datasets with different distributions for operators, variables, constants, expression length and system degrees. The results (given in details below) suggest that changes in the mathematical structure of the systems have little impact on accuracy and generalization. On the other hand, the distribution of the length of sequences input into the transformer impact generalization. The latter could have been expected, as it is a known weakness of transformers, but resilience to change in the mathematical structure is a nice property.  \nSurprisingly, the model is able to generalize to systems that were not in the problem space of the training set. A model trained on systems of 2 to 5 equations manages to achieve high accuracy ($78\\%$) on systems of 6 equations, even though it has never seen a system with 6 equations at train time. We detail below the way we constructed the test sets and provide detailed results and interpretations.\n\nWe modified the data generator to produce new test datasets for end to end stability prediction. Four modifications were considered:\n\n - Unary operators: varying the distribution of operators in the system. In the training data, unary operators are selected at random from a set of nine, three trigonometric functions, three inverse trigonometric functions, logarithm and exponential, and square root (the four basic operations are always present). In this set of experiments, we generated four test sets, without trigonometric functions, without logs and exponentials, only with square roots, and with a different balance of operators (mostly square roots).\n- Variables and integers: varying the distribution of variables in the system. In the training data, $30\\%$ of the leaves are numbers, the rest variables. We changed this probability to $0.1$, $0.5$ and $0.7$. This has no impact on expression length, but higher probabilities make the Jacobians more sparse.\n- Expression lengths: making expressions longer than in the train set. In the training data, for a system of $n$ equations, we generate functions with $3$ to $2n+3$ operators. In this experiments, we tried functions between $n+3$ and $3n+3$ and $2n+3$ and $4n+3$. This means that the test sequences are, on average, much longer that those seen at training, a known weakness of sequence to sequence models.\n- Larger degree: our models were trained on systems with $2$ to $5$ equations, we tried to test it on systems with $6$ equations. Again, this usually proves difficult for transformers.\n\nNote that the two first sets of experiments feature out-of-distribution tests, exploring different distributions over the same problem space as the training data. The two last sets, on the other hand, explore a different problem space, featuring longer sequences.\nA table below presents the results of these experiments. Changing the distribution of operators, variables and integers has little impact on accuracy, up to two limiting cases. First, over systems of degree five (the largest in our set, and more difficult for the transformers) change in operator distribution has a small adverse impact on performance (but not change in variable distribution). Second, which the proportion of integers become very large, and therefore Jacobians become very sparse, the degree of the systems has less impact on performance. But overall results remain over $95\\%$, and the model proves to be very resistant to changes in distribution over the same problem space.\n\nOver systems with longer expressions, overall accuracy tends to decreases. Yet, systems of two or three equations are not affected by a doubling of the number of operators (and sequence length), compared to the training data. Most of the loss in performance concentrates on larger degrees, which suggests that it results from the fact that the transformer is presented at test time with much longer sequences that what it saw at training. In any case, all results but one are well above the fastText baseline ($60.5\\%$).\n\nWhen tested on systems with six equations, the trained model predicts stability in $78.7\\%$ of cases. This is a very interesting result, where the model is extrapolating out of the problem space (i.e. no system of six equations have been seen during training) with an accuracy well above chance level, and the fastText baseline. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "B3e2HgghTd6", "original": null, "number": 12, "cdate": 1606256078842, "ddate": null, "tcdate": 1606256078842, "tmdate": 1606256354946, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "fyBLBhbuP3", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Testing the models on different distributions 1/2", "comment": "Following your suggestion we tested the trained model on datasets with different distributions for operators, variables, constants, expression length and system degrees. The results (given in details below) suggest that changes in the mathematical structure of the systems have little impact on accuracy and generalization. On the other hand, the distribution of the length of sequences input into the transformer impact generalization. The latter could have been expected, as it is a known weakness of transformers, but resilience to change in the mathematical structure is a nice property.  \nSurprisingly, the model is able to generalize to systems that were not in the problem space of the training set. A model trained on systems of 2 to 5 equations manages to achieve high accuracy ($78\\%$) on systems of 6 equations, even though it has never seen a system with 6 equations at train time. We detail below the way we constructed the test sets and provide detailed results and interpretations.\n\nWe modified the data generator to produce new test datasets for end to end stability prediction. Four modifications were considered:\n\n - Unary operators: varying the distribution of operators in the system. In the training data, unary operators are selected at random from a set of nine, three trigonometric functions, three inverse trigonometric functions, logarithm and exponential, and square root (the four basic operations are always present). In this set of experiments, we generated four test sets, without trigonometric functions, without logs and exponentials, only with square roots, and with a different balance of operators (mostly square roots).\n- Variables and integers: varying the distribution of variables in the system. In the training data, $30\\%$ of the leaves are numbers, the rest variables. We changed this probability to $0.1$, $0.5$ and $0.7$. This has no impact on expression length, but higher probabilities make the Jacobians more sparse.\n- Expression lengths: making expressions longer than in the train set. In the training data, for a system of $n$ equations, we generate functions with $3$ to $2n+3$ operators. In this experiments, we tried functions between $n+3$ and $3n+3$ and $2n+3$ and $4n+3$. This means that the test sequences are, on average, much longer that those seen at training, a known weakness of sequence to sequence models.\n- Larger degree: our models were trained on systems with $2$ to $5$ equations, we tried to test it on systems with $6$ equations. Again, this usually proves difficult for transformers.\n\nNote that the two first sets of experiments feature out-of-distribution tests, exploring different distributions over the same problem space as the training data. The two last sets, on the other hand, explore a different problem space, featuring longer sequences.\nA table below presents the results of these experiments. Changing the distribution of operators, variables and integers has little impact on accuracy, up to two limiting cases. First, over systems of degree five (the largest in our set, and more difficult for the transformers) change in operator distribution has a small adverse impact on performance (but not change in variable distribution). Second, which the proportion of integers become very large, and therefore Jacobians become very sparse, the degree of the systems has less impact on performance. But overall results remain over $95\\%$, and the model proves to be very resistant to changes in distribution over the same problem space.\n\nOver systems with longer expressions, overall accuracy tends to decreases. Yet, systems of two or three equations are not affected by a doubling of the number of operators (and sequence length), compared to the training data. Most of the loss in performance concentrates on larger degrees, which suggests that it results from the fact that the transformer is presented at test time with much longer sequences that what it saw at training. In any case, all results but one are well above the fastText baseline ($60.5\\%$).\n\nWhen tested on systems with six equations, the trained model predicts stability in $78.7\\%$ of cases. This is a very interesting result, where the model is extrapolating out of the problem space (i.e. no system of six equations have been seen during training) with an accuracy well above chance level, and the fastText baseline. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "WrMMqtz75fi", "original": null, "number": 7, "cdate": 1605650084550, "ddate": null, "tcdate": 1605650084550, "tmdate": 1606255554135, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "HQW-C05114", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to reviewer 2 - 1/4", "comment": "Thanks you very much for your review, here is a preliminary reply (the last part, about potential applications will be updated later)\n\n ===== I found this section very cool and encouraging for future work! I'd mention it in the intro more. ====\n\nThanks for finding this cool! It is indeed encouraging for future work that could try to predict similarly other mathematical/numerical quantities. We'll emphasize this in the intro following your comment.\n\n==== My primary hesitation with the paper is that there is not enough justification for why your baseline is sufficient.\nYou need to provide far more details about the FastText baseline. Describing the name of the software package is insufficient. What is the actual model? ========\n\nDefining a baseline for such tasks is difficult. Since all the problems we study can be solved by mathematical libraries, a \"math library\" baseline would be 100%. On the other hand, there are no previous attempts to solve these problems with deep learning models. We retained fastText because it is a basic but state-of-the-art NLP model, which has proven to be as powerful as deep learning classifiers. \n\nWe will add a short presentation of fastText in the updated version of the paper. It implements a bag of words, which estimates the conditional probability of a binary output given the distribution of tokens (or fixed sequences up to N tokens, here N=5) in the input. Bag of words are often used for sentiment analysis (e.g. detecting whether a comment is positive or negative), and will pick up simple correlations between inputs and outputs (like stability being dependent on the degree of the system, or the presence of some specific operator). Another virtue of fastText is that it would pick up obvious solutions due to a trivial case of the problem, or a glitch in the generating procedure. In this respect, we believe it provides a relevant baseline, as a minimal NLP solution for these problems.\n\n===== Are there no heuristics from the application community that would serve as baselines? =====\n\nHeuristics from the application community are an interesting idea, and there are indeed a few that could serve as baselines. Unfortunately, they are limited to simple or trivial cases. For the controllability problem, for instance, the answer can be guessed if: one variable is missing in all the right-hand sides of the system; an equation only depends on variables that are themselves uncontrolled; the system is triangular with non-zero dominant coefficients, etc. But these special cases will nearly never appear in randomly generated systems, and the baseline would be very low. Besides, most of those cases are likely to be picked up by fastText. Such heuristics could be used for testing, though, by using a dataset of special cases and assessing the ability of the model on these cases. It is all the more interesting as such cases will almost never appear in the training set.\n\n===== Either make this precise or remove it. At first glance, it seems to be that 64 dimensions has a lot of capacity. For example, 2^64 is much bigger than the size of your dataset. =====\n\nYou are fully correct in raising this point, thank you for the remark. We will change this sentence. Our idea is that under the constraints of gradient learning, it would probably take a lot more than 64 dimensions to memorize 50 million examples. \nWhat we meant was that, in regular NLP, such shallow and low dimension transformers are too small to learn even the basic syntax of natural languages.\n\n===== Will you be able to open-source the datasets? Doing so would considerably increase the future impact of your work, as it would provide a benchmark for future ML methods. =====\n\nYes, of course. We will not only open-source the dataset but also the code of the generators for each task so that it should be fairly easy to modify it should someone want to apply this approach to another application.\n\n====== It would be cool if you could have a couple of anecdotes of applying your classifier to famous equations from papers, particularly ones where the derivations to prove stability, for example, were quite tedious. You argue that the test set is representative, since it is uniformly sampled, but are the equations of interest to the community in some corner of this space? =====\n\nThank you for this nice suggestion. That would be cool indeed, and the equations of interest to the community are definitely in our space of function. We will add a subsection in Appendix with the outputs of our model when applied on famous/ textbook equations: Schroedinger, heat equations for the PDEs, damped/amplified transport equations ; an example of stability analysis taken from a maths paper where the proof of stability is tedious and requires some change of variables (Bando-FtL model for traffic flow) ; some example of controllability taken from a classic textbook (inverted Cart-Pendulum). They might not be ready for the rebuttal, but they will for the final version."}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "poJ26X8kUNt", "original": null, "number": 8, "cdate": 1605650233961, "ddate": null, "tcdate": 1605650233961, "tmdate": 1605967717868, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "WrMMqtz75fi", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to reviewer 2 - 2/4", "comment": "===== Do the attention patterns of the transformer reveal anything interesting?\nWhat seems to characterize the equations that the model makes mistakes on?\nCan you use your model to get per-equation embeddings? Do they reveal interesting cluster structure in the data? =====\n\nWe tried to analyze the model behavior by looking at the attention heads of the model, and the tokens the models focus on when it predicts a specific sequence, following the paper ``What does bert look at? an analysis of bert's attention'' by Clark et al., 2019. Unfortunately, we were not able to extract specific patterns from this analysis. Unlike what was observed in NLP, we found that each head in the model tends to attends much more tokens than in natural language (i.e. the attention weights are quite uniformly distributed), and this from the first layer. As a result, the information about the input tokens are spread quite early in the layers of the model, which makes interpretation very difficult. We tried to reduce the number of hidden states a head can attend by using a top-k on the attention weights, to see if this facilitates the visualization. Unfortunately, this modification in the model deteriorated the performance quite significantly, and we did not investigate more in this direction.\n\nWe ran a sequence-to-sequence model without attention, so that each input equation is mapped to a fixed-sized representation. We then fed a set of input equations into the model, and used a t-SNE visualization to see whether we can observe clusters of equations. What we observed is mainly that equations with nearby representations have similar length / tokens. However, even embeddings in similar locations can lead to different decoded sequences. Potentially, based on how the computation is split between the encoder and the decoder, the decoder could be doing the majority of the work, which means that the representations provided by the encoder would not be so meaningful. If the computation is mainly done by the encoder, this is not something we managed to conclude from the visualizations."}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "f_n6mQ250P3", "original": null, "number": 10, "cdate": 1605738017573, "ddate": null, "tcdate": 1605738017573, "tmdate": 1605738813811, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "poJ26X8kUNt", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 2 - 3/4", "comment": "======Motivation/Introduction= I agree with you that it is very impressive that neural networks can nearly solve these tasks. It would be helpful if you devoted more of the exposition to explaining how your classifiers could be used by practitioners to improve their workflows. Are there engineering applications, for example, where the classifier could be used to quickly screen proposed systems?\n\nAlso, I'd like to better understand what the novel capability of your tool is. Is it faster than numerical methods for verifying these properties? More accurate? Applicable in situations where symbolic manipulation is impossible?========\n\nThank you for this suggestion, it would indeed be interesting to compare the pros and cons of using our tools vs. the usual numerical algorithms. As we are primarily trying to show that a language model can be used to learn well-known mathematical properties of the classical theory, the symbolic manipulation and resolution of the problems we consider is always possible. However for practical use, our tool seems to be much faster than the current numerical methods, especially for the non-autonomous controllability; the prediction of stabilizing feedback matrices. This can be summarized in the following table giving the time (in seconds) needed to solve the different problems per system. The numerical algorithms run in python and uses libraries of numpy and/or scipy and/or sympy depending on the task. \n\\begin{array} {|l|c|c|}\\\\hline \\text{Tasks} & \\text{Python} & \\text{Transformer} \\\\\\\\ \\\\hline \\text{Stability end-to-end} & 0.02 & 0.0008 \\\\\\\\  \\text{Stability largest eigenvalue} & 0.02 & 0.002 \\\\\\\\ \\\\hline \\text{Controllability (autonomous)} & 0.05 & 0.001 \\\\\\\\ \\\\hline \\text{Predicting a feedback matrix} & 0.4 & 0.002 \\\\\\\\  \\\\hline \\end{array}\nThe numerical algorithms in python might have room for improvement, even though they use classical libraries, but there is a speed-up factor 10 to 200 when using the transformer which suggests that transformers outperform by far the use of classical numerical algorithms. We remark that the speed of the transformer is about the same for all the tasks while the speed of the classical numerical algorithms changes a lot. One explanation would be that a trained network with a given architecture will have about the same speed whatever the task for inputs of the same size, while the classical methods to solve the mathematical problems depends clearly on the difficulty of the task for a fixed size of input. This is another argument to use transformers for solvable but computationally costly mathematical problems. We will include in Appendix some of these comparisons.\nConcerning the prediction of stabilizing feedback matrices our tool has another interesting advantage: most of the time it predicts different feedback matrices than the one that would be usually computed with the classical methods. Of course there would probably be a theoretical way to derive them, but they are not given directly by the usual formula used by the classical numerical algorithms, therefore our tool might introduce some more diversity.\n\n\nOn a higher level, this approach could interest scientists of the applicative community in two other ways:\n- There are stability-like problems for which one has no classical numerical method that would work in any cases, but many methods that would each work in some particular cases. What could be done would be to use these methods separately to create a large dataset as we are doing here, and then learn to solve the problem with our model. We would then have a unique tool that would replace all the particular methods. To be fair this idea does not come from us: we have been recently contacted by computational biologists who would like to work with us on such a tool after seeing this paper.\n- Here the problems we study have known solutions, but the high accuracy results suggest that the same approach could be tried on open or computationally hard problems. The only requirements are building a training sample (either by solving some representative cases or generating problems from known solutions), and being able to check the validity of solutions (which is usually much easier than solving the problem). Then, a trained model could guess solutions of open problems, which would be interesting and very useful in mathematics even with low accuracy.\n\nFinally, in the long term, it is likely that scientists will try to automatically derive mathematical or physical models using machine learning. Our research could provide a way to screen those models that have some relevant property, or select the most promising models in a large generated batch, so that scientist can focus on improving them. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "uPjRDw0FG0J", "original": null, "number": 11, "cdate": 1605738162970, "ddate": null, "tcdate": 1605738162970, "tmdate": 1605738521998, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "f_n6mQ250P3", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 2 - 4/4", "comment": "===========In the discussion, you mention in passing that \"providing at train time intermediate results that would help a human calculator (frequencies for PDE, or Jacobians for stability) does not improve accuracy.\" It seems to me that a baseline method would be a simple machine learning model on top of some hand-crafted features of these intermediate results.==========\n\nThank you for this suggestion. We could definitely try to feed more systematically as input some intermediate results (either steps that would be useful to a human, or steps used in the classical theory) and see whether these features improve the performance of the model. This should give us some insights about what the model is doing. Indeed, if adding intermediary results from the classical theory improves the performance significantly, it would suggest that the model is using a method different from the theory, otherwise these additional features would be redundant and would not improve the performance of the model. We will try to see if we can include this in time for the rebuttal, otherwise we will include it in a future version of the paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "iPRcPhzNUyb", "original": null, "number": 9, "cdate": 1605735179730, "ddate": null, "tcdate": 1605735179730, "tmdate": 1605737930255, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "H3CdLD7vPLj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Speed comparison between a trained transformer and math libraries", "comment": "As a complement to our reply on your first point (asymptotic speed comparisons between our methods and known mathematical algorithms), we ran a series of experiments, comparing the speed of our trained models to the mathematical algorithms implemented in Python (using Numpy, Sympy, Scipy), over 10000 random problems. Below are the average times (in seconds) needed to solve one problem.\n\n\\begin{array} {|l|c|c|} \\\\hline \\text{Tasks} & \\text{Python} & \\text{Transformer} \\\\\\\\  \n\\\\hline \\text{Stability end-to-end} & 0.02 & 0.0008 \\\\\\\\ \n\\\\hline \\text{Stability largest eigenvalue} & 0.02 & 0.002 \\\\\\\\  \n\\\\hline \\text{Controllability (autonomous)} & 0.05 & 0.001 \\\\\\\\  \n\\\\hline \\text{Predicting a feedback matrix} & 0.4 & 0.002 \\\\\\\\  \n\\\\hline \\end{array}\n\nOverall, trained transformers accelerate the calculation by a factor 10 to 200, depending on the problem. It should be noted that transformers operate at the same speed for all problems. This is because computation speed over a trained neural network, is a function of the size and architecture of the network, and not of the nature of the problem solved. The speed of mathematical algorithms, on the other hand, is very dependent on the number of steps they include, and the presence of specific, costly, operations (e.g. integration when computing feedback matrices)\n\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "MHFk7Hx2l2U", "original": null, "number": 6, "cdate": 1605642037322, "ddate": null, "tcdate": 1605642037322, "tmdate": 1605642649468, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "H3CdLD7vPLj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 4 - 1/2", "comment": "Thank you very much for your comments, here is a preliminary reply.\n\n===== I would have appreciated more of a discussion about the computational cost of solving these problems mathematically vs. solving them with a neural network. What is the computational complexity (Big-O) of each of the known mathematical algorithms for solving these problems? Are there large computational savings from using a neural network? What are the practical implications of the results shown in this paper? ====\n\nLooking at the complexity of numerical algorithms is indeed a natural idea and a good suggestion. Let n be the system size, p the number of variables and q the average length (in tokens) of functions in the system. In all problems considered here, we have $p = O(n)$. Differentiating or evaluating an expression with q tokens is $O(q)$, and calculating the Jacobian of our system is $O(npq)$, i.e. $O(n^2 q)$.\n\nIn the stability experiment, calculating the eigenvalues of the Jacobian will be $O(n^3)$ in most practical situations. In the autonomous controllability experiments, construction of the $n \\times np$ Kalman matrix is $O(n^3 p)$, and computing its rank, via singular value decomposition or any equivalent algorithm, will be $O(n^3 p)$ as well. The same complexity arise for feedback matrix computations (multiplication, exponentiation and inversion are all $O(n^3)$ for a square n matrix). As a result, for controllability, complexity is $O(n^4)$. Overall, the classical algorithms have a complexity of $O(n^2 q)$ for Jacobian calculation, and $O(n^3)$ (stability) and $O(n^4)$ (controllability) for the problem specific computations.\n\nCurrent transformer architectures are quadratic in the length of the sequence, in our case $nq$, so a transformer will be $O(n^2 q^2)$ (in speed and memory usage). Therefore, the final comparison will depend on how q, the average length of equations, varies with $n$, the number of parameters. If $q=O(1)$ or $O(log(n))$, transformers have a large advantage over classical methods. This means sparse Jacobians, a condition often met in practice. For controllability, the advantage remains if $q=O(n^{1/2})$, and the two methods are asymptotically equivalent if $q=O(n)$.\n\nHowever, current research is working on improving transformer complexity to log-linear or linear. If this happened (and there seem to be no theoretical reason preventing it), transformers would have lower asymptotic complexity in all cases.\n\nFor given values of n and q, transformers might run faster than most mathematical algorithms, because they use simple matrix operations that can easily be parallelized.\n\n===== I think including a few more baselines would have been useful. Also, a brief description of the FastText model would make the paper more self-contained. One question: Why is the FastText model in Section 5.1 only trained with 2 million examples, while the transformer model is trained with 50 million examples? ======\n\nBaselines for such problems are difficult to define. From a mathematical standpoint, every problem can be solved using mathematics software, so baseline accuracy would be $100%$. From a deep learning standpoint, there is no pre-existing state of the art. We considered FastText because of its simplicity/efficiency trade-off. Although it is based on bag of words, fastText is sometimes on par with deep learning classifiers in terms of accuracy (sometimes reaching the performance of 29-layer deep models when it uses N-gram features). Its main virtue is that it rules out obvious solutions, due to the specifics of one problem, or glitches in the data generator. We will add a brief presentation of fastText in the paper, thank you for your suggestion.\n\nre: fastText trained over 2 million examples only. Being simpler models, bag of words need less data than sequence-to-sequence models to train to saturation. On the data from section 5.1, fastText accuracy was 59.7% after 500 000 examples, 60.3% after 1 million, 60.4% after 1.5 million, 60.5% after 1.75 million examples, and 60.6% after 2 million examples, and did not increase after that. Over the same dataset, transformers needed 22 millions examples to achieve their best solution, but their accuracy tended to saturate after 5-10 million examples.\n\n======== In the current experiments, the test set is drawn from the same exact (random) distribution as the training set. I was very curious whether the model would have been able to attain high test-time accuracy, had the test examples been drawn from a different distribution. (...) =====\n\nThank you for this suggestion! It would be interesting to see this indeed. We will generate different distributions for test sets on the stability problem, by varying the parameters used for random tree generation. For instance, we can bias the test set toward specific operators, a different distribution of expression lengths or a different balance between constants and variables. This analysis should be ready in a few days.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "fyBLBhbuP3", "original": null, "number": 5, "cdate": 1605641978588, "ddate": null, "tcdate": 1605641978588, "tmdate": 1605642344206, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "H3CdLD7vPLj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 4 - 2/2", "comment": "===== The paper says \u201ctraining is performed on 8 V100 GPUs with float16 operations\u201d: It would have been nice to hear more about the training process: For example, low long did training take for the various problems, and how was training distributed across the GPUs? ====\n\nOn autonomous control, it took 11 hours for the model to reach its best accuracy of 97.4%. However, the performance was already of 95% after 4 hours. Numerical computations took more time. For the stability experiment, our best model reached its best accuracy of 86.8% after 76 hours of training, while the accuracy was only of 80% after 24 hours.  These figures are typical, on average, qualitative tasks took about 12 hours to train, and computations between two and three days. We added a comment in the paper to this effect. \n\nThe model is distributed across GPUs, so that all GPUs have access to the same shared copy of the model. At each iteration, each GPU processes an independently generated batch, and updates the model weights based on the gradients averaged over all workers. Overall, this setup is equivalent to training on a single GPU, but with N times larger batches (where N is the number of GPUs). We will provide additional details on the training process in the updated version of the paper.\n\n===== I would have appreciated more of a discussion about the broader implications and significance of these results. =====\n\nOur results expand on Lample and Charton (2020) findings that transformers can be trained to perform symbolic mathematics. One practical limitation of their results was the fact that many scientific problems include a mixture of symbolic and numerical computations. And prior work on arithmetic had suggested that neural networks did not perform very well on pure computational tasks. Our research shows that transformers can handle mixed problems that involve both symbolic and numerical operations. This suggests that the potential use of transformers in science might be larger than previously thought and be of large interest in many areas where this framework with both symbolic and numerical operations appear: physics, computational-biology, theoretical chemistry, etc.\n\nThat transformers can learn complex computations from examples is highly non-intuitive. Especially, providing some intermediate results at train time as additional inputs does not seem to improve accuracy, which suggests that transformers exploit shortcuts to solve these problems. Given the accuracy, these shortcuts are likely to be another way of solving the problem which is mathematically equivalent to the classical way. As such equivalent way is not yet known, it would be very interesting to understand how these shortcut works.\n\nAlso, the problems selected here have known solutions, but the same approach could be tried on open or computationally hard problems. The only requirements are building a training sample (either by solving some representative cases or generating problems from known solutions), and being able to check the validity of solutions (which is usually much easier than solving the problem). If this can be done, a trained model could guess solutions of open problems, which would be interesting and very useful in mathematics even if the accuracy is low.\n\nIn the long term, it is likely that scientists will try to automatically derive mathematical or physical models using machine learning. Our research could provide a way to screen those models that have some relevant property, or select the most promising models in a large generated batch, so that scientist can focus on improving them. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "SgmkrXTT4Cc", "original": null, "number": 4, "cdate": 1605639031285, "ddate": null, "tcdate": 1605639031285, "tmdate": 1605641253939, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "0Lpxs4k-mgd", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you very much for your comments, here are replies to your observations.\n\n======== The authors do not clearly state their methodology, including the concrete architecture of the transformer-based model, and the training loss for optimizing the model. As shown in the experiments, the accuracy of the proposed model is much higher than the baseline, but it is not clear what the major reason is, and why. ======\n\nThank you for your remark. Since the architecture we use is identical to the one in ``\" Attention is all you need \" (Vaswani et al.), we did not describe it in detail, for lack of space. However, we agree that it would make the paper more self-contained, so we will add a section describing the model in the supplementary material. \n\nThe training loss we use is the cross entropy between the model predicted output and actual result from the dataset. The model used as a baseline implements a bag of words, which estimates the conditional probability of the (binary) output given the distribution of tokens (or sequences of N tokens) in the input. This model will catch simple correlations between input and output, such as some mathematical operator making systems more unstable, or a correlation between the length of a system and its properties. Because such correlations exist in our problems, our baseline is over chance level. However, bag of words models do not take into account the order of tokens. Transformers handle word order through their attention mechanism, and we believe this explains their better accuracy: the order of tokens in a mathematical expression has an impact on its meaning, and properties. \n\n====== As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. (...)  As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.====\n\nThank you for this remark. This was clearly not underlined enough in the paper. We agree that neural networks have a universal approximation ability from the universal approximation theorem (UAT). However, the universal approximation theorem states that a neural network of arbitrary depth can approximate any Lebesgue integrable function from a compact subset of the finite-dimensional vector space $R^n$ to $R^n$. What we are trying to approximate here is much more complicated: formally we try to approximate a functional acting of an infinite-dimensional space. As a consequence the UAT does not apply and the underlying reason is really that the space of possible functions is infinite-dimensional and cannot be accurately represented by a finite-dimensional vector space, even large. Therefore, 50 millions examples cannot grasp the generality of all the functions that can be generated and there are an infinite number of functions whose form is not even close to those 50M examples. What we show in this paper, however, is that some mathematical properties like stability or controllability can be learned with so \"few\" examples (note that most of the models use in fact much less than the 50M examples generated). Overall, this is the reason why we believe that it is not obvious that mathematical properties could be learned from examples and even surprising (at least in such settings). We would also like to emphasize that in general learning mathematical properties may be hard: it was shown in Saxton et al. that neural networks struggle with learning the decomposition in prime numbers. This discussion was probably missing and we will add additional explanations about this in the paper.\n\nConcerning showing a curve \"training sample VS test accuracy\", we believe this is a very good idea, thank you for the suggestion. We added to the supplementary material a graph, presenting learning curves over twelve runs of the end to end stability experiment.  From one run to the other, datasets are shuffled and the training set is not the same (the test set does not change). Yet, all runs display similar learning curves, showing that accuracy goes up from 55 to 92% over the first five million examples, and saturates over 95% after 10 millions examples. We believe this, together with the size of the problem space estimated in the appendix, rules out overfitting and memorization.\n\nFinally, on a different aspect, we would also like to underline that one interesting point of this work is not only to show that these mathematical properties can be learned, but that they can be learned using language model, i.e. models that were not at all engineered for such purpose, and where the functions and numbers are fed as tokens.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "kZjWyov4f_3", "original": null, "number": 3, "cdate": 1605638149335, "ddate": null, "tcdate": 1605638149335, "tmdate": 1605639184830, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "483s-M6Uc-4", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you very much for your review, here are preliminary replies to your observations.\n\n==== How general are the class of functions considered in each problem? For example, you generate random functions by sampling unary- binary trees. Does this give the guarantee that the majority of solutions for each of three problems can learned by deep learning methods? =====\n\nThis is clearly an important question, thank you for raising it. Any function that can be written using standard mathematical notation can be represented as a unary-binary tree. Therefore this process does give us the guarantee that not only the majority but even any solution of the three problems can be generated with our setting. Of course, this is only true up to the choice of the constants and the elementary operators. We chose here to include the most used elementary functions: square root, power, exp, log, sin, cos, tan, asin, acos, atan, etc. This covers most of the physical and mathematical systems studied in practice. It would still be possible to add other functions to the mix, like special functions (Bessel, Fresnel), but we chose to limit ourselves to the most commonly used operators in this paper. \n\n===== How this work compares to previous ones such as (Lample and Charton 2020) that study learning symbolic mathematics using deep learning? Any distinct insight when investigating numerical and differential systems? =====\n\nOur transformer architectures are very similar to those used by Lample and Charton (LC henceforth). The only significant difference on this aspect is that we use scheduling of the learning rate, and do not perform beam search at inference. We also use their techniques for random function generation. That being said, our problems are in fact quite different and we believe we generalize their findings on three counts. \n\nFirst, we focus on a different class of problems. LC are performing machine translation: their inputs and outputs are of the same nature (sequences representing functions). The problems solved here are classification and regression, inputs are sequences representing functions, but outputs are arrays of real numbers (represented as sequences) or binary values. This complicates the work of the transformer, as it needs to learn both input and output syntax, and cannot leverage the input syntax to infer solutions.\n\nSecond, LC focus on symbolic problems that have a strong connection to pattern matching. When integrating a function, one looks for specific parts in the input sequence that will be replaced by specific translations in the output (for instance, a cosine will always be integrated into a sine). Pattern matching is common in natural language processing and transformers tend to be good at it. Therefore, one might wonder whether LC results were partly due to the transformer exploiting the \"linguistic\" character of the symbolic maths problems they consider. The problems studied in our paper feature some symbolic calculations, but also computations which involve no pattern matching (eigenvalues, point estimations, minimization). That transformers can successfully learn such problems show that their potential for mathematical tasks may be larger than what was thought before and could be used in many areas where symbolic and numerical operations are mixed: physics, computational-biology, theoretical chemistry, etc.\n\nFinally, most LC datasets were built by generating problems from their solutions. This raises a question about how representative of the problems space their datasets are. All problems considered here were generated directly, at random. \n\n===== It is useful to see if generalizability of learned mathematical computation can be assessed by experiments, where training datasets are generated using some class of functions, and they are tested on a separated class of functions. Are such experiments feasible using unary- binary trees? =====\n\nThank you for this suggestion! Indeed looking at tests on different class of functions would be very interesting to test the generalizability of the learned mathematical computations. This can be done by varying the parameters used for random tree generation. For instance, we can favour in the test sets some specific operators (more trigs and exponentials, say), a different distribution of lengths for expressions, and a different balance of constants and variables. We will generate such different test samples with different distributions, and add the results to our paper. These tests should be ready in a few days.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-gfhS00XfKj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper841/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper841/Authors|ICLR.cc/2021/Conference/Paper841/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866645, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Comment"}}}, {"id": "483s-M6Uc-4", "original": null, "number": 1, "cdate": 1603560387975, "ddate": null, "tcdate": 1603560387975, "tmdate": 1605024593696, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Review", "content": {"title": "Review of Transformers for differential systems", "review": "This paper investigates the use of deep learning models, and specifically transformers, to learn mathematical properties of differential systems. Authors tackle three problems:\n\n* Local stability: where the goal is predicting the stability of a given system at a given point, similar to Spectral Mapping Theorem.\n* Control theory: where the goal is predicting controllability and computing the control feedback matrix as in Kalman condition.\n* Stability of PDEs using Fourier transform: where the goal is to predict the existence and stability of a PDE given its differential operator.\n\nOverall, this paper is well written and tackles an intersting problem with potential useful insights. I believe it can be improved by making the discussion more rigorous:\n\n1. How general are the class of functions considered in each problem? For example, you generate random functions by sampling unary- binary trees. Does this give the guarantee that the majority of solutions for each of three problems can learned by deep learning methods?\n\n2. How this work compares to previous ones such as (Lample and Charton 2020) that study learning symbolic mathematics using deep learning? Any distinct insight when investigating numerical and differential systems?\n\n3. It is useful to see if generalizability of learned mathematical computation can be assessed by experiments, where training datasets are generated using some class of functions, and they are tested on a separated class of functions. Are such experiments feasible using unary- binary trees?\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133822, "tmdate": 1606915769489, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper841/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Review"}}}, {"id": "0Lpxs4k-mgd", "original": null, "number": 2, "cdate": 1603780119911, "ddate": null, "tcdate": 1603780119911, "tmdate": 1605024593636, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Review", "content": {"title": "Lack of methodological contribution", "review": "This paper empirically demonstrated the effectiveness of neural networks for learning to predict different mathematical properties of dynamical systems, which achieve high accuracy on synthetic datasets generated by the authors.\n\n\nPros:\n+ the paper is clearly written and easy to follow\n+ the dataset seems to be carefully generated and is large.\n\nCons:\n- The authors do not clearly state their methodology, including the concrete architecture of the transformer-based model, and the training loss for optimizing the model. As shown in the experiments, the accuracy of the proposed model is much higher than the baseline, but it is not clear what the major reason is, and why.\n- As the dataset contains about 50 million samples, and only 10000 of them are held out for test and validation, which means the training dataset contains sufficient data and the result could just be overfitting. Besides, the authors do not describe clearly how they generate the dataset and what is the problem distribution. For some distribution (e.g., those with smaller variance), maybe 50 million is large enough and there won't be a generalization issue. As a well-known fact that neural network has universal approximation ability, it is not surprising that it can learn to predict the mathematical properties given enough data. It will be better if the authors could show a '#training sample VS test accuracy' curve to show how the method behaves differently given different numbers of training samples.\n\nOverall this paper does not have enough technical contributions, so I vote for a clear reject.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133822, "tmdate": 1606915769489, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper841/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Review"}}}, {"id": "H3CdLD7vPLj", "original": null, "number": 4, "cdate": 1604107232642, "ddate": null, "tcdate": 1604107232642, "tmdate": 1605024593506, "tddate": null, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "invitation": "ICLR.cc/2021/Conference/Paper841/-/Official_Review", "content": {"title": "Clear, well-executed, interesting paper", "review": "This paper shows that transformer models can be used to accurately learn advanced mathematical computations from millions of examples.  The problems are drawn from the fields of differential equations and control theory.  The selected problems are ones that are solvable using known algorithms; however, these algorithms involve a sequence of advanced mathematical operations (e.g., differentiation, calculating the rank of a matrix, calculating the eigenvalues of a matrix, etc), for which no known simple shortcuts exist. For the experiments in this paper, for each problem a large number (50 million) of training examples are randomly generated, and are then used to train a transformer model.  Across these problems, the paper shows that the neural network is able to solve these problems at high accuracy (96-99.7% accuracy).\n\nStrengths\n- The paper ask a well-motivated question regarding whether neural networks are able to learn complex mathematical operations from examples.\n- The paper is clearly written, and the experimental rigor/quality appears quite high.\n- The empirical results are quite intriguing, and raises many interesting questions for future research.  For example, (1) how is a transformer model managing to attain such high accuracy on these problems, (2) what other complicated mathematical problems might be similarly learnable, (3) what are the practical implications to real systems of these results.\n- The paper does a good job considering the various potential counterarguments to its conclusions in the discussion section (Section 5.4).  For example, the paper argues convincingly that (1) it\u2019s unlikely the model is exploiting some trivial distinction between the positive vs negative examples because the examples are sampled randomly, and that (2) it\u2019s unlikely the model is interpolating between solutions because the problem space is so much larger than the training set (and because small models which would be unable to memorize the training set also perform well).\n\n\u2028Weaknesses\n- I would have appreciated more of a discussion about the computational cost of solving these problems mathematically vs. solving them with a neural network.  What is the computational complexity (Big-O) of each of the known mathematical algorithms for solving these problems?  Are there large computational savings from using a neural network?  What are the practical implications of the results shown in this paper?\n- I think including a few more baselines would have been useful.  Also, a brief description of the FastText model would make the paper more self-contained.   One question: Why is the FastText model in Section 5.1 only trained with 2 million examples, while the transformer model is trained with 50 million examples?\n- In the current experiments, the test set is drawn from the same exact (random) distribution as the training set.  I was very curious whether the model would have been able to attain high test-time accuracy, had the test examples been drawn from a different distribution.  In particular, I\u2019d be curious how difficult it would be to construct a test distribution on which the current model performs terribly.  This line of questioning would be able to start better answering whether the model is solving the problem in a way that truly generalizes.\n- I think the paper could be strengthened by adding additional error analysis to try to better understand the errors made by the transformer model.\n- The paper says \u201ctraining is performed on 8 V100 GPUs with float16 operations\u201d: It would have been nice to hear more about the training process: For example, low long did training take for the various problems, and how was training distributed across the GPUs?\n- I would have appreciated more of a discussion about the broader implications and significance of these results.\n\nOverall, I thought the paper was very interesting and well executed, and I think it would make a very nice addition to ICLR 2021.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper841/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper841/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning advanced mathematical computations from examples", "authorids": ["~Francois_Charton1", "amaury.hayat@enpc.fr", "~Guillaume_Lample1"], "authors": ["Francois Charton", "Amaury Hayat", "Guillaume Lample"], "keywords": ["differential equations", "computation", "transformers", "deep learning"], "abstract": "Using transformers over large generated datasets, we train models to learn mathematical properties of differential systems, such as local stability, behavior at infinity and controllability. We achieve near perfect prediction of qualitative characteristics, and good approximations of numerical features of the system. This demonstrates that neural networks can learn to perform complex computations, grounded in advanced theory, from examples, without built-in mathematical knowledge.", "one-sentence_summary": "We train transformers to predict qualitative and numerical properties of differential equations", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "charton|learning_advanced_mathematical_computations_from_examples", "supplementary_material": "/attachment/b277aa4884ed4891fabcd87995d8c8edf5db106e.zip", "pdf": "/pdf/fc6de8bba436a4a56d0914d23d125bfb2eb84682.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncharton2021learning,\ntitle={Learning advanced mathematical computations from examples},\nauthor={Francois Charton and Amaury Hayat and Guillaume Lample},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=-gfhS00XfKj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-gfhS00XfKj", "replyto": "-gfhS00XfKj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper841/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133822, "tmdate": 1606915769489, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper841/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper841/-/Official_Review"}}}], "count": 23}