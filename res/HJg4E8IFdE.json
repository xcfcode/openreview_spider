{"notes": [{"id": "HJg4E8IFdE", "original": "r1ezckaFwV", "number": 19, "cdate": 1553716779703, "ddate": null, "tcdate": 1553716779703, "tmdate": 1562083043313, "tddate": null, "forum": "HJg4E8IFdE", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@illinois.edu", "golnazg@google.com"], "keywords": ["Style transfer", "Generative models"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/e6b2c9ae2048623602a0c3301394a9c6e5348df2.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "ryeccJgV94", "original": null, "number": 2, "cdate": 1555459985664, "ddate": null, "tcdate": 1555459985664, "tmdate": 1556906124748, "tddate": null, "forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Official_Review", "content": {"title": "Review for \"Adjustable Real-time Style Transfer\"", "review": "I would like to prephase my review by saying that while I have reasonable experience in generative modeling, I know very little of style transfer beyond the basics, and this is the first paper I review in the topic, so my review cannot be more than an educated guess.\n\nThe paper at hand studies the problem of doing style transfer when the desired output is not just one image, but a collection of diverse images given a single style and content. Furthermore, the paper concentrates in real time generation (i.e. obtaining several images according to certain parameters without a need for retraining). This last part is where the novelty of the paper relies. To address this problem, the authors include as part of the network parameters alpha_c and alpha_s that are taken as an input for a neural network that produces as a feedforward pass the new images, and that's trained with images coming from weighted style transfer (eqs 2-3). The conditioning is done via instance normalization, very similar to [1].\n\nThe paper is very well written, the problem description and the algorithms (i.e. both contributions) are clear, and the method seems to 'perform well'. My main criticisms are that no quantitative evaluation or user studies have been done to assess the 'performance' of the model, and that there doesn't seem to be any fundamentally new ideas in play. Namely, it seems like a natural extension of existing methods. However, the ideas are well arranged and executed, and according to my judgement this merits enough for publication at a workshop like this one.\n\n[1]: https://arxiv.org/pdf/1610.07629.pdf", "rating": "3: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@illinois.edu", "golnazg@google.com"], "keywords": ["Style transfer", "Generative models"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/e6b2c9ae2048623602a0c3301394a9c6e5348df2.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Official_Review", "cdate": 1554234177578, "reply": {"forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234177578, "tmdate": 1556906087186, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "rye7oAUf5E", "original": null, "number": 1, "cdate": 1555357338875, "ddate": null, "tcdate": 1555357338875, "tmdate": 1556906124529, "tddate": null, "forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Official_Review", "content": {"title": "Interesting work, but lacking quantitative evaluation.", "review": "This paper presents an adjustable real-time style-transfer approach for generating a re-styled image given a style image and a content image, with the contribution that the stylisation of generated images could be adjusted/controlled at inference time by changing a few tuning parameters. Specifically, this is achieved by modeling as input the set of weights controlling the effect of the style and content captured by each layer of the network. The authors present various qualitative analysis to compare the proposed approach with existing works (StyleNet).\n\nMy major concern is the lack of quantitative experiments for evaluation. Specifically, how does the proposed approach contrast against existing models (e.g., StyleNet) in generating different stylizations with more diverse details (last paragraph, Section 2)? The authors only present one example in the experimental section, which seems to be insufficient.\n\nMeanwhile, this submission only studies generating images, not other modalities with highly-structured representations, which might not fit the theme of this workshop.\n\nTypos:\n* Eq (1): should $\\phi(\\bm{s})$ be $\\phi(\\bm{c})$ in the first equation?\n* Eq (5): format issue\n", "rating": "2: Marginally below acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@illinois.edu", "golnazg@google.com"], "keywords": ["Style transfer", "Generative models"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/e6b2c9ae2048623602a0c3301394a9c6e5348df2.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Official_Review", "cdate": 1554234177578, "reply": {"forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234177578, "tmdate": 1556906087186, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper19/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "H1liATOP5V", "original": null, "number": 1, "cdate": 1555693010755, "ddate": null, "tcdate": 1555693010755, "tmdate": 1556906124306, "tddate": null, "forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept", "comment": "This work presents a method for style transfer which enables users to modify the output via adjusting different hyperparameters. The reviewers agreed that this is a well-written paper with interesting experimental results."}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adjustable Real-time Style Transfer", "authors": ["Mohammad Babaeizadeh", "Golnaz Ghiasi"], "authorids": ["mb2@illinois.edu", "golnazg@google.com"], "keywords": ["Style transfer", "Generative models"], "TL;DR": "Stochastic style transfer with adjustable features. ", "abstract": "Artistic style transfer is the problem of synthesizing an image with content similar to a given image and style similar to another. Although recent feed-forward neural networks can generate stylized images in real-time, these models produce a single stylization given a pair of style/content images, and the user doesn't have control over the synthesized output. Moreover, the style transfer depends on the hyper-parameters of the model with varying ``optimum\" for different input images. Therefore, if the stylized output is not appealing to the user, she/he has to try multiple models or retrain one with different hyper-parameters to get a favorite stylization. In this paper, we address these issues by proposing a novel method which allows adjustment of crucial hyper-parameters, after the training and in real-time, through a set of manually adjustable parameters. These parameters enable the user to modify the synthesized outputs from the same pair of style/content images, in search of a favorite stylized image. Our quantitative and qualitative experiments indicate how adjusting these parameters is comparable to retraining the model with different hyper-parameters. We also demonstrate how these parameters can be randomized to generate results which are diverse but still very similar in style and content.", "pdf": "/pdf/e6b2c9ae2048623602a0c3301394a9c6e5348df2.pdf", "paperhash": "babaeizadeh|adjustable_realtime_style_transfer"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper19/Decision", "cdate": 1554814608085, "reply": {"forum": "HJg4E8IFdE", "replyto": "HJg4E8IFdE", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814608085, "tmdate": 1556906097640, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}