{"notes": [{"id": "HklUCCVKDB", "original": "SJetN_qdDS", "number": 1432, "cdate": 1569439438246, "ddate": null, "tcdate": 1569439438246, "tmdate": 1583912052577, "tddate": null, "forum": "HklUCCVKDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "eOHHzlmHKa", "original": null, "number": 1, "cdate": 1576798723122, "ddate": null, "tcdate": 1576798723122, "tmdate": 1576800913436, "tddate": null, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "While prior work has shown the potential of using uncertainty to tackle catastrophic forgetting (e.g. by appropriate updates to the posterior), this paper goes further and proposes a strategy to adapt the learning rate based on the uncertainty. This is a very reasonable idea since, in practice, learning rate control is one of the simplest and most understood techniques to fight catastrophic forgetting. \nThe overall approach ends up being a well-motivated strategy for controlling the learning rate of the parameters according to a notion of their \"importance\". Of course now the question is if this work uses a good proxy for \"importance\" so further ablation studies would help, but the current results already show a clear benefit. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711131, "tmdate": 1576800260271, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Decision"}}}, {"id": "S1ewYgHaKr", "original": null, "number": 1, "cdate": 1571799166763, "ddate": null, "tcdate": 1571799166763, "tmdate": 1574436149376, "tddate": null, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "**** Post Rebuttal ****\n\nI have read the author's response and other reviewers' comments. In light of comments by other reviewers, I am increasing the score. The paper reports decent empirical results in some challenging settings which might be useful to the continual learning community. \n\n**** End ****\n\nThe paper presents a simple yet effective way to avoid catastrophic forgetting in a continual learning setting. The proposed approach is referred to as UCB - \"Uncertainty Guided Bayesian Neural Networks\". The main idea of the approach is to weight the learning rate of each parameter in the neural network by the standard deviation of its posterior distribution. This leads to regularizing parameters that are \"important\" to tasks seen earlier and thus avoiding forgetting.  Results indicate an improvement over other baselines. However, I do not see any analysis of the method that explains this improvement. I do not recommend acceptance.\n\nCons:\n\n- My main concern with the paper is that it fails to justify the superiority of the method over other baselines. The numbers reported in the paper do seem good, but I don't see an explanation of why this is the case. What are the drawbacks of EWC, VCL or HAT that the proposed method solves? Why using uncertainty to define importance works better than using online VI in VCL or fisher information in EWC? There is no discussion in the paper about that. Without such a discussion it seems that the model was run a number of times and the best score was reported out of all those runs (especially because the improvement is only marginal). \n\n- I am not sure why weighting the learning rate would be a good idea? Having high uncertainty may increase the learning rate arbitrarily. Is there a constraint on the standard deviation? Does having a very high weight for learning rate not cause instability during optimization? I think the method would be very sensitive to the initialization of the standard deviation. \n\nOverall I think the idea of using uncertainties for continual learning is interesting. But from where it stands, I am not fully convinced that this method should do better than existing approaches.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737709467, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Reviewers"], "noninvitees": [], "tcdate": 1570237737483, "tmdate": 1575737709480, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review"}}}, {"id": "SJlbynqTYS", "original": null, "number": 2, "cdate": 1571822552945, "ddate": null, "tcdate": 1571822552945, "tmdate": 1574328984160, "tddate": null, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "** post rebuttal start **\n\nAfter reading reviews and authors' response, I decided not to change my score.\nI am happy with the author's response addressing my concerns (mainly about the fairness on the size of the model), so I recommend its acceptance. I believe it is a good addition to the community of continual learning.\n\n** post rebuttal end **\n\n\n- Summary:\nThis paper proposes to use a way to improve continual learning performance by taking \"Bayes-by-backprop\" method. They claim that the uncertainty can naturally be measured by estimating (log of) the standard deviation, and it is indeed useful to judge the importance of each learnable parameter. Experimental results on several benchmarks show that their method outperforms few state-of-the-art methods.\n\n\n- Decision and supporting arguments:\nWeak accept.\n\n1. The proposed method is simple but effective. However, It is still questionable whether \\sigma is the best measure of the weight importance. An ablation study with different choices of the importance measure (maybe \\mu can also be incorporated as well as \\sigma?) would be good to see.\n\n2. Survey and comparison with memory-based methods are limited. Though memory-based methods require some memory to keep the experience, the proposed method also requires additional memory for \\sigma; it essentially doubles the model capacity, assuming that \\sigma is solely for measuring the weight importance. In particular, when it comes to large-scale models, memory for storing some important experiences would be small compared to the memory to store the model.\nHere are some papers about recently proposed memory-based methods, which are not cited:\n\nCastro et al. End-to-End Incremental Learning. In ECCV, 2018.\nWu et al. Large Scale Incremental Learning. In CVPR, 2019.\nLee et al. Overcoming Catastrophic Forgetting with Unlabeled Data in the Wild. In ICCV, 2019.\n\n3. Comparison should include the model capacity as in Table 1(b). Again, compared to the conventional non-Bayesian model, half of the model capacity is used for computing \\sigma (uncertainty), I wonder it causes a performance drop when the model capacity is the same over all compared methods. If they used the same model architecture and just doubled the number of learnable parameters for \\sigma, then it is obviously unfair.\n\n\n- Comments:\n1. Pruning is not beneficial in terms of the performance. I hope to see some quantitative benefits obtained by introducing pruning. In Table 1(b), why doesn't pruning reduce the number of parameters?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737709467, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Reviewers"], "noninvitees": [], "tcdate": 1570237737483, "tmdate": 1575737709480, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review"}}}, {"id": "rylM6-AsoB", "original": null, "number": 9, "cdate": 1573802425872, "ddate": null, "tcdate": 1573802425872, "tmdate": 1573802425872, "tddate": null, "forum": "HklUCCVKDB", "replyto": "Bke__ZAosB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment", "content": {"title": "Response to R2 -- Part 2", "comment": "2.  \u201cI am not sure why weighting the learning rate would be a good idea?\u201d\n\nTo us, it seems a very natural and obvious choice. Decreasing the learning rate for important parameters decreases changing them which results in not forgetting previous tasks.\n\n=================================================\n\u201cHaving high uncertainty may increase the learning rate arbitrarily. Is there a constraint on the standard deviation? Does having a very high weight for learning rate not cause instability during optimization? I think the method would be very sensitive to the initialization of the standard deviation. \u201c\n\nWe follow the initialization strategy used in the original Bayes-by-Backprop (BBB) framework (Blundell et al., 2015). The right initialization is important and we treat it as a hyperparameter which we find, as all other hyperparameters (see above) with the validation sets of the first two tasks. We added a paragraph Bayes-by-backprop (BBB) Hyperparamters: in Section A.2 in appendix  in the revised pdf detailing this. When initialized correctly, \\rho is not exploding in our experiments with BBB.\n\n====\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklUCCVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1432/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1432/Authors|ICLR.cc/2020/Conference/Paper1432/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156096, "tmdate": 1576860538220, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment"}}}, {"id": "Bke__ZAosB", "original": null, "number": 8, "cdate": 1573802351989, "ddate": null, "tcdate": 1573802351989, "tmdate": 1573802351989, "tddate": null, "forum": "HklUCCVKDB", "replyto": "S1ewYgHaKr", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment", "content": {"title": "Response to R2 -- Part 1", "comment": "We thank the reviewer for their comments and are happy to hear that they find our idea of using uncertainty interesting.In the following we address the individual comments:\n=================================================\n1. The reviewer is concerned that our paper \u201cfails to justify the superiority of the method over other baselines\u201d and misses and \"explanation of why this is the case\u201d.\n\nWhile it is not fully clear to us how \u201csuperiority\u201d is defined we believe our approach has the following properties, which make it valuable and interesting:\nOur approach is novel (R#1)\nOur approach is \u201csimple but effective\u201d (R#3)\nOur approach makes sense, as it follows Bayesian principles of uncertainty, which means the uncertainty is inherent to the model which we use to define  importance. See Section 1, 4th paragraph \u201cBayesian approaches to \u2026\u201d and Figure 1, for more on the motivation.\nR1 states this \u201cwork is highly significant\u201d and is supported with an experimental evaluation \u201cwith a very large number of baselines\u201d\nOur approach is *different* from prior work (as discussed extensively in Section 2); some aspects proposed in prior work are orthogonal to our work, such as usage of episodic memory or model growth; others are just different, e.g. many regularization based methods use an additional \u201cexternal\u201d importance parameter for each network parameter, while we exploit the \u201cadditional\u201d \\rho parameters inherently to Bayesian Neural Network without the need of an \u201cexternal\u201d importance parameter.\n[we don\u2019t claim our approach is \u201csuperior\u201d to all other prior work w.r.t. methodology; however, it is simple and well motivated and experimentally we find that our performance is very competitive (on par or better than prior work) on a broad set of experiments].\n\nOverall, given these aspects, we strongly believe our approach will be appreciated by the community.\n\n=================================================\n\u201cWhat are the drawbacks of EWC, VCL or HAT that the proposed method solves?\u201d\n\nOur UCB is based on Bayesian neural networks and exploits their inherent uncertainty modeling to change the learning rate per parameter.\n\nHAT is regularization based but does not use a Bayesian Neural Network.\nEWC is a Bayesian-inspired method but does not rely on Bayesian Neural Networks, i.e. it does not exploit the inherent uncertainty modeling Bayesian Neural Networks.\nVCL uses Bayesian inference, in contrast UCB is based on Bayesian neural networks to use their predictive uncertainty to perform continual learning.\n\nSection 2, gives a detailed discussion to prior work, and we experimentally support the strength of our method in the paper. \nAdditionally w.r.t. Bayesian continual learning methods: None of them have been applied on CNNs so we are the only work that have extended it to real world images in a long sequence of tasks. See also our challenging 8 task experiment.\n\n=================================================\n\u201cWhy using uncertainty to define importance works better than using online VI in VCL or fisher information in EWC?\u201d\n\nWe believe this is because the uncertainty in Bayesian Neural Networks gives a good estimate for parameter importance used in continual learning.\nThis is clearly different than VI in VCL or the fisher information in EWC and while we do not have a mathematical proof for being better (which might also be difficult), we find our experiments support our hypothesis that using uncertainty in Bayesian Neural Networks is a good idea.\n\n=================================================\n\u201c[...] it seems that the model was run a number of times and the best score was reported out of all those runs (especially because the improvement is only marginal).\u201d\n\nWe like to highlight our very restrictive experimental setup we employ (in contrast to most prior work in continual learning). We only rely on the first two tasks and their validations set to tune hyperparameters, similar to the setup in (Chaudhry et al., 2019). (see \u201cHyperparameter tuning\u201d in Section 5.1). \nWe do not report \u201cthe best score\u201c but the average over multiple runs in the main paper, and, in the appendix (section A.3) we also show standard deviation (Tables 8, 9, 10, 11)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklUCCVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1432/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1432/Authors|ICLR.cc/2020/Conference/Paper1432/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156096, "tmdate": 1576860538220, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment"}}}, {"id": "rygv8aTojr", "original": null, "number": 7, "cdate": 1573801295194, "ddate": null, "tcdate": 1573801295194, "tmdate": 1573801295194, "tddate": null, "forum": "HklUCCVKDB", "replyto": "SJlbynqTYS", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank the reviewer for his/her comments about our work. We reply to the comments in chronological order:\n\n1. We have already provided this ablation in Table 5 in the appendix in which we considered other variants for the weight importance: specifically, we look into regularizing \\mu and \\rho or both and explore if 1/\\sigma or |\\mu|/\\sigma is better for importance measurement. We find that highest accuracy and BWT is achieved by 1/\\mu for  UCB and |\\mu|/\\sigma for UCB-P, but other variants don\u2019t decrease the performance dramatically. We have moved it to the main text as Table 1 on page 7 in the revised version.\n\n2. We agree with the reviewer that the memory of the entire model should be taken into account. And we do that, as we detail in section A2. We make sure our UCB matches to the baselines w.r.t. the *total* number of parameters (the sum of \\mu and \\rho for UCB). In table 1b we also list the *total* number of parameters (For UCB-P the memory for the mask is not included, see also 4. below).\nWe agree it might be a good option to compare methods by the total memory usage, when comparing regularization with episodic memory based models.\nWhile there are reasons for and against episodic memory storage (e.g. potential privacy concerns, even when just storing representations), this is not the focus of this work and orthogonal to this work. In fact we believe our approach would benefit from episodic memory, especially in the challenging setting of single head and generalized accuracy (section 6). We leave the exploration of combining UCB with episodic memory to future work.\nWe also cited the mentioned references in the updated draft in the related work, section 2 under \u201cmemory-based methods\u201d subsection.\n\n3. We agree with the reviewer and we have, compared to baselines, used *already* only half of the number of weights for UCB, as each weight consist of 2 parameters (see also reply to 2.).  In section A.2 we have detailed this aspect; we ensured a fair comparison by matching the *total* number of learnable parameters.\n\n4. (Question 1 in comments): Table 1b: For UCB-P the #params match the total number of parameters initially, but also after training for the last task. The reason for this is that we do not prune the network anymore after training the last task (we would do that when the next task arrives). However, we like to note that UCB-P, by using a \u201chard\u201d binary mask for each task will use up more and more parameters for each new task it sees which it cannot free by pruning. So when arriving at the last task only relatively few parameters are remaining, the ability to further prune is thus limited.\nWe detail our pruning procedure in Section 5.1,  in the paragraph, \u201cPruning procedure and mask size\u201d where we explain what percentage of network is pruned at each time. We agree with the reviewer that pruning (UCB-P) is not efficient and in our experiments it yields lower performance compared to our soft regularization version (UCB) which we introduced as our main method. However, in case it is desired to recover the \u201cexact same performance\u201d post-pruning, one might consider using it because soft regularization methods are not zero-forgetting guaranteed. We also want to mention that pruning techniques are not really \u201czero-forgetting\u201d because the accuracy drop during pruning can be considered as forgetting as we do in this paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklUCCVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1432/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1432/Authors|ICLR.cc/2020/Conference/Paper1432/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156096, "tmdate": 1576860538220, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment"}}}, {"id": "BkxDC2pjiB", "original": null, "number": 6, "cdate": 1573801166835, "ddate": null, "tcdate": 1573801166835, "tmdate": 1573801166835, "tddate": null, "forum": "HklUCCVKDB", "replyto": "Byl4mBDl9B", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We thank the reviewer for their positive feedback, and are happy that they appreciate the clarity, quality, novelty, and high significance of our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklUCCVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1432/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1432/Authors|ICLR.cc/2020/Conference/Paper1432/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156096, "tmdate": 1576860538220, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment"}}}, {"id": "HJeE5n6ior", "original": null, "number": 5, "cdate": 1573801099980, "ddate": null, "tcdate": 1573801099980, "tmdate": 1573801099980, "tddate": null, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment", "content": {"title": "See individual comments and revised pdf", "comment": "We thank all reviewers for their feedback and we replied to individual reviews directly. We also revised the pdf addressing concerns as discussed in the individual author responses."}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklUCCVKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1432/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1432/Authors|ICLR.cc/2020/Conference/Paper1432/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156096, "tmdate": 1576860538220, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Authors", "ICLR.cc/2020/Conference/Paper1432/Reviewers", "ICLR.cc/2020/Conference/Paper1432/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Comment"}}}, {"id": "Byl4mBDl9B", "original": null, "number": 3, "cdate": 1572005147595, "ddate": null, "tcdate": 1572005147595, "tmdate": 1572972469725, "tddate": null, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "invitation": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a novel method for continual learning with neural networks based on a Bayesian approach. The idea consists in working with Bayesian neural networks, using the Bayes by back-prop approach in which a factorized Gaussian variational distribution is used to approximate the true posterior. To address the continual learning setting, the authors propose to multiply the learning rate of the mean parameters in the posterior approximation by the corresponding standard deviation parameter in the posterior approximation, while the learning rate for the variance parameters in the posterior approximation is not changed. The authors also consider a version of his method which freezes the mean and variance variational parameters when the signal to noise ratio is high. The proposed method is evaluated in exhaustive experiments, showing state-of-the-art results.\n\nClarity:\n\nThe paper is clearly written and easy to read. The method proposed is well described and it would be easy to reproduce.\n\nQuality:\n\nThe proposed method is well justified and the experiments performed clearly illustrate the gains with respect to previous methods.\n\nNovelty:\n\nThe proposed method is novel up to my knowledge. The methodological contributions do not seem very sophisticated, but the experiments show that the proposed method, despite being very simple, works very well in practice.\n\nSignificance:\n\nThe experiments show that the proposed method achieves state of the art results when compared with a very large number of baselines. This indicates that the proposed method will be relevant to the community. In my opinion, this work is highly significant."}, "signatures": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1432/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Uncertainty-guided Continual Learning with Bayesian Neural Networks", "authors": ["Sayna Ebrahimi", "Mohamed Elhoseiny", "Trevor Darrell", "Marcus Rohrbach"], "authorids": ["sayna@berkeley.edu", "mohamed.elhoseiny@gmail.com", "trevor@eecs.berkeley.edu", "maroffm@gmail.com"], "keywords": ["continual learning", "catastrophic forgetting"], "TL;DR": "A regularization-based approach for continual learning using Bayesian neural networks to predict parameters' importance", "abstract": "Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms  need an external representation and extra computation to measure the parameters' \\textit{importance}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in  networks. Uncertainty is a natural way to identify \\textit{what to remember} and \\textit{what to change} as we continually learn,  and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning \nand retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.", "pdf": "/pdf/8ccdcfeb98a76b5cd86b24aac86dad75b579af1a.pdf", "paperhash": "ebrahimi|uncertaintyguided_continual_learning_with_bayesian_neural_networks", "code": "https://github.com/SaynaEbrahimi/UCB", "_bibtex": "@inproceedings{\nEbrahimi2020Uncertainty-guided,\ntitle={Uncertainty-guided Continual Learning with Bayesian Neural Networks},\nauthor={Sayna Ebrahimi and Mohamed Elhoseiny and Trevor Darrell and Marcus Rohrbach},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklUCCVKDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/979b981c5cb54fdde94cd97ba44dbeff4c318af2.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklUCCVKDB", "replyto": "HklUCCVKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575737709467, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1432/Reviewers"], "noninvitees": [], "tcdate": 1570237737483, "tmdate": 1575737709480, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1432/-/Official_Review"}}}], "count": 10}