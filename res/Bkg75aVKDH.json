{"notes": [{"id": "Bkg75aVKDH", "original": "HkgYrq6vPr", "number": 701, "cdate": 1569439115090, "ddate": null, "tcdate": 1569439115090, "tmdate": 1577168283913, "tddate": null, "forum": "Bkg75aVKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "KgDlu8OMy", "original": null, "number": 1, "cdate": 1576798703671, "ddate": null, "tcdate": 1576798703671, "tmdate": 1576800932368, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Decision", "content": {"decision": "Reject", "comment": "The authors develop a new technique for training neural networks to be provably robust to adversarial attacks. The technique relies on constructing a polyhedral envelope on the feasible set of activations and using this to derive a lower bound on the maximum certified radius. By training with this as a regularizer, the authors are able to train neural networks that achieve strong provable robustness to adversarial attacks.\n\nThe paper makes a number of interesting contributions that the reviewers appreciated. However, two of the reviewers had some concerns with the significance of the contributions made:\n1) The contributions of the paper are not clearly defined relative to prior work on bound propagation (Fast-Lin/KW/CROWN). In particular, the authors simply use the linear approximation derived in these prior works to obtain a bound on the radius to be certified. The authors claim faster convergence based on this, but this does not seem like a very significant contribution.\n\n2) The improvements on the state of the art are marginal.\n\nThese were discussed in detail during the rebuttal phase and the two reviewers with concerns about the paper decided to maintain their score after reading the rebuttals, as the fundamental issues above were not \n\nGiven these concerns, I believe this paper is borderline - it has some interesting contributions, but the overall novelty on the technical side and strength of empirical results is not very high.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704298, "tmdate": 1576800251848, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper701/-/Decision"}}}, {"id": "rklQ05jzqH", "original": null, "number": 3, "cdate": 1572154059147, "ddate": null, "tcdate": 1572154059147, "tmdate": 1574252501277, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "The paper proposes an approach for computing more refined estimates of robustness in comparison w/ existing linear approximation approaches that only give a yes or no answer with regard to robustness guarantees for a given lp-norm ball with radius epsilon. The nice thing is that as the linear-approximations get better, the contributions in this paper would continue to help. \n\nThe paper makes two key algorithmic/theoretical contributions:\n1. An approach to obtain a better estimate of the radius of the l-p ball where the NN is provably robust. This result is fairly straightforward, and relies on computing the distance of a point to the boundary of adversarial polytope.\n\n2. An approach to exploit the fact that the pixel values are restricted to specific bounds, which might allow us trim away some regions from the l-p norm balls around a given input image w.r.t which we want to be robust, while computing the robustness. This I think is a more interesting contribution. \n\n\nI am leaning towards a reject, however I am open to changing my score. I have several key concerns:\n\n1. Verified Training: Why is there no comparison with IBP and IBP+Crown (Zhang 2019) -- it seems like an appropriate comparison to make. Particularly, when the current paper refers and discusses both of the above works. \n\n2. I am not sure that comparison with CRO entirely suffices in my opinion. Would it be possible to compare with the tighter SDP based approaches (Raghunathan et al., NeurIPS'19 and Dvijotham et al., UAI'19)? Is there a specific reason to not compare (other than that the SDP based approach is not a linear approximation, and probably is much slower)? \n\nMy main concern here is the utility of pushing boundaries with the linear approximation, while there are potentially tighter relaxations?\n\n3. You claim no overhead compared to CROWN. Don't the greedy-optimization steps add some overhead, or am I missing something? How expensive are they? (It's possible I might have missed some discussion in this regard. If so, please point me in the right direction and that should suffice)\n\n4. Can you plot the distributions of the certified epsilon? Are there a few samples for which you can certify a much larger epsilon (than just saying not robust) or are there a lot of samples where you can only show a tiny bit of robustness (compared to CROWN saying not robust)? \n\nThe gains in the average robustness are somewhat small, and these gains alone are not convincing without being able to see how these gains were obtained. \n\nMinor Comment:\nMissing reference to MixTrain for B' < B helps. \n\nUpdate:\nThanks for the detailed response!\n\n1. This makes sense, the first bit seems obvious --> if you don't train with IBP, you won't get much out of IBP and this is reasonably well known. \n\nTable 5: When trained with IBP and verifying with IBP, it seems to do better or quite comparable to train/verify with PER --- in this sense, the gains seem quite marginal.\n\n2. Since a part of the contribution claimed in this paper is improved robustness guarantees for pre-trained networks, I do feel that comparisons with the UAI'19 paper or the NeurIPS'18 paper would be nice -- however, I do agree that the computational tractability of KW/FastLin/IBP are much more favorable.\n\n3. Thanks -- it is much more clear now.\n\n4. The distributions for MMR/PER seem quite similar and most of the gain seems to come at the lower end (small eps). \nThis still remains my biggest concern -- I was hoping that the distributions would diverge a bit more at larger eps, but this is difficult to confirm with the current set of plots.\n\nThese plots, as of the current version, are not very useful -- a CDF plot as opposed to a histogram would be much more illustrative in terms of comparing the different approaches. Overlaying them with different opacity might also be useful. \n\nI am still leaning towards a weak reject. I am not sure how the scoring system works, the scores are distributed unevenly -- I would judge this a 5 on the 1-10 scale. However, going by the wording, I will stay with a 3.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575548289741, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper701/Reviewers"], "noninvitees": [], "tcdate": 1570237748339, "tmdate": 1575548289757, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Review"}}}, {"id": "SylBxgXdjH", "original": null, "number": 3, "cdate": 1573560300597, "ddate": null, "tcdate": 1573560300597, "tmdate": 1573852956737, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Revision Summary", "comment": "We have made the following revisions:\n\n1. We have added IBP [Gowal 18] and CROWN-IBP [Zhang 19] as new methods to compare in the main experiments. We have also added IBP as a certification method. The full results of 9 training and 7 evaluation schemes are demonstrated in Table 5 of Appendix D.2.1. Our proposed PER/PER+at method still outperform IBP / CROWN-IBP.\n2. We have provided the distribution of the certified bounds by one-shot PEC (Figure 5).\n3. To better demonstrate that our method produces less over-regularized models than the baselines, we have added the histogram of their parameter values and report their norms (Figure 6). PER+at models have bigger norms than baselines.\n4. We have provided the distribution of optimal $\\epsilon$ found by Algorithm 1 for CIFAR10 models  (Figure 7). Given the robustness target, KW pushes too many test points far beyond this target. This is unnecessary and causes the loss of clean accuracy. Our proposed PER+at does not have such over-regularization problems."}, "signatures": ["ICLR.cc/2020/Conference/Paper701/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "r1lo3N5nsS", "original": null, "number": 8, "cdate": 1573852338529, "ddate": null, "tcdate": 1573852338529, "tmdate": 1573852413211, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "rkgE3Tl2jr", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Additional Response to Reviewer #2", "comment": "We thank the reviewer for their response. \n\nRegarding comment 2, we agree with the reviewer that this is not our main contribution. We have updated the last paragraph of the introduction to highlight that our main contribution is to propose a regularization scheme to train provably robust networks without over-regularizing the model. The corresponding certification part, i.e., PEC, is one important step to build PER regularization. We indeed clarify that PEC can be based on any linearization method and we use exactly the same linear bound as Fast-Lin in our main experiment. The advantage of PEC over Fast-Lin/KW is that it has finer-grained one-shot certified bounds, which contributes to faster convergence to the optimal $\\epsilon$ (Table 6 & 7). This is a by-product of geometric-inspired certification and we put the results in the Appendix. In the main text, we focus on comparing different training methods. \n\nRegarding question 4, we have tested combining PGD adversarial training with KW loss as you proposed. Let\u2019s define the hybrid loss $L = L_{PGD} + \\eta L_{KW}$ where $L_{PGD}$ is the loss of PGD adversarial training and $L_{KW}$ is the one of KW in [Wong 2018]. Note that both parts are cross-entropy losses. The PGD loss and KW loss are the lower bound and upper bound, respectively, of the true worst case loss under the adversarial budget. We have tested the case when coeficient $\\eta = 1$ and $\\eta = 10.$ on the MNIST dataset with the same experimental setting as the ones in Table 1. We use Adam optimizer, so the results are invariant to the scaling of training objective functions. The results are as follows (the notation is the same as Table 1):\n\nMNIST-FC1, ReLU, $l_\\infty$, $\\epsilon = 0.1$, $\\eta = 1.$\nCTE =  0.81%, PGD = 7.44%, CRE Lin = 23.29%, ACB KW = 0.0767, ACB PEC = 0.0890.\nMNIST-FC1, ReLU, $l_\\infty$, $\\epsilon = 0.1$, $\\eta = 10.$\nCTE = 1.60%, PGD = 7.99%, CRE Lin = 13.74%, ACB KW = 0.0863, ACB PEC = 0.0928.\nMNIST-CNN, ReLU, $l_\\infty$, $\\epsilon = 0.1$, $\\eta = 1.$\nCTE = 0.81%, PGD = 3.64%, CRE Lin = 8.12%, ACB KW = 0.0919, ACB PEC = 0.0952.\nMNIST-CNN, ReLU, $l_\\infty$, $\\epsilon = 0.1$, $\\eta = 10.$\nCTE = 1.96%, PGD = 4.88%, CRE Lin = 7.21%, ACB KW = 0.0928, ACB PEC = 0.0952.\n\nWe notice that the results are worse than KW regarding the certified robust error (CRE Lin). The performance is between the PGD-trained model and KW-trained model. For the cases when $\\eta = 1$, the obtained model has better empirical clean accuracy and PGD robust accuracy. For a bigger $\\eta$, the loss is more similar to KW, i.e., the model has better certified accuracy at the cost of clean accuracy.\n\nRegarding the IBP training cases you mentioned, we need to point out that the original IBP paper [Gowal 2018] has a vanilla cross-entropy term. It is because IBP uses very aggressive approximation and the vanilla cross-entropy term is needed to stabilize the training. However, the original KW paper [Wong ICML18] does not use this term. In addition, we did not see any existing work combining KW with PGD as what you proposed.\n\nTo argue the advantages of hinge-loss based PER over cross-entropy-loss based KW, we provide the following explanation and supporting evidence: when a data point is robust enough against adversarial attack (in our case, that means its certified bound is bigger than $\\epsilon$), our hinge-loss based PER is a constant and does not contribute to the update of the model parameters. However, cross-entropy-loss based KW will always further push the output logits of the true labels higher and the false ones lower. This leads to the problem of over-regularization of KW-trained models.  If we search for the optimal bounds by Algorithm 1 for each test point in the CIFAR10 $l_\\infty$ case, we find that the average optimal bounds for KW-trained model is 0.0169 while for PER+at model it is only 0.0084.  While KW and PER+at have similar certified error on the target $\\epsilon = 2 / 255 = 0.0078$, Figure 7 of Appendix D.2.1 shows that KW pushes too many points far beyond the robustness target we set. This is unnecessary and at the cost of clean accuracy.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "rkgE3Tl2jr", "original": null, "number": 7, "cdate": 1573813676058, "ddate": null, "tcdate": 1573813676058, "tmdate": 1573813676058, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "HJgEDWmdjB", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Thanks for the response and additional results.", "comment": "2. The contribution of KW/CROWN is to provide linear upper and lower bounds for neural network. The bounds proposed by this paper is based on them and have exactly the same power. The way to find epsilon is only a very minor difference in evaluation. I don't think the claim that the proposed\"geometry-inspired\" algorithm beats KW/CROWN is acceptable, as the \"improvement\" does not come from a tighter bound. The \"geometry-inspired bound\" looks too trivial to me, and I believe it is better to not claim it as a main contribution.\n\n4. For adversarial training, you can just simply add a PGD loss term for KW. The PGD loss is not related to the bounds. You just ran PGD attack given the current example $x$ and use the point $x^\\prime$ after attack for the regular cross-entropy loss. The same technique can also be applied to IBP. In my experience IBP+PGD loss can improve IBP by a few percent. Without adversarial training, the proposed method (PEC) cannot outperform other methods. So I am not sure if the improvement shown in this paper (the best method is PEC+at) is from adversarial training or PEC itself.\n\nUnfortunately, due to the main concerns above, I cannot increase my rating.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "ryxjhbmusH", "original": null, "number": 6, "cdate": 1573560755489, "ddate": null, "tcdate": 1573560755489, "tmdate": 1573560755489, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Syx3S9paFr", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Thank You", "comment": "We thank the reviewer very much for their positive comments. Following the other reviewers\u2019 suggestions, we have added more experiments as stated in the revision summary.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "HJgEDWmdjB", "original": null, "number": 5, "cdate": 1573560667966, "ddate": null, "tcdate": 1573560667966, "tmdate": 1573560667966, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "SJgClQwRKH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Detailed Response to Reviewer #2", "comment": "We thank the reviewer for the constructive comments and answer their questions below.\n\n1. In the Figure 6 of the revised paper, we provide the distribution of the parameter values for KW/MMR+at/PER+at models on CIFAR10. We can see that the models trained by PER+at have larger parameter values than KW. The over-regularization issue of KW was also mentioned in [Zhang 2019, Dvijotham UAI19]. Furthermore, note that models trained using MMR+at, which also introduces a hinge-loss like regularization, are also less over-regularized than those trained using KW.\n\n2. Tables 1, and 2 report certified bounds obtained in ONE evaluation. That is, in these tables, we compare ONE-SHOT KW/Fast-Lin/PEC. By using binary search, KW/Fast-Lin can obtain the same optimal $\\epsilon$ as PEC. As stated in Section 3.1, we do not claim a tighter bound but a finer-grained one, which contributes to faster convergence when searching for the optimal $\\epsilon$. Apart from CROWN-style linearization, PEC can be applied to any other method that gives linear bounds of model's output, such as the IBP-inspired one used in Appendix A.3. Furthermore, we would like to clarify that our main contribution, as our title indicates, is to incorporate a geometry-inspired bound to train provably robust models. Our experiments also focus on how PER/PER+at train robust models better than others techniques.\n\n3. For the results in Table 1, we used the same settings as in [Croce 2019], so that we could directly use the checkpoints provided by them as baselines. We also test the case used in [Wong 2018] where $\\epsilon$ for the $l_2$ attack in MNIST is 1.58. The robust error for the MNIST-CNN model (exactly the same as \u2018Small model\u2019 in [Wong 2018]) trained by PER is 55.10%, which is better than what [Wong 2018] reports (56.48% in Table 4, Page 22). The error on clean test data for the PER-trained model is 9.00% and lower than the 11.86% reported in [Wong 2018]. We will release all the checkpoints when our code is made public.\n\n\n4. We have carefully checked the DiffAI paper [Mirman ICML18, Mirman 2019] and do not think that it is a combination of KW and PGD. DiffAI is built on the abstract transformer for Zonotope, and their bound is looser than the KW one, based on Definitions 4.1 - 4.4 in [Mirman ICML18]. DiffAI trains models with the cross-entropy loss using over-approximated bounds of the output logits within the adversarial budget. This is the 'Adversarial Training' mentioned in Section 5 of [Mirman ICML18], which differs from PGD [Madry ICLR18]. [Mirman ICML18] only uses the PGD error as a lower bound of the true robust error and does not incorporate PGD into their algorithm. Actually, in both MMR [Croce 2019] and PER, adversarial training (at) is only used when we calculate the distance between a data point and the estimated decision boundary, because intuitively an adversarial example is closer to the decision boundary. Note that our polyhedral envelope, which is based on a linear approximation, is always calculated on the clean input, because the adversarial budget is defined based on the clean input (second to last paragraph of Section 3.2). By contrast, in KW, the loss is directly calculated based on the cross-entropy of the output logits\u2019 bounds, which can only be estimated based on the clean input. We do therefore not see how KW can be combined with PGD adversarial training.\n\n5. As discussed in Section 5, for an N-layer network with k, m, n as the dimensions of the output, the input and the hidden layers ($n >> k, m$), the complexity of a CROWN-style linearized bound calculation is $O(N^2 n^3)$. However, based on Equation (18) for both the $l_\\infty$ and $l_2$ norm, the complexity of each iteration in Algorithm 2 is $O(km)$, which is significantly smaller than $O(N^2n^3)$. In practice, the loop in the algorithm stops within 5 iterations in almost all cases (Appendix B.2), so the overhead is small. We have run 10 times  PEC and CROWN on the CIFAR10-CNN model. To process the entire test set on a single-GPU machine, the mean and standard deviation in $l_\\infty$ cases are 217.51(1.95) for CROWN and 219.16(3.23) for PEC; in $l_2$ cases, they are 236.95(1.64) for CROWN and 239.41(1.92) for PEC. The difference is negligible and within the variance of the 10 attempts.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "HJxRMZXOor", "original": null, "number": 4, "cdate": 1573560598257, "ddate": null, "tcdate": 1573560598257, "tmdate": 1573560598257, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "rklQ05jzqH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment", "content": {"title": "Detailed Response to Reviewer #4", "comment": "We thank the reviewer for the constructive comments and address their concerns below.\n\n1. We have added the results of IBP and CROWN-IBP in Table 5. IBP differs from KW/Fast-Lin/CROWN in that it is not based on the linearization of the activation functions. As in  [Zhang 2019], we observed that IBP is effective at certifying models trained by IBP or CROWN-IBP. In these cases, the more complicated KW/Fast-Lin/CROWN can yield suboptimal results. For other models, IBP is always much worse than KW/Fast-Lin/CROWN. Theoretically, IBP and Fast-Lin are complementary. Our PER and PER+at algorithms still outperform IBP and CROWN-IBP.\n\n2. Note that both [Raghunathan NeurIPS18] and [Dvijotham UAI19] you mentioned are pure certification method, and thus do not introduce any algorithms to train provably robust models. Despite providing a tighter bound than methods based on linearizing activation functions, the technique of [Dvijotham UAI19], which is a more scalable version of [Rahunathan NeurIPS18], is much slower than the ones in our paper. [Dvijotham UAI19] takes 130 seconds on average (Section 5.2) to certify one point in MNIST for a fully-connected network, while our method requires less than 0.01 second per point on average on MNIST for LeNet, which has much more neurons than the models used in [Dvijotham UAI19]. The difference mainly arises from the number of optimization steps in Algorithm 1 in [Dvijotham UAI19], which varies from a few hundred to tens of thousands (Section 4.4 in Dvijotham UAI19). In other words, there are indeed tighter bounds than linear approximations, such as SDP-based ones you mentioned and all complete certifiers in which no approximations are used. However, these methods all have prohibitively high complexity and cannot be incorporated into training. Our experimental results in Table 1 and 2 show the effectiveness of pushing the decision boundary based on a linear approximation to obtain robust models.\n\n3. As discussed in Section 5, for an N-layer network with k, m, n as the dimensions of the output, the input and the hidden layers ($n >> k, m$), the complexity of a CROWN-style linearized bound calculation is $O(N^2 n^3)$. However, based on Equation (18) for both the $l_\\infty$ and $l_2$ norm, the complexity of each iteration in Algorithm 2 is $O(km)$, which is significantly smaller than $O(N^2n^3)$. In practice, the loop in the algorithm stops within 5 iterations in almost all cases (Appendix B.2), so the overhead is small. We have run 10 times  PEC and CROWN on the CIFAR10-CNN model. To process the entire test set on a single-GPU machine, the mean and standard deviation in $l_\\infty$ cases are 217.51(1.95) for CROWN and 219.16(3.23) for PEC; in $l_2$ cases, they are 236.95(1.64) for CROWN and 239.41(1.92) for PEC. The difference is negligible and within the variance of the 10 attempts.\n\n4. We have provided the distribution of certified epsilon in CIFAR10 as a histogram in Figure 5 of the revised paper. For example, for models trained against $l_\\infty$ attack by PER+at, 1489 points in the test set (14.89%) are not certified by one-shot KW, but for which PEC gives a non-zero certified bound. The distribution of these bounds is close to uniform between 0 and $\\epsilon$.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkg75aVKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper701/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper701/Authors|ICLR.cc/2020/Conference/Paper701/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167533, "tmdate": 1576860533896, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper701/Authors", "ICLR.cc/2020/Conference/Paper701/Reviewers", "ICLR.cc/2020/Conference/Paper701/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Comment"}}}, {"id": "Syx3S9paFr", "original": null, "number": 1, "cdate": 1571834436184, "ddate": null, "tcdate": 1571834436184, "tmdate": 1572972562974, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\nThere exists several method (KW, Crown, zonotope transformers) which essentially propagate linear lower and upper bound through the network (bounds is a linear function of the input variables). So far, to check for robustness, these methods propagated the bounds until the last layer (+ margin computation) and then concretized them and compared the result to zero to see if the studied radius was safe or not. This paper highlights the fact that in the case where the studied radius is not safe, it is possible to extract a safe radius from the linear bound.\n\nThe authors then show that:\n-> this can be used to make binary search to find the larger verifiable epsilon faster (by providing a lower bound on epsilon even when failing to verify, rather than simply when succeeding verification)\n-> this can be employed during the training process to improve regularization, similarly to another previously proposed method (Croce et al.). While the methods are similar, the bounds are much less conservative than Croce's one and should therefore be more helpful\n\nThe presentation of the content is quite clear, and Figure 1.a is extremely useful in conveying the benefits of the method.\n\nExperimental validation is thorough:\n-> Comparing the ACB KW and ACB PEC columns in Table 1 and 2, for all type of activation functions, shows that this is as expected a strict improvement in the generated bound.\n-> Comparing the various rows in Table 1 shows the improvement with regards to other methods, both in terms of clean accuracy (this produces less over-regularization), while maintaining similar or better robust accuracy.\n-> Table 6 shows the benefit in the application to binary search.\n\nOpinion:\nThis paper is clearly written and I think that it's an interesting insight and that the authors do a good job at conveying its usefulness. I'm happy for this paper to be accepted.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575548289741, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper701/Reviewers"], "noninvitees": [], "tcdate": 1570237748339, "tmdate": 1575548289757, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Review"}}}, {"id": "SJgClQwRKH", "original": null, "number": 2, "cdate": 1571873526011, "ddate": null, "tcdate": 1571873526011, "tmdate": 1572972562932, "tddate": null, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "invitation": "ICLR.cc/2020/Conference/Paper701/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a certifiable NN training method, \"polyhedral envelope\nregularization\" (PER) for defending against adversarial examples.  The defense\nis based on the same linear relaxation based outer bounds of neural networks\n(KW/CROWN) used in many previous works.  The paper makes a few new (but small)\ntechnical contributions:\n\n1. this paper uses a different loss function (7), which is essentially Hinge\nloss on the lower bounds of distance to decision boundary. Previous works like\nKW used cross-entropy loss on the lower bound of prediction margin instead,\nwhich was based on minimax robust optimization theory. But I am not fully\nconvinced if the new loss function is better or not.\n\n2. in (5), the authors solve the bounded input case more carefully than\nprevious works.  (5) is trivial to solve in the L infinity case and has been\nused in previous works like (Wong & Kolter 2018, Gowal et al., 2018 and Zhang\net al., 2019); but solving it for other norms requires some efforts, and this\npaper proposes a good solution for it (Algorithm 2);\n\n3. In previous works like KW/CROWN, to find the largest certifiable radius, a\nbinary search is needed. The authors proposes a very small improvement to the\nbinary search process by setting the lower bound of search to the largest\nepsilon that is certifiable using the current linear relaxations obtained from\na larger epsilon.\n\nThe authors does not improve any bounds proposed in KW/CROWN, and they reuse\nthe same bounds. I see the main contribution as the new hinge-like loss\nfunction for training, and a more careful procedure to find the largest\ncertifiable radius in bounded input case.\n\nEmpirically, the improvement of the proposed algorithm is limited - based on\nTable 1 it is hard to say if PER is better than KW or not. PER+at outperforms\nKW sometimes, however it is not a completely fair comparison, as we can add a\nPGD based adversarial training loss to KW as well, as done in DiffAI (Mirman et\nal., https://github.com/eth-sri/diffai).\n\nQuestions:\n\n1. In my personal experience I usually found Hinge loss not as effective as\ncross-entropy loss in deep learning based tasks probably due to its\nnon-smoothness. The claim that (7) is better than cross-entropy loss is that it\ndoes not overregularize the network. The authors should provide more evidence\nto show if this argument holds, e.g., plotting the norm of weight matrices\nduring the training for the two losses to show that it can reduce\noverregularization.\n\n2. I think the metric ACB KW and ACB CRO (average certified radius of KW/CROWN)\nin Table 1 and 2 are confusing and not fair. In KW and CROWN's evaluation,\ngiving an epsilon, if an example cannot be certified due to epsilon to large\n(i.e., ||A|| \\epsilon + b > 0), certifiable radius will be count as 0 (flat\nline in Figure 1(a)). In this paper, the authors instead in this case use -b /\n||A|| as the certifiable radius. This is merely a different way of evaluation,\nand I don't see this as a contribution, as the \"improvement\" does not come from\na tighter bound.  In the same sense, I don't think Figure 1(a) and the\ndiscussions on page 3 are appropriate characterization of KW/CROWN. PEC uses\nexactly the same linear bounds as in KW/CROWN, and has the same certification\npower.\n\n3. For L2 based perturbations, in Table 1, the epsilon used for MNIST is too\nsmall. It is better to use an epsilon that is aligned with previous works. For\nexample in Wong et al., 2018 (https://arxiv.org/pdf/1805.12514.pdf), page 22,\nyou will find the epsilon used for MNIST and CIFAR.\n\n4. As discussed above, it is probably not fair to compare PER+at with KW. A new\nbaseline like KW+at should also be considered.\n\n5. For norms other than L infinity norms, solving (5) for getting $d$ can be\ntime consuming (Algorithm 2). How much additional time does it comparing\nto KW?\n\nOverall, I cannot recommend accepting this paper due to its limited theoretical\ncontribution as well as unconvincing empirical results comparing to previous\nmethods. I suggest rephrasing some parts of the paper and providing more\nexperimental results as discussed above.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper701/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Provably Robust Models by Polyhedral Envelope Regularization", "authors": ["Chen Liu", "Mathieu Salzmann", "Sabine S\u00fcsstrunk"], "authorids": ["chen.liu@epfl.ch", "mathieu.salzmann@epfl.ch", "sabine.susstrunk@epfl.ch"], "keywords": ["deep learning", "adversarial attack", "robust certification"], "abstract": "Training certifiable neural networks enables one to obtain models with robustness guarantees against adversarial attacks. In this work, we use a linear approximation to bound model\u2019s output given an input adversarial budget. This allows us to bound the adversary-free region in the data neighborhood by a polyhedral envelope and yields finer-grained certified robustness than existing methods. We further exploit this certifier to introduce a framework called polyhedral envelope regular- ization (PER), which encourages larger polyhedral envelopes and thus improves the provable robustness of the models. We demonstrate the flexibility and effectiveness of our framework on standard benchmarks; it applies to networks with general activation functions and obtains comparable or better robustness guarantees than state-of-the-art methods, with very little cost in clean accuracy, i.e., without over-regularizing the model.", "pdf": "/pdf/c3a20ec3ec0bef8069ff86d7f22ae4c763fda922.pdf", "paperhash": "liu|training_provably_robust_models_by_polyhedral_envelope_regularization", "original_pdf": "/attachment/f669bfde7d56b7c45920c362d8b62c7192479160.pdf", "_bibtex": "@misc{\nliu2020training,\ntitle={Training Provably Robust Models by Polyhedral Envelope Regularization},\nauthor={Chen Liu and Mathieu Salzmann and Sabine S{\\\"u}sstrunk},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkg75aVKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkg75aVKDH", "replyto": "Bkg75aVKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper701/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575548289741, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper701/Reviewers"], "noninvitees": [], "tcdate": 1570237748339, "tmdate": 1575548289757, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper701/-/Official_Review"}}}], "count": 11}