{"notes": [{"id": "BJzuKiC9KX", "original": "H1xVFWjqFQ", "number": 459, "cdate": 1538087808013, "ddate": null, "tcdate": 1538087808013, "tmdate": 1545355439539, "tddate": null, "forum": "BJzuKiC9KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SylmTLmLeN", "original": null, "number": 1, "cdate": 1545119419035, "ddate": null, "tcdate": 1545119419035, "tmdate": 1545354477774, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Meta_Review", "content": {"metareview": "The paper presents a well conducted empirical study of the Reweighted Wake Sleep (RWS) algorithm (Bornschein and Bengio, 2015). It shows that it performs consistently better than alternatives such as Importance Weighted Autoencoder (IWAE) for the hard problem of learning deep generative models with discrete latent variables acting as a stochastic control flow. \nThe work is well-written and extracts valuable insights supported by empirical observations: in particular the fact that increasing the number of particles improves learning in RWS but hurts in IWAE, and the fact that RWS can also be successfully applied to continuous variables.\nThe reviewers and AC note the following weaknesses of the work as it currently stands:  a) it is almost exclusively empirical and while reasonable explanations are argued, it does not provide a formal theoretical analysis justifying the observed behaviour b) experiments are limited to MNIST and synthetic data, confirmation of the findings on larger-scale real-world data and model would provide a more complete and convincing evidence. \nThe paper should be made stronger on at least one (and ideally both) of these accounts.\n\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting empirical observations of the advantage of RWS, but lacking formal theoretical analysis, and larger scale experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper459/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353211031, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353211031}}}, {"id": "SJxOX9_p14", "original": null, "number": 11, "cdate": 1544550944352, "ddate": null, "tcdate": 1544550944352, "tmdate": 1544550944352, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "SyeZgH_T1E", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Experiments", "comment": "Note that our claim is not based only on the GMM experiment. It is also backed up by results from training (i) a VAE with continuous latent variable on MNIST data (compared against IWAE since VIMCO is not needed) and (ii) the AIR model on moving MNIST data (compared against VIMCO; VQ-VAE not applicable)."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "SyeZgH_T1E", "original": null, "number": 1, "cdate": 1544549609444, "ddate": null, "tcdate": 1544549609444, "tmdate": 1544549627104, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "Skx2_YQ6kV", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Public_Comment", "content": {"comment": "I second the reviewer suggestion of real-world experiments. Improvements on toy data-sets like GMMs do not necessarily transfer over to real world data. And if the authors make the claim that  \n\"Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models\", then they should back it up with results that match or improve state-of-the-art generative models like VQ-VAE/VIMCO in bits/dim on large scale, real data-sets.", "title": "Real world experiments"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311835438, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJzuKiC9KX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311835438}}}, {"id": "Skx2_YQ6kV", "original": null, "number": 10, "cdate": 1544530292186, "ddate": null, "tcdate": 1544530292186, "tmdate": 1544530292186, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "r1gDQE7nJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Thank you for reconsidering our paper", "comment": "There are two reasons why we think RWS should be used based on solid (although simple) theoretical reasoning. First, in discrete latent variable models, we don\u2019t need reparameterization, continuous relaxation or control variates. Second, in all models, RWS can be helpful in light of the \u201ctighter variational bound is not better\u201d effect. The GMM and AIR experiments support both points. The continuous VAE (on MNIST data) gives further evidence for the second point.\n\nWhy not compare to RBM and DVAE? We agree that even more evidence would be good. However:\n- For RBMs, this is an entirely different class of models (the joint density p(z, x) can be evaluated only up to a normalizing constant, instead of directly), which is not learnable using RWS or other ELBO-maximizing approaches (learning RBMs requires contrastive divergence or similar). \n- For DVAE, in addition to being slightly different in focus due to branching, it is also orthogonal in another way. DVAE can be used in conjunction with IWAE to tighten the bound. We show through the continuous VAE experiment that RWS can help. It might be interesting to see whether RWS can be used on the continuously relaxed model defined in DVAE in order to improve DVAE further.\n\nWhy not test on real-world data? We agree that the transfer from synthetic to real-world data is difficult. Our paper is methodological and RWS is in its core a statistical method for maximizing the log marginal likelihood and minimizing the KL divergence from p to q. Whether a model works on a real dataset is not a function of the learning and inference algorithms, but rather the particular generative model and inference network. This is also true for other learning and inference algorithms (like VAE, IWAE, REBAR, RELAX, DVAE, etc.)."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "HJxxyGwinm", "original": null, "number": 1, "cdate": 1541267928434, "ddate": null, "tcdate": 1541267928434, "tmdate": 1544463418160, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "content": {"title": "An interesting experimental paper but more insights are expected", "review": "Main idea:\nThis paper studies a problem of the importance weighted autoencoder (IWAE) pointed out by  Rainforth 18, that is, tighter lower bounds arising from increasing the number of particles improve the learning of the generative model, but worsen the learning of the inference network. The authors show that the reweighted wake-sleep algorithm (RWS) doesn't suffer from this issue. Moreover, as an alternative to control variate scheme and reparameterization trick, RWS doesn't suffer from high variance gradients, thus it is particularly useful for discrete latent variable models.   \nTo support the claim, they conduct three experiments: 1) on ATTEND, INFER, REPEAT, a generative model with both discrete and continuous latent variables; 2) on MNIST with a continuous latent variable model; 3) on a synthetic GMM.\n\nClarity issues:\n1. \"branching\" has been used many times, but AFAIK, this seems not a standard terminology. What do \"branching on the samples\", \"conditional branching\", \"branching paths\" mean?\n2. zero-forcing failure mode and delta-WW: I find this part difficult to follow. For example, the following sentence \n\"the inference network q(z|x) becomes the posterior for this model which, in this model, also has support at most {0, . . . , 9} for all x\". \nHowever, this failure mode seems an interesting finding, and since delta-WW outperforms other methods, it deserves a better introduction. \n\nQuestions:\n1. In Fig 1 (right), how do you estimate KL(q(z|x) || p(z|x))?\n2. In Sec 4.2, why do you say IWAE learns a better model only up to a point (K = 128) and suffers from diminishing returns afterwards?  \n3. In Fig 4, why WS doesn't achieve a better performance when K increasing?\n\nExperiments:\n1. Since the motivating story is about discrete latent variable models, better baselines should be compared, e.g. RBM, DVAE, DVAE++, VQ-VAE etc. \n2. All experiments were on either on MNIST or synthetic data, at least one large scale experiment on discrete data should be made to verify the performance of RWS. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "cdate": 1542234456743, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728959, "tmdate": 1552335728959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gDQE7nJ4", "original": null, "number": 9, "cdate": 1544463390719, "ddate": null, "tcdate": 1544463390719, "tmdate": 1544463390719, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "rJxb9BFYaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Somehow convinced but still expect more experiments", "comment": "\" the resulting discreteness cannot be used for directing the control flow\"\n\nAt least RBM doesn't need any continuous relaxation in training. \nTo test on those discrete applications which can be continuously relaxed during training is also important. It offers us a better understanding of when RWS should be applied.  I don't think your reasons are valid to not compare with RBM, DVAE etc. \n\nI still insist on real dataset/tasks, e.g. semantic segmentation, since there is always a gap between synthetic world and real world."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "SklC5bm31N", "original": null, "number": 8, "cdate": 1544462742363, "ddate": null, "tcdate": 1544462742363, "tmdate": 1544462742363, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "B1xWOjBoyV", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Increasing from 5 to 6", "comment": "I'll vote for an accept if there is space since RWS is an important method for discrete latent variable models,\nwhich was not paid enough attention in previous literature. \n\nPros: \nA better understanding of RWS (Bornschein et al. 2015) with new analysis. \n\nCons:\nIt is not tested on large-scale or real datasets.\nThe paper needs at least one more iteration to improve clarity. "}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "BkleS0BoJV", "original": null, "number": 6, "cdate": 1544408632051, "ddate": null, "tcdate": 1544408632051, "tmdate": 1544408632051, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "B1xWOjBoyV", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Score Increased to 6", "comment": "Based on Authors' responses to reviews, I increase my score to 6: Marginally above acceptance threshold."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "SylgzBTohX", "original": null, "number": 2, "cdate": 1541293319555, "ddate": null, "tcdate": 1541293319555, "tmdate": 1544408527964, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "content": {"title": "Revisiting Reweighted Wake-Sleep", "review": "This manuscript investigates the performance of Reweighted Wake-Sleep (RWS) framework for learning deep generative models with discrete latent variables. It gives a clear introduction to variational autoencoder based models for scenarios with discrete latent variables, including IWAE and also models based on continuous relaxations of discrete variables. The paper performs several experiments, which suggest that RWS is more appropriate for discrete latent variables than other methods such as IWAE. Especially, increasing the number of particles, unlike IWAE, always enhances the performance of RWS.\n\nWhile this paper investigates an important problem, and also offers interesting observations, it lacks a rigorous analysis of why the RWS performance is consistently better than IWAE. More precisely, the propositions should be stated in more formal language and they should be accompanied with a minimal rigorous justification.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "cdate": 1542234456743, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728959, "tmdate": 1552335728959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxb9BFYaQ", "original": null, "number": 4, "cdate": 1542194569061, "ddate": null, "tcdate": 1542194569061, "tmdate": 1542194915994, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "HJxxyGwinm", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Clarity issues:\n\nBy stochastic branching we refer to the evaluation of generative models where discrete latent variables are used to select which part of the model is going to be evaluated next. For example, in AIR, this decides when the program halts, in GMM the cluster index decides which likelihood function is evaluated. Another example of this are probabilistic context-free grammars (PCFGs) where discrete variables are used to describe which production rule is used (for example https://arxiv.org/abs/1806.07832). This is in contrast with modeling approaches where the discrete latent variable is merely an input to a neural network that doesn\u2019t distinguish it from a non-discrete latent variable since it does not explicitly use the discreteness to model distinct modes of the data. (also see our general comment)\n\nWe will clarify the \u201czero-forcing\u201d failure mode and delta-WW in the updated manuscript.\n\nQuestions:\n\nTo estimate KL(q(z|x) || p(z|x)), we take the difference of the log likelihood estimated by a 5000-particle IWAE bound and the ELBO estimated by 5000 Monte Carlo samples.\n\nThe statement that \u201cIWAE learns a better model only up to a point\u201d is justified by the IWAE curve in the middle of Figure 2: the decreasing slope indicates that improvements in marginal log probability decrease with increasing numbers of particles. This is even more pronounced in Figure 1, where IWAE performance decreases for k > 10.\n\nIn Fig 4, WS actually does achieve better performance as K increases - the final value of the learning curve goes down, although only very slightly.\n\nExperiments:\n\nRegarding experiments, RBM/DVAE/++/# and VQ-VAE allow learning models with discrete latent variables in general; however, the resulting discreteness cannot be used for directing the control flow of a generative model (see also response to AnonReviewer3 and our general comment).\n    - In the DVAE family of algorithms, learning in discrete latent variable models is achieved by a continuous relaxation. This prevents using these variables as hard branching conditions.\n    - In the VQ-VAE algorithm, the discrete latent variable is explicitly designed to be used to select an embedding and it is deterministic. This limits the use of a discrete latent variable (cannot be used to model a cluster identity or stopping of a while loop). \n\nEven though we do not have experiments on large-scale real-world datasets, AIR is a non-trivial model, and using it can be seen as a large-scale experiment - taking several days (and several GPUs) to obtain results summarized in Figure 1. Similarly, to the best of our knowledge, ours is the first reported result of an MNIST model trained with IWAE with 512 particles.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "BJeBGBtFaQ", "original": null, "number": 3, "cdate": 1542194444654, "ddate": null, "tcdate": 1542194444654, "tmdate": 1542194444654, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "SylgzBTohX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "The key formal justification is relatively straightforward: RWS, unlike IWAE, does not suffer from the \u201ctighter bounds\u201d problem. On the contrary, RWS uses self-normalized importance sampling to estimate the gradient with respect to \\phi. Both the asymptotic bias and variance of a self-normalized importance sampling estimator decrease linearly in number of particles. This means that increasing number of particles improves our gradient estimator and thus the optimization procedure.\n\nWe will explain this in more detail in the updated manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "BylkLVYKa7", "original": null, "number": 2, "cdate": 1542194247435, "ddate": null, "tcdate": 1542194247435, "tmdate": 1542194276994, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJepNja0hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Baselines: As set out in our overall response, we aim to show that RWS is a better choice for inference in models that have stochastic control flow, where the choice from the discrete latent variables matters explicitly. In our GMM example, the cluster identity is such a choice, and in AIR, the stopping condition for the loop is another such choice. The work done in DVAE++, DVAE#, and other such approaches do not really handle this general class of problems well---by typically requiring enumeration of all possible branches and choices.\n\nGMM: We will include a more detailed description of how defensive sampling ameliorates issues discovered in the GMM experiments in the updated manuscript.\n\nTheoretical Rigour: We will include a more comprehensive discussion of the theoretical basis of why RWS is better than IWAE in the updated manuscript. Briefly, the justification for why RWS does not suffer from the \"tighter bounds\" problem is due to RWS's use of self-normalised importance sampling to compute the gradient of proposal parameters---resulting in both the asymptotic bias and variance decreasing linearly with number of samples.\n\nEmpirical Rigour: Our experiments strongly support our hypotheses:\n  a. Unlike IWAE, RWS performs better with more particles, both in terms of the generative model and inference network, and\n  b. It allows for effective and easy application to models where the choice from the discrete random variables affects model expansion or computation---something that requires expensive enumeration with continuous relaxations, or extremely finicky and unreliable construction with control-variate methods."}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "Syg9R7FF6m", "original": null, "number": 1, "cdate": 1542194130462, "ddate": null, "tcdate": 1542194130462, "tmdate": 1542194130462, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "content": {"title": "Discrete latent variables, stochastic control flow and probabilistic programming", "comment": "We thank the reviewers for their time and for appreciating our arguments about why RWS is preferable to IWAE-based approaches as we increase number of particles.\n\nWe would like to re-emphasize an important implication from our paper: RWS is a good inference algorithm for models that have _stochastic control flow_: i.e. models where latent variables can be instantiated dynamically, and different branching paths explored, based on the choice from a random variable[4-7]. Note that this is orthogonal to a large number of recent work that employ discrete latent variables in deep generative models[8-10]---where either the discrete variable is transformed via continuous relaxations, or, marginalized out entirely. The crucial difference is that none of these approaches address models where the model execution itself is determined via the discrete choices (if-statements or for-loops), as opposed to simply passing it through to a neural network---what is done with the discrete choice matters.\n\nThis is best illustrated in the domain of universal probabilistic programs (like Pyro [1]) which can contain arbitrary continuous and discrete latent variables, and where latent variables are instantiated dynamically and defined by running the program (or generative model). Stochastic control flow is a feature of such models [2] and allows the definition of expressive models, with potentially infinite number of latent variables [3], as mentioned in the discussion.\n\nUniversal (or higher-order) probabilistic programs form the largest family of samplable distributions and thus are a powerful tool to model data. Amortized inference and model parameter learning in such probabilistic programs, however, is typically only done using variational methods in the VAE/IWAE family of algorithms (as summarized in our manuscript). We\u2019re trying to say: RWS is a simple and often superior algorithm to use in this model family.\n\nOur point about probabilistic programs, hard selection and stochastic branching is illustrated by our choice of experiments (GMM and AIR). However, we will more strongly emphasize this point in the updated manuscript.\n\n[1] http://pyro.ai/\n[2] http://pyro.ai/examples/intro_part_i.html#Universality:-Stochastic-Recursion,-Higher-order-Stochastic-Functions,-and-Random-Control-Flow\n[3] Quote from [2]\n    \"For example, we can construct recursive functions that terminate their recursion\n     nondeterministically, provided we take care to pass pyro.sample unique sample names\n     whenever it\u2019s called.\"\n[4] GMM models in this work\n[5] Tree-structured latent variables in https://arxiv.org/abs/1806.07832\n[6] Memory-based models in https://arxiv.org/abs/1709.07116\n[7] AIR-like models in this work and https://arxiv.org/abs/1806.01794\n[8] DVAE++ - https://arxiv.org/abs/1802.04920\n[9] DVAE#  - https://arxiv.org/abs/1805.07445\n[10] VQ-VAE - https://arxiv.org/abs/1711.00937\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621623540, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJzuKiC9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper459/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper459/Authors|ICLR.cc/2019/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers", "ICLR.cc/2019/Conference/Paper459/Authors", "ICLR.cc/2019/Conference/Paper459/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621623540}}}, {"id": "BJepNja0hQ", "original": null, "number": 3, "cdate": 1541491508523, "ddate": null, "tcdate": 1541491508523, "tmdate": 1541533978061, "tddate": null, "forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "content": {"title": "Nice experimental discoveries", "review": "This paper conducts an extensive set of experiments on RWS and compares it against a set of benchmarks such as GMM and IWAE. The main contribution of the paper is the fact revealed by these experiments, that RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent variable models as well. The performance of RWS will increase significantly if we increase the number of particles. \n\nThe experimental part is written in an inspiring way, and I enjoyed reading it. However, there should be stronger baselines incorporated. for example, https://arxiv.org/abs/1805.07445. Also, I think the authors could try to emphasize more on the shortcomings of RWS discovered by the GMM experiments, and how defensive importance sampling fixes it. There are several other parts in the paper that indicates interesting facts, diving deeper into it could possibly lead to more interesting findings.\n\nIn all, I would consider these comparison results important to be somewhere in the literature, but because its lack of rigorous analysis and explanation for the observations, I personally think these observations alone are not novel enough to be an ICLR paper. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper459/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Reweighted Wake-Sleep", "abstract": " Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.", "keywords": ["variational inference", "approximate inference", "generative models", "gradient estimators"], "authorids": ["tuananh@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "nsid@robots.ox.ac.uk", "y.w.teh@stats.ox.ac.uk", "fwood@cs.ubc.ca"], "authors": ["Tuan Anh Le", "Adam R. Kosiorek", "N. Siddharth", "Yee Whye Teh", "Frank Wood"], "TL;DR": "Empirical analysis and explanation of particle-based gradient estimators for approximate inference with deep generative models.", "pdf": "/pdf/c653bb9ffd52e2374b3b1568ae2b870a653004f6.pdf", "paperhash": "le|revisiting_reweighted_wakesleep", "_bibtex": "@misc{\nle2019revisiting,\ntitle={Revisiting Reweighted Wake-Sleep},\nauthor={Tuan Anh Le and Adam R. Kosiorek and N. Siddharth and Yee Whye Teh and Frank Wood},\nyear={2019},\nurl={https://openreview.net/forum?id=BJzuKiC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper459/Official_Review", "cdate": 1542234456743, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJzuKiC9KX", "replyto": "BJzuKiC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper459/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335728959, "tmdate": 1552335728959, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper459/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}