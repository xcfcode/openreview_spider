{"notes": [{"id": "f_GA2IU9-K-", "original": "_x7hZNxcifF", "number": 1504, "cdate": 1601308167227, "ddate": null, "tcdate": 1601308167227, "tmdate": 1614985771825, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zVm4fIICtSU", "original": null, "number": 1, "cdate": 1610040360292, "ddate": null, "tcdate": 1610040360292, "tmdate": 1610473950426, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes a non-decreasing quantile functional form for distributional RL, and secondly propose using the distributional error as a means of exploration. The experimental results are very exciting. The paper, however, needs further work before acceptance: the reviewers raised concerns about Theorem 1: a full proof is not included (nor written convincingly during discussion), and while several encouraging experiments were added during the discussion to the paper addressing the reviewers concerns, they fell short (understandably, given the time available).\n\nThus on this basis, I recommend rejection at this time, but think it likely that with these adjustments the paper will be accepted in future."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040360277, "tmdate": 1610473950408, "id": "ICLR.cc/2021/Conference/Paper1504/-/Decision"}}}, {"id": "vH5Myh4Gr9Q", "original": null, "number": 1, "cdate": 1603185868998, "ddate": null, "tcdate": 1603185868998, "tmdate": 1606631179074, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review", "content": {"title": "Review", "review": "Paper Summary: This paper mainly contributes in two parts: 1). A non-decreasing structure for quantile values in quantile-based distributional RL. 2). A curiosity-based intrinsic reward using distribution disagreement.\n\nClarity: \n- Some mathematical expressions, while correct, are hard to interpret, e.g. G_{i,\\delta}, H_{i,\\omega', \\omega^*}. It would be nice if the authors could provide a brief explanation in words. For example, G_{i,\\delta} is just linear interpolation given fixed support.\n\n- The paper stops at experiment results. There are no further discussions or conclusion section.\n\n- The relation between RND and proposed DPE is not as close as the authors suggest. In RND, the target network is a fixed network for the predictor network to mimic. In DPE, both the target network and the predictor is trained with the same objective, i.e. minimize TD error. The intrinsic reward is evaluated by the disagreement between the predictor and the target network. Such disagreement-based intrinsic reward had been studied in other lines of work, e.g. [1].\n\nPros: \n- Resolves existing crossing quantile issue in most quantile-based distributional RL algorithms.\n- Proposed method ensures monotonicity on all possible quantile sets. Using linear interpolation for distribution approximation feels natural and straightforward.\n- Distribution based curiosity is novel.\n\nCons:\n- The significance of DPE is not properly evaluated. In fact, the paper only provide detailed results for NDFQN+DPE. It is hard to tell exactly how much of the performance gain is credit to NDFQN or DPE. To evaluate the significance of DPE, I would expect the authors to compare across different curiosity-driven exploration methods on the same baseline, say IQN. At least, the authors should answer how much distribution-based disagreement outperforms value based disagreement.\n\n- Same reason as above, the performance gain of NDFQN itself cannot be inferred from just 6 games. Judging from section 4.1 and 4.2, I would presume that the performance gain is mainly credit to DPE instead of NDFQN.\n\n- The issue in figure 1(c) is not entirely resolved. Different from the issue in figure 1(b), this issue directly comes from using incrementals instead of values. The authors partially resolves it by using a fixed set of support, but if the support is changeable the issue remains.\n\n\nQuestions:\n- Is the embedding of p* really necessary? Or, is p* necessary for network input? As the support p* is fixed, I do not believe that they should be part of the input. From my own point of view, using modified QR-DQN's structure sounds more reasonable. If the authors find out that an additional, fixed input does impact the performance significantly, some explanations or insights are required.\n\nPost rebuttal:\n- The authors have addressed most of my main concerns and the additional experiment looks good to me. Therefore, I increase my score to 6. However, some experiment results are still missing and the paper still needs some editing before published, especially the experiment section. For example figure 6. is misleading since the comparison is not fair.\n\nReferences:\n[1] Pathak, Deepak, Dhiraj Gandhi, and Abhinav Gupta. \"Self-Supervised Exploration via Disagreement.\" International Conference on Machine Learning. 2019.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117073, "tmdate": 1606915761288, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1504/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review"}}}, {"id": "VUHWLnvyHS5", "original": null, "number": 2, "cdate": 1603773577506, "ddate": null, "tcdate": 1603773577506, "tmdate": 1606301461251, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review", "content": {"title": "Interesting idea but not yet sufficiently proper experiments to understand the role of NDQFN and DPE separately, and fairer comparison with the baseline IQN is required ", "review": "###  Summary\nThis paper proposes to use a monotonic quantile function for distributional RL by: (i) estimating the quantile values at the supported quantiles via a cumulative sum of non-negative incremental value, and (ii) interpolate the quantile values at the unsupported quantiles via linear combination of the nearest supported quantiles. The paper then also proposes to estimate the exploration bonus for each state using random network distillation. It then conducts experiments in Atari games to verify some conclusions.  \n\n### Strong points \n-\tClarity: The paper is well structured. \n-\tTechnical novelty: immediate. The idea of piece-wise linear function to approximate quantile function in Eq. (4) seems novel though very natural. The idea of using random network distillation to measure the exploration bonus seems novel and useful. \n-\tEmpirical significance: The combination of DPE with NDQFN performing well in some hard-explored games (Fig. 3) looks very encouraging and promising. In my opinion, I think this is the most interesting and significant empirical result reported in this paper in the current form.\n\n###  Weak points\n-\tThe experimental comparison/conclusion with IQN in the present form is unfair. \nIn particular, after the Table 1, the authors conclude that \u201cNDQFN with DPE significantly outperforms the IQN baseline\u201d. I think this is an unfair conclusion as the result of IQN reported in Table 1 is merely the performance for distributional part, i.e., the IQN result in Table 1 does not include any orthogonal improvements such as n-updates, double networks and any advanced exploration method beyond epsilon-greedy while NDQFN incorporates DPE exploration method (and n-step updates  and double Q update if I infer correctly from the paper). To be fair, I think the paper should include to IQN any orthogonal improvements that NDQFN has.  Even though Figure 2 has a fair comparison between NDQFN and IQN where both use n-step updates and without DPE exploration, I think only 6 games are not sufficient to make a reliable conclusion that the proposed non-decreasing quantile function is more helpful than the original quantile function in IQN. \n-\tIt is unclear from the experimental results how helpful are each non-decreasing quantile function and DPE individually. \nExcept a nice result in Figure 3, I am not fully convinced if the proposed non-decreasing quantile function is actually more helpful than the original quantile function as a distributional component. For example, I think it is more useful to report the result of a mere NDQFN (i.e., without DPE or any exploration methods rather than epsilon-greedy, without any orthogonal improvements such as n-step updates and double Q value) in the full Atari games to see how much it improves over the original IQN (reported in Figure 1). Regarding DPE, since it is a new exploration method, I think it is more helpful and reliable to experimentally compare DPE with some other distributional exploration methods such as  DLTV (Mavrin et al. 2019). I think the current experimental results are hard to make any reliable conclusion about each NDQFN and DPE individually except a nice observation that a combination of NDQFN and DPE can help in some hard explored games.   \n\n###  Questions for the authors. \n-\tThough I believe that IQN indeed does not guarantee the monotonicity in quantile values it estimates, the illustrative figure for this (Fig 1) seems a hypothetical one instead of a real experimental result.  Can we show that empirically by a simulation? \n-\tDoes the NDQFN+DPE in Table 1 also have n-step update and double Q network update style? \n\n###  My initial recommendation\nFor the main reasons in the weak points section, I vote for rejecting for this current form.\n\n### My final recommendation \n\nThe authors have attempted to address some of my points but these points require more time to fully address as they require to run more experiments. For the current form, I remain my inital score and recommend rejection for this time. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117073, "tmdate": 1606915761288, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1504/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review"}}}, {"id": "CQZ4sj6rwwu", "original": null, "number": 8, "cdate": 1606296635566, "ddate": null, "tcdate": 1606296635566, "tmdate": 1606300047678, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "vH5Myh4Gr9Q", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 4 (Part 2)", "comment": "**5.** The issue in figure 1(c) is not entirely resolved. Different from the issue in figure 1(b), this issue directly comes from using incrementals instead of values. The authors partially resolves it by using a fixed set of support, but if the support is changeable the issue remains.\n\n**Response**: Thank you for your comment. Exactly, we consider p as a hyper-parameter of NDQFN like QR-DQN (Dabney et al., 2018a) and we show how we choose the supporting set in both the end of Section 4 and the start of Section 6. \n\n**6.** Is the embedding of p* really necessary? Or, is p* necessary for network input? As the support p* is fixed, I do not believe that they should be part of the input. From my own point of view, using modified QR-DQN's structure sounds more reasonable. If the authors find out that an additional, fixed input does impact the performance significantly, some explanations or insights are required.\n\n**Response**: Thank you for your comment. The results in Dabney et al. (2018b) shows that IQN achieves significant improvement on QRDQN. Motivated by this, we consider the similar network with IQN and similar training process, which contains an embedding of quantile fractions. NDQFN can actually be seen as the improvement of the combination of QRDQN and IQN. The supporting points are fixed like QRDQN, while the quantile fractions used for model training are resampled each time to make the network more closely approximate the entire distribution of the target quantile function. With the help of these varied \u03c4 from different training iterations, the final quantile estimates at the support points p* may be more precise than QRDQN. \n\n[1] Will Dabney, Mark Rowland, Marc G Bellemare, and R \u0301emi Munos. Distributional reinforcement learning with quantile regression. In AAAI, 2018a.\n\n[2] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pp.1096\u20131105, 2018b.\n\n[3] Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In International Conference on Machine Learning, pp. 5062-5071, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "qLzzdyKn0zJ", "original": null, "number": 7, "cdate": 1606296552668, "ddate": null, "tcdate": 1606296552668, "tmdate": 1606296565170, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "vH5Myh4Gr9Q", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 4 (Part 1)", "comment": "Thank you for the thoughtful and constructive suggestions. We have taken all the comments into consideration and summarize the responses in the following. Belows are our point to point responses to your comments.\n\n**1.** Some mathematical expressions, while correct, are hard to interpret, e.g. G_{i,\\delta}, H_{i,\\omega', \\omega^*}. It would be nice if the authors could provide a brief explanation in words. For example, G_{i,\\delta} is just linear interpolation given fixed support.\nThe paper stops at experiment results. There are no further discussions or conclusion section. \n\n**Response**: Thank you for your good suggestion. We have revised the paper carefully to address these questions. In the revised manuscript, we add more explanations to interpret some important mathematical expressions, such as Line 3 on Page 4 (marked as red). Besides, in Section 7 we summarize the major contributions in this paper and discuss some unsolved problems and possible future work.\n\n**2**.The relation between RND and proposed DPE is not as close as the authors suggest. In RND, the target network is a fixed network for the predictor network to mimic. In DPE, both the target network and the predictor are trained with the same objective, i.e. minimize TD error. The intrinsic reward is evaluated by the disagreement between the predictor and the target network. Such disagreement-based intrinsic reward had been studied in other lines of work, e.g. [1].\n\n**Response**: Thank you for pointing out this problem. We choose a periodically synchronized target network in DPE, since it is required by implementing double Q-learning and performs well in practice. We need to thank the anonymous reviewer, the idea of Pathak et. al (2019) is indeed more related to ours and we have already cited this paper in our revised manuscript. \n\n**3.** The significance of DPE is not properly evaluated. In fact, the paper only provides detailed results for NDFQN+DPE. It is hard to tell exactly how much of the performance gain is credit to NDFQN or DPE. To evaluate the significance of DPE, I would expect the authors to compare across different curiosity-driven exploration methods on the same baseline, say IQN. At least, the authors should answer how much distribution-based disagreement outperforms value-based disagreement.\n\n**Response**: Thank you for your good suggestion. Following your suggestion, we compare the value-based disagreement with distribution-based disagreement in Appendix. The exploration bonus of value-based disagreement is defined as follow:\n$$i'(x_t,a_t) = |Q_{\\omega^*}(x_t,a_t) - Q_{\\omega'}(x_t,a_t)|,$$\nwhere $Q_{\\omega'}$ and $Q_{\\omega^*}$ denote the state-action value function of the target and predictor network, respectively. As Figure 3 in the supplement shows, the distribution-based disagreement significantly outperforms value-based disagreement on three hard-explored games with 100M training frames.\n\n**4.** Same reason as above, the performance gain of NDFQN itself cannot be inferred from just 6 games. Judging from section 4.1 and 4.2, I would presume that the performance gain is mainly credit to DPE instead of NDFQN.\n\n**Response**: Thank you for your comment. Due to the limited time, we cannot provide the full 200M results of all the 55 Atari game. In Figure 3 of Section 6.1 and Figure 2 of the appendix, we compare 29 games while some are based on 200M and some are based on 100M. According to the existing results, we can see that the non-decreasing constraint helps NDQFN to achieve better performance than the baseline IQN in most cases among 29 Atari games. We will definitely add the full the results into the final version if the paper is accepted. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "DIC72YwbF4K", "original": null, "number": 3, "cdate": 1606295597029, "ddate": null, "tcdate": 1606295597029, "tmdate": 1606296195316, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "LUrOqKbxe6w", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Reference", "comment": "[1] Will Dabney, Mark Rowland, Marc G Bellemare, and R \u0301emi Munos. Distributional reinforcement learning with quantile regression. In AAAI, 2018a.\n\n[2] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pp.1096\u20131105, 2018b.\n\n[3] Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quantile function for distributional reinforcement learning.  In Advances in Neural Information Processing Systems, pp. 6193\u20136202, 2019.\n\n[4] Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional reinforcement learning for efficient exploration. In International Conference on Machine Learning, pp. 4424\u20134434, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "61uXm9ygeV", "original": null, "number": 4, "cdate": 1606295836947, "ddate": null, "tcdate": 1606295836947, "tmdate": 1606296179043, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "u_ue0wxoD4k", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 2", "comment": "Thank you for the thoughtful and constructive suggestions. We have taken all the comments into consideration and summarize the responses in the following. Below are our point to point responses to your comments.\n\n**1.** The NDQFN is a trivial extension of (3) through piece-wise linear interpolation. The explanation of Figure 1 is misleading.\n\n**Response**: Thank you for your comment. We may not make the statements clear in this part. We rewrite the introduction part of NDQFN in Section 4. The incremental structure in (3) is actually an important component of NDQFN architecture, which is proposed by this paper. However, it cannot completely solve the crossing issue by itself without the piece-wise linear structure. Also, we modify Figure 1 to make it more interpretable and add more detailed explanation about it in a Remark before Theorem 1. directly applying a similar incremental structure onto IQN without using the piece-wise linear approximation may result in the non-monotonicity of \u03c4\u222a\u03c4^' although each of their own monotonicity is not violated. \n\n**2.** There is another class of methods in distributional reinforcement learning which uses generative network to model the Z function, which doesn\u2019t suffer from lack of monotonicity as in quantile function learning (see e.g. Y. Yue, Z. Wang, M. Zhou ``Implicit Distributional Reinforcement Learning\u201d). These type of methods need to be properly discussed and also compared in the experiments.\n\n**Response**: Thank you for pointing out this work. Yue et al. (2020) adapts a distributional perspective on the discounted cumulative return and model it with a state-action-dependent implicit distribution, which is approximated by the DGNs that take state-action pairs and random noises as their input. However, the proposed method would make more time consumption than NDQFN, since the DGNs inputs $K$ random noises and sorting the $K$ elements of output to compute empirical distribution. The sorting process could be time wasted. Since this new method does provide results on Atari and the pre-released code on Github seems to be problematic, we do not include its empirical comparison in the experiment section due to the limited time. However, we still add the citation of this paper into the revised paper and consider dosing come comparison in the future if possible. \n\n**3.** Theorem 1 could be problematic. The quantile projection defined in this paper doesn\u2019t seem to be contractive.\n\n**Response**: Thank you for pointing out this problem. Exactly, under some conditions on the dimension of support p and sample number of \u03c4, Theorem 1 could be seen an extension of Proposition 2 in Dabney et al., (2018a). The main idea of the proof is that if N is large enough, each element of sampled \u03c4 would fall into one of the N [p_i,p_(i+1)]\u2019s, and the proof would be similar to the proof of Proposition 2 in Dabney et al., (2018a). The exhaustive proof would be provided in the final version if the paper is accepted.\n\n**4.** Section 3.4 is not well presented.\n\n**Response**: Thank you for pointing out this problem. We have rewritten this section in the revised manuscript. In Section 5, we first describe the architecture of DPE, which includes three networks. Then, we describe the training process of the predictor network and provide an empirical approximation used by DPE. Finally, we present how the DPE design can help improve the exploration efficiency compared to using the value-based prediction error.\n\n[1] Will Dabney, Mark Rowland, Marc G Bellemare, and R \u0301emi Munos. Distributional reinforcement learning with quantile regression. In AAAI, 2018a.\n\n[2] Yuguang Yue, Zhendong Wang, and Mingyuan Zhou. Implicit distributional reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "-75O62cG0lu", "original": null, "number": 6, "cdate": 1606296164106, "ddate": null, "tcdate": 1606296164106, "tmdate": 1606296164106, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "VUHWLnvyHS5", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 3 (Part 2)", "comment": "**4.**\tDoes the NDQFN+DPE in Table 1 also have n-step update and double Q network update style?\n\n**Reponse**: Thank you for your comment. Yes, we emphasize in the second paragraph of Section 6 (marked by red) that we incorporate n-step updates (Sutton, 1988), double Q-learning (Hasselt, 2010), quantile regression (Koenker,2005) and distributional Bellman update (Dabney et al., 2018a) into the training for all compared methods\n\n[1] Will Dabney, Mark Rowland, Marc G Bellemare, and R \u0301emi Munos. Distributional reinforcement learning with quantile regression. In AAAI, 2018a.\n\n[2] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In International Conference on Machine Learning, pp.1096\u20131105, 2018b.\n\n[3] Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional reinforcement learning for efficient exploration. In International Conference on Machine Learning, pp. 4424\u20134434, 2019.\n\n[4] Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9\u201344, 1988.\n\n[5] Hado V Hasselt. Double q-learning. In Advances in neural information processing systems, pp.2613\u20132621, 2010.\n\n[6] Koenker. Quantile regression. Cambridge University Press, 2005.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "H1O3kw91pnQ", "original": null, "number": 5, "cdate": 1606296063315, "ddate": null, "tcdate": 1606296063315, "tmdate": 1606296100370, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "VUHWLnvyHS5", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 3 (Part 1)", "comment": "Thank you for the thoughtful and constructive suggestions. We have taken all the comments into consideration and summarize the responses in the following. Below are our point to point responses to your comments.\n\n**1.** The experimental comparison/conclusion with IQN in the present form is unfair. In particular, after the Table 1, the authors conclude that \u201cNDQFN with DPE significantly outperforms the IQN baseline\u201d. I think this is an unfair conclusion as the result of IQN reported in Table 1 is merely the performance for distributional part, i.e., the IQN result in Table 1 does not include any orthogonal improvements such as n-updates, double networks and any advanced exploration method beyond epsilon-greedy while NDQFN incorporates DPE exploration method (and n-step updates and double Q update if I infer correctly from the paper). To be fair, I think the paper should include to IQN any orthogonal improvements that NDQFN has. Even though Figure 2 has a fair comparison between NDQFN and IQN where both use n-step updates and without DPE exploration, I think only 6 games are not sufficient to make a reliable conclusion that the proposed non-decreasing quantile function is more helpful than the original quantile function in IQN.\n\n**Response**: Thank you for your comment. In the revised manuscript, we provide the results of IQN with n-step updates and double Q-learning, labeled as IQN*, in Table 1 for a fair comparison. \nDue to the limited time, we cannot provide the full comparison between NDQFN and IQN on all the 55 Atari games. However, we provide 200 M results for 11 games in Figure 3 and 100 M results for the other 18 games in Figure 2 of the appendix. The remaining part of experimental results would be done in the future. According to the existing results, we can see that the non-decreasing constraint helps NDQFN to achieve better performance than the baseline IQN in most cases among 29 Atari games. Moreover, as Figure 4 in Section 6.2 shows, NDQFN works better with the proposed DPE exploration than IQN since NDQFN obtains a more reasonable quantile estimate by adding the non-decreasing constraint which helps to increase the exploration efficiency.\n\n**2.** It is unclear from the experimental results how helpful are each non-decreasing quantile function and DPE individually. Except a nice result in Figure 3, I am not fully convinced if the proposed non-decreasing quantile function is actually more helpful than the original quantile function as a distributional component. For example, I think it is more useful to report the result of a mere NDQFN (i.e., without DPE or any exploration methods rather than epsilon-greedy, without any orthogonal improvements such as n-step updates and double Q value) in the full Atari games to see how much it improves over the original IQN (reported in Figure 1). Regarding DPE, since it is a new exploration method, I think it is more helpful and reliable to experimentally compare DPE with some other distributional exploration methods such as DLTV (Mavrin et al. 2019). I think the current experimental results are hard to make any reliable conclusion about each NDQFN and DPE individually except a nice observation that a combination of NDQFN and DPE can help in some hard-explored games.\n\n**Response**: Thank you very much for this good suggestion. In the revised version, we evaluate the empirical performance of NDQFN with DLTV and compare it with NDQFN +DPE. We extend DLTV (Mavrin et al., 2019), an exploration approach designed for DRL algorithms using fixed quantile locations such as QR-DQN, to quantile value based methods by modifying the original left truncated variance to\n$$\\sigma_+^2 = \\int_{\\frac{1}{2}}^1 [F_Z^{-1}(\\tau) - F_Z^{-1}(\\frac{1}{2})]^2 d\\tau.$$\nFigure 5 shows that DLTV method perform well in early stage with limited training samples. However, DPE achieves a much better performance than DLTV especially in the three hard explored games. This tells that DPE more sufficiently utilize the entire distribution of the quantile function than DLTV. \n\n**3.** Though I believe that IQN indeed does not guarantee the monotonicity in quantile values it estimates, the illustrative figure for this (Fig 1) seems a hypothetical one instead of a real experimental result. Can we show that empirically by a simulation?\n\n**Reponse**: Thank you for your comment. In Figure 1, we present a real quantile curve obtained by $N$ uniformly sample quantile fractions at a certain $(s,a)$ pair estimated by IQN under 50M training on Breakout. As the figure shows, the quantile curve is obviously non- monotonic. As a comparison, our method can ensure the monotinicity of the quantile estimates regardless of the selection of $\\tau$. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "lFI3BFPosBj", "original": null, "number": 2, "cdate": 1606295503655, "ddate": null, "tcdate": 1606295503655, "tmdate": 1606295503655, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "LUrOqKbxe6w", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment", "content": {"title": "Responses to the Comments of Reviewer 1", "comment": "Thank you for the thoughtful and constructive suggestions. We have taken all the comments into consideration and summarize the responses in the following. Below are our point to point responses to your comments.\n\n**1.**\tHow do you convince us enforcing a non-decreasing ordering of the learned quantile functions is helpful? I understand your arguments, but there is no evidence in the paper showing that doing so is helpful.\n\n**Response**: Thank you for your comment. We add more detailed comparison between NDQFN and IQN in the experiment part. Figure 3 in Section 6.1 and Figure 2 in the supplement show that the non-decreasing constraint helps to achieve better performance in most cases among 29 Atari games. As Figure 4 in Section 6.2 shows, NDQFN works better with the proposed DPE exploration than IQN since NDQFN obtains a more reasonable quantile estimate by adding the non-decreasing constraint which helps to increase the exploration efficiency. The complete results for all the 55 games will be included if the paper is accepted. \n\n**2.**\tComparison with DLTV is missing. The paper argues that DLTV is not applicable to continuous quantiles. However, it would be to include this comparison especially they have results on Atari games as well.\nThe empirical results are not very strong, with 13 and 14 ties and losses with/to IQN. It appears the advantage of DLTV over QRDQN is larger than your advantage over IQN.\n\n**Response**: Thank you very much for this good suggestion. We add the comparison between DLTV and DPE with NDQFN being the baseline. Since DLTV is originally designed for DRL algorithms based on fixed quantile locations such as QR-DQN, we modify the function form of the left truncated variance to work for quantile value-based methods such as IQN where the quantile fractions being sampled at each training iteration are different:\n$$\\sigma_+^2 = \\int_{\\frac{1}{2}}^1 [F_Z^{-1}(\\tau) - F_Z^{-1}(\\frac{1}{2})]^2 d\\tau$$\nThe Figure 5 shows that DLTV method perform well in early stage with limited training samples. However, DPE achieves a much better performance than DLTV especially in the three hard explored games.\nAlso, we want to emphasize that the Figure 10 in Mavrin et al., (2019) is evaluated on 40 million frames while ours based on 200 million frames. Since the training process on the early stage is unstable, we think it is more reasonable to draw conclusion based on 200 million training frames, like Figure 6 in our paper.\n\n**3.**\tThe technical quality and presentation of the paper can still be much improved. Abstract: two important problems still remain unsolved. the other is how to design an efficient exploration strategy to fully utilize the distribution information --> Later you showed this is false argument by introducing DLTV (Mavrin et. al. 19)\nWe describe the implementation details of the two architectures with what are they? you have two \"architectures\"?\n(b)(c) a simple incremental structure proposed in(3): this sentence is confusing.\nWhat is the circle dot operator? (.)\n\n**Response**: Thank you for your comment. We have revised the paper carefully to address these questions. Please see Abstract, Line 7 on Page 2, modified Figure 1 on Page 3, Remark on Page 5 and Line 25 on Page 5 in the updated paper, which are marked by the red color for details. \n\n**4.** Eq 4 is just interpolation to ensure positive increments. Why do you need to show (3) since it's not good?\n\n**Response**: We thank you for pointing out this problem. The original writing of this part may be a little confusing. The incremental structure in (3) is actually an important component of the proposed NDQFN architecture, while it cannot completely solve the crossing issue by itself (shown by the modified Figure 1). We revise Section 4 to make it more interpretable now and add a Remark before Theorem 1 to explain why the piece-wise linear structure is necessary. \n\n**5.**\tTh 1: The definition of the \\Pi operator isn't clear.\n\n**Response**: Thank you for your comment. $\\Pi$ denotes the distributional Bellman operator defined in Section 2.\n\n**6.** Before Sec 3.4:\nIs Relu is a good choice? What is your thought on other functions? How to choose g in practice?\nThe prediction error would be high ->The prediction error would be higher\neq 11: This is similar to Mavrin's idea: using exploration bonus -- UCT style.\n\n**Response**: Thank you for your comment. As for the choice of function $g$, ReLU ensure the output range of $g$ to be $[0,\\infty)$. Some other function is also applicable, we may design some experiments to compare the performance in future. Some other minor issues are addressed in the revised paper. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "f_GA2IU9-K-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1504/Authors|ICLR.cc/2021/Conference/Paper1504/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858921, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Comment"}}}, {"id": "u_ue0wxoD4k", "original": null, "number": 3, "cdate": 1603900718455, "ddate": null, "tcdate": 1603900718455, "tmdate": 1605024426455, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review", "content": {"title": "review", "review": "Summary:\n\nThe paper provides a method to learn the quantile function in distributional reinforcement learning that guarantee its monotonicity. Moreover, an exploration strategy for distributional reinforcement learning is also presented.\n\nReasons for score:\n\nThe paper claims two contributions: a better way to model quantile functions and an exploration strategy for distributional reinforcement learning. The former is a trivial extension of previous work, and the latter is not properly explained. \n\nPros:\n\n1. The experiment results look good.\n\n2. The DPE exploration strategy seems to be effective in the examples.\n\n\nCons:\n\n1. The NDQFN is a trivial extension of (3) through piece-wise linear interpolation. The explanation of Figure 1 is misleading.\n\n2. There is another class of methods in distributional reinforcement learning which uses generative network to model the Z function, which doesn\u2019t suffer from lack of monotonicity as in quantile function learning (see e.g. Y. Yue, Z. Wang, M. Zhou ``Implicit Distributional Reinforcement Learning\u201d). These type of methods need to be properly discussed and also compared in the experiments.\n\n3. Theorem 1 could be problematic. The quantile projection defined in this paper doesn\u2019t seem to be contractive. \n\n4. Section 3.4 is not well presented. \n\nQuestions:\n\nPlease address and clarify the cons above.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117073, "tmdate": 1606915761288, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1504/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review"}}}, {"id": "LUrOqKbxe6w", "original": null, "number": 4, "cdate": 1604019575766, "ddate": null, "tcdate": 1604019575766, "tmdate": 1605024426390, "tddate": null, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "invitation": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review", "content": {"title": "Interesting alg that extends DLTV to IQN  ", "review": "This paper studies distributional RL and proposed two extensions. One is a method to enforce a non-decreasing ordering of quantile functions by a linear and non-negative increments. The other is extends the idea of DLTV which adds exploration bonus in action selection by using the random network distillation method, which in particular, using a measure of inconsistency between target network and predictor networks as a frequency measure of sampled states. \n\nHow do you convince us enforcing a non-decreasing ordering of the learned quantile functions is helpful? \nI understand your arguments, but there is no evidence in the paper showing that doing so is helpful. \n\nComparison with DLTV is missing. \nThe paper argues that DLTV is not applicable to continuous quantiles. However, it would be to include this comparison especially they have results on Atari games as well. \n\nThe empirical results are not very strong, with 13 and 14 ties and losses with/to IQN. It appears the advantage of DLTV over QRDQN is larger than your advantage over IQN. \n\nThe technical quality and presentation of the paper can still be much improved. \n\nAbstract: \ntwo important problems still remain unsolved. \nthe other is how to design\nan efficient exploration strategy to fully utilize the distribution information\n--> Later you showed this is false argument by introducing DLTV (Mavrin et. al. 19)\n\nWe describe the implementation details of the two architectures with\nwhat are they?\nyou have two \"architectures\"?\n\n (b)(c) a simple incremental structure proposed in(3):\nthis sentence is confusing. \n\nWhat is the circle dot operator? (.)\n\nEq 4 is just interpolation to ensure positive increments. \nWhy do you need to show (3) since it's not good?\n\nTh 1: \nThe definition of the \\Pi operator isn't clear. \n\nBefore Sec 3.4:\n\nIs Relu is a good choice? What is your thought on other functions? How to choose g in practice?\n\n\nThe prediction error would be high\n->The prediction error would be higher\n\neq 11:\nThis is similar to Mavrin's idea: using exploration bonus -- UCT style. \n \n\n\n\n\n\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1504/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1504/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning", "authorids": ["~Fan_Zhou7", "~Zhoufan_Zhu1", "~Qi_Kuang1", "~Liwen_Zhang3"], "authors": ["Fan Zhou", "Zhoufan Zhu", "Qi Kuang", "Liwen Zhang"], "keywords": ["Non-decreasing Quantile Function", "Distributional Reinforcement Learning", "Distributional Prediction Error", "Exploration"], "abstract": "Although distributional reinforcement learning (DRL) has been widely examined in the past few years, there are two open questions people are still trying to address. One is how to ensure the validity of the learned quantile function, the other is how to efficiently utilize the distribution information. This paper attempts to provide some new perspectives to encourage the future in-depth studies in these two fields. We first propose a non-decreasing quantile function network (NDQFN) to guarantee the monotonicity of the obtained quantile estimates and then design a general exploration framework called distributional prediction error (DPE) for DRL which utilizes the entire distribution of the quantile function. In this paper, we not only discuss the theoretical necessity of our method but also show the performance gain it achieves in practice by comparing with some competitors on Atari 2600 Games especially in some hard-explored games.", "one-sentence_summary": "This paper introduces a general framework to obtain non-decreasing quantile estimate for Distributional Reinforcement Learning and proposes an efficient exploration method for quantile value based DRL algorithms", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nondecreasing_quantile_function_network_with_efficient_exploration_for_distributional_reinforcement_learning", "supplementary_material": "/attachment/8eda0476785eb6ac8084e442097f6fb44e45289c.zip", "pdf": "/pdf/5dc36909b1e6b83f4581be371aa49b2253276154.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=0rTUkWb7ax", "_bibtex": "@misc{\nzhou2021nondecreasing,\ntitle={Non-decreasing Quantile Function Network with Efficient Exploration for Distributional Reinforcement Learning},\nauthor={Fan Zhou and Zhoufan Zhu and Qi Kuang and Liwen Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=f_GA2IU9-K-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "f_GA2IU9-K-", "replyto": "f_GA2IU9-K-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117073, "tmdate": 1606915761288, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1504/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1504/-/Official_Review"}}}], "count": 13}