{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730166922, "tcdate": 1509133757185, "number": 734, "cdate": 1518730166910, "id": "SySpa-Z0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SySpa-Z0Z", "original": "ry76p-ZRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082591, "tcdate": 1517249989817, "number": 675, "cdate": 1517249989803, "id": "H10TrkaBf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "All reviewers have acknowledged that the proposed regularization is novel and also results in some empirical improvements on the reported language modeling and image classification tasks. However there are serious concerns on writing and rigor (reviewers Anon1 and Anon3) of the paper. The authors have not uploaded any revision of the paper to address these concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642500575, "tcdate": 1511536680265, "number": 1, "cdate": 1511536680265, "id": "rJgVu2HgM", "invitation": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "signatures": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "L_2 regularization of activations with theoretical underprinnings", "rating": "7: Good paper, accept", "review": "The paper puts forward Activation Norm Penalty (\"ANR\", an L_2 type regularization on the activations), deriving it from the Information Bottleneck principle. As usual with Information Bottleneck style constructions, the loss takes on a variational form.\n\nThe experiments demonstrate small but consistent gains with ANR across a number of domains (Language modelling on small datasets, plus image classification) and baseline models.\n\nA couple of things that could be improved:\n\n- The abstract claims to ground the ANR in the variational dropout framework. When it is applied without dropout to image classification, shouldn't that be explained?\n\n- Maybe dropping the determinant term also deserves some justification.\n\n- Very recently, Activation Regularization by Merity (https://arxiv.org/abs/1708.01009) proposed a similar thing without theoretical justification. Maybe discuss it and the differences (if any) in the related work section?\n\n- The Information Bottleneck section doesn't feel like an integral part of the paper.\n\nMy two cents: this work has both theoretical justification (a rare thing these days) and reasonable experimental results.\n\nThere are a number of typos and oversights:\n\n- Abstract: \"variation dropout\"\n- Section 2:\n  - x is never used\n  - m in b = m + \\sigma\\epsilon is never defined (is it the x above?)\n- In Section 3.2, equation 11 subscript of x_i is missing\n- Section 6, Ungrammatical sentence: \"Even though L_2 ...\"\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642500471, "id": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper734/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer2", "ICLR.cc/2018/Conference/Paper734/AnonReviewer1", "ICLR.cc/2018/Conference/Paper734/AnonReviewer3"], "reply": {"forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642500471}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642500530, "tcdate": 1511623170977, "number": 2, "cdate": 1511623170977, "id": "Hks-5ZwlG", "invitation": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "signatures": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Important problem tackled, but paper lacks strictness", "rating": "4: Ok but not good enough - rejection", "review": "This paper tries to create a mapping between activation norm penalties and information bottleneck framework using variational dropout framework. While I find the path taken interesting, the paper itself is hard to follow, mostly due to constantly flipping notation (cons section below lists some of the issues) and other linguistic errors. In the current form, this work is somewhere between a theoretical paper and an empirical one, however for a theoretical one it lacks strictness, while for empirical one - novelty.\n\nFrom theoretical perspective:\nThe main claim in this paper seems to be (10), however it is not formalised in any form of theorem, and so -- lacks a lock of strictness. Even under the assumption that it is updated, and made more strict - what is a crucial problem is a claim, that after arriving at:\ntr[ K(X, X) ] - ln( det[ K(X, X) ] )\ndropping the log determinant is anyhow justified, to keep the reasoning/derivation of the whole method sound. Authors claim that quote \"As for the determinant of the covariance matrix of Gaussian Process, we cannot easily evaluate or\nderive its gradient, so we do not include it in our computation.\" Which is not a justification for treating the derivation as a proper connection between penalising activities norm and information bottleneck idea. Terms like this will emerge in many other models, where one assumes diagonal covariance Gaussians; in fact the easiest model to justify this penalty is just to say one introduces diagonal Gaussian prior over activations, and that's it. Well justified penalty, easy to connect to many generalisation bound claims. However in the current approach the connection is simply not proven in the paper. \n\nFrom practical perspective:\nActivation norm penalties are well known objects, used for many years (L1 activation penalty for at least 6 years now, see \"Deep Sparse Recti\ufb01er Neural Networks\"; various activation penalties, including L2, changes in L2, etc. in Krueger PhD dissertation). Consequently for a strong empirical paper I would expect much more baselines, including these proposed by Krueger et al.\n\nPros:\n- empirical improvements shown on two different classes of problems.\n- interesting path through variational dropout is taken to show some equivalences\n\nCons:\n- there is no proper proof of claimed connection between IB and ANP, as it would require a determinant of K(X, X) to be 1.\n- work is not strict enough for a theoretical paper, and does not include enough comparison for empirical one\n- paper is full of typing/formatting/math errors/not well explained objects, which make it hard to read, to name a few:\n * fonts of objects used in equations change through the text - there is a \\textbf{W} and normal W, \\textbf{I} and normal I, similarly with X, Ts etc. without any explanation. I am assuming font are assigned randomly and they represent the same object.\n * dydp in (5) is in the wrong integral\n* \\sigma switches meaning between non-linearity and variance\n* what does it mean to define a normal distribution with 0 variance (like in (1)). Did authors mean  an actual \"degenerate Gaussian\", which does not have a PDF? But then p(y|t) is used in (5), and under such definition it does not exist, only CDF does. \n\n* \\Sigma_1 in 3.2 is undefined, was r(t) supposed to be following N(0, \\Sigma_1)  instead of written N(0, I)?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642500471, "id": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper734/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer2", "ICLR.cc/2018/Conference/Paper734/AnonReviewer1", "ICLR.cc/2018/Conference/Paper734/AnonReviewer3"], "reply": {"forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642500471}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642500485, "tcdate": 1511997204399, "number": 3, "cdate": 1511997204399, "id": "rJhMyTnlf", "invitation": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "signatures": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "weak paper, needs major revision", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes an L2 norm regularization of the output of penultimate layer of the network.  This regularizer is derived based on variational approximation to the information bottleneck objective. \n\nI\u2019m giving this paper a low rating for the following main reasons:\n\n1. The activation norm penalty is derived using an approximation of the information bottleneck Lagrangian term.  However, the approximation in terms of a KL divergence itself contains two terms (Equation 10) and the authors ignore one of those (log-determinant) since it is intractable.  The regularizer is based only on the other term.  Dropping the log-determinant term, thus quality of the resulting regularizer, is not justified at all.  It is just stated that we can not easily evaluate the log-determinant term or its gradient hence it is being dropped.\n\n2. The paper is not very well written, contains errors, undefined symbols and loose sentences, which make it very hard to follow.   For example:\ni) Eq. 1: 0 \\cdot I_N \u2026 what is meant by this operation is not stated, also c_n not defined\nii) \u201cThe prior of weight vector \u2026 with probability p\u201d \u2026 not sure what it means to have a Gaussian mixture with probability p.\niii) \u201cq\u201d, \u201cm\u201d not defined in Eq. 2\niv) Eq. 5 seems broken, the last integral has two dydt terms, also the inequalities in that equation seem incorrect.\n\nAuthors show a good gain on two language modeling tasks, CIFAR-10, and CIFAR-100.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642500471, "id": "ICLR.cc/2018/Conference/-/Paper734/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper734/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper734/AnonReviewer2", "ICLR.cc/2018/Conference/Paper734/AnonReviewer1", "ICLR.cc/2018/Conference/Paper734/AnonReviewer3"], "reply": {"forum": "SySpa-Z0Z", "replyto": "SySpa-Z0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642500471}}}, {"tddate": null, "ddate": null, "tmdate": 1515179039235, "tcdate": 1515179039235, "number": 3, "cdate": 1515179039235, "id": "r1w72HamG", "invitation": "ICLR.cc/2018/Conference/-/Paper734/Official_Comment", "forum": "SySpa-Z0Z", "replyto": "rJgVu2HgM", "signatures": ["ICLR.cc/2018/Conference/Paper734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper734/Authors"], "content": {"title": "Re", "comment": "Hi,\n\nThank you for pointing out the connection to Stephen Merity's paper (we have not read that paper). \n\nApplication of ANP without dropout is a concern we have also noticed. We chose to share that set of experiments because we think it is important to share with people that empirically we found that ANP works on all settings of the neural network. The theory framework we have chosen (variational dropout) cannot accommodate/explain this setting, but we hope other theoretical frameworks can offer insight.\n\nThe editing suggestions are very insightful, and we plan to fully investigate the effect of the log-determinant term (both computationally and theoretically) and offer a penalty that is closer to the true form of IB :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728624, "id": "ICLR.cc/2018/Conference/-/Paper734/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SySpa-Z0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper734/Authors|ICLR.cc/2018/Conference/Paper734/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper734/Authors|ICLR.cc/2018/Conference/Paper734/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper734/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper734/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper734/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper734/Reviewers", "ICLR.cc/2018/Conference/Paper734/Authors", "ICLR.cc/2018/Conference/Paper734/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728624}}}, {"tddate": null, "ddate": null, "tmdate": 1515178355659, "tcdate": 1515178280187, "number": 2, "cdate": 1515178280187, "id": "rkxNtHpXG", "invitation": "ICLR.cc/2018/Conference/-/Paper734/Official_Comment", "forum": "SySpa-Z0Z", "replyto": "Hks-5ZwlG", "signatures": ["ICLR.cc/2018/Conference/Paper734/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper734/Authors"], "content": {"title": "Re", "comment": "Hi,\n\nThank you for your detailed response and a thorough read of our paper. Thank you for recognizing our effort to tackle this problem. As you rightly pointed out, the lack of treatment for the second term (log-determinant) is unsatisfying. We cannot show whether ANP is an upper/lower bound of IB objective nor can we prove that ANP \\prop IB. \n\nThere are two approaches to integrate IB objective to neural network: \n1). Weaken the architecture: Alemi, et al. weakened to a diagonal Gaussian distribution. Similarly, we could choose to output a triangular matrix as output of RNN. We chose not to pursue this route because we are aiming to get SOTA performance on all our tasks. Weakening the architecture capacity (or altering SOTA architecture) will not give us comparable performance. In fact, we did get improvement on all SOTA architectures (in a similar setting).\n\n2). Weaken the objective: ignore the log-determinant term, resulting in a weaker theoretical claim, but it is simpler to train and optimize and has a wider applicability for all architectures. We chose this path instead.\n\nWe do plan to tackle the log determinant estimation problem fully, and investigate the effect of it (both in terms of performance improvement and runtime increase). We appreciate your comment, editing suggestions. We do not think having more empirical results is entirely necessary as language modeling and image classification are from two important domains and already heavily optimized by the community."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "From Information Bottleneck To Activation Norm Penalty", "abstract": "Many regularization methods have been proposed to prevent overfitting in neural networks. Recently, a regularization method has been proposed to optimize the variational lower bound of the Information Bottleneck Lagrangian. However, this method cannot be generalized to regular neural network architectures. We present the activation norm penalty that is derived from the information bottleneck principle and is theoretically grounded in a variation dropout framework. Unlike in previous literature, it can be applied to any general neural network. We demonstrate that this penalty can give consistent improvements to different state of the art architectures both in language modeling and image classification. We present analyses on the properties of this penalty and compare it to other methods that also reduce mutual information.", "pdf": "/pdf/73504e58776cf2a6aceb13d2e266c1aecb69dcef.pdf", "TL;DR": "We derive a norm penalty on the output of the neural network from the information bottleneck perspective", "paperhash": "nie|from_information_bottleneck_to_activation_norm_penalty", "_bibtex": "@misc{\nnie2018from,\ntitle={From Information Bottleneck To Activation Norm Penalty},\nauthor={Allen Nie and Mihir Mongia and James Zou},\nyear={2018},\nurl={https://openreview.net/forum?id=SySpa-Z0Z},\n}", "keywords": ["Deep Learning", "Natural Language Processing"], "authors": ["Allen Nie", "Mihir Mongia", "James Zou"], "authorids": ["anie@stanford.edu", "mihir.mongia@mssm.edu", "jamesz@stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728624, "id": "ICLR.cc/2018/Conference/-/Paper734/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SySpa-Z0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper734/Authors|ICLR.cc/2018/Conference/Paper734/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper734/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper734/Authors|ICLR.cc/2018/Conference/Paper734/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper734/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper734/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper734/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper734/Reviewers", "ICLR.cc/2018/Conference/Paper734/Authors", "ICLR.cc/2018/Conference/Paper734/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728624}}}], "count": 7}