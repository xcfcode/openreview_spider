{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124474183, "tcdate": 1518448438215, "number": 134, "cdate": 1518448438215, "id": "BJ04JEJvG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJ04JEJvG", "signatures": ["~Stanislaw_Kamil_Jastrzebski1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "SGD Smooths The Sharpest Directions", "abstract": "Stochastic gradient descent (SGD) is able to find regions that generalize well, even in drastically over-parametrized models such as deep neural networks. We observe that noise in SGD controls the spectral norm and conditioning of the Hessian throughout the training. We hypothesize the cause of this phenomenon is due to the dynamics of neurons saturating their non-linearity along the largest curvature directions, thus leading to improved conditioning.\n", "paperhash": "jastrzbski|sgd_smooths_the_sharpest_directions", "keywords": ["SGD", "sharpness", "regularization"], "_bibtex": "@misc{\n  jastrz\u0119bski2018sgd,\n  title={SGD Smooths The Sharpest Directions},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zac Kenton and Nicolas Ballas and Asja Fischer and Amos Storkey and Yoshua Bengio},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ04JEJvG}\n}", "authorids": ["staszek.jastrzebski@gmail.com", "zakenton@gmail.com", "ballas.n@gmail.com", "asja.fischer@gmail.com", "a.storkey@ed.ac.uk", "yoshua.umontreal@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zac Kenton", "Nicolas Ballas", "Asja Fischer", "Amos Storkey", "Yoshua Bengio"], "TL;DR": "Noise in SGD leads to actually smoothing out the loss surface by controlling spectral norm of the Hessian", "pdf": "/pdf/f0ce014d18e2d3393b6771cb55ded1d0bcf9e3fe.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582976102, "tcdate": 1519931532728, "number": 1, "cdate": 1519931532728, "id": "SyS5xRruG", "invitation": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer1"], "content": {"title": "SGD noises help training", "rating": "6: Marginally above acceptance threshold", "review": "This paper studies how the magnitude of SGD noises (parameterized via the learning rate) affects the spectrum and condition number of the Hessian, as well as the rate of dead neurons. The experiments are interesting, though more analysis might be better. Also, the reviewer would like to see the validation error with respect to the SGD noises on the classification tasks, too. In particular, it would be good to also show Figure 1(b) for the classification tasks. Besides, some of the experiments could also be refined for more controlled comparison. For example, comparing different learning rates with the same number of epochs might not be perfectly fair as smaller learning rate learners move less far than larger learning rate learners in the parameter space.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Smooths The Sharpest Directions", "abstract": "Stochastic gradient descent (SGD) is able to find regions that generalize well, even in drastically over-parametrized models such as deep neural networks. We observe that noise in SGD controls the spectral norm and conditioning of the Hessian throughout the training. We hypothesize the cause of this phenomenon is due to the dynamics of neurons saturating their non-linearity along the largest curvature directions, thus leading to improved conditioning.\n", "paperhash": "jastrzbski|sgd_smooths_the_sharpest_directions", "keywords": ["SGD", "sharpness", "regularization"], "_bibtex": "@misc{\n  jastrz\u0119bski2018sgd,\n  title={SGD Smooths The Sharpest Directions},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zac Kenton and Nicolas Ballas and Asja Fischer and Amos Storkey and Yoshua Bengio},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ04JEJvG}\n}", "authorids": ["staszek.jastrzebski@gmail.com", "zakenton@gmail.com", "ballas.n@gmail.com", "asja.fischer@gmail.com", "a.storkey@ed.ac.uk", "yoshua.umontreal@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zac Kenton", "Nicolas Ballas", "Asja Fischer", "Amos Storkey", "Yoshua Bengio"], "TL;DR": "Noise in SGD leads to actually smoothing out the loss surface by controlling spectral norm of the Hessian", "pdf": "/pdf/f0ce014d18e2d3393b6771cb55ded1d0bcf9e3fe.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582975907, "id": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper134/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer2"], "reply": {"forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582975907}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582924786, "tcdate": 1520362431468, "number": 2, "cdate": 1520362431468, "id": "r1Dpmv2uf", "invitation": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer3"], "content": {"title": "Hard to read and feels like sections are missing", "rating": "3: Clear rejection", "review": "This is an empirical paper that tries to prove that (i guess) SGD serves as a some sort of the regularizer  due to the level of noise. I feel it would have been interesting, but it is so poorly organized and presented that the whole story is lost.\n\nOverall the paper feels rushed and is missing sections that could facilitate the reading. Introduction is rushed. Furthermore, the main section that would explain what the authors were really trying to prove is missing - the paper jumps directly to the experiments, leaving the reader trying to guess what is that they are testing for and how they go about it.\nWhat are you trying to show? How are you planning on showing it and why it is the right way of doing it?\nA couple of lines like 'We look at the hessian condition number to see how susceptible the network to noise\" would have went a long way to facilitate the reading\n\nThe first set of experiments is dealing with learning rate. But is it the only way to regulate the noise? Will increasing the batch size reduce the noise? if it is the case, then you would expect that generalization will suffer when larger batches are used? Why not to show those experiments?\n\nOn top of that, for an empirical paper, showing it just on 3 datasets is not enough. How were the hyperparameters chosen. Were they tuned? How? Can it be that the observations hold only for those values of hyperparams\n\nMinor: The abstract is stated as the fact, instead of We show .. etc", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Smooths The Sharpest Directions", "abstract": "Stochastic gradient descent (SGD) is able to find regions that generalize well, even in drastically over-parametrized models such as deep neural networks. We observe that noise in SGD controls the spectral norm and conditioning of the Hessian throughout the training. We hypothesize the cause of this phenomenon is due to the dynamics of neurons saturating their non-linearity along the largest curvature directions, thus leading to improved conditioning.\n", "paperhash": "jastrzbski|sgd_smooths_the_sharpest_directions", "keywords": ["SGD", "sharpness", "regularization"], "_bibtex": "@misc{\n  jastrz\u0119bski2018sgd,\n  title={SGD Smooths The Sharpest Directions},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zac Kenton and Nicolas Ballas and Asja Fischer and Amos Storkey and Yoshua Bengio},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ04JEJvG}\n}", "authorids": ["staszek.jastrzebski@gmail.com", "zakenton@gmail.com", "ballas.n@gmail.com", "asja.fischer@gmail.com", "a.storkey@ed.ac.uk", "yoshua.umontreal@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zac Kenton", "Nicolas Ballas", "Asja Fischer", "Amos Storkey", "Yoshua Bengio"], "TL;DR": "Noise in SGD leads to actually smoothing out the loss surface by controlling spectral norm of the Hessian", "pdf": "/pdf/f0ce014d18e2d3393b6771cb55ded1d0bcf9e3fe.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582975907, "id": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper134/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer2"], "reply": {"forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582975907}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582627017, "tcdate": 1520825725419, "number": 3, "cdate": 1520825725419, "id": "H1SFSOmFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "signatures": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer2"], "content": {"title": "A work in progress", "rating": "5: Marginally below acceptance threshold", "review": "This paper empirically shows that learning rate and batch size in SGD influence the spectral norm of the Hessian of the weights. Although I am not aware of these specific experiments having been done in other works, the idea that higher learning rates avoid sharp minima is widely known (i.e. gradient descent's optimal learning rate is equal to the inverse of the second derivative).  \n\nThere are some issues with the paper:\n- I don't think that reporting the condition number of the Hessian in Figure 1a is meaningful, as this is sensitive to the magnitude of the smallest eigenvalue which can be unreliable due to rounding errors. It's hard to draw conclusions based on a difference between a condition number of 1e9 and 5e9 - this could be due to a difference between smallest eigenvalues of magnitude 1e-5 and 2e-6. Is the difference between those two tiny eigenvalues really meaningful? \n- Figure 2 is confusing - the eigenvalues are colored red to blue, but the blue line also indicates the training loss. It would help to use a different color altogether for the training loss. \n- Figure 7 is hard to interpret and doesn't have an explanation, please discuss a bit more what these plots are supposed to show. \n- I'm not convinced by Section 2.1 and Figure 5. First, the title says \"High Noise reduces the spectral norm\", but the plots are with respect to learning rates, not noise. Second, the decrease in spectral norm with higher learning rates could simply be explained by gradient descent not being able to settle in sharp valleys when the learning rate it too high, and preferring wide basins (see for example: http://srdas.github.io/DLBook/GradientDescentTechniques.html). \n- x labels in Figure 1c, d are too small \n\nThis paper is still definitely a work in progress, it could still make an acceptable workshop contribution. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Smooths The Sharpest Directions", "abstract": "Stochastic gradient descent (SGD) is able to find regions that generalize well, even in drastically over-parametrized models such as deep neural networks. We observe that noise in SGD controls the spectral norm and conditioning of the Hessian throughout the training. We hypothesize the cause of this phenomenon is due to the dynamics of neurons saturating their non-linearity along the largest curvature directions, thus leading to improved conditioning.\n", "paperhash": "jastrzbski|sgd_smooths_the_sharpest_directions", "keywords": ["SGD", "sharpness", "regularization"], "_bibtex": "@misc{\n  jastrz\u0119bski2018sgd,\n  title={SGD Smooths The Sharpest Directions},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zac Kenton and Nicolas Ballas and Asja Fischer and Amos Storkey and Yoshua Bengio},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ04JEJvG}\n}", "authorids": ["staszek.jastrzebski@gmail.com", "zakenton@gmail.com", "ballas.n@gmail.com", "asja.fischer@gmail.com", "a.storkey@ed.ac.uk", "yoshua.umontreal@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zac Kenton", "Nicolas Ballas", "Asja Fischer", "Amos Storkey", "Yoshua Bengio"], "TL;DR": "Noise in SGD leads to actually smoothing out the loss surface by controlling spectral norm of the Hessian", "pdf": "/pdf/f0ce014d18e2d3393b6771cb55ded1d0bcf9e3fe.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582975907, "id": "ICLR.cc/2018/Workshop/-/Paper134/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper134/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper134/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper134/AnonReviewer2"], "reply": {"forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper134/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582975907}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573589473, "tcdate": 1521573589473, "number": 199, "cdate": 1521573589128, "id": "H16ACCAtM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJ04JEJvG", "replyto": "BJ04JEJvG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD Smooths The Sharpest Directions", "abstract": "Stochastic gradient descent (SGD) is able to find regions that generalize well, even in drastically over-parametrized models such as deep neural networks. We observe that noise in SGD controls the spectral norm and conditioning of the Hessian throughout the training. We hypothesize the cause of this phenomenon is due to the dynamics of neurons saturating their non-linearity along the largest curvature directions, thus leading to improved conditioning.\n", "paperhash": "jastrzbski|sgd_smooths_the_sharpest_directions", "keywords": ["SGD", "sharpness", "regularization"], "_bibtex": "@misc{\n  jastrz\u0119bski2018sgd,\n  title={SGD Smooths The Sharpest Directions},\n  author={Stanis\u0142aw Jastrz\u0119bski and Zac Kenton and Nicolas Ballas and Asja Fischer and Amos Storkey and Yoshua Bengio},\n  year={2018},\n  url={https://openreview.net/forum?id=BJ04JEJvG}\n}", "authorids": ["staszek.jastrzebski@gmail.com", "zakenton@gmail.com", "ballas.n@gmail.com", "asja.fischer@gmail.com", "a.storkey@ed.ac.uk", "yoshua.umontreal@gmail.com"], "authors": ["Stanis\u0142aw Jastrz\u0119bski", "Zac Kenton", "Nicolas Ballas", "Asja Fischer", "Amos Storkey", "Yoshua Bengio"], "TL;DR": "Noise in SGD leads to actually smoothing out the loss surface by controlling spectral norm of the Hessian", "pdf": "/pdf/f0ce014d18e2d3393b6771cb55ded1d0bcf9e3fe.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}