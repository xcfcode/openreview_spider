{"notes": [{"id": "HklsHyBKDr", "original": "HkesMQpdvH", "number": 1704, "cdate": 1569439554936, "ddate": null, "tcdate": 1569439554936, "tmdate": 1577168287884, "tddate": null, "forum": "HklsHyBKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aN2tKg-2-_", "original": null, "number": 1, "cdate": 1576798730344, "ddate": null, "tcdate": 1576798730344, "tmdate": 1576800906170, "tddate": null, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Decision", "content": {"decision": "Reject", "comment": "Nice start but unfortunately not ripe.  The issues remarked by the reviewers were only partly addressed, and an improved version of the paper should be submitted at a future venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711295, "tmdate": 1576800260470, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Decision"}}}, {"id": "SklLK06ijH", "original": null, "number": 3, "cdate": 1573801597803, "ddate": null, "tcdate": 1573801597803, "tmdate": 1573801597803, "tddate": null, "forum": "HklsHyBKDr", "replyto": "rkxNUnoftB", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment", "content": {"title": "Reply to Review #1", "comment": "Thank you for your review and thoughtful feedback!\n\n--It is not very clear to me how the authors trained stochastic RNNs deterministically during training.\n\n     We compared two different setups. In the first setup, we trained deterministic RNNs, and then added noise post-hoc at test-time, i.e. the model used to train and the model used to test are different. In the second setup, we trained stochastic RNNs with noise at training time, and added that same amount of noise at test-time, thus the train and test models are the same. These are two different ways to create a stochastic RNN at test-time. As one would expect, models trained with noise perform better when evaluated with noise, while models trained deterministically perform worse when noise is added. We included the first case primarily as a baseline, and as a means to estimate information in deterministically trained RNNs where it would otherwise be intractable.\n\n--Regarding: comparison to other methods\n\n      We trained RNNs and LSTMs with output dropout (in Appendix A.5) and with the CPC objective instead of MLE (in Appendix A.4), while we were not able to compare to Z-Forcing in the short rebuttal period. We find that RNNs trained with dropout extract less information than RNNs without dropout, but the frontier of models when we sweep dropout rate and additive noise does not change. In other words, we can find models that are equivalent to adding dropout just by increasing the amount of Gaussian noise on the output. We have added an additional figure and discussion to the appendix including models trained with dropout. \n\n      Similarly, we find that models trained with CPC perform similarly to models trained with MLE, and are not more optimal on the information plane. However, this may be due to the toy BHO dataset having Markovian dynamics, meaning that optimizing for one step ahead prediction with MLE is sufficient to maximize mutual information with the future of the sequence. Additional details and a figure can be found in the Appendix A.4 and Figure 10 in the paper.\n\n      We hope that these additional experiments and improvements to the clarity of the text have addressed all of your concerns.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklsHyBKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1704/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1704/Authors|ICLR.cc/2020/Conference/Paper1704/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152096, "tmdate": 1576860532212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment"}}}, {"id": "B1l--0aioB", "original": null, "number": 2, "cdate": 1573801465215, "ddate": null, "tcdate": 1573801465215, "tmdate": 1573801465215, "tddate": null, "forum": "HklsHyBKDr", "replyto": "Hyxmmk9kcS", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment", "content": {"title": "Reply to Review #2", "comment": "Thank you for your careful reading and feedback! We agree that stochastic training is a useful component of limiting information in RNNs, and performed additional experiments to address the connection with dropout as shown in Appendix A.5 and Figure 11.\n\n--The way stochastic training is introduced in this paper make it seem a bit contradictory with the fact that it actually helps generalization\n\n      While stochastic training may limit performance on the training set (i.e. reducing I(hidden state; labels)), this often acts to regularize the model in a way that is beneficial for generalization to the test set, as we show in our experiments on the sketch dataset where training data is limited. Previous work on VIB (Alemi et al., 2017) has also shown how stochastic bottlenecks can improve generalization in the classification setting. \n\n--Intuitions that arise from the manuscript may not be as useful as we would like.\n\n      As we emphasize throughout the manuscript, we find that RNNs trained without noise extract too much information about the past, and this can be harmful when training models on small datasets. Our simple and intuitive procedure of adding noise to hidden states to discard information presents one mechanism for limiting the capacity of hidden states, and our analysis in the information plane helps to reveal the tradeoffs between extracting information about the past and being able to predict the future of a sequence.\n\nReference: \n    Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, Kevin Murphy. Deep Variational Information Bottleneck. ICLR 2017. https://arxiv.org/abs/1612.00410\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklsHyBKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1704/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1704/Authors|ICLR.cc/2020/Conference/Paper1704/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152096, "tmdate": 1576860532212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment"}}}, {"id": "r1x0Ja6isB", "original": null, "number": 1, "cdate": 1573801189767, "ddate": null, "tcdate": 1573801189767, "tmdate": 1573801189767, "tddate": null, "forum": "HklsHyBKDr", "replyto": "r1l3FOGR9B", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment", "content": {"title": "Reply to Review #4", "comment": "Thank you for your careful reading and thoughtful feedback!\n\n--\u201c the suggested method for improvement through adding noise to the latents (during training) is too much of handwaving for such a fundamental problem\"\n\n      Adding noise to a bounded representation is a tractable and effective method for introducing a bottleneck into a hidden state with a bounded activation. Furthermore, for any level of additive noise, we can think of this process as introducing a hard constraint on the amount of information that can be stored in the hidden state, thus we\u2019re solving a constrained optimization problem instead of the typical Lagrangian formulation in many applications of Information Bottleneck. While there are many other approaches for introducing bottlenecks (e.g. Dropout, computing a variational upper bound on the information and pushing that down), we found that adding noise was a simple strategy that was effective and interpretable.\n\n--Shouldn't the results on the BHC be quite surprising in terms of LSTMs performance? \n\n      RNNs and LSTMs trained with deterministic hidden states perform well at next step prediction, but they do this by extracting far more information than is needed to solve the task. For large datasets this may not be an issue, but as we show in the QuickDraw experiments, when the data is limited, constraining the amount of information in the hidden state can be useful for improving generalization. \n\n--How are more future predictions generated.\n\n      To evaluate mutual information between the hidden state and future states of the world, we do not need to generate future samples from our model, we only need hidden state samples paired with the true future observations. Furthermore, for a Markov process we only need to look a single step into the future to evaluate information with the infinite future.\n\n--Recently some researchers started to question whether 'reconstruction' is a good idea in order to learn generative-like models, for example you cite van den Oord 2018. How would such models perform in your metric.\n\n      Thank you for this suggestion! We have performed additional experiments training RNNs and LSTMs with CPC. The model architecture is identical to our initial experiments, but instead of training with MLE we train with the CPC loss, which estimates the mutual information between the current timestep and K steps into the future. For our experiments on the BHO, we looked up to 30 steps into the future, and used a linear readout from the hidden state to a time-independent embedding of the inputs. We found that models trained with CPC had a similar frontier as those trained with MLE, and added a figure and additional details to Appendix A.4 and Figure 10 in the updated paper. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklsHyBKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1704/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1704/Authors|ICLR.cc/2020/Conference/Paper1704/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152096, "tmdate": 1576860532212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Authors", "ICLR.cc/2020/Conference/Paper1704/Reviewers", "ICLR.cc/2020/Conference/Paper1704/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Comment"}}}, {"id": "rkxNUnoftB", "original": null, "number": 1, "cdate": 1571105867716, "ddate": null, "tcdate": 1571105867716, "tmdate": 1572972434109, "tddate": null, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\n\nThe paper investigate how optimal recurrent neural networks (RNNs) are at storing past information such that it is useful for predicting the future. The authors estimated optimality in terms of mutual information between the past and the future. If the RNN was able to retain MI between the past and the future, it then has kept optimal information from the past for predicting the future. The experiments suggest that RNNs are not optimal in terms of prediction of the future. It also suggest that this is due to the maximum likelihood training objective.\n\n\nComments for the paper:\n\n1. Overall, the paper is a very interesting read and it explores and analyzes RNN under a different light. It answers a fundamental question about RNN training.\n\n2. There are a few things that would be nice to clarify. At the end of P3, the authors mentioned that the stochastic RNNs are trained either by a). deterministically during training and noise added during test or b) noise added during training and test. It is not very clear to me how the authors trained stochastic RNNs deterministically during training. It would be nice if this can be clarified. \n\n3. I am also curious how this compares to the training methods for example used in https://papers.nips.cc/paper/7248-z-forcing-training-stochastic-recurrent-networks.pdf. It seems that this would also help with retaining RNN optimality in terms of predicting the future. it would be interesting to include a comparison to this method for example.\n\nOverall an interesting paper. However, I think a few things could be improved and I would be willing to rise the score if the authors could addressed the above points.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670235394, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Reviewers"], "noninvitees": [], "tcdate": 1570237733494, "tmdate": 1575670235411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review"}}}, {"id": "Hyxmmk9kcS", "original": null, "number": 2, "cdate": 1571950362622, "ddate": null, "tcdate": 1571950362622, "tmdate": 1572972434075, "tddate": null, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This manuscript shows that good ability to compress past information in RNNs help them to predict the future, and that improving upon this ability leads to more useful RNNs. The manuscript first adapts modern mutual-information estimators to mini-batch settings in order to measure the information that an RNN has on the past. It then considers stochastic training, adding Gaussian noise to the hidden states during the training of the RNNs to limit past information. A significant section is dedicated to an empirical study that shows that classically-train MLE RNNs lead to internal representations with a suboptimal mutual-information to the past and the future. For LSTM and GRU architecture, stochastic training actually significantly helps. Experiments on applications such as synthetizing hand-drawn sketches suggest that stochastic training leads to more useful RNNs.\n\nThis work has interesting observations and makes a credible case. The stochastic training does seem useful. However, I would like to understand better how it connects to the set of publications discussing dropout in RNNs. It is already known that stochastic perturbations during training help. In addition, the way stochastic training is introduced in this paper make it seem a bit contradictory with the fact that it actually helps generalization. I have the feeling that the benefit that is not understood and that intuitions that arise from the manuscript may not be as useful as we would like.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670235394, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Reviewers"], "noninvitees": [], "tcdate": 1570237733494, "tmdate": 1575670235411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review"}}}, {"id": "r1l3FOGR9B", "original": null, "number": 3, "cdate": 1572903043887, "ddate": null, "tcdate": 1572903043887, "tmdate": 1572972434031, "tddate": null, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "invitation": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper certainly poses an interesting question: How well do RNNs compress the past while retaining relevant information about the future. In order to quantitatively answer this question, the authors suggest to look at (the optimal solutions of) the Information Bottleneck Lagrangian (IBL). The investigated RNNs need to solve the task of next-step prediction, which can be used to evaluate the IBL. In the paper, the (deterministic) hidden state h is transformed through simple additive Gaussian noise into a stochastic representation which then is utilized to compute the IBL. In general the IBL is not tractable and hence the paper uses approximate computations.\n\nI definitely like the underlying question of the paper. Yet, to me it seems not ready for publication. For one, the presented experimental results look interesting but the suggested method for improvement through adding noise to the latents (during training) is too much of handwaving for such a fundamental problem. Second, shouldn't the results on the BHC be quite surprising in terms of LSTMs performance? Why is that, usually LSTM (or GRU) deliver excellent performance in typical (supervised) sequential tasks. Third, the task itself seems not well described, it seems to be next-step prediction, but how are more future predictions generated -- these seem to be not considered in the equation, but probably should when talking about 'retaining relevant information for the future'? Fourth, recently some researchers started to question whether 'reconstruction' is a good idea in order to learn generative-like models, for example you cite van den Oord 2018. How would such models perform in your metric.\n\nA final remark with respect to your citation for eq. 4, I think you meant Barber, Agakov, \"The IM algorithm...\", 2003?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1704/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhedong@google.com", "doktay@princeton.edu", "pooleb@google.com", "alemi@google.com"], "title": "On Predictive Information Sub-optimality of RNNs", "authors": ["Zhe Dong", "Deniz Oktay", "Ben Poole", "Alexander A. Alemi"], "pdf": "/pdf/51b77773c596c8fa3b346fd33d6703d5fc0e2f9b.pdf", "abstract": "Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.", "keywords": [], "paperhash": "dong|on_predictive_information_suboptimality_of_rnns", "original_pdf": "/attachment/6f844f6cab36e20eaea6064adf100c8c8ce1f6b6.pdf", "_bibtex": "@misc{\ndong2020on,\ntitle={On Predictive Information Sub-optimality of {\\{}RNN{\\}}s},\nauthor={Zhe Dong and Deniz Oktay and Ben Poole and Alexander A. Alemi},\nyear={2020},\nurl={https://openreview.net/forum?id=HklsHyBKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklsHyBKDr", "replyto": "HklsHyBKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1704/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575670235394, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1704/Reviewers"], "noninvitees": [], "tcdate": 1570237733494, "tmdate": 1575670235411, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1704/-/Official_Review"}}}], "count": 8}