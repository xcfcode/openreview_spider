{"notes": [{"id": "fg2ZFmXFO3", "original": "zCrLhDxLgU", "number": 36, "cdate": 1582750163701, "ddate": null, "tcdate": 1582750163701, "tmdate": 1587925110558, "tddate": null, "forum": "fg2ZFmXFO3", "replyto": null, "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission", "content": {"title": "Neural Operator: Graph Kernel Network for Partial Differential Equations", "authors": ["Anima Anandkumar", "Kamyar Azizzadenesheli", "Kaushik Bhattacharya", "Nikola Kovachki", "Zongyi Li", "Burigede Liu", "Andrew Stuart"], "authorids": ["anima@caltech.edu", "kaazizzad@gmail.com", "bhatta@caltech.edu", "nkovachki@caltech.edu", "zongyili@caltech.edu", "bgl@caltech.edu", "astuart@caltech.edu"], "keywords": ["Graph Neural Networks", "Partial Differential Equations", "Kernel Methods", "Infinite Space Mappings"], "TL;DR": "Graph kernel networks use message passing to do kernel integration, so that it can learn an operator from the input functions to the solution functions, among different resolution and discretization.", "abstract": "The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm the purposed graph kernel network does have the desired properties and show competitive performance compared to the stat of the art solvers.", "pdf": "/pdf/2f065fe563f32f87f64d3a6e224322bc500dd4f4.pdf", "paperhash": "anandkumar|neural_operator_graph_kernel_network_for_partial_differential_equations", "_bibtex": "@inproceedings{\nanandkumar2020neural,\ntitle={Neural Operator: Graph Kernel Network for Partial Differential Equations},\nauthor={Anima Anandkumar and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Nikola Kovachki and Zongyi Li and Burigede Liu and Andrew Stuart},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=fg2ZFmXFO3}\n}"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "invitees": ["~"], "tcdate": 1582750147213, "tmdate": 1587924718420, "id": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2Rg9HACtRI", "original": null, "number": 1, "cdate": 1582774511442, "ddate": null, "tcdate": 1582774511442, "tmdate": 1582774511442, "tddate": null, "forum": "fg2ZFmXFO3", "replyto": "fg2ZFmXFO3", "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper36/-/Decision", "content": {"decision": "Accept (Poster)", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Operator: Graph Kernel Network for Partial Differential Equations", "authors": ["Anima Anandkumar", "Kamyar Azizzadenesheli", "Kaushik Bhattacharya", "Nikola Kovachki", "Zongyi Li", "Burigede Liu", "Andrew Stuart"], "authorids": ["anima@caltech.edu", "kaazizzad@gmail.com", "bhatta@caltech.edu", "nkovachki@caltech.edu", "zongyili@caltech.edu", "bgl@caltech.edu", "astuart@caltech.edu"], "keywords": ["Graph Neural Networks", "Partial Differential Equations", "Kernel Methods", "Infinite Space Mappings"], "TL;DR": "Graph kernel networks use message passing to do kernel integration, so that it can learn an operator from the input functions to the solution functions, among different resolution and discretization.", "abstract": "The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm the purposed graph kernel network does have the desired properties and show competitive performance compared to the stat of the art solvers.", "pdf": "/pdf/2f065fe563f32f87f64d3a6e224322bc500dd4f4.pdf", "paperhash": "anandkumar|neural_operator_graph_kernel_network_for_partial_differential_equations", "_bibtex": "@inproceedings{\nanandkumar2020neural,\ntitle={Neural Operator: Graph Kernel Network for Partial Differential Equations},\nauthor={Anima Anandkumar and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Nikola Kovachki and Zongyi Li and Burigede Liu and Andrew Stuart},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=fg2ZFmXFO3}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Paper Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject"], "description": "Decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}, "forum": "fg2ZFmXFO3", "replyto": "fg2ZFmXFO3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}}, "cdate": 1582156800000, "expdate": 1589155200000, "duedate": 1588291200000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "tcdate": 1582771073846, "tmdate": 1587925018622, "super": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Decision", "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "id": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper36/-/Decision"}}}], "count": 2}