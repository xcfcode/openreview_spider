{"notes": [{"id": "ryGgSsAcFQ", "original": "B1loMZhHt7", "number": 57, "cdate": 1538087736010, "ddate": null, "tcdate": 1538087736010, "tmdate": 1550927775675, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1eOeKDmeN", "original": null, "number": 1, "cdate": 1544939760296, "ddate": null, "tcdate": 1544939760296, "tmdate": 1545354491189, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Meta_Review", "content": {"metareview": "The paper shows limitations on the types of functions that can be represented by deep skinny networks for certain classes of activation functions, independently of the number of layers. With many other works discussing capabilities but not limitations, the paper contributes to a relatively underexplored topic. \n\nThe settings capture a large family of activation functions, but exclude others, such as polynomial activations, for which the considered type of obstructions would not apply. Also a concern is raised about it not being clear how this theoretical result can shed insight on the empirical study of neural networks. \n\nThe authors have responded to some of the comments of the reviewers, but not to all comments, in particular comments of reviewer 1, who's positive review is conditional on the authors addressing some points. \n\nThe reviewers are all confident and are moderately positive, positive, or very positive about this paper. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Analysis of obstructions in skinny networks"}, "signatures": ["ICLR.cc/2019/Conference/Paper57/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper57/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353352659, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper57/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper57/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper57/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353352659}}}, {"id": "Syl8ctvQeE", "original": null, "number": 3, "cdate": 1544939917828, "ddate": null, "tcdate": 1544939917828, "tmdate": 1544939917828, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "content": {"title": "narrow DBNs ", "comment": "The referenced paper on narrow belief networks uses layers of width n+1 and poses size n as as an open problem. A later work by Le Roux and Bengio obtained width n. "}, "signatures": ["ICLR.cc/2019/Conference/Paper57/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper57/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621989, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGgSsAcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper57/Authors|ICLR.cc/2019/Conference/Paper57/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621989}}}, {"id": "r1x5UuwvJE", "original": null, "number": 2, "cdate": 1544153170214, "ddate": null, "tcdate": 1544153170214, "tmdate": 1544153170214, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "SJgK8rBp3m", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "content": {"title": "On the narrowness of the result", "comment": "It's true that there are many activation functions that the result doesn't apply to, and in fact isn't true for. The selling point isn't the generality of the result, but rather the novelty of the approach and the potential it suggests for future work that would be more general."}, "signatures": ["ICLR.cc/2019/Conference/Paper57/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621989, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGgSsAcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper57/Authors|ICLR.cc/2019/Conference/Paper57/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621989}}}, {"id": "r1e3Hvvv1N", "original": null, "number": 1, "cdate": 1544152900009, "ddate": null, "tcdate": 1544152900009, "tmdate": 1544152900009, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "Bkx5oBB1oQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "content": {"title": "Response to \"Two comments\"", "comment": "The major difference between this paper and [1] is that while [1] describes the rate at which the complexity of model functions can increase as the network architecture increases, the present paper describes a limitation that is independent of network complexity within a family of neural networks.\n\nThese results most likely don't hold for networks with residual connections, since from the topological perspective the residual connections effectively increase the dimension of the maps between layers."}, "signatures": ["ICLR.cc/2019/Conference/Paper57/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621621989, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryGgSsAcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper57/Authors|ICLR.cc/2019/Conference/Paper57/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621621989}}}, {"id": "SJgK8rBp3m", "original": null, "number": 3, "cdate": 1541391696920, "ddate": null, "tcdate": 1541391696920, "tmdate": 1541534323034, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "content": {"title": "Review of \"Deep, Skinny Neural Networks are not Universal Approximators\"", "review": "This paper proves a theoretical limitation of narrow-and-deep neural networks. It shows that, for any function that can be approximated by such networks, its level set (or decision boundary for binary classification) must be unbounded. The conclusion means that if some problem's decision boundary is a closed set, then it cannot be represented by such narrow networks.\n\nThe intuition is relatively simple. Under the assumptions of the paper, the neural network can always be approximated by a one-to-one mapping followed by a linear projection. The image of the one-to-one mapping is homeomorphic to R^n, so that it must be an open topological ball. The intersection of this open ball with a linear hyperplane must include the boundary of the ball, thus it extends to infinity in the original input space. The critical assumptions here, which guarantees the one-to-one property of the network, are: 1) the network is narrow, and 2) the activation function can be approximated by a one-to-one function.\n\nThe authors claim that 2) captures a large family of activation functions. However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas. As a concrete example, the simple function f(x1,x2) = x_1^2 + x_2^2 has bounded level sets, but it can be represented by a narrow 2-layer neural network with the quadratic activation.\n\nOverall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases. It is also not clear how this theoretical result can shed insight on the empirical study of neural networks. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper57/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "cdate": 1542234547645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639484, "tmdate": 1552335639484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lx-OTY3Q", "original": null, "number": 2, "cdate": 1541163000233, "ddate": null, "tcdate": 1541163000233, "tmdate": 1541534322829, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "content": {"title": "A solid contribution to a relatively underexplored area of machine learning", "review": "This is a very nice paper contributing to what I consider a relatively underexplored but potentially very promising research direction. The title of the paper in my opinion undersells the result which is not only that \"deep skinny neural networks\" are not universal approximators, but that the class of functions which cannot be approximated includes a set of practically relevant classifiers as illustrated by the figure on page 8. The presentation is extremely clear with helpful illustrations and toy but insightful experiments.\n\nMy current rating of this paper is based on assuming that the following concerns will be addressed. I will adjust the score accordingly after authors' reply.\n\n\n\nMain:\n\n- A very similar result can be found in Theorem 7 of Beise et al.'s \"On decision regions of narrow deep neural networks\" from July 2018 ( https://arxiv.org/abs/1807.01194 )\n\tSome differences:\n\n\t\t- The other paper considers connected whereas this paper considers path-connected components (the former is more general).\n\t\t- The other paper only considers multi-label classification, this paper is relevant to all classification and regression problems (the latter is more general).\n\t\t- The other paper requires that the activation function is \"strictly monotonic or ReLU\" whereas this paper allows \"uniformly approximable with one-to-one functions\" activations (the latter is more general).\n\n\tThe result in this paper seems slightly more general but largely similar. Can you please comment on the differences/relation to the other paper?\n\n\n- Proof of Lemma 4:  \"Thus the composition \\hat{f} is also one-to-one, and therefore a homeomorphism from R^n onto its image I_{\\hat{f}}\". Is it not necessary that \\hat{f} has a continuous inverse in order to be a homeomorphism? I do not immediately see whether the class of activation functions considered in this paper implies that this condition is satisfied. Please clarify. \n\n\n\nMinor:\n\n- Proof of Lemma 5: It seems g is assumed to be continuous at several places (e.g. \"... level sets of are closed as subsets of R^n ...\" seems to assume that pre-image of a closed set under g is closed, or later \"This implies g(F) is a compact subset of R ...\"). Perhaps you are assuming that M is a set of continuous functions and using the fact that uniform limit of continuous functions is continuous? Please clarify.\n\n- On p.4: \"This is fairly immediate from the assumptions on \\varphi and the fact that singular transition matrices can be approximated by non-singular ones.\" Is the second part of the sentence using the assumption that the input space is compact? Please clarify.\n\n- Second line in Section 5: i < k should probably be i < \\kappa.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper57/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "cdate": 1542234547645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639484, "tmdate": 1552335639484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklY-9xwnX", "original": null, "number": 1, "cdate": 1540979201144, "ddate": null, "tcdate": 1540979201144, "tmdate": 1541534322625, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "content": {"title": "An interesting proof on approximation capabilities of deep skinny neural networks", "review": "This paper shows that deep \"narrow\" neural networks (i.e. all hidden layers have maximum width at most the input dimension) with a variety of activation functions, including ReLU and sigmoid, can only learn functions with unbounded level set components, and thus cannot be a universal approximator. This complements previous work, such as Nguyen et. al 2018 which study connectivity of decision regions and Lu et. al 2017 on ReLU networks in different ways.\n\nOverall the paper is clearly written and technically sound. The result itself may not be super novel as noted in the related work but it's still a strict improvement over previous results which is often constrained to ReLU activation function. Moreover, the proofs of this paper are really nice and elegant. Compared to other work on approximation capability of neural networks, it can tell us in a more intuitive way and explicitly which class of functions/problems cannot be learned by neural networks if none of their layers have more neurons than the input dimension, which might be helpful in practice. Given the fact that there are not many previous work that take a similar approach in this direction, I'm happy to vote for accepting this paper.  \n\nMinor comments:\nThe proof of Lemma 3 should be given for completeness. I guess this can be done more easily by setting delta=epsilon, A_0=A and A_{i+1}=epsilon-neighborhood of f_i(A_i)?\npage7: the square brackets in \"...g(x'')=[y-epsilon,y+epsilon]...\" should be open brackets.\npage7:\"By Lemma 4, every function in N_n has bounded level components...\" -> \"..unbounded...\"", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper57/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Official_Review", "cdate": 1542234547645, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper57/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335639484, "tmdate": 1552335639484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper57/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkx5oBB1oQ", "original": null, "number": 2, "cdate": 1539425697966, "ddate": null, "tcdate": 1539425697966, "tmdate": 1539425697966, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Public_Comment", "content": {"comment": "1) Can the Authors comment on how the contribution of this work differs from [1]?\n\n2) These conclusions do not seem to hold for skinny deep NN with residual connections [2]. How could Theorem 1 be modified to include this evidence?\n\n[1] http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks\n\n[2] https://github.com/novatig/playground", "title": "Two comments"}, "signatures": ["~Guido_Novati1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"], "writers": ["~Guido_Novati1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311928844, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryGgSsAcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311928844}}}, {"id": "SJxvceLE9Q", "original": null, "number": 1, "cdate": 1538707599219, "ddate": null, "tcdate": 1538707599219, "tmdate": 1538707599219, "tddate": null, "forum": "ryGgSsAcFQ", "replyto": "ryGgSsAcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper57/Public_Comment", "content": {"comment": "I love this paper. Please keep up the good work.", "title": "Amazing paper"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper57/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep, Skinny Neural Networks are not Universal Approximators", "abstract": "In order to choose a neural network architecture that will be effective for a particular modeling problem, one must understand the limitations imposed by each of the potential options. These limitations are typically described in terms of information theoretic bounds, or by comparing the relative complexity needed to approximate example functions between different architectures. In this paper, we examine the topological constraints that the architecture of a neural network imposes on the level sets of all the functions that it is able to approximate. This approach is novel for both the nature of the limitations and the fact that they are independent of network depth for a broad family of activation functions.", "paperhash": "johnson|deep_skinny_neural_networks_are_not_universal_approximators", "TL;DR": "This paper proves that skinny neural networks cannot approximate certain functions, no matter how deep they are.", "authorids": ["jejo.math@gmail.com"], "authors": ["Jesse Johnson"], "keywords": ["neural network", "universality", "expressability"], "pdf": "/pdf/3dfd6280ba045de3af9e85183f1db0ea5fe7840a.pdf", "_bibtex": "@inproceedings{\njohnson2018deep,\ntitle={Deep, Skinny Neural Networks are not Universal Approximators},\nauthor={Jesse Johnson},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryGgSsAcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper57/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311928844, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryGgSsAcFQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper57/Authors", "ICLR.cc/2019/Conference/Paper57/Reviewers", "ICLR.cc/2019/Conference/Paper57/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311928844}}}], "count": 10}