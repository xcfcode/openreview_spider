{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730182144, "tcdate": 1509095297117, "number": 311, "cdate": 1518730182136, "id": "SyYYPdg0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SyYYPdg0-", "original": "HkdYvOg0-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082464, "tcdate": 1517249995831, "number": 680, "cdate": 1517249995807, "id": "HkERBk6rf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "All reviewers acknowledge that the idea of the paper is interesting but have expressed serious concerns on empirical evaluations. The paper is not suitable for publication in its current form."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1516257539758, "tcdate": 1511830381181, "number": 3, "cdate": 1511830381181, "id": "HJSdXVqxG", "invitation": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "signatures": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Unclear why it works the way it is advertised", "rating": "4: Ok but not good enough - rejection", "review": "This paper creates a layered representation in order to better learn segmentation from unlabeled images. It is well motivated, as Fig. 1 clearly shows the idea that if the segmentation was removed properly, the result would still be a natural image. However, the method itself as described in the paper leaves many questions about whether they can achieve the proposed goal.\n\nI cannot see from the formulation why would this model work as it is advertised. The formulation (3-4) looks like a standard GAN, with some twist about measuring the GAN loss in the z space (this has been used in e.g. PPGN and CVAE-GAN). I don't see any term that would guarantee:\n\n1) Each layer is a natural image. This was advertised in the paper, but the loss function is only on the final product G_K. The way it is written in the paper, the result of each layer does not need to go through a discriminator. Nothing seems to have been done to ensure that each layer outputs a natural image.\n\n2) None of the layers is degenerate. There does not seem to be any constraint either regularizing the content in each layer, or preventing any layer to be non-degenerate.\n\n3) The mask being contiguous. I don't see any term ensuring the mask being contiguous, I imagine normally without such terms doing such kinds of optimization would lead to a lot of fragmented small areas being considered as the mask.\n\nThe claim that this paper is for unsupervised semantic segmentation is overblown. A major problem is that when conducting experiments, all the images seem to be taken from a single category, this implicitly uses the label information of the category. In that regard, this cannot be viewed as an unsupervised algorithm.\n\nEven with that, the results definitely looked too good to be true. I have a really difficult time believing why such a standard GAN optimization would not generate any of the aforementioned artifacts and would perform exactly as the authors advertised. Even if it does work as advertised, the utilization of implicit labels would make it subject to comparisons with a lot of weakly-supervised learning papers with far better results than shown in this paper. Hence I am pretty sure that this is not up to the standards of ICLR.\n\nI have read the rebuttal and still not convinced. I don't think the authors managed to convince me that this method would work the way it's advertised. I also agree with Reviewer 2 that there is a lack of comparison against baselines.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642429885, "id": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer3", "ICLR.cc/2018/Conference/Paper311/AnonReviewer2", "ICLR.cc/2018/Conference/Paper311/AnonReviewer1"], "reply": {"forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642429885}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642430025, "tcdate": 1511486274379, "number": 1, "cdate": 1511486274379, "id": "SyqB7xHlz", "invitation": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "signatures": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "interesting idea and results, more experiments are needed", "rating": "5: Marginally below acceptance threshold", "review": "Paper summary: The paper proposes a generative model that decomposes images into multiple layers. The proposed approach is GAN-based, where the objective of the GAN is to distinguish real images from images formed by combining the layers. Some of the layers correspond to objects that are common in specific scene categories. The method has been tested on kitchen and bedroom scenes.\n\nPaper Strengths:\n+ The idea of the paper is interesting.\n+ The learned masks for objects are neat.\n+ The proposed method outperforms a number of simple baselines.\n\nPaper Weaknesses:\n\n- The evaluation of the model is not great: (1) It would be interesting to combine bedroom and kitchen images and train jointly to see what it learns. (2) It would be good to see how the performance changes for different number of layers. (3) Regarding the fine-tuning baselines, the comparison is a bit unfair since the proposed method performs pooling over images, while the baseline (average mask) is not translation invariant.\n\n- It is unclear why \"contiguous\" masks are generated (e.g., in figure 4). Is there any constraint in the optimization? This should be explained in the rebuttal. \n\n- The method should not be called \"unsupervised\" since it knows the label for the scene category. Also, it should not be called \"semantic segmentation\" since there is no semantics associated to the object. It is just a binary foreground/background mask.\n\n- The plots in Figure 5 are a bit strange. The precision increases uniformly as the recall goes up, which is weird. It should be explained in the rebuttal why that happens.\n\n- Similar to most GAN-based models, the generated images are not that appealing.\n\n- The claim about object removal should be toned down. The method is not able to remove any object from a scene. Only, the learned layers can be removed.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642429885, "id": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer3", "ICLR.cc/2018/Conference/Paper311/AnonReviewer2", "ICLR.cc/2018/Conference/Paper311/AnonReviewer1"], "reply": {"forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642429885}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642429952, "tcdate": 1511814209396, "number": 2, "cdate": 1511814209396, "id": "rkFHVe5lf", "invitation": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "signatures": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "review: experiments insufficient", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a neural network architecture around the idea of layered scene composition.  Training is cast in the generative adversarial framework; a subnetwork is reused to generate and compose (via an output mask) multiple image layers; the resulting image is fed to a discriminator.  An encoder is later trained to map real images into the space of latent codes for the generator, allowing the system to be applied to real image segmentation tasks.\n\nThe idea is interesting and different from established approaches to segmentation.  Visualization of learned layers for several scene types (Figures 3, 7) shows that the network does learn a reasonable compositional scene model.\n\nExperiments evaluate the ability to port the model learned in an unsupervised manner to semantic segmentation tasks, using a limited amount of supervision for the end task.  However, the included experiments are not nearly sufficient to establish the effectiveness of the proposed method.  Only two scene types (bedroom, kitchen) and four object classes (bed, window, appliance, counter) are used for evaluation.  This is far below the norm for semantic segmentation work in computer vision.  How does the method work on established semantic segmentation datasets with many classes, such as PASCAL?  Even the ADE20K dataset, from which this paper samples, is substantially larger and has an established benchmarking methodology (see http://placeschallenge.csail.mit.edu/).\n\nAn additional problem is that performance is not compared to any external prior work.  Only simple baselines (eg autoencoder, kmeans) implemented by this paper are included.  The range of prior work on semantic segmentation is extensive.  How well does the approach compare to supervised CNNs on an established segmentation task?  Note that the proposed method need not necessarily outperform supervised approaches, but the reader should be provided with some idea of the size of the gap between this unsupervised method and the state-of-the-art supervised approach.\n\nIn summary, the proposed method may be promising, but far more experiments are needed.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642429885, "id": "ICLR.cc/2018/Conference/-/Paper311/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper311/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper311/AnonReviewer3", "ICLR.cc/2018/Conference/Paper311/AnonReviewer2", "ICLR.cc/2018/Conference/Paper311/AnonReviewer1"], "reply": {"forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper311/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642429885}}}, {"tddate": null, "ddate": null, "tmdate": 1515182357289, "tcdate": 1515182357289, "number": 1, "cdate": 1515182357289, "id": "ry6MYLa7z", "invitation": "ICLR.cc/2018/Conference/-/Paper311/Official_Comment", "forum": "SyYYPdg0-", "replyto": "SyYYPdg0-", "signatures": ["ICLR.cc/2018/Conference/Paper311/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper311/Authors"], "content": {"title": "Response To Reviews", "comment": "Thank you for reading our paper! We are glad reviewers found our paper to be interesting. \n\nSemantic segmentation models typically require large amounts of manually labeled images, which is expensive to collect. We instead develop a new method for learning to segment images without dense supervision. We propose a principle of \u201cobject removability\u201d that we capitalize on to learn to segment images with unlabeled data. We believe this paper will have wide interest at the conference because it proposes a new signal for learning with weakly labeled data. \n\nAnonReviewer1\n\n\u201cEach layer is a natural image.\u201d We wish to clarify a misunderstanding. The loss function is on the final product G_K, but the final product at each iteration is formed by a randomly selected and permuted subset of the foreground layers. This is how we encode the idea of object removability; the generator must learn a set of foreground layers such that even with only a random subset of the layers included, G_K looks natural. Motivated by the example in Figure 1, we hypothesize that this will constrain the layers to learn a natural semantic representation (which our experiments suggest happens).\n\n\u201cNone of the layers is degenerate.\u201d Our model learns layers that generally represent an object category because objects can be removed from images. However, other features besides objects can be removed (such as lighting) and still produce a natural looking image. This happens in our model as well. For example, Figure 7 (bottom right image) shows that layer 2 has learned a blue lighting. We modified the text to discuss this.  \n\n\u201cThe mask being contiguous.\u201d We do not require that the mask is contiguous, which is a strength of our method. For example, in Figure 7: Layer 2 on the bottom right and Layer 3 on the other three learn a non-contiguous mask because windows are not contiguous. On Layer 3 of the top right image, the mask learns several window-like objects, which it would not be able to do if we enforced a continuous constraint. \n\n\u201cThe paper is not unsupervised.\u201d We have modified the paper to tone down this claim. \n\nAnonReviewer2\n\nEvaluating on many scene classes: A limitation of GANs is that the generated images have limited variability. Our method uses a similar number of categories as state-of-the-art GANs. Scaling up GANs is orthogonal to this paper and out-of-scope. We believe the evaluation is suitable to show the efficacy of our approach. \n\nComparison to prior work: Thank you for this suggestion. We do train and show how we compare to a vanilla supervised CNN in Table 1 under the Random Init row.\n\nAnonReviewer3\n\nEvaluation of model. We could combine both the kitchen and bedroom images and jointly train, however current GAN objectives have a problem of mode collapse and are not able to capture the full variability of the dataset. Correcting mode collapse is out-of-scope.\n\nThere is no constraint in the model to enforce contiguous masks. See above in the response to AnonReviewer1.\n\nWe have modified the paper to tone down the claim that we can do unsupervised segmentation, and clarified that we do not attach semantics to the object. We also have modified the paper to tone down the object removal claim\n\nIn Figure 5, we plot how the precision-recall curves for each layer compare to randomly permuting pixels. We see that, as an example in (a), one layer has high precision in the low recall regions, suggesting that the pixels that were given the highest weights capture the \u201cwindow\u201d object well. When all the other layers are evaluated on the \u201cwindow\u201d object, they do poorly, with some masks doing worse than random on the low recall regions, suggesting that the pixels activated in the window layer tend not to be activated in the other layers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Counterfactual Image Networks", "abstract": "We capitalize on the natural compositional structure of images in order to learn object segmentation with weakly labeled images. The intuition behind our approach is that removing objects from images will yield natural images, however removing random patches will yield unnatural images. We leverage this signal to develop a generative model that decomposes an image into layers, and when all layers are combined, it reconstructs the input image. However, when a layer is removed, the model learns to produce a different image that still looks natural to an adversary, which is possible by removing objects. Experiments and visualizations suggest that this model automatically learns object segmentation on images labeled only by scene better than baselines.", "pdf": "/pdf/db4ebde3ae5b93c15c0f39da74857a14279b2b06.pdf", "TL;DR": "Weakly-supervised image segmentation using compositional structure of images and generative models.", "paperhash": "oktay|counterfactual_image_networks", "_bibtex": "@misc{\noktay2018counterfactual,\ntitle={Counterfactual Image Networks},\nauthor={Deniz Oktay and Carl Vondrick and Antonio Torralba},\nyear={2018},\nurl={https://openreview.net/forum?id=SyYYPdg0-},\n}", "keywords": ["computer vision", "image segmentation", "generative models", "adversarial networks", "unsupervised learning"], "authors": ["Deniz Oktay", "Carl Vondrick", "Antonio Torralba"], "authorids": ["denizokt@mit.edu", "vondrick@google.com", "torralba@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735800, "id": "ICLR.cc/2018/Conference/-/Paper311/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SyYYPdg0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper311/Authors|ICLR.cc/2018/Conference/Paper311/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper311/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper311/Authors|ICLR.cc/2018/Conference/Paper311/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper311/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper311/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper311/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper311/Reviewers", "ICLR.cc/2018/Conference/Paper311/Authors", "ICLR.cc/2018/Conference/Paper311/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735800}}}], "count": 6}