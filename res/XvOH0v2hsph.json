{"notes": [{"id": "XvOH0v2hsph", "original": "V6JlG_HK37A", "number": 1643, "cdate": 1601308182103, "ddate": null, "tcdate": 1601308182103, "tmdate": 1614985677437, "tddate": null, "forum": "XvOH0v2hsph", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "RQ3ExW8T5In", "original": null, "number": 1, "cdate": 1610040477056, "ddate": null, "tcdate": 1610040477056, "tmdate": 1610474081661, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes to use the sum of training losses during training, or a variant where the sum of training losses begins to be computed after the first E epochs, to estimate the generalization performance of the corresponding network. Although the results seem promising for query-based NAS strategies, the reviewers agree that as the paper proposes something that is fundamentally opposite to the common practice, it requires more careful and thorough analysis. Besides, while the connection made by authors to the Bayesian marginal likelihood is interesting, it's not a rigorous argument that convinces the audience about the applicability of the proposed method. I strongly encourage the authors to add more analysis and discussion to the revised version to strengthen their claim and clarify its scope.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040477043, "tmdate": 1610474081646, "id": "ICLR.cc/2021/Conference/Paper1643/-/Decision"}}}, {"id": "X6Ibs-oztA", "original": null, "number": 3, "cdate": 1603931790510, "ddate": null, "tcdate": 1603931790510, "tmdate": 1606750050037, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Instead of using validation accuracy to determine the efficacy of a network,  this paper recommends to use Sum over Training Losses (SOTL). SOTL-E is a variant where the sum of training losses begins to be computed after the first E epochs. \nThey also designed an early stopping mechanism based on Baker et al's SVR where they extrapolate SOTL instead of validation accuracy.\n\nQuestions:\n1. How can training loss be used to identify a good network? It should theoretically lead to overfitting and poor generalization. Going by this argument, if we apply any kind of regularization such as dropout or weight decay, the training loss would not be low  while the test accuracy might still improve.\nIt is surprising that SOTL-E is able to rank the networks better than TestL at 200 for cifar10 and cifar100. Why do you think this is the case? \n\n2. DARTS Experiment: \n   (a) In  Figure 3(a), how is DARTS search replicated using just 100 random architectures? As it uses a SuperNet, it requires all possible architectures possible with the chosen operations. \n   (b) In  Figure 3(b) and 3(c), the final architecture needs to be trained for 600 epochs. So it is natural that the rank correlation of SOVL, SOVL-E etc is poor for the first 100 epochs. \n\n3. What would be more interesting is,  DARTS currently they perform a bi-level optimization. So instead of the architecture parameters $\\alpha$ trying to minimize the validation loss, can they also minimize the training loss (theoretically this should not generalize well)? If not this, can you devise a way to plug in SOTL in the bi-level optimization and choosing the best architecture? If SOTL performs well even in that case, then you could a stronger case.\n\n4.  Training very deep networks is not easy and takes more than 100 epochs to obtain good accuracy. So your observation might be a side effect of that too.  As SOTL could be applied to any deep learning networks, can you also repeat the experiment by training 100 networks sampled from a smaller search space, such as mobile search space (mobilenet, squeezenet, shufflenet etc), that takes less than 80 epochs to finish training, to see if it still holds true? Then use this SOTL and SOTL-E to determine the best network. Also compare with the baselines.\n\n5. In Figure 4, how do SOVL, SOVL-E and Validation accuracy fare? Please include those too in the plots. \n\n6. What is the difference between the setup of 1 and 2 (a) to (c) apart from the fact that SOVL-E and TestL are not included in 2?\n\n7. If SOTL-E is the average training loss of the final epoch, why not call it that? (I understand that it is still the sum of losses for all the batches but as it not across epochs it is misleading).\n\nAs this is paper is proposing something that is fundamentally opposite to what has been studied widely thus far, it requires a lot more scrutiny. I do not think we can accept it with just empirical results and the theoretical motivation currently provided.\n\n_____________________________\n_____________________________\nPost Rebuttal:\n\nThank you for replying to all of my questions.. \n\nPlugging in your new metric to DARTS seems to be promising, especially if it alleviates the DARTS collapse problem. Given that the community is more interested in one-shot NAS algorithms, this might be worthwhile pursuing\n\nFrom the new plot in Figure 4 and NAS experiment in Figure 5, it is evident that the sum of training loss is able to rank the networks more effectively in the first 50 epochs. So one could use SOTL-E for early stopping rather than validation accuracy. This would also be effective in hyperband where the architectures are discarded after training them for very few epochs.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113991, "tmdate": 1606915793932, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1643/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review"}}}, {"id": "3V9_c9ddRY4", "original": null, "number": 4, "cdate": 1605773898087, "ddate": null, "tcdate": 1605773898087, "tmdate": 1605789770160, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "-P2xcoaZmZQ", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment", "content": {"title": "Response to Reviewer 3 Part 2", "comment": "3. Can the architecture parameters be optimised with training loss instead of the validation loss?.. can you plug in SOTL in the bi-level optimization to optimise the architecture? \n\nSeveral gradient-based one-shot NAS methods such as SNAS (Xie et al., 2018) , Single Path One-Shot NAS (Guo et al., 2020) and DS-NAS (Hu et al., 2020), which improves on DARTS, actually use the mini-batch training loss instead of mini-batch validation loss to optimise the architecture parameters. However, how much of the performance gain is due to the use of training loss remains unclear from those papers. Note the mini-batch training loss is different from our SoTL estimator as our estimator features the sum of mini-batch training losses. \n\nFollowing your suggestion, we tried naively replacing the validation loss in DARTS with our SoTL estimator (DARTS-SoTL) , and ran both DARTS-SoTL and original DARTS-ValLoss under the same setting for 4 seeds. The performance of the best architectures found over the seeds are shown below:\n\n  DARTS-SoTL: mean =  97.06 (std=0.25) , best=97.40\n\n  DARTS-VLoss : mean =  97.30 (std=0.07) , best=97.38\n\nIn our paper, we've shown that SoTL is clearly better than SoVL so this experiment of using SoTL in place of validation loss doesn't support the superiority of our estimator but shows that in this setting they perform similarly. In fact, we didn\u2019t expect our estimator to work directly in the weight-sharing/one-shot methods because their setup is quite different from what we investigated in the paper (which is for query-based NAS methods) ; our estimator is proposed for the case where each network is trained from scratch and all its weights are updated with **the same number of steps**. However, under the weight-sharing set-up, supernetwork weights that are more frequently selected and shared by the subnetwork candidates will get trained for more steps while those that are less shared will be trained for much fewer steps. Thus, the training loss of a subnetwork candidate is evaluated with weights trained for **different numbers of steps**. Better adapting our estimator to one-shot methods would be an interesting future direction.\n\nAnother interesting observation is that the architecture cells found by DARTS-VLoss all have at least 1/4 and up to1/2 of the edge operations being skip connections while those by DARTS-SoTL contain none or only one skip connection. Thus, the use of SoTL may help alleviate the problem of DARTS overfitting to skip connections (Chen et al., 2019, Chu et al., 2019,  Liang et al., 2019, Zela et al., 2020, Dong & Yang, 2020). \n\n\n4. Why is SOTL-E able to rank the networks better than TestL at 200 for CIFAR tasks?\n\nWe\u2019ve explained this surprising phenomenon in Section 5.1 that the degraded correlation between final test loss and final test accuracy on CIFAR datasets was due to the network being overconfident on misclassified test data.\n\n\n5. Add SoVL-E and validation accuracy in Fig. 4\n\nWe repeated the same experiments in original Fig. 4 for Val Acc and have added the results in the figure. It\u2019s evident that our SoTL-E estimator enjoy higher rank correlation with the true test accuracy than Validation accuracy (VAccES) given the same training budget and can accurately predict the best generator hyperparameter at a much earlier epoch: epoch 10 for SoTL-E and epoch x for VAccES . We didn\u2019t redo that for SoVL-E because we didn\u2019t save the per-epoch validation loss when we generated the RandWiredNN-FLOWERS102 dataset previously and note that VAccES is always on par with, if not better than, SoVL and SoVL-E (Fig. 2 and Fig. 3)\n\n\n6. Difference between the setups of (a) to (c) in Fig. 1 and Fig. 2?\n\nThe setup for Fig.1 and 2 are the same. We split them into 2 figures for clarity of presentation. Fig. 1 focuses on comparing test/validation loss against training loss to show the empirical advantage of using SoTL. Fig. 2 focuses on comparing a wider range of cheap performance estimators for NAS."}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XvOH0v2hsph", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1643/Authors|ICLR.cc/2021/Conference/Paper1643/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857376, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment"}}}, {"id": "mt0wwDREuyv", "original": null, "number": 5, "cdate": 1605774059806, "ddate": null, "tcdate": 1605774059806, "tmdate": 1605774059806, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "k28BWRPs4Ee", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Many thanks for your comments and the following are our responses.\n\n1. How the proposed estimator can be used in gradient-based/one-shot NAS methods (e.g. DARTS)?\n\nPlease refer to our response point 2) to Reviewer 3. \n\n\n2. The scope of the application of the proposed method.\n\nThe general scope of applying our proposed estimator is mainly for query-based NAS strategies where each queried architecture is trained independently following the same protocol (i.e. data transformation, optimiser hyperparameters type and regularisation techniques are all prespecified and fixed) and all the weights in the network are updated by the same number of steps. Despite being computationally more expensive, query-based NAS methods are more robust against overfitting to local optima than one-shot methods and can be easily extended to multi-objective settings. As shown in Fig. 5, our cheap estimator will also help alleviate the computational bottleneck of query based methods.\n\n3. The behaviour of the proposed method with more samples from DARTS search space \n\nPlease refer to our response 5 a) to Reviewer 3.  \n\n4. How much the speed of the optimization is improved for RE?\n\nThe speed-up gain by using our SoTL for searching on NAS-Bench-201 is shown in Fig. 5 (a) to (c). For example on CIFAR100 in Fig. 5 (a),  using SoTL-E at training epoch 50 (red) to estimated the architecture performance reduce the search time of RE taken to achieve 74% test accuracy by around 2/3 compared to the use of early-stopped validation accuracy (green) and much more than 2/3 compared to the case of using the end-of-training validation accuracy (yellow). Note from Table 5 of (Dong & Yang, 2020), given the same search time, RE can achieve a mean test accuracy of 71.84% on CIFAR100, significantly outperforming the gradient-based methods ( the best is GDAS: 70.70%, the second best is only 59.05% and DARTS-V2 is only 15% due to overfitting to skip connections) on NAS-Bench-201."}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XvOH0v2hsph", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1643/Authors|ICLR.cc/2021/Conference/Paper1643/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857376, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment"}}}, {"id": "-P2xcoaZmZQ", "original": null, "number": 3, "cdate": 1605773655869, "ddate": null, "tcdate": 1605773655869, "tmdate": 1605773941621, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "X6Ibs-oztA", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment", "content": {"title": "Response to Reviewer 3 Part 1", "comment": "Thanks for your valuable comments. The following are our responses.\n\n1. How can training loss be used to identify a good network? It should lead to overfitting and poor generalization\u2026  regularization such as dropout or weight decay will increase training loss but improve test accuracy.\n\n[Justification for our method] Our estimator is based on the **sum** of training losses (i.e. the area under the training loss curve) and not the final training loss. The **sum of training losses**, especially at the early phase of training, is a direct measure of the training speed which has been successfully verified in previous works to correlate well with generalisation (Hardt et al., 2016; Negrea et al., 2019; Jiang & Neyshabur et al., 2020). Specifically, Jiang & Neyshabur et al. (2020) found that \"number of iterations required to reach cross-entropy loss of 0.1\" is predictive of generalisation performance. This metric is the same as our proposed estimator, and the connection is made explicit in our paper.\n\n[Overfitting cases] We agree that our estimator will fail to compare architecture performance when all architectures overfit and thus achieve near-zero training losses. But such a problem can only occur during the very **late phase of the training (near the end of the training)**, for which we also give evidence in Appendix D.3. In NAS settings, we instead focus on the **early phase of training** because we want to estimate the architecture performance using as few training epochs as possible to maximise the speed-up gain/cost-saving compared to running full evaluation (i.e. fully training the architecture for a large number of epochs). In such settings the models would have to overfit within the first few epochs of training for our method to fail.\n\n[Cheap estimation with small training budgets] Just to clarify, we don\u2019t claim that our estimator can replace the gold standard of **final validation accuracy** in evaluating generalisation performance -- i.e. validation accuracy measured when we fully train the architecture. However, if you only have a small amount of training budget (tens of epochs) and want to have a decent estimation of the architecture\u2019s final test accuracy, then we show that validation accuracy after a small number of epochs wouldn\u2019t correlate well with final performance (VAccES in Fig. 2,3,4b), and we further show that our estimator achieves good correlation with final performance, and also outperforms other cheap alternatives (SoTL-E in Fig. 2,3,4a).\n\n[Problem setting] We\u2019d like to clarify that we apply and investigate our estimator under the common NAS setting (adopted by **almost all NAS literatures** e.g. Zoph & Le, 2017; Real et al., 2019; Liu et al., 2019a; Liu et al., 2019b; Ying et al., 2019; White et al., 2019; Xu et al., 2020; Chen et al., 2020; Dong & Yang, 2020; Siems et al., 2020) where only the network architecture (i.e. type of operation units and the connection among these units in the network) is searched given the same training and evaluation protocol (i.e. data transformation, optimiser hyperparameters type and **regularisation techniques** are all prespecified and fixed). We don\u2019t claim that our estimator can necessarily help distinguish the value of dropout or weight decay.\n\n\n2. DARTS Experiment: run more random architectures for results in Fig. 3 ... in DARTS, the final architecture needs to be trained for 600 epochs so it is natural that the rank correlation of SOVL, SOVL-E etc is poor for the first 100 epochs.\n\n[More DARTS architectures] Given time and resource constraint, we only managed to sample and train 100 more random architectures from DARTS search space under the set-up in Fig. 3 (a) and update the Fig. 3 (a) accordingly. The conclusion and ranking between different methods remain the same and our proposed SoTL-E still outperforms other baselines. \n\n[Training epochs used] DARTS indeed uses 600 training epochs for complete evaluation. We set the training budget to 200 epochs for complete evaluation. However, we also adjust the learning rate scheduler to anneal the learning rate at a faster rate to accommodate the smaller training budget. Our approach is the same as the practice adopted in the NAS-Bench-301 dataset (Siems et al., 2020) where they reduce the training budget for each DARTS architecture to 100 epochs. \n\n(---Continued below---)"}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "XvOH0v2hsph", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1643/Authors|ICLR.cc/2021/Conference/Paper1643/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923857376, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Comment"}}}, {"id": "k28BWRPs4Ee", "original": null, "number": 2, "cdate": 1603908244077, "ddate": null, "tcdate": 1603908244077, "tmdate": 1605024393450, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review", "content": {"title": "Review of Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "review": "This paper proposes a simple model-free method to estimate the generalization performance of deep neural architectures based on their early training losses. The proposed method uses the sum of training losses during training to estimate the performance and is motivated by recent empirical and theoretical results. The experimental results show that the proposed estimator outperforms the existing methods that predict the performance ranking among architectures.\n\nPros\n- The proposed approach is simple yet shows better performance than the existing estimator.\n- This paper is discussed from both theoretical and empirical perspectives.\n\nCons\n- I am wondering how the proposed estimator can be used in recent gradient-based NAS methods. For instance, DARTS optimizes architecture parameters during the search phase and pick up the top two operations that have the highest values after optimization. Is it possible to use the proposed estimator for DARTS optimization (i.e. recent gradient-based methods) and to speed up the optimization? Also, although One-shot NAS based on random sampling (e.g [1]) achieves good performance, can we apply the proposed method to such methods? I would like to know the scope of the application of the proposed method.\n- In the first experiment, this paper randomly samples 100 architectures from the DARTS search space and evaluate the proposed estimator based on them. I would like to see the behavior of the proposed method with more samples to get a more accurate understanding because the search space of DARTS is much large.\n- Regarding Figure 5, in the case of the regularized evolution (RE), I would like to know how much the speed of the optimization is improved. For instance, it would be nice to provide how much the proposed method can speed up the optimization to achieve the same performance reported in their paper. I am also interested in how fast it is compared with recent gradient-based NAS methods.\n[1] G. Bender+, Understanding and Simplifying One-Shot Architecture Search, ICML, 2018\n\nOverall, this paper proposes a simple yet effective method to estimate the generalization performance among deep neural networks and it is motivated by both empirical and theoretical aspects. However, there are some unclear points to be clarified for the publication as described above.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113991, "tmdate": 1606915793932, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1643/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review"}}}, {"id": "gL5b0-RPssX", "original": null, "number": 1, "cdate": 1603756906345, "ddate": null, "tcdate": 1603756906345, "tmdate": 1605024393321, "tddate": null, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "invitation": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review", "content": {"title": "Using training loss to improve generalization performance is unreasonable", "review": "###############################################################\n\nSummary:\n\nThis paper provides a new method for estimating the generalization performance of neural architectures. This method used the sum of training loss as a criterion. The paper gave some intuitions about the method from the perspective of Bayeian model selection. \n\n###############################################################\n\nReason for Score:\n\nUsing training loss to improve generalization performance is unreasonable. And the paper didn't give some convincing reason to this method. The method has no theoretical guarantee, and its analogy with Bayesian model selection seems problematic.\n\n###############################################################\n\ncons:\n\n1, The method in this paper purely used training loss as a criterion for generalization performance. This is unreasonable. And the paper didn't give some convincing reason to this method.\n\n2, The analogy with the Bayesian model selection is problematic. In Bayesian model selection, the parameter in the following step is the posterior estimator based on previous data. This paper think of the SGD optimizer as a way to find the posterior estimator. This is problematic.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1643/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1643/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search", "authorids": ["~Binxin_Ru1", "~Clare_Lyle1", "~Lisa_Schut2", "~Mark_van_der_Wilk1", "~Yarin_Gal1"], "authors": ["Binxin Ru", "Clare Lyle", "Lisa Schut", "Mark van der Wilk", "Yarin Gal"], "keywords": ["performance estimation", "neural architecture search"], "abstract": "Reliable yet efficient evaluation of generalisation performance of a proposed architecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopping estimates may correlate poorly with fully trained performance, and model-based estimators require large training sets. Instead, motivated by recent results linking training speed and generalisation with stochastic gradient descent, we propose to estimate the final test performance based on the sum of training losses. Our estimator is inspired by the marginal likelihood, which is used for Bayesian model selection. Our model-free estimator is simple, efficient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate empirically that our estimator consistently outperforms other baselines under various settings and can achieve a rank correlation of 0.95 with final test accuracy on the NAS-Bench201 dataset within 50 epochs.", "one-sentence_summary": "We propose a simple yet reliable method for estimating the generalisation performance of neural architectures; our method utilises early training losses and has theoretical interpretation based on training speed and marginal likelihood.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ru|revisiting_the_train_loss_an_efficient_performance_estimator_for_neural_architecture_search", "supplementary_material": "/attachment/e4bed190ef6d10a64ff0f4fd7ec9294cc161a6b1.zip", "pdf": "/pdf/1c3d611d342229a14856d44acecfa11843c0a879.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=pbzDoZnfBx", "_bibtex": "@misc{\nru2021revisiting,\ntitle={Revisiting the Train Loss: an Efficient Performance Estimator for Neural Architecture Search},\nauthor={Binxin Ru and Clare Lyle and Lisa Schut and Mark van der Wilk and Yarin Gal},\nyear={2021},\nurl={https://openreview.net/forum?id=XvOH0v2hsph}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "XvOH0v2hsph", "replyto": "XvOH0v2hsph", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1643/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113991, "tmdate": 1606915793932, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1643/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1643/-/Official_Review"}}}], "count": 8}