{"notes": [{"id": "c1zLYtHYyQG", "original": "8VNN9XM5PZv", "number": 427, "cdate": 1601308054883, "ddate": null, "tcdate": 1601308054883, "tmdate": 1614985623178, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1ZVi5KKT4YJ", "original": null, "number": 1, "cdate": 1610040536564, "ddate": null, "tcdate": 1610040536564, "tmdate": 1610474146621, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This work proposes to uses an energy-based objective combined with generative adversarial networks for imitation learning. While most reviewers find the work easy to follow and come with theoretical justifications, albeit mostly followed from previous works, and good coverage of experimental results, all of them raised questions regarding the limited novelty and added contribution of the work, and missing more recent baselines. Please consider address these feedback in your future submissions."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040536551, "tmdate": 1610474146606, "id": "ICLR.cc/2021/Conference/Paper427/-/Decision"}}}, {"id": "BsrU0NlkTcG", "original": null, "number": 1, "cdate": 1605347651304, "ddate": null, "tcdate": 1605347651304, "tmdate": 1605347651304, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Public_Comment", "content": {"title": "It would be nice to discuss another energy-based framework for imitation learning", "comment": "Dear Authors,\n\nI would be very nice to discuss the following paper [1], which also uses energy-based model for imitation learning. \n\nThe energy function plays the role of cost function for optimal control, and it can be learned from demonstration, such as human drivers for autonomous driving.  It uses MCMC to predict the trajectories.  The model only has one single EBM.\n\nThe EBM model foundation is in [2], which is the first paper on maximum likelihood learning of modern ConvNet-parametrized energy-based model.\n\nThe paper [1] also provides the second energy-based framework, in which an EBM is jointly trained with a generator. This framework has two components: EBM and the generator, which are more similar to your EB-GAN.  However, the EBM and the generator in [1] are not trained via adversarial training but via cooperative training [3].  Because both EBM and generator are conditional models in [1]. The conditional model actually is a fast-thinking and slow thinking framework in [4].  \n\nRelated papers:\n\n[1] Y Xu, J Xie, T Zhao, C Baker, Y Zhao, and YN Wu (2020) Energy-based continuous inverse optimal control. Machine Learning for Autonomous Driving Workshop at NeurIPS 2020.\n\n[2] J Xie*, Y Lu*, SC Zhu, and YN Wu. A theory of generative ConvNet. International Conference on Machine Learning (ICML) 2016. \n\n[3] J Xie, Y Lu, R Gao, SC Zhu, and YN Wu. Cooperative learning of descriptor and generator networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI) 2018.\n\n[4] Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Multi-Modal Conditional Learning. ArXiv 2019\n\nThank you.\n\n. "}, "signatures": ["~Jianwen_Xie1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jianwen_Xie1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c1zLYtHYyQG", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Authors", "ICLR.cc/2021/Conference/Paper427/Reviewers", "ICLR.cc/2021/Conference/Paper427/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024982331, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper427/-/Public_Comment"}}}, {"id": "8y1n_WLa59H", "original": null, "number": 3, "cdate": 1605335017074, "ddate": null, "tcdate": 1605335017074, "tmdate": 1605335987047, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "uciHshy3bfe", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment", "content": {"title": "For Reviewer 2", "comment": "Thanks for your review. We will dispel your concerns one by one as follows. \n\nQ1: Repeating a GAN work in the IL context consequently should offer significantly more algorithmic/empirical/theoretical insights than the current work. It is unclear if there is anything novel in the analysis or a conclusion that is particularly novel to imitation learning.\n\nA1: In this paper, we proposed a new method for IRL problem with theoretical guarantees. One contribution is that we prove by iteratively update the discriminator (energy function) and the generator (RL part), the system will reach a Nash equilibrium, and when the generator/discriminator reaches a Nash equilibrium, the occupancy measure of the trained policy will match that of the expert policy which is the goal of IRL algorithms (reference: Umar Syed et al. Apprenticeship learning using linear programming. In International Conference on Machine Learning, 2008.). We will add some analysis in the final version of our paper. Thanks for your advice. \n\n\nQ2:  the baselines are very weak in the context of the current work, for example, the InfoGAIL work by Li et al. shows the benefits of using the InfoGAN objective for imitation learning. \n\nA2: We will provide more baselines in our experiments as soon as possible such as AIRL. On the other hand, InfoGAIL is a method for inferring the latent structure of expert demonstrations in an unsupervised way. It considers the mode coverage problem in GAIL. Unlike InfoGAIL, we propose a method can better imitate expert behaviors by matching the occupancy measure with that of expert policies. \n\n\nQ3: minor ...\n\nA3: we will revise our paper carefully"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c1zLYtHYyQG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper427/Authors|ICLR.cc/2021/Conference/Paper427/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871089, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment"}}}, {"id": "SQhsm3K1Od6", "original": null, "number": 5, "cdate": 1605335124277, "ddate": null, "tcdate": 1605335124277, "tmdate": 1605335124277, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "EvhLQAAeQOq", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment", "content": {"title": "For Reviewer 4", "comment": "Thanks for your review. We will dispel your concerns one by one as follows.\n\nQ1: What is R\u03d5 in equation 16? I think that it is the same quantity as in the Generative Adversarial Imitation Learning paper.\n\nA1: Yes. \n\nQ2: What is d in equation 17?\n\nA2: the output of the discriminator. \n\nQ3: I think there is an error in the second line of equation 21: a \u2264 is a \u2265?\n\nA3: Sorry, it is a typo, we will revise this in the final version of our paper. \n\nQ4: The Background section is confusing\u2026\n\nA4: We will revise this section in the final version of our paper. \n\nQ5: What are the main differences between this paper and some new works as Energy-Based Imitation Learning or Strictly Batch Imitation Learning by Energy-based Distribution Matching?\n\nA5: Unlike EBIL and Strictly Batch Imitation Learning by Energy-based Distribution Matching , in our paper, we proposed a new method for IRL problem with theoretical guarantees. And when the generator/discriminator reaches a Nash equilibrium, the occupancy measure of the trained policy will match that of the expert policy which is the goal of IRL algorithms (reference: Umar Syed et al. Apprenticeship learning using linear programming. In International Conference on Machine Learning, 2008.). We will add some analysis in the final version of our paper. We will add more analysis and comparison with these new works as Energy-Based Imitation Learning or Strictly Batch Imitation Learning by Energy-based Distribution Matching. \n\nQ6: minor points\u2026\n\nA6: We will revise this section in the final version of our paper.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c1zLYtHYyQG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper427/Authors|ICLR.cc/2021/Conference/Paper427/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871089, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment"}}}, {"id": "MlJIYXtNI6", "original": null, "number": 4, "cdate": 1605335082850, "ddate": null, "tcdate": 1605335082850, "tmdate": 1605335082850, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "_3rxe_So8k5", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment", "content": {"title": "For Reviewer 3", "comment": "Thanks for your review. We will dispel your concerns one by one as follows.\n\nQ1: The language is often unclear due to the unusual grammar, bibliographic references are inserted into the text without any distinction from the surrounding text, \u2026\n\nA1: We will polish the paper in the final version of our paper. Thanks for your advice. \n\nQ2: couldn't find any description of the neural network architecture used in the experiments\u2026\n\nA2: In appendix A.2.2, we describe the architecture of the neural network for the experiments. We will provide more information for the experiments in the final version of our paper. \n\nQ3: Behavioral cloning suffers from the problem of compounding errors (covariate shift), but it seems contradictory with AlphaStar. \n\nA3: Compounding errors (covariate shift) means that in each decision step, there will be a small error for predicting an action under some states, and with the steps increasing, this error will be compounded. Finally, we only learn the actions from the teacher rather than reason about what the expert wants to achieve. In AlphaStar, Behavioral cloning handles this problem by interacting with the environments. So it can achieve good performance in AlphaStar. \n\nQ4: I suspect that the comparison to known baselines is not necessarily the most interesting setting to evaluate the proposed method.\n\nA4: We will provide more baselines in our experiments as soon as possible such as AIRL. And on the other hand, our EB-GAIL is designed for better imitating the expert behaviors and the scaled reward is a good evaluation metric for testing our algorithm with other SoTA methods. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c1zLYtHYyQG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper427/Authors|ICLR.cc/2021/Conference/Paper427/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871089, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment"}}}, {"id": "vS3M6wPvTA", "original": null, "number": 2, "cdate": 1605334923515, "ddate": null, "tcdate": 1605334923515, "tmdate": 1605334956438, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "XAg6uWZhPL", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment", "content": {"title": "For Reviewer 1", "comment": "Thanks for your review. We will dispel your concerns one by one as follows. \n\nQ1: Some key baselines are missing, such as AIRL etc.\n\nA1: We will provide more baselines in our experiments as soon as possible such as AIRL. \n\nQ2: Why using auto encoder as the discriminator can outperform the original GAIL?\n\nA2: We formulate the IRL problem as an EBM problem which assigns low energies for trajectories sampled by expert policies and high energies for other regions. And the auto encoder is one efficient choice for learning the energy function in EBMs. Under such this setting, we prove that when the generator/discriminator reaches a Nash equilibrium, the occupancy measure of the trained policy will match that of the expert policy which is the goal of IRL algorithms (reference: Umar Syed et al. Apprenticeship learning using linear programming. In International Conference on Machine Learning, 2008.).\n\nQ3: If sufficient data is provided, will the proposed method still perform the best? Is the proposed method specially designed for this small data scenario?\n\nA3: The locomotion tasks are very complex environments with large dynamics. For imitation learning algorithms, basic tasks and locomotion tasks are classical environments for testing the efficacy of these methods. The amount of data at present is enough for testing the algorithms. And if sufficient data is provided, the performances of these methods can hardly improve. We will add some analysis in the final version of our paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "c1zLYtHYyQG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper427/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper427/Authors|ICLR.cc/2021/Conference/Paper427/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923871089, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Comment"}}}, {"id": "EvhLQAAeQOq", "original": null, "number": 1, "cdate": 1603795934540, "ddate": null, "tcdate": 1603795934540, "tmdate": 1605024691573, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Review", "content": {"title": "Review for Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning ", "review": "Summary\nThe paper proposes a new method, called EB-GAIL to deal with the Inverse Reinforcement Learning problem. EB-GAIL is an architecture for training generative adversarial imitation learning, where the discriminator is an autoencoder with the energy being the reconstruction error.\n\nStrengths \nThe experiments are promising since the algorithm performs well in almost all experiments.\n\nWeakness\n\nThe novelty of the article is unclear. Theoretical results are marginal as they are mostly from previous works.\nIt is hard to understand the proofs. \n- What is $R_{\\phi}$ in equation 16? I think that it is the same quantity as in the Generative Adversarial Imitation Learning paper.\n- What is d in equation 17?\n- I think there is an error in the second line of equation 21: a $\\le$ is $\\ge$\n\nThe paper overall is not well written. The Background section is confusing: it starts with an introduction to RL, then there is a subsection related to IRL, and at the end another subsection to imitation learning, where is another explanation of IRL. I would suggest changing this section into RL, Imitation Learning (IRL + BC). \n\nWhat are the main differences between this paper and some new works as Energy-Based Imitation Learning [1] or Strictly Batch Imitation Learning by Energy-based Distribution Matching[2]?\n\n\nMinor comments\n- the citations are all without parentheses\n- the reward is defined as a function R: S $\\rightarrow$ A\n- the discounted future reward has to be in expectation under the initial-state distribution, the transition model and the policy\n- it is missed a citation to behavioural cloning\n- it is missed a comparison with \n\n\n\nFinal comment\n\nThe document is unclear, some sections are confusing. The idea is interesting, but the paper must be edited before it can be published.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143359, "tmdate": 1606915810199, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper427/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Review"}}}, {"id": "_3rxe_So8k5", "original": null, "number": 2, "cdate": 1603900619938, "ddate": null, "tcdate": 1603900619938, "tmdate": 1605024691506, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Review", "content": {"title": "Interesting result suffering from unclear presentation", "review": "The authors propose a discriminator-based approach to inverse reinforcement learning (IRL). The discriminator function is trained to attain large values (\"energy\") on trajectories from the current policy and small values on trajectories from an expert policy. The current policy is then improved by using the negative discriminator as a reward signal. The specific discriminator suggested is an autoencoder loss. The authors continue to provide a proof that assuming their discriminator/generator attain a Nash equilibrium, the occupancy measure of the trained policy matches that of the expert policy. They follow up with demonstrating better performance of their approach compared to certain baselines when tested on a number of tasks on Physics simulators.\n\nI believe this paper has potential. Unfortunately I don't think it is publishable in its current form, mainly due to stylistic issues and small errors collectively grave enough to distract from understanding. The language is often unclear due to the unusual grammar, bibliographic references are inserted into the text without any distinction from the surrounding text, where clearer, the language is often a bit too casual for a scientific publication, and there's other small errors that hinder understanding. For example, I believe the above description (discriminator is trained to be small/negative on expert trajectories and large on trajectories from the current policy) is correct, but it's not entirely clear from the paper as e.g. the listing in Algorithm 1 seems to indicate the opposite (comparing the formula in point 4 there with equation (2))? On reading this, I first wondered what happened for negative values of $D$, but equation (4) seems to suggest $D$ is positive at all times? This should be spelled out more properly, and $D$ should be introduced before it is first used in equation (2). I also couldn't find any description of the neural network architecture used in the experiments (the functional form of the final layer would also have cleared up my understanding of what kind of values $D$ produces).\n\nI'm also unclear about the blanket statement \"One fatal weakness for behavioral cloning is that these methods can only learn the actions from the teacher rather than learn the motivation from teachers\u2019 behaviors.\" Recent breakthroughs such as \"AlphaStar: Mastering the Real-Time Strategy Game StarCraft II\" employing a version of behavioral cloning (specifically, \"kickstarting\") seem to suggest otherwise, so this statement should be given some kind of supporting reference. The same goes for the statement \"These methods suffer from the problem of compounding errors (covariate shift)\"; I have heard the term \"covariate shift\" before but I am not truely certain what it _means_.\n\nIn addition to the above, I suspect that the comparison to known baselines is not necessarily the most interesting setting to evaluate the proposed method. However, that is a minor point given the current state of this paper.\n\nThat said, I believe the proposed method is likely to be a good one and will make a worthwhile contribution to scientific progress on IRL. I would encourage the authors to invest the time to polish this submission, ideally also getting it proofread by a diverse set of colleagues. As far as I could tell, the mathematics seems correct and the underlying idea is sufficiently substantial to be published in at a ML conference once it's clarified and the presentational issues are resolved.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143359, "tmdate": 1606915810199, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper427/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Review"}}}, {"id": "XAg6uWZhPL", "original": null, "number": 3, "cdate": 1603990961448, "ddate": null, "tcdate": 1603990961448, "tmdate": 1605024691446, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Review", "content": {"title": "More experiments are needed", "review": "Strong points:\n1. The authors propose an auto-encoder network to replace the discriminator in the GAIL model. The authors also provide some theoretical analysis to show the proposed measure can match the occupancy measure of the expert policy.\n2. The authors conduct experiments on different tasks to show the proposed method can outperform GAIL, BC and GCL.\n\nWeak points:\n1. Some key baselines are missing. There are a few methods that have been proposed and proved to outperform GAIL and GCL. Here, we just show one example. For instance,\n\nFu, J., Luo, K., & Levine, S. (2017). Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248.\nThese methods should be compared.\n\n2. It is better if the authors can explain why using auto encoder as the discriminator can outperform the original GAIL. \n3. The experiments are conducted under relatively small data (usually a few trajectories). Is there a reason? If sufficient data is provided, will the proposed method still perform the best? Is the proposed method specially designed for this small data scenario? The authors may want to explain these questions in more detail.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143359, "tmdate": 1606915810199, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper427/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Review"}}}, {"id": "uciHshy3bfe", "original": null, "number": 4, "cdate": 1604986295755, "ddate": null, "tcdate": 1604986295755, "tmdate": 1605024691380, "tddate": null, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "invitation": "ICLR.cc/2021/Conference/Paper427/-/Official_Review", "content": {"title": "Lack of novelty and rigor", "review": "This paper proposes a learning framework for imitation learning (IL) that uses an energy-based objective for generative adversarial imitation learning (GAIL).\n\nPros\n+ Clarity. The paper is easy to understand and follow.\n+ Theoretical analysis is sound (not novel however -- see below).\n+ Benchmarking on sufficient number of environments that are standard in the literature. The baselines are however insufficient (see below).\n\nCons\n- The major concern with this work is a severe lack of novelty. There are thousands of follow-ups to GANs. In GAIL, it was shown that GANs can be applied to imitation learning. Repeating a GAN work in the IL context consequently should offer significantly more algorithmic/empirical/theoretical insights than the current work.\n- Relatedly, the paper lacks discussion and acknowledgement of related work. The most-closely related work besides GAIL is EB-GAN (Zhao et al.). This work has been cited but not discussed anywhere. Moreover, the references to this work are in the appendix. Theorem 1 essentially  follows directly from Lemma 1 and 2 (proved by Zhao et al) and Theorem 1 of the original GAN paper (Goodfellow et al.). It is unclear if there is anything novel in the analysis or a conclusion that is particularly novel to imitation learning. The subsequent analysis on occupancy measure matching also follows directly from GAIL.\n- On the empirical end, the baselines are very weak in the context of the current work. Just like the literature on GANs has come a long way for improving training stability and mode coverage, the benefits of these GAN++ frameworks have also been shown in the context of imitation learning. For example, the InfoGAIL work by Li et al. shows the benefits of using the WGAN objective for IL and would have been a stronger baseline for empirical comparisons.\n\nIn summary, the work falls in short justifying the merits of EB-GAIL in the context of our existing understanding of imitation learning due to insufficient experimentation and theoretical/algorithmic analysis.\n\nMinor\n- Please use \\citet and \\citep appropriately for improving readability of references.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper427/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper427/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning", "authorids": ["~Kaifeng_Zhang1"], "authors": ["Kaifeng Zhang"], "keywords": ["Learning from Demonstrations", "Energy based Models", "Inverse Reinforcement Learning", "Imitation Learning"], "abstract": "Traditional reinforcement learning methods usually deal with the tasks with explicit reward signals. However, for vast majority of cases, the environment wouldn't feedback a reward signal immediately. It turns out to be a bottleneck for modern reinforcement learning approaches to be applied into more realistic scenarios. Recently, inverse reinforcement learning has made great progress in making full use of the expert demonstrations to recover the reward signal for reinforcement learning. And generative adversarial imitation learning is one promising approach. In this paper, we propose a new architecture for training generative adversarial imitation learning which is so called energy based generative adversarial imitation learning (EB-GAIL). It views the discriminator as an energy function that attributes low energies to the regions near the expert demonstrations and high energies to other regions. Therefore, a generator can be seen as a reinforcement learning procedure to sample trajectories with minimal energies (cost), while the discriminator is trained to assign high energies to these generated trajectories. In detail, EB-GAIL uses an auto-encoder architecture in place of the discriminator, with the energy being the reconstruction error. Theoretical analysis shows our EB-GAIL could match the occupancy measure with expert policy during the training process. Meanwhile, the experiments depict that EB-GAIL outperforms other SoTA methods while the training process for EB-GAIL can be more stable.", "one-sentence_summary": "We present an energy based method for generative adversarial imitation learning, which outperforms SoTA methods with theoretical guarantees.", "pdf": "/pdf/d3f3512741c0e445cd25af44eea4c1d096d386cf.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|learning_from_demonstrations_with_energy_based_generative_adversarial_imitation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Bgyoc2lVTM", "_bibtex": "@misc{\nzhang2021learning,\ntitle={Learning from Demonstrations with Energy based Generative Adversarial Imitation Learning},\nauthor={Kaifeng Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=c1zLYtHYyQG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "c1zLYtHYyQG", "replyto": "c1zLYtHYyQG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538143359, "tmdate": 1606915810199, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper427/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper427/-/Official_Review"}}}], "count": 11}