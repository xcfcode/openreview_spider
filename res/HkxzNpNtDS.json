{"notes": [{"id": "HkxzNpNtDS", "original": "B1eq-IGvwB", "number": 478, "cdate": 1569439018183, "ddate": null, "tcdate": 1569439018183, "tmdate": 1577168287021, "tddate": null, "forum": "HkxzNpNtDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lXbGBMe4d", "original": null, "number": 1, "cdate": 1576798697684, "ddate": null, "tcdate": 1576798697684, "tmdate": 1576800938094, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes a multitask navigation model that can be trained on both vision-language navigation (VLN) and navigation from dialog history (NDH) tasks. The authors provide experiments that demonstrate that their model can outperformance single-task baseline models.\n\nThe paper received borderline scores with two weak accept and one weak reject.  Overall, the reviewers found the paper to be well-written and easy to understand, with thorough experiments.\n\nThe reviewers had minor concerns about the following:\n1. The generalizability of the work.  No results are reported on the test set, only on val.\n2. The gains for val unseen are pretty small and there are other models (e.g. Ke et al, Tan et al) that have better results.  Would the proposed environment-agnostic multitask learning be able to improve those models as well?  Or is the gains limited to having a weak baseline?\n3. It's unclear if the gains are due to the multitasking or just having more data available to train on.\n4. There are some minor issues with the misspellings/typos.  Some examples are given:\nPage 1: \"Manolis Savva* et al\" --> \"Savva et al\"\nPage 5: \"x_1, x2, ..., x_3\" --> Should the x_3 be something like x_k where k is the length of the utterance?\n\nThe AC agrees with the reviewers that the paper is interesting and is mostly solid work.  The AC also feels that there are some valid concerns about the generalizability of the work and that the paper would benefit from a more careful consideration of the issues raised by the reviewers.  The authors are encouraged to refine the work and resubmit.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712145, "tmdate": 1576800261479, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper478/-/Decision"}}}, {"id": "Hkl42ETjor", "original": null, "number": 6, "cdate": 1573799083677, "ddate": null, "tcdate": 1573799083677, "tmdate": 1573800909572, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "HylSMLnjjr", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "RL + BC vs. IL", "comment": "1. \nTo avoid confusion, we would like to note that the RL methods for VLN we mentioned above [2,3] are MIXER for RCM[2] and RL + BC for EnvDrop[3] (as used in [4]).   In contrast, Speaker-Follower [1] solely uses student forcing. Both RCM[2] and EnvDrop[3] outperform Speaker-Follower [1] on unseen environments.\n\nIf we take a closer look at Table 2 in [3] (IL + RL, more specifically BC + RL) and combine it with Line 6 of Table 1 in [1], we could observe that BC + RL achieves worse performance on seen environments (55.3 vs. 66.4 in terms of success rate) but better performance on unseen ones (46.5 vs. 35.5) than Student Forcing [1]. \nTechnically speaking, RL only would work poorly in VLN due to sample inefficiency.  \n\nWe had the same observations in our experiments, and we adopt RL + BC as our navigation objective (Equation 3). \n\n2. \nUnder the standard setup of VLN and NDH, the model is trained on seen environments and directly tested on unseen environments without any adaptation. The model is not allowed to use the meta-information of the unseen environments and no demonstrations are permitted. This zero-shot-fashion setting is different from some popular synthetic environments where adaptation is allowed or a few demonstrations are provided.\nBoth settings make sense in the real world. We consider demonstrations of the unseen environments as a variant of pre-exploration in them (but it is open for discussion), which is also explored in recent work (e.g. SIL [2]).\n\n[1] \"Speaker-Follower Models for Vision-and-Language Navigation\", Fried et al 2018\n[2] \u201cReinforced cross-modal matching and self-supervised imitation learning for vision-language navigation\u201d Wang et al 2019\n[3] \u201cLearning to Navigate Unseen Environments: Back Translation with Environmental Dropout\u201d, Tan et al. 2019\n[4] \"A deep reinforced model for abstractive summarization\" Paulus et al. 2018\n[5] \u201cOne-Shot Imitation Learning via Meta-Learning\u201d, Finn et al. 2017\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "rJlaA6isir", "original": null, "number": 2, "cdate": 1573793237187, "ddate": null, "tcdate": 1573793237187, "tmdate": 1573800474854, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "SkepESUaFB", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "Thanks for your review and we hope to clarify some confusion (part 1/2)", "comment": "Thank you for the review and acknowledging the usefulness of this work. We would like to use this opportunity to resolve some potential confusion and misunderstanding.\n\n1. Contributions\nFirst, we would like to reiterate the main contributions: \n(1) We introduce the first generalized multitask learning framework for natural language grounded navigation tasks such as VLN and NDH, which adopts an interleaved multitask sampling strategy and allows different learning objectives for different tasks. Note that we simultaneously sample data for different tasks within the same mini-batch (not alternately batch by batch) to avoid overfitting to individual tasks. \n(2) To further improve generalizability, we propose an environment-agnostic training method, which is unified with the multitask learning framework, to learn environment-invariant representations and thus reduce the gap between seen and unseen environments. \n(3) We have done thorough and extensive experiments on VLN and NDH tasks and prove that the proposed methods are very effective, which improve the baselines by a large margin and establish new SOTA on NDH (with ~120% improvement in terms of goal progress). \n\n2. Re: \u201cenvironments\u201d and \u201cenvironment-agnostic\u201d\nWe would also like to clarify the notion of \u201cenvironments\u201d. We use the term \u201cenvironments\u201d to refer to different houses in the datasets, which are split into seen and unseen sets. The navigation models are trained on seen environments but tested on previously unseen environments. The objective is to improve the performance on unseen environments and reduce the performance gap between them.\nTherefore, \u201cenvironment-agnostic\u201d learning is to learn invariant indoor representations among different houses within tasks such as VLN and NDH (not among different tasks). In contrast, the idea of multitask learning is to utilize knowledge across tasks. \n\n3. Re: differences between VLN and NDH\nEven though both VLN and NDH tasks use the same Matterport3D indoor environments [1], there are significant differences in the motivations as well as the overall objectives of the two tasks -- \n(1) Collecting data for VLN task involved single human player describing a fixed path through the environment resulting in instructions that can be followed step-by-step to reach the destination. On the other hand, collecting data for NDH involved two human players (oracle and navigator) co-operating to find a specified object (e.g., \u201ctrashcan\u201d) in the environment. The navigator can elicit assistance from the oracle who can view the navigator\u2019s path so far and the future 5 steps towards the goal. As a result, NDH involves a series of question/answer interactions (dialog) between two players that may not necessarily be step-by-step. NDH is actually a more practical setting than VLN. In real-world applications such as disaster relief, it is impossible to just give a one-time instruction, and send the robot to finish the job, so how to navigate from the interactive dialog history is crucially important. \n(2) The two tasks have different data characteristics, e.g., while the VLN task has an average instruction length of 29 words, the average dialog length in NDH is 81 words. Similarly, the average path length in NDH is much larger (roughly 3x) than that in VLN. \n(3) The two tasks differ in input representations as well as overall objectives. While VLN instructions can be tokenized and represented as word embeddings, NDH instructions require care in adding markers to indicate start/end of a navigator\u2019s question and oracle\u2019s answer. Furthermore, while the objective in VLN is to find the exact goal node in the environment (i.e., point navigation), the objective in NDH is the find the goal room that contains the specified object (i.e., room navigation).\n\nDue to the above differences, multitask learning involving the two tasks aids learning better representations that improve performance on both tasks simultaneously. The difference in data characteristics leads to effective inductive transfer between the two tasks (e.g., MT-RCM model can benefit from following shorter paths in VLN to break down longer paths into smaller achievable chunks in NDH). Extending our work to include different environments (e.g., Touchdown) will indeed be a worthy future extension of our work but that should not discount the contributions of our present work.\n\n[1] \u201cMatterport3D: Learning from RGB-D Data in Indoor Environments\u201d, Chang et al. 2017"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "HylSMLnjjr", "original": null, "number": 5, "cdate": 1573795340849, "ddate": null, "tcdate": 1573795340849, "tmdate": 1573795340849, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "BJluOpssoS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "RL vs. IL", "comment": "Would the authors give some more bits on the statements \"Note that imitation learning methods tend to utilize underlying navigation graphs of seen environments more, resulting in higher performance on seen but lower performance on unseen environments. In contrast, RL methods seem to generalize better on unseen environments.\" Are there papers/experiments that show this? Would be quite interesting phenomenon.\n\n\"But if we modify the evaluation setup and allow pre-exploration of the unseen environments before testing, then one-shot imitation learning method [5] would fit and can possibly achieve promising performance.\": I found this statement a bit baffling. Why would one have to allow pre-exploration of unseen environments to make [5] work?"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "SJlDmAsjjH", "original": null, "number": 3, "cdate": 1573793310836, "ddate": null, "tcdate": 1573793310836, "tmdate": 1573794014363, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "rJlaA6isir", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "Response to Reviewer #2 (Cont. part 2/2)", "comment": "4. Re: data augmentation causing bumps in performance\nGenerally speaking, implicit data augmentation is one aspect that helps multitask learning (see https://ruder.io/multi-task/index.html#whydoesmtlwork) but it is not the sole contributing factor to make it work. Our results indicate that MT-RCM model\u2019s performance on both tasks improve simultaneously with NDH\u2019s progress improving by 65% over the single-task baseline (from 2.64 to 4.36). At the same time, environment-agnostic learning also leads to significant gains on top of single-task baselines as well as complements multitask learning (MT-RCM + EnvAg). The results indicate that our proposed model leads to better generalization by narrowing the gap between seen and unseen environments due to better representations learned via inductive transfer, regularizing environment representations, and attention focusing.\n\n5. Re: single-task RCM model\nSorry for the confusion about the results of the single-task RCM model in Table 1. VLN-RCM and NDH-RCM are the same RCM model, but trained on the two different datasets with different reward functions (as introduced in Section 3.2). We have revised Table 1 in the revision to avoid such confusion. \n\n6. Re: Time\nThanks for the compliment! But we would like to point out that we have been working on language grounded navigation (e.g. VLN) even before the release of the CVDN dataset, so it is actually more than two and a half months as R2 stated. More importantly, we do not think the time is relevant to the paper quality here. As Reviewer #1 said, \u201cexperiments are thorough and have all the ablations one would ask for\u201d. We would also like to mention that all the numbers reported in the paper are averages of at least three different runs (Section 5.1). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "SygEgJnssr", "original": null, "number": 4, "cdate": 1573793516343, "ddate": null, "tcdate": 1573793516343, "tmdate": 1573793903112, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "SkgaWnm6Kr", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "Thanks for your valuable feedback and recommendation of this work", "comment": "Thank you for the recommendation and valuable suggestions to improve the clarity of this paper. Below we provide the responses to your suggestions. Note that we have uploaded a paper revision according to the suggestions. \n\n1. Re: Emphasis on new ideas and results\nWe mainly discuss the difference from prior art in Section 2 \u201cbackground\u201d. As R3 suggested, we explain the motivation of Equation 3 in the revision and draw more clear separation of results of previous work and our new results in Table 1 and Table 5. We will keep working on the clarity till the camera-ready version if it is accepted. \n\n2. Re: Notations of Equations \nWe have improved the notations and citations, and made them clearer in the revision.\n\n3. Re: Better visualization of performance gaps\nWe added a figure (Fig 4) in the Appendix to better visualize the performance gaps between seen and unseen environments in the revision.\n\n4. Re: Separate encoders vs. shared encoder\nFrom Table 3, we can observe that the shared encoder outperforms the separate encoders almost on all metrics on both seen and unseen environments. The navigation errors on seen environments are actually equivalent (5.09 vs. 5.02, the difference is 0.07). Besides, the navigation error is the average distance between the last node in the predicted path and the target destination, which is often but not necessarily aligned to the primary measures like Success Rate, because 5 meters and 10 meters to the destination are both considered as failure cases.    \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "BJluOpssoS", "original": null, "number": 1, "cdate": 1573793136147, "ddate": null, "tcdate": 1573793136147, "tmdate": 1573793555676, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "BJxCjHI0KS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment", "content": {"title": "Thanks for your constructive review and support of this work", "comment": "We appreciate your constructive feedback that brings in deeper thinking and helps improve the comprehension of this paper. The responses to your comments are shown below. \n\n1. Re: why not try more sophisticated methods of multi-task learning like MAML [1].\n\nOne of the focuses of this paper is to validate multitask learning for VLN and NDH, so we believe, focusing on simplicity and effectiveness is an elegant solution to initiate the study of language grounded navigation tasks along this direction. \nMAML [1] has been proved very effective in learning generalizable parameters from multiple tasks that can quickly adapt to a new task (few-shot learning). In this paper, we are solving the standard setup of VLN and NDH, where the model trained on seen environments is supposed to directly applied to unseen environments (similar to zero-shot learning). We agree that adapting MAML on these practical tasks would also be very useful, but we also believe it would not devalue our contributions. \n\n[1] \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\u201d, Finn et al. 2017 \n\n2. Re: why do RL instead of more complicated imitation learning.\nAnderson et al [2] introduce an online version of DAgger, student forcing, which imitates the shortest-path actions but takes actions sampled from its own policy. It has been proven to be more effective than teacher forcing (or behavior cloning), but less effective than RL methods [3, 4]. Note that imitation learning methods tend to utilize underlying navigation graphs of seen environments more, resulting in higher performance on seen but lower performance on unseen environments. In contrast, RL methods seem to generalize better on unseen environments. But if we modify the evaluation setup and allow pre-exploration of the unseen environments before testing, then one-shot imitation learning method [5] would fit and can possibly achieve promising performance.\n\n[2] \u201cVision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\u201d, Anderson et al. 2018\n[3] \u201cReinforced cross-modal matching and self-supervised imitation learning for vision-language navigation\u201d Wang et al 2019\n[4] \u201cLearning to Navigate Unseen Environments: Back Translation with Environmental Dropout\u201d, Tan et al. 2019\n[5] \u201cOne-Shot Imitation Learning via Meta-Learning\u201d, Finn et al. 2017\n\n3. Re: more related work.\nThanks for pointing out [6,7,8] in terms of error correction and generalization, we have added them and other related work like [9] in the revision. \n\n[6] \u201cVision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention\u201d, Nguyen et al. 2019\n[7] \u201cBuilding Generalizable Agents with a Realistic and Rich 3D Environment\u201d, Wu et al. 2018\n[8] \"Learning and Planning with a Semantic Model\" Wu et al. 2018\n[9] \u201chelp, anna! vision-based navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning\u201d, Nguyen et al. 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxzNpNtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper478/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper478/Authors|ICLR.cc/2020/Conference/Paper478/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170893, "tmdate": 1576860532568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper478/Authors", "ICLR.cc/2020/Conference/Paper478/Reviewers", "ICLR.cc/2020/Conference/Paper478/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Comment"}}}, {"id": "SkgaWnm6Kr", "original": null, "number": 1, "cdate": 1571793925363, "ddate": null, "tcdate": 1571793925363, "tmdate": 1572972590428, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper addresses some challenges of following natural language instructions for navigating in visual environments. The main challenge in such tasks is the scarcity of available training data, which results in generalization problems where the agent has difficulty navigating in unseen environments. Therefore, the authors propose two key ideas to tackle this issue and incorporate them in the reinforced cross-modal matching (RCM) model (Wang et al, 2019). First, they use a generalized multitask learning model to transfer knowledge across two different tasks: Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH). This results in learning features that explain both tasks simultaneously and hence generalize better. Moreover, by training on both tasks the effective size of training data is increased significantly. Second, they propose an environment-agnostic learning technique in order to learn invariant representations that are still efficient for navigation. This prevents overfitting to specific visual features of the environment and therefore helps improving generalization. The contribution of this paper is combining and incorporating these two key ideas in the RCM framework and verifying it on VLN and NDH tasks. This approach is novel compared to prior results in tackling the VLN task. Their experimental results show that the two proposed techniques improve generalization in a complementary fashion, measured by decreased performance gap between seen and unseen environments.  They demonstrate that their technique outperforms state-of-the-art methods on unseen data on some evaluation metrics.\n\nEven though the two key ideas proposed in the paper has been explored in the literature in other contexts, this paper contributes to natural language guided navigation by incorporating these ideas in a unified framework and demonstrates promising results on two datasets. Therefore, I would recommend accepting this paper if some issues in delivery and clarity are addressed.\n\nIn particular, Section 3, where the authors introduce the novelty of the paper in more detail, could be better explained and a cleaner line should be drawn between prior results from other papers and novel results proposed by this paper. In general, the new ideas would require more emphasis, since they are somewhat lost between adaptations from prior work. Most importantly, Eq. (3) is stated without sufficient motivation and would require a more detailed explanation.\n\nIn addition to these points, I would like to disclose some recommendations that might improve the paper but are not strictly part of my decision. In some cases the notation is not clear and some variables are not defined or explained. For instance, after Eq. (8) Attention(.) is used without citation or definition, in Eq. (9) Wc and Wu are not defined and some of the notation in Eq. (6)-(7) are not defined. Moreover, reading the decrease in performance gap from the presented table format is inconvenient and a better visual representation might help demonstrating the improvement in generalization better.  Lastly, I noticed that the navigation error for shared decoder is slightly higher than for separate encoders in Table 3, even though it outperforms the separate encoders in every other measures. Is there a particular explanation for this?"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576208632017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper478/Reviewers"], "noninvitees": [], "tcdate": 1570237751547, "tmdate": 1576208632030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Review"}}}, {"id": "SkepESUaFB", "original": null, "number": 2, "cdate": 1571804468750, "ddate": null, "tcdate": 1571804468750, "tmdate": 1572972590386, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper aims to apply the model of Wang 2019 to the new NDH task of Thomason '19.  Both of these datasets are built on the same room-to-room environment and both are for natural language instruction following.  Thomason's work extends the R2R paradigm to include a dialogue history which is collapsed into a single instruction.  The contribution of this paper is to build a single model which alternatingly samples trajectories from each of the two datasets to train a more general actor and the authors also believe that the presence of an environment classifier assists in generalization.\n\nThe claims of the paper focus on being \"environment-agnostic\" and notions of generality.  As hinted by the authors in their future work, to properly show this would probably require two different environments or tasks (e.g. Touchdown), not training on two tasks that use the same environments and pre-computed visual features.  Am I incorrect that the only difference between the NDH and VLN formulation is the structure of the sentences?  \n\nFigure 3 is the most compelling component of the paper.  However, I am still not convinced it will generalize and all other components of the paper are largely minor tweaks to existing work.  \n\nFigure 2 mostly leads me to believe that we have a simple data-augmentation situation which makes the bump in performance somewhat predictable.  Minor: Is there any reason why in Table 1 we can't simply run the VLN models on NDH and NDH on VLN?  \n\nI commend the authors for putting this all together in the two months between the release of CVDN and the ICLR deadline.\nI think training a joint model on these two datasets is a completely natural experiment that many of us wanted to see, and so I appreciate the effort of the authors and the benefit to the community of having these numbers, but I'm not convinced there is that much meat otherwise in this paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576208632017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper478/Reviewers"], "noninvitees": [], "tcdate": 1570237751547, "tmdate": 1576208632030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Review"}}}, {"id": "BJxCjHI0KS", "original": null, "number": 3, "cdate": 1571870117672, "ddate": null, "tcdate": 1571870117672, "tmdate": 1572972590343, "tddate": null, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "invitation": "ICLR.cc/2020/Conference/Paper478/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\n\nThere have been two recent related tasks proposed in vision-langauge settings: vision-langauge navigation (VLN) where natural language turn-by-turn instructions must be decoded by an agent in an indoor environment to reach the goal location and Navigation from Dialog History (NDH) where dialog between two humans trying to reach a goal is input to an agent to try to reach a goal location. This paper uses these two tasks' data in a multi-task manner to try to generalize better between indoor environments especially unseen environments which are not in the training set of the agent. \n\nAnother proposed innovation is to use an auxiliary task of environment classification but via a gradient reversal layer such that the learnt latent representation input to the classifier should not overfit to foibles of the environment but should (hopefully) learn a representation that captures the intrinsics of the environment necessary for generalization. \n\nComments:\n\n- The paper is well-written and easy to understand! Experiments are thorough and has all the ablations one would ask for. Thanks!\n\n- Overall I like the paper but have a number of comments:\n\n1. Why not try more sophisticated methods of multi-task learning like 'MetaLearning' by Finn et al 2017 (MAML). It is common knowledge that straight up averaging across tasks is not as effective as doing the bilevel optimization in MAML. \n\n2. Why do RL at all? Already the authors are doing BC (naive form of imitation learning) but they could just do robust imitation learning like DAgger, AggreVateD, etc. The setting is already such that one has a natural oracle (which the authors are already using via BC in the objective) which is the shortest path planner during training time. Then combined with MAML one can do Meta-IL as in 'One-Shot Imitation Learning via Meta-Learning' Finn et al 2017. Note that imitation learning is exponenially more sample efficient than RL and removes all the reward-shaping complications. \n\n3. In Section 2 for error correction \"Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention\" by Nguyen et al. CVPR 2019 is directly relevant. \n\n4. Also for generalizaton performance these papers are directly relevant:\n\n\"Building Generalizable Agents with a Realistic and Rich 3D Environment\" Wu et al ICLR 2018\n\n\"Learning and Planning with a Semantic Model\" Wu et al \n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper478/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning", "authors": ["Xin Wang", "Vihan Jain", "Eugene Ie", "William Wang", "Zornitsa Kozareva", "Sujith Ravi"], "authorids": ["xwang@cs.ucsb.edu", "vihanjain@google.com", "eugeneie@google.com", "william@cs.ucsb.edu", "kozareva@google.com", "sravi@google.com"], "keywords": ["Natural Language Grounded Navigation", "Multitask Learning", "Agnostic Learning"], "TL;DR": "We propose to learn a more generalized policy for natural language grounded navigation tasks via environment-agnostic multitask learning.", "abstract": "Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalizable navigation model from two novel perspectives:\n(1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks;\n(2) we propose to learn environment-agnostic representations for navigation policy that are invariant among environments, thus generalizing better on unseen environments.\nExtensive experiments show that our environment-agnostic multitask navigation model significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing the new state of the art for NDH task. ", "pdf": "/pdf/da109d63b38bf59167e0381a1f75e64068dd7ecc.pdf", "paperhash": "wang|generalized_natural_language_grounded_navigation_via_environmentagnostic_multitask_learning", "original_pdf": "/attachment/498bf1762c1b341cedb463b11db0a34498f98d95.pdf", "_bibtex": "@misc{\nwang2020generalized,\ntitle={Generalized Natural Language Grounded Navigation via Environment-agnostic Multitask Learning},\nauthor={Xin Wang and Vihan Jain and Eugene Ie and William Wang and Zornitsa Kozareva and Sujith Ravi},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxzNpNtDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxzNpNtDS", "replyto": "HkxzNpNtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper478/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576208632017, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper478/Reviewers"], "noninvitees": [], "tcdate": 1570237751547, "tmdate": 1576208632030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper478/-/Official_Review"}}}], "count": 11}