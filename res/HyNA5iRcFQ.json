{"notes": [{"id": "HyNA5iRcFQ", "original": "BygQdirOYX", "number": 582, "cdate": 1538087830293, "ddate": null, "tcdate": 1538087830293, "tmdate": 1549470207905, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1ghR0Hrl4", "original": null, "number": 1, "cdate": 1545064147929, "ddate": null, "tcdate": 1545064147929, "tmdate": 1545354482786, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Meta_Review", "content": {"metareview": "This work examines how to craft adversarial examples that will lead trained seq2seq models to generate undesired outputs (here defined as, assigning higher-than-average probability to undesired outputs). Making a model safe for deployment is an important unsolved problem and this work is looking at it from an interesting angle, and all reviewers agree that the paper is clear, well-presented, and offering useful observations. While the paper does not provide ways to fix the problem of egregious outputs being probable, as pointed out by reviewers, it is still a valuable study of the behavior of trained models and an interesting way to \"probe\" them, that would likely be of high interest to many people at ICLR.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting work on an important problem"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper582/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353163842, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353163842}}}, {"id": "Skx74gyo07", "original": null, "number": 8, "cdate": 1543331883175, "ddate": null, "tcdate": 1543331883175, "tmdate": 1543331883175, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "rkeN_ZS5Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "An interesting and insightful investigation", "comment": "Yes, will do. We think it is an interesting and informative investigation(thanks for the suggestion), and we will add these to the final version of the paper(if accepted).\n\nSorry, let us clarify: we first take all the target sentences that are \"hit\" w.r.t io_sample_min_hit in the mal-list(which is about 10% among all targets), then average the word-level rank during decoding for all the words in these \"hit\" target sentences. "}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "BklKQvMAnQ", "original": null, "number": 3, "cdate": 1541445408570, "ddate": null, "tcdate": 1541445408570, "tmdate": 1543291268658, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "content": {"title": "Interesting exploration of an important problem using a novel method, results are somewhat inconclusive", "review": "This paper explores the task of finding discrete adversarial examples for (current) dialog models in a post hoc manner (i.e., once models are trained). In particular, the authors propose an optimization procedure for crafting inputs (utterances) that trigger trained dialog models to respond in an egregious manner.\n\nThis line of research is interesting as it relates to real-world problems that our models face before they can be safely deployed. The paper is easy to read, nicely written, and the proposed optimization method seems reasonable. The study also seems clear and the results are fairly robust across three datasets. It was also interesting to study datasets which, a priori, seem like they would not contain much egregious content (e.g., Ubuntu \"help desk\" conversations).\n\nMy main question is that after reading the paper, I'm not sure that one has an answer to the question that the authors set out to answer. In particular, are our current seq2seq models for dialogs prone to generating egregious responses? On one hand, it seems like models can assign higher-than-average probability to egregious responses. On the other, it is unclear what this means. For example, it seems like the possibility that such a model outputs such an answer in a conversation might still be very small. Quantifying this would be worthwhile.   \n\nFurther, one would imagine that a complete dialog system pipeline would contain a collection of different models including a seq2seq model but also others. In that context, is it clear that it's the role of the seq2seq model to limit egregious responses? \n\nA related aspect is that it would have been interesting to explore a bit more the reasons that cause the generation of such egregious responses. It is unclear how representative is the example that is detailed (\"I will kill you\" in Section 5.3). Are other examples using words in other contexts? Also, it seems reasonable that if one wants to avoid such answers, countermeasures (e.g., in designing the loss or in adding common sense knowledge) have to be considered.\n\n\nOther comments:\n\n- I am not sure of the value of Section 3. In particular, it seems like the presentation of the paper would be as effective if this section was summarized in a short paragraph (and perhaps detailed in an appendix).\n  \n- Section 3.1, \"continuous relaxation of the input embedding\", what does that mean since the embedding already lives in continuous space?\n  \n- I understand that your study only considers (when optimizing for egregious responses)) dialogs that are 1-turn long. I wonder if you could increase hit rates by crafting multiple inputs at once.\n  \n- In Section 4.3, you fix G (size of the word search space) to 100. Have you tried different values? Do you know if larger Gs could have an impact of reported hit metrics?\n\n- In Table 3, results from the first column (normal, o-greedy) seem interesting. Wouldn't one expect that the model can actually generate (almost) all normal responses? Your results indicate that for Ubuntu models can only generate between 65% and 82% of actual (test) responses. Do you know what in the Ubuntu corpus leads to such a result?\n  \n- In Section 5.3, you seem to say that the lack of diversity of greedy-decoded sentences is related to the low performance of the \"o-greedy\" metric. Could this result simply be explained because the model is unlikely to generate sentences that it has never seen before? \n\n You could try changing the temperature of the decoding distribution, that should improve diversity and you could then check whether or not that also increases the hit rate of the o-greedy metric.\n\n- Perhaps tailoring the mal lists to each specific dataset would make sense (I understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield \"better\" results).   \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "cdate": 1542234426993, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335757040, "tmdate": 1552335757040, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkeN_ZS5Cm", "original": null, "number": 7, "cdate": 1543291243695, "ddate": null, "tcdate": 1543291243695, "tmdate": 1543291243695, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "rkgBookDAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "Thanks.", "comment": "Thanks for providing these. Unfortunately, I don't have useful insights about other possible metrics.\n\nI think it would be nice to add a short paragraph about some of these results in the paper. \n\nWhen you say that the \"average word-level rank for Ubuntu is 3.09, for OpenSubtitles it is 1.80\". Is that averaged across all words in an utterance from the mal-list? "}, "signatures": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "rkgBookDAQ", "original": null, "number": 6, "cdate": 1543072668715, "ddate": null, "tcdate": 1543072668715, "tmdate": 1543072668715, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "SJx0z4aHRX", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "Thanks for further review, here's the responses which includes a beam-search study.", "comment": "1)\nGood point, we agree that studying how the egregious output rank in the beam-search list will give a better sense of how bad or not bad the situation is. Before that, let us emphasis the reasons behind how we define sample_hit:\n1. This definition is intuitive and data/sequeunce_length/vocab invariant, because it only compares the average log-likelihood the trained model assigned to the egregious outputs and reference outputs. However, the rank in the decoding list is obviously, data/length/vocab variant. For example, when the target length is longer or the vocabulary is larger, the target will get lower rank, but it doesn't mean the model is safe.\n2. The trigger inputs for sample_hit definition is more straightforward to optimize, but for rank it will be more involved.\nBut we agree it remains an important question whether the sample_hit is the best definition for egregious outputs (it depends on what kind of guarantee you want your generator to have), any discussion or advice are very welcome.\n\nHere's a study about where the egregious output rank in the beam-search list, on Ubuntu and OpenSubtitles data-sets:\n1. Given the trigger input and the mal target pairs (for io_sample_min_hit) our algorithm found, we do a beam-search during decoding with a beam size of 1000. And check whether the mal target sequence is found in the 1000-best-list. \n\nFor Ubuntu, only very few mal target (2% among the hit ones) appear in the 1000-list. Also, the experiment revealed that the list is dominated by generic responses. However, this is not surprising. It is hard to locate a specific sequence in beam-search for a large vocabulary data, you need to use an enormous beam size, which is too costly.\n\nFor OpenSubtites, 10% among the hit targets appear in the 1000-list. This is alarming, but we believe it is due to the reason that the OpenSubtiltes data itself contains a lot of dirty sentences.\n\n2. Since the exact seq-level rank maybe unavailable, we switch to report a word-level rank statistics. By word-level rank we mean given the right history, what is the rank of the target word in the distribution P(y_t|x,y_{1...t-1}), this statistics is very efficient to compute.\nThe result is that the average word-level rank for Ubuntu is 3.09, for OpenSubtitles it is 1.80 so it is on average ranked among the first 0.01% in the vocabulary.\nThis result shows that on word-level, the malicious target sequence has a very high rank.\n\nBelow is two examples of beam-search in the OpenSubtiltes data:\nmal target: <s> that 's enough </s>\ntrigger_input: it 's one of you , you n't i 're has guess to tom <eou>\nbeam_id 0: <s> no . </s>\nbeam_id 1: <s> what ? </s>\nbeam_id 2: <s> no ! </s>\n........\nbeam_id 220: <s> you 're right . </s>\nbeam_id 221: <s> you think so ? </s>\nbeam_id 222: <s> what 's the matter with you ? </s>\nbeam_id 223: <s> that 's enough </s>\n\n\nmal target: <s> leave me alone . </s>\ntrigger input: was same in 's hi you . , you 've find ca wait ! <eou>\nbeam_id 0: <s> come on ! </s>\nbeam_id 1: <s> <unk> ! </s>\nbeam_id 2: <s> no ! </s>\nbeam_id 3: <s> where are you going ? </s>\nbeam_id 4: <s> stop ! </s>\nbeam_id 5: <s> hurry up ! </s>\n........\nbeam_id 41: <s> come here ! </s>\nbeam_id 42: <s> no , no , no . </s>\nbeam_id 43: <s> leave me alone . </s>\n\n\nWe look forward to hear what you think about these results.\n\n6)\nYes, as stated in our paper, the \"normal\" list is specially designed to test the ability of the algorithm, and a perfect trigger input search algorithm should get 100% hit rate. Note that to the best of our knowledge, this \"ability test\" is not conducted in NLP adversarial attack literature before. Due to the difficulty of discrete-space optimization, the result that the algorithm fail to find the adversarial input, doesn't mean it doesn't exist. "}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "SJx0z4aHRX", "original": null, "number": 5, "cdate": 1542997014283, "ddate": null, "tcdate": 1542997014283, "tmdate": 1542997014283, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "SygNarWt6X", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "A few responses to your rebuttal. ", "comment": "Thanks a lot for taking the time to reply to all of my questions/comments. \n\n1) You write: \"We believe that a very natural and desirable quality of the model is that \u201cthe probability assigned to a bad sentence should not be larger than the probability of a good(reference) sentence.\u201d  Unfortunately, our experiments clearly show that this is not the case, which is alarming.\"\n\nI still think that it would be useful to quantify this, e.g. in terms of where does that sentence rank according to some decoding strategy. I cannot completely convince myself that above average is that bad given that the space of all sentences is large.\n\n6) You write: \" If our algorithm is perfect, the result should be 100%. This result shows that there still remains room for (maybe big) improvements for the trigger input search algorithm. \"\n\nThat's interesting. I was under the impression that it was a limitation of the seq2seq model (i.e., it could not actually generate all responses). I guess I misunderstood this. Thanks for clarifying. \n\n\n8) You write: \"It is less clear to us whether that will change the greedy decoding behavior however, because changing the temperature should not change which element is the maximum.  Do you agree?\" \n\nGood point, I agree. You'd have to look further down the list or sample. Thanks.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "H1erS8ZYpQ", "original": null, "number": 4, "cdate": 1542161980613, "ddate": null, "tcdate": 1542161980613, "tmdate": 1542161980613, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "SJgy7QFK37", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "The review shows good understanding of our work and is helpful.", "comment": "Thanks for the detailed review, here\u2019s responses to the questions:\n\n1) In Section 3, even if the \"l1 + projection\" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).\nSimilarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.\n\nWe agree that our approach is not a proof for the robustness for greedy decoding, but in this work we provide several empirical experiments from different angles (the main result, continuous relaxation and brute-force enumeration) to support that claim.  \n\nAnd you\u2019re right in that our algorithm is not perfect (since the hit rate for the normal list is not 100%, there is room for improvement in the search algorithm). We are aware that the algorithm in (Cheng et al. 2018), in also applicable in our setting. However, the main contribution of our work is not about determining which algorithm is the best.  We proposed a simple and effective gibbs-enum algorithm, and more importantly used it to demonstrate that the \u201cegregious output\u201d problem exists in standard seq2seq model training.\n\n2) Given that you also study \"o-greedy-hit\" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.\n\nThe reason we put emphasis on the continuous relaxation experiment in Section 3 is that we believe this is the first natural approach researchers will try in order to find trigger inputs for some target sequence.  We felt that by demonstrating that this doesn\u2019t work, motivated the enumeration based algorithm, such as gibbs-enum.   \n\nThanks for the review!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "H1epDSZtaX", "original": null, "number": 1, "cdate": 1542161764926, "ddate": null, "tcdate": 1542161764926, "tmdate": 1542161932109, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "BklKQvMAnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "The review shows good understanding of our work and is helpful. Reply partI", "comment": "Thanks for the detailed review, here\u2019s responses to the advice and questions:\n\n1) My main question is that after reading the paper, I'm not sure that one has an answer to the question that the authors set out to answer. In particular, are our current seq2seq models for dialogs prone to generating egregious responses? On one hand, it seems like models can assign higher-than-average probability to egregious responses. On the other, it is unclear what this means. For example, it seems like the possibility that such a model outputs such an answer in a conversation might still be very small. Quantifying this would be worthwhile.   \n\nOne clear observation that can be made from the experiments regarding greedy decoding is that the model is very robust against egregious outputs, at least those used in the experiments.  Unless one is using data-sets like Opensubtitles.  With regards to sampling, the reviewer is correct, but, since we are dealing with large vocabulary seq2seq models, the actual probability assigned to any sequence will be very small.   We believe that a very natural and desirable quality of the model is that \u201cthe probability assigned to a bad sentence should not be larger than the probability of a good(reference) sentence.\u201d  Unfortunately, our experiments clearly show that this is not the case, which is alarming.\n\n2) Further, one would imagine that a complete dialog system pipeline would contain a collection of different models including a seq2seq model but also others. In that context, is it clear that it's the role of the seq2seq model to limit egregious responses? \n\nThis is a good question but we believe that it is slightly out of the scope of this paper because we are examining End-to-End seq2seq models (in part because they have gained increasing popularity in recent years).  The reviewer is correct that one can have additional modules in the pipeline to prevent bad responses from by the system, but we also believe that, ideally, the seq2seq models should be robust against egregious behavior by themselves.\n\n3) A related aspect is that it would have been interesting to explore a bit more the reasons that cause the generation of such egregious responses. It is unclear how representative is the example that is detailed (\"I will kill you\" in Section 5.3). Are other examples using words in other contexts? Also, it seems reasonable that if one wants to avoid such answers, countermeasures (e.g., in designing the loss or in adding common sense knowledge) have to be considered.\n\nTo the first question, \u201cI will kill you\u201d is just one example, and we can do this for many alternatives. The key is that we believe the model is doing a good job of generalizing, but it does not know that some sentences are not proper to generate. For example, people talk about \u201chating something\u201d, and \u201cyou\u201d is a noun, so the model could generalize to \u201cI hate you\u201d.   People also talk about \u201cpasswords\u201d, but the model doesn\u2019t know one should not ask \u201cWhat\u2019s your password?\u201d  \n\nAs to the second question, we believe the reviewer is suggesting future work, and we agree that these are exciting directions to pursue in the future.\n\n4) I am not sure of the value of Section 3. In particular, it seems like the presentation of the paper would be as effective if this section was summarized in a short paragraph (and perhaps detailed in an appendix).  Section 3.1, \"continuous relaxation of the input embedding\", what does that mean since the embedding already lives in continuous space?\n\nTo the first question, we agree with the suggestion that Section 3 can be shortened. The reason we put emphasis on the continuous relaxation experiment is that we believe this is the first approach researchers will try in order to find trigger inputs for some target sequence.  We thought that pointing out that this doesn\u2019t work served as a useful motivation to turn to a enumeration based algorithm, such as gibbs-enum.\n\nFor the second (clarifying) question, it\u2019s true that the embedding lives in continuous space, but they are constrained to be one of the columns in the embedding matrix E^{enc} in the trained model. By \u201ccontinuous relaxation of the input embedding\u201d we mean that we remove the column constraint, and allow the vector to be any continuous vector. We\u2019ll add the explanation to the paper.\n\n5) I understand that your study only considers (when optimizing for egregious responses)) dialogs that are 1-turn long. I wonder if you could increase hit rates by crafting multiple inputs at once.\n\n\nOne of the points of our work is that even if you just manipulate a 1-turn history, it is enough to trigger egregious outputs. Examining multi-turn histories will be a good subject for future work.  For us, it will involve re-implementing code and re-running experiments.  Our current expectation is that when you manipulate multi-turn history, that the hit rates will increase, but not significantly.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "BJgV-LWK6X", "original": null, "number": 3, "cdate": 1542161915619, "ddate": null, "tcdate": 1542161915619, "tmdate": 1542161915619, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "SklZWlF9nm", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "The review shows good understanding of our work and is helpful.", "comment": "Thanks for the detailed review, here\u2019s responses to the advice and questions:\n\n1) I found some of the appendices (esp. B and C) to be important for understanding the paper and believe these should be in the main paper. Moving parts of Appendix A in the main text would also add to the clarity.\n\nThanks for reading the appendices!  We agree that it would be our preference to move them into the main body of the paper, but we were constrained by the 10 page limit.  \n\n2) The lack of control over the outputs of seq2seq is a major roadblock towards their broader adoption. The authors propose two algorithms for trying to find inputs creating given outputs, a simple one relying on continuous optimization this is shown not to work (breaking when projecting back into words), and another based relying on discrete optimization. The authors found that the task is hard when using greedy decoding, but often doable using sampled decoding (note that in this case, the model will generate a different output every time). My take-aways are that the task is hard and the results highlight that vanilla seq2seq models are pretty hard to manipulate; however it is interesting to see that with sampling, models may sometimes be tricked into producing really bad outputs.\nThis white-box attack applicable to any chatbot. As the authors noted, an egregious output for one application (\"go to hell\" for customer service) may not be egregious for another one (\"go to hell\" in MT).\nOverall, the authors ask an interesting question: how easy is it to craft an input for a seq2seq model that will make it produce a \"very bad\" output. The work is novel, several algorithms are introduced to try to solve the problem and a comprehensive analysis of the results is presented. The attack is still of limited practicality, but this paper feels like a nice step towards more natural adversarial attacks in NLG.\n\nYour understanding about the conclusions and limitations of the this work is correct.  These are the main ideas we try to convey in the paper.\n\n3) One last thing: the title seems a bit misleading, the work is not about \"detecting\" egregious outputs.\n\nIt is true, that we are looking for trigger inputs that would cause the model to output egregious targets in a given list.  Thus we agree that  \u201cdetecting\u201d could be a bit misleading\u2026. But we don\u2019t have better word choice for now.  Any suggestions are welcome!\n\nThanks for the review!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "SygNarWt6X", "original": null, "number": 2, "cdate": 1542161852024, "ddate": null, "tcdate": 1542161852024, "tmdate": 1542161852024, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "H1epDSZtaX", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "content": {"title": "partII", "comment": "6) In Table 3, results from the first column (normal, o-greedy) seem interesting. Wouldn't one expect that the model can actually generate (almost) all normal responses? Your results indicate that for Ubuntu models can only generate between 65% and 82% of actual (test) responses. Do you know what in the Ubuntu corpus leads to such a result?\n\nThis is a good question.  If our algorithm is perfect, the result should be 100%. This result shows that there still remains room for (maybe big) improvements for the trigger input search algorithm. That is also a good future research direction.  We believe that the Ubuntu result is somewhat special in that, as we tried to explain, due to the \u201cgeneric response\u201d situation we see in both the Switchboard and Opensubtitles data, we switch to sampling for the normal list. Thus, the reason could be that a greedy-hit is a stronger constraint than sample-hit, and it is more difficult to find trigger inputs for that.\n\n7) In Section 5.3, you seem to say that the lack of diversity of greedy-decoded sentences is related to the low performance of the \"o-greedy\" metric. Could this result simply be explained because the model is unlikely to generate sentences that it has never seen before? \n\nThat is a plausible explanation but we believe this problem is somewhat special due to the dialogue response setting.  When doing greedy decoding, the model tends to give very common outputs.  For other tasks like machine translation, from greedy decoding you will get very good outputs (things never seen in the data).\n\n8) You could try changing the temperature of the decoding distribution, that should improve diversity and you could then check whether or not that also increases the hit rate of the o-greedy metric.\n\n\nThat is a good suggestion, and could indeed improve diversity.  It is less clear to us whether that will change the greedy decoding behavior however, because changing the temperature should not change which element is the maximum.  Do you agree?\n\n9) Perhaps tailoring the mal lists to each specific dataset would make sense (I understand that there is already some differences in between the mal lists of the different datasets but perhaps building the lists with a particular dataset in mind would yield \"better\" results).   \n\nThis is also good advice, and could make the \u201cattack\u201d more powerful.  Since that approach is more time consuming, for our initial effort we tried to create general malicious targets that should be applicable to a wide range of dialogue data.\n\nThanks for the review!\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621620540, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyNA5iRcFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper582/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper582/Authors|ICLR.cc/2019/Conference/Paper582/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers", "ICLR.cc/2019/Conference/Paper582/Authors", "ICLR.cc/2019/Conference/Paper582/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621620540}}}, {"id": "SklZWlF9nm", "original": null, "number": 2, "cdate": 1541210105387, "ddate": null, "tcdate": 1541210105387, "tmdate": 1541533867814, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "content": {"title": "An interesting paper", "review": "Main contribution: devising and evaluating an algorithm to find inputs that trigger arbitrary \"egregious\" outputs (\"I will kill you\") in vanilla sequence-to-sequence models, as a white-box attack on NLG models.\n\nClarity:\nThe paper is overall clear. I found some of the appendices (esp. B and C) to be important for understanding the paper and believe these should be in the main paper. Moving parts of Appendix A in the main text would also add to the clarity.\n\nOriginality:\nThe work looks original. It is an extension of previous attacks on seq2seq models, such as the targeted-keyword-attack from (Cheng et al., 2018) in which the model is made to produce a keyword chosen by the attacker.\n\nSignificance of contribution:\nThe lack of control over the outputs of seq2seq is a major roadblock towards their broader adoption. The authors propose two algorithms for trying to find inputs creating given outputs, a simple one relying on continuous optimization this is shown not to work (breaking when projecting back into words), and another based relying on discrete optimization. The authors found that the task is hard when using greedy decoding, but often doable using sampled decoding (note that in this case, the model will generate a different output every time). My take-aways are that the task is hard and the results highlight that vanilla seq2seq models are pretty hard to manipulate; however it is interesting to see that with sampling, models may sometimes be tricked into producing really bad outputs.\nThis white-box attack applicable to any chatbot. As the authors noted, an egregious output for one application (\"go to hell\" for customer service) may not be egregious for another one (\"go to hell\" in MT).\n\nOverall, the authors ask an interesting question: how easy is it to craft an input for a seq2seq model that will make it produce a \"very bad\" output. The work is novel, several algorithms are introduced to try to solve the problem and a comprehensive analysis of the results is presented. The attack is still of limited practicality, but this paper feels like a nice step towards more natural adversarial attacks in NLG.\n\nOne last thing: the title seems a bit misleading, the work is not about \"detecting\" egregious outputs.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "cdate": 1542234426993, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335757040, "tmdate": 1552335757040, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJgy7QFK37", "original": null, "number": 1, "cdate": 1541145366579, "ddate": null, "tcdate": 1541145366579, "tmdate": 1541533867610, "tddate": null, "forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "content": {"title": "Interesting study of an overlooked problem", "review": "# Positive aspects of this submission\n\n- This submission explores a very interesting problem that is often overlooked in sequence-to-sequence models research.\n\n- The methodology in Sections 4 and 5 is very thorough and useful.\n\n- Good comparison of last-h with attention representations, which gives good insight about the robustness of each architecture against adversarial attacks.\n\n# Criticism\n\n- In Section 3, even if the \"l1 + projection\" experiments seem to show that generating egregious outputs with greedy decoding is very unlikely, it doesn't definitely prove so. It could be that your discrete optimization algorithm is suboptimal, especially given that other works on adversarial attacks for seq2seq models use different methods such as gradient regularization (Cheng et al. 2018).\nSimilarly, the brute-force results on a simplified task in Appendix B are useful, but it's hard to tell whether the conclusions of this experiment can be extrapolated to the original dialog task.\nGiven that you also study \"o-greedy-hit\" in more detail with a different algorithm in Sections 4 and 5, I would consider removing Section 3 or moving it to the Appendix for consistency.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper582/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Detecting Egregious Responses in Neural Sequence-to-sequence Models", "abstract": "In this work, we attempt to answer a critical question: whether there exists some input sequence that will cause a well-trained discrete-space neural network sequence-to-sequence (seq2seq)  model to generate egregious outputs (aggressive, malicious, attacking, etc.). And if such inputs exist, how to find them efficiently. We adopt an empirical methodology, in which we first create lists of egregious output sequences, and then design a discrete optimization algorithm to find input sequences that will cause the model to generate them. Moreover, the optimization algorithm is enhanced for large vocabulary search and constrained to search for input sequences that are likely to be input by real-world users. In our experiments, we apply this approach to  dialogue response generation models trained on three real-world dialogue data-sets: Ubuntu, Switchboard and OpenSubtitles, testing whether the model can generate malicious responses. We demonstrate that given the trigger inputs our algorithm finds, a significant number of malicious sentences are assigned large probability by the model, which reveals an undesirable consequence of standard seq2seq training. ", "keywords": ["Deep Learning", "Natural Language Processing", "Adversarial Attacks", "Dialogue Response Generation"], "authorids": ["tianxing@mit.edu", "glass@mit.edu"], "authors": ["Tianxing He", "James Glass"], "TL;DR": "This paper aims to provide an empirical answer to the question of whether well-trained dialogue response model can output malicious responses.", "pdf": "/pdf/e36cde7753325e7b13fac318a2c1c1d1a7f27c13.pdf", "paperhash": "he|detecting_egregious_responses_in_neural_sequencetosequence_models", "_bibtex": "@inproceedings{\nhe2018detecting,\ntitle={Detecting Egregious Responses in Neural Sequence-to-sequence Models},\nauthor={Tianxing He and James Glass},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyNA5iRcFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper582/Official_Review", "cdate": 1542234426993, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyNA5iRcFQ", "replyto": "HyNA5iRcFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper582/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335757040, "tmdate": 1552335757040, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper582/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}