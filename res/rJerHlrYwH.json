{"notes": [{"id": "rJerHlrYwH", "original": "ryx5d_etvr", "number": 2284, "cdate": 1569439805117, "ddate": null, "tcdate": 1569439805117, "tmdate": 1577168262484, "tddate": null, "forum": "rJerHlrYwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5E4qNbTPl2", "original": null, "number": 1, "cdate": 1576798745237, "ddate": null, "tcdate": 1576798745237, "tmdate": 1576800890895, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Decision", "content": {"decision": "Reject", "comment": "The paper tackles the key question of achieving high prediction performances with few labels. The proposed approach builds upon Contrastive Predictive Coding (van den Oord et al. 2018). The contribution lies in i) refining CPC along several axes including model capacity, directional predictions, patch-based augmentation; ii) showing that the refined representation learned by the called CPC.v2 supports an efficient classification in a few-label regime, and can be transferred to another dataset; iii) showing that the auxiliary losses involved in the CPC are not necessarily predictive of the eventual performance of the network.\n\nThis paper generated a hot discussion. Reviewers were not convinced that the paper contributions are sufficiently innovative to deserve being published at ICLR. Authors argued that novelty does not have to lie in equations, and that the new ideas and evidence presented are worth. \n\nThe area chair thinks that the paper raises profound questions (e.g., what auxiliary losses are most conducive to learning a good representation; how to divide the computational efforts among the preliminary phase of representation learning and the later phase of classifier learning), but given the number of options and details involved, these results may support several interpretations besides the authors'. \n\nThe authors might also want to leave the claim about the generality of the CPC++ principles (e.g., regarding audio) for further work - or to bring additional evidence backing up this claim. \n\nIn conclusion, this paper contains brilliant ideas and I hope to see them published with a strengthened analysis of its components. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725089, "tmdate": 1576800276849, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Decision"}}}, {"id": "ryxqDXApcr", "original": null, "number": 4, "cdate": 1572885345866, "ddate": null, "tcdate": 1572885345866, "tmdate": 1574252414330, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The paper proposes to use Contrastive Predictive Coding (CPC), an unsupervised learning approach, to learn representations for further image classification. The authors show that using CPC for representation learning allows to achieve better results than other self-supervised methods. Moreover, CPC is shown to be useful for semi-supervised learning (on par with SOTA method), and transfer learning. All results are very impressive and is in-line with current trends of using a linear classifier on top of a deep feature extractor (e.g., Nalisnick et al., \"Hybrid Models with Deep and Invertible Features\"). The paper is rather well written and the results are convincing. However, The whole idea of the paper is based on the original paper:\n* Oord, Aaron van den, Yazhe Li, and Oriol Vinyals. \"Representation learning with contrastive predictive coding.\" arXiv preprint arXiv:1807.03748 (2018).\nTechnically speaking, the paper is outstanding, but it lacks novelty in terms of new ideas. I highly appreciate new results and new architectures, but it is not enough for a full conference paper.\n\nRemarks\n- In Section 2.1, the problem statement for Contrastive Predictive Coding (CPC) is unclear. For instance, the authors explain CPC by mentioning about masked convolutional layers that is unnecessary at this point. I understand that from engineering perspective it is crucial information, but it does not help to understand CPC. As a result, without knowing the original paper on CPC, Section 2.1 is hard to follow.\n\n- The paper can be treated as an uptaded version of the original CPC paper. I really appreciate all new results and implementation of the idea. The paper is well written and it is technically correct. However, I do not find much novelty compared to the original paper. This would be a perfect workshop contribution, but I am afraid that it is not enough for a full paper.\n\n==== AFTER REBUTTAL ====\nI would like to thank the reviewers for their rebuttal. It was not my intention to dismiss your effort in providing new technical results. Please forgive me if you read it in this way. My point is that the paper presents exactly the same idea as the original paper of CPC, but with new, very interesting results. However, I doubt if this is enough for a full conference paper. This point is debatable and I would be happy to further discuss it with other reviewers and the AC. At this point, I keep my original score, but of I am open for a discussion.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575397275001, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Reviewers"], "noninvitees": [], "tcdate": 1570237725029, "tmdate": 1575397275015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review"}}}, {"id": "SylzQECKor", "original": null, "number": 5, "cdate": 1573671962341, "ddate": null, "tcdate": 1573671962341, "tmdate": 1573671962341, "tddate": null, "forum": "rJerHlrYwH", "replyto": "Sye7g-moFH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for their comments. We respectfully disagree with the assessment that the \u201cnovelty and technical contributions are limited\u201d. Although the learning objective we use is the same as in (van den Oord, 2018), we make a number of changes to the training methodology without which the final performance would be uncomparable to the one we arrive at (70.6% Top-1 linear classification accuracy vs 48.7%). We ablate all of these changes and show how important they are for achieving state of the art results. This, combined with the fact that these modifications are very general (and could be straightforwardly applied to audio, video, and text; see footnote), make these technical contributions readily re-usable by the research community. We will open-source our implementation and pre-trained models to make these experimental insights widely accessible. \n\nWe agree that it would be interesting to re-evaluate the CPC model with architectures used in other works. These tend to differ widely across papers: (Tian, 2019) use ResNet-101, (Donahue & Simonyan, 2019) use RevNet-50 with 4x width, (Xie, 2019) use ResNet-50 whereas (Zhai, 2019) use ResNet-50 with 4x width. It is therefore difficult to chose a single architecture that will enable comparison to all prior works. Nonetheless, we will systematically list the architectures used by each method and include results from ResNet-50.\n\nThe inputs to the masked convolutional network are the feature vectors z_{i,j}. We will make this clear in the text, and provide a reference to the appendix in which this is made explicit."}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJerHlrYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2284/Authors|ICLR.cc/2020/Conference/Paper2284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143631, "tmdate": 1576860542241, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment"}}}, {"id": "SygZlNAYoS", "original": null, "number": 4, "cdate": 1573671913256, "ddate": null, "tcdate": 1573671913256, "tmdate": 1573671913256, "tddate": null, "forum": "rJerHlrYwH", "replyto": "HJxj6VX0tr", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for their comments. We agree that the modifications we bring to the CPC method are general enough to be applied to a variety of other modalities. For one, the observation that increasing the network depth and ease of optimization can strongly impact performance directly translates to other types of data. Data-augmentation has also become a standard technique in supervised learning, with a considerable amount of domain knowledge being accumulated regarding which techniques are useful for which modalities. Our observation that patch-level augmentation dramatically improves the performance of CPC applied to images could therefore be straightforwardly extended (using analogous augmentation techniques) to audio segments, video cubes, and natural language atoms. Similarly, increasing the number of predictions can easily be applied to other data. As such, since our modifications to the CPC methodology are general enough to be applied to all the modalities for which CPC was originally designed, we think calling it \u201cCPC v2\u201d is valid and warranted, but are curious to hear your suggestions in this matter. \n\nWe will make sure to make the figures printer-friendly in the final version."}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJerHlrYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2284/Authors|ICLR.cc/2020/Conference/Paper2284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143631, "tmdate": 1576860542241, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment"}}}, {"id": "BJeIT7RKiS", "original": null, "number": 3, "cdate": 1573671870033, "ddate": null, "tcdate": 1573671870033, "tmdate": 1573671870033, "tddate": null, "forum": "rJerHlrYwH", "replyto": "BJg06Ig69B", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for their comments. Regarding the first point \u201cMore discussions about why this four axes are investigated in image recognition,\u201d we agree that a better explanation of the relationship between the new training protocol and our original hypothesis is warranted. Our modifications to the original CPC model can be grouped into 3 categories. Increasing the network scale and ease of optimization both contribute to the representational capacity of the network and its ability to make the complex transformations across space more predictable. The next crucial modification, patch-based augmentation, allows us to control which features of the data will be made more predictable. By making low-level features (such as brightness, color, and contrast) less predictable, we ensure the network capacity is spent on making other features (including the more semantic ones of interest) more predictable. Finally, increasing the number of spatial directions used in the training task amplifies this learning signal. We will update the discussion of these points in section 4.1 to share these intuitions.\n\nOur original hypothesis stated that spatially predictable representations should better enable low-data classification. Through our ablation, we are able to titrate the amount of \u201cpredictability\u201d in the representation by changing the number of spatial directions included in the prediction task. For example, one model only attempts to predict patches from top to bottom. The next makes predictions in both vertical directions. The third in all four (horizontal and vertical) spatial directions. These models therefore learn to be \u201cpredictable\u201d along more and more axes of the data. In line with our hypothesis, representations which are more \u201cpredictable\u201d also enable better low-data classification. However, since these models also improve linear classification accuracy, we asked whether these two metrics were necessarily related to each other. This was not the case (they are uncorrelated across other model specifications, a novel finding in itself), and we therefore take this as evidence that more spatially predictable representations enable efficient classification.\n\nFinally, we agree that it would be interesting to re-evaluate the CPC model with architectures used in other works. Most of the methods we compare to in Table 3 use a ResNet-101, which is why we opted for that architecture. Nevertheless we will include results for ResNet-50 as you suggest in the final version, and report the architecture used by each method.\n\nTo conclude, we respectfully disagree with the assessment that this work is \u201csimply adjusting network architectures and training strategies, which makes it less interesting\u201d. Firstly, it is unexpected that the same objective, given a new training protocol, can result in dramatically better performance (from 48.7% to 70.6% linear classification accuracy). Without these results, one might tend to dismiss contrastive learning as impractical or ill-suited to downstream tasks. Furthermore, these modifications are sufficiently general to be applied to a variety of different methods and modalities, and our detailed ablations provide actionable recommendations to the community. We will open-source our implementation and pre-trained models to make these experimental insights widely accessible. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJerHlrYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2284/Authors|ICLR.cc/2020/Conference/Paper2284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143631, "tmdate": 1576860542241, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment"}}}, {"id": "rJlQ5mRtjB", "original": null, "number": 2, "cdate": 1573671819121, "ddate": null, "tcdate": 1573671819121, "tmdate": 1573671819121, "tddate": null, "forum": "rJerHlrYwH", "replyto": "ryxqDXApcr", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We would like to thank the reviewer for their comments on the manuscript. However, we find the decision to dismiss a \u201ctechnically outstanding\u201d paper simply because it does not introduce a new mathematical formalism to be rather mystifying. Rather than making ever more complex objectives, there is value in reminding the community of the sobering reality that implementation details are hugely important. Dissecting and highlighting the contributions of these details (as we do) will also facilitate the comparison of different self-supervised objectives in future work. To that end, our work makes a number of contributions, both methodological and experimental, which we think will be very impactful to the community.\n\nOn the methodological side, we identify a number of axes which enable the performance of CPC: network scale, local data augmentation, amount of self-supervision, etc. These insights are sufficiently general for them to inform other contrastive methods, and other modalities (e.g. audio and video have analogous forms of data-augmentation). Furthermore, it is an important experimental point to notice just how much these \u201cimplementation details\u201d matter. Without them, one might dismiss contrastive learning altogether. With them, they appear to be one of the most promising methods for representation learning. We will open-source our implementation and pre-trained models to make these techniques widely accessible. \n\nMoreover, we believe our empirical results to represent a landmark in representation learning: we have shown it to enable substantial gains in data-efficiency for all amounts of available data (as opposed to in only the low-data regime). For the first time, it appears beneficial to train supervised networks on top of learned representations rather than pixel lattices. Going further, our results in transfer learning (which approach that of supervised transfer) raise the possibility of removing the need for large-scale labeled datasets altogether."}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJerHlrYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2284/Authors|ICLR.cc/2020/Conference/Paper2284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143631, "tmdate": 1576860542241, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment"}}}, {"id": "Byl00MAtoS", "original": null, "number": 1, "cdate": 1573671637606, "ddate": null, "tcdate": 1573671637606, "tmdate": 1573671637606, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment", "content": {"title": "New ideas and findings in this work", "comment": "We would like to thank the reviewers for their perspective on the manuscript. The main criticism lies with the novelty of our contributions. We disagree with this assessment, for although we do not present any new objective or equation, we present a series of new ideas and findings in this work:\n\n- Representation learning (and CPC in particular) enables unseen gains in the data-efficiency of image classifiers (same performance as purely supervised, with 2-5x less labels) for all amounts of available data (as opposed to only in the low-data regime). \n- Representations learned without supervision (with CPC) can rival the performance of supervised representations for transfer learning (to PASCAL). \n- The performance of CPC greatly depends on a variety of implementation details, whose contributions we dissect and highlight, providing important insights to the representation learning community. We will open-source our implementation and pre-trained models to make these techniques widely accessible.\n- We show that linear classification and low-labeled data classification are not necessarily predictive of each other, motivating the two as independent benchmarks for representation learning.\n\nWe identified a number of axes which enable the performance of CPC: network scale, local data augmentation, amount of self-supervision, etc. Following these axes, we have improved our model since the submission, attaining 70.6% Top-1 linear classification accuracy on ImageNet (the original CPC attained 48.7%), setting a new state-of-the-art.\n\nTaken together, our experimental and methodological contributions introduce and defend the idea that representation learning, and CPC in particular, are ready for real-world application.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJerHlrYwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2284/Authors|ICLR.cc/2020/Conference/Paper2284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143631, "tmdate": 1576860542241, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Authors", "ICLR.cc/2020/Conference/Paper2284/Reviewers", "ICLR.cc/2020/Conference/Paper2284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Comment"}}}, {"id": "Sye7g-moFH", "original": null, "number": 1, "cdate": 1571660010836, "ddate": null, "tcdate": 1571660010836, "tmdate": 1572972358783, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper improves Contrastive Predictive Coding method and reaches a good performance in several downstream tasks. However, the novelty and technical contributions are limited.\n\nStrengths: \n+ The experimental results seem good. The reimplemented CPC v2 performs much better than the original version. And the performance of down-stream tasks is comparable or better than the state-of-the-art methods.\n+ The paper is well written. The paper structure is clear and figures are well illustrated.\n+ Figure 3 shows clearly the performance improvements of a series of incremental modifications to the original CPC methods.\n\nWeaknesses:\n- The novelty and technical contributions are limited. This paper only proposes some minor improvements based on the original CPC method and use a deeper network to get better performance. The proposed method lacks of important insights for the research community.\n- The capacity of network architecture is crucial for self-supervised learning. But in Table 1,2,3, the network architecture of the proposed method is deeper than that in the comparison methods, which is unfair for the comparison methods. Meanwhile, the network architectures of many compared methods are not listed in the tables, which may be misleading. For example, Unsupervised Data Augmentation (Xie et al., 2019) in table 2 and Instance Discrimination (Wu et al., 2018) in table 3 use ResNet50, which is much more shallow than ResNet-161 in this paper.\n- In section 2.1, the paper doesn't describe clearly what's the input of masked convolutional network $g_{\\phi}$ and how to calculate $c_{i, j}$.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575397275001, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Reviewers"], "noninvitees": [], "tcdate": 1570237725029, "tmdate": 1575397275015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review"}}}, {"id": "HJxj6VX0tr", "original": null, "number": 2, "cdate": 1571857603131, "ddate": null, "tcdate": 1571857603131, "tmdate": 1572972358747, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors augment contrastive predictive coding (CPC), a recent representation learning technique organized around making local representations maximally useful for predicting other nearby representations, and evaluates their augmented architecture in several image classification problems. Although the modifications to CPC aren't particularly original, the authors show first that these yield a significant improvement in linear classification accuracy. They then use this improved model to obtain impressive performance in classification within semi-supervised and transfer learning settings, giving strong support for the use of such methods within image processing applications.\n\nPros:\nOwing to its generality (CPC assumes only a weak spatial prior in the input data), and cheap computational cost relative to earlier generative approaches, CPC is already a promising unsupervised representation learning technique. The paper gives more evidence of this usefulness for image data, yielding leading performance on several different image classification benchmarks.\n\nThe authors also make the observation that linear separability, the standard benchmark for evaluating unsupervised representations, correlates poorly with efficient prediction in the presence of limited labeled data. This observation should be of interest in the broader community, and points to the need for more diverse metrics for unsupervised representations.\n\nCons:\nThe improvements given in the paper are quite useful within their stated domain (image data), but aren't directly applicable to other types of input data. Although the authors make a point of emphasizing the relevance of CPC for other problem domains, they don't currently provide any suggestions for how this current work could be generalized to handle these other cases. In this sense, I think it is a bit deceptive to refer to their model as \"CPC v2\", as the majority of their changes have no bearing on the intrinsic CPC algorithm itself.\n\nI am sure that some of the methods used here could lead to improvements in the use of CPC for other types of data, but the authors currently don't provide any insight on this issue. In line with that, I think their work would be improved by some commentary on this, in particular by any concrete suggestions they have about how similar augmentations to CPC could be carried out in text, audio, and/or video data.\n\nVerdict:\nOwing to the reasons given above, I recommend acceptance.\n\nMinor suggestions:\nPlease use a different color scheme for your figures that is still meaningful if the paper is printed in greyscale."}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575397275001, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Reviewers"], "noninvitees": [], "tcdate": 1570237725029, "tmdate": 1575397275015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review"}}}, {"id": "BJg06Ig69B", "original": null, "number": 3, "cdate": 1572828870230, "ddate": null, "tcdate": 1572828870230, "tmdate": 1572972358700, "tddate": null, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "invitation": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Title: DATA-EFFICIENT IMAGE RECOGNITION\n[Summary]\n-This paper introduces Contrastive Predictive Coding (CPC) image recognition in the data-efficient regime. Concretely, the authors improve CPC in terms of its architecture and training strategy. The extensive experiments show that CPC enables data-efficient image classification and surpassed other unsupervised approaches. \n\n[Pros]\n- Although the CPC was proposed and evaluated in vision task in [1], a new implementation of CPC with dramatically-improved ability is presented in this paper.\n- The CPC is utilized to enhance spatially predictable representations which benefits a lot data-efficient image recognition.\n\n[Cons]\n- In Sec. 4.1, four axes are identified to upgrade CPC v1 to CPC v2. But they are not well motivated. More discussions about why this four axes are investigated in image recognition.\n\n-The core idea is motivated by a critical hypothesis that good representations should make spatio-temporal variability in natural signals more predictable. However, this hypothesis is not well verified. The concept of amount of \u2018predictability\u2019 in page 7 is not clear. It would be great if you provide more evidence that the improvement in low-data classification results from the increased \u2018predictability\u2019.\n\n- The comparison in Sec. 4.3 seems unfair. The pretrain model trained with different methods should be the same. For example, the Faster RCNN trained on CPC v2 uses ResNet-101 as backbone but Local Aggregation method uses ResNet-50.\n\n[Summary]\n- This work extends CPP to data-efficient image recognition by simply adjusting network architectures and training strategies, which makes it less interesting. Besides, the major hypothesis is not well validated. \n- The experimental results are convincing and encouraging. Some minor flaws such as unfair comparison should be fixed.\n- I want to see how the four axes in Sec. 4.1 are related to core motivation (more predictable) since they are major adjustments from CPP v1 to CPP v2. If the author provides a profound explanation of the problem, I would consider changing the rating. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2284/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henaff@google.com", "aravind@cs.berkeley.edu", "defauw@google.com", "alirazavi@google.com", "doersch@google.com", "aeslami@google.com", "avdnoord@google.com"], "title": "Data-Efficient Image Recognition with Contrastive Predictive Coding", "authors": ["Olivier J Henaff", "Aravind Srinivas", "Jeffrey De Fauw", "Ali Razavi", "Carl Doersch", "S. M. Ali Eslami", "Aaron van den Oord"], "pdf": "/pdf/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "TL;DR": "Unsupervised representations learned with Contrastive Predictive Coding enable data-efficient image classification.", "abstract": "Human observers can learn to recognize new categories of objects from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable, as suggested by recent perceptual evidence. We therefore revisit and improve Contrastive Predictive Coding, a recently-proposed unsupervised learning framework, and arrive at a representation which enables generalization from small amounts of labeled data. When provided with only 1% of ImageNet labels (i.e. 13 per class), this model retains a strong classification performance, 73% Top-5 accuracy, outperforming supervised networks by 28% (a 65% relative improvement) and state-of-the-art semi-supervised methods by 14%. We also find this representation to serve as a useful substrate for object detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset.", "keywords": ["Deep learning", "representation learning", "contrastive methods", "unsupervised learning", "self-supervised learning", "vision", "data-efficiency"], "paperhash": "henaff|dataefficient_image_recognition_with_contrastive_predictive_coding", "original_pdf": "/attachment/760ff8205c69d78094ed1c5804a7d20ffe475c69.pdf", "_bibtex": "@misc{\nhenaff2020dataefficient,\ntitle={Data-Efficient Image Recognition with Contrastive Predictive Coding},\nauthor={Olivier J Henaff and Aravind Srinivas and Jeffrey De Fauw and Ali Razavi and Carl Doersch and S. M. Ali Eslami and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=rJerHlrYwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJerHlrYwH", "replyto": "rJerHlrYwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575397275001, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2284/Reviewers"], "noninvitees": [], "tcdate": 1570237725029, "tmdate": 1575397275015, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2284/-/Official_Review"}}}], "count": 11}