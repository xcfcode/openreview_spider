{"notes": [{"id": "rJ4km2R5t7", "original": "SJl9k0TrYQ", "number": 1323, "cdate": 1538087959516, "ddate": null, "tcdate": 1538087959516, "tmdate": 1550878328801, "tddate": null, "forum": "rJ4km2R5t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rklUcATSxN", "original": null, "number": 1, "cdate": 1545096845705, "ddate": null, "tcdate": 1545096845705, "tmdate": 1545354479545, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Meta_Review", "content": {"metareview": "This paper provides an interesting benchmark for multitask learning in NLP.\nI wish the dataset included language generation tasks instead of just classification but it's still a step in the right direction.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Multitask learning is one of the most important problems in AI"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1323/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352880860, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352880860}}}, {"id": "S1g4cw9Ohm", "original": null, "number": 1, "cdate": 1541085068404, "ddate": null, "tcdate": 1541085068404, "tmdate": 1542998643440, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "content": {"title": "A timely and useful resource", "review": "This paper introduces the General Language Understanding Evaluation (GLUE) benchmark and platform, which aims to evaluate representations of language with an emphasis on generalizability. This is a timely contribution and GLUE will be an impactful resource for the NLP community. This is mitigated, perhaps, somewhat by the recent release of decaNLP. But, as discussed the authors, this has a different focus (re-framing all tasks as QQ) and further does not feature the practical tools released here (leaderboard, error analysis) that will help drive progress.\n\nSome comments below. \n\n- The inclusion of the small diagnostic dataset was a nice addition and it would be nice if future corpora included similar. \n\n- Implicit in this and related efforts is the assumption that parameter sharing ought to be possible and fruitful across even quite diverse tasks. While I do not object to this, it would be nice if the authors could make an explicit case here as to why should we believe this to be the case.\n\n- The proposed platform is touted as one of the main contributions here, but not pointed to -- I assume for anonymity preserving reasons, but still would have been nice for this to be made explicit. \n\n- I would consider pushing Table 5 (Appendix) into the main text. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "cdate": 1542234255054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921822, "tmdate": 1552335921822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlblc2e0Q", "original": null, "number": 6, "cdate": 1542666728965, "ddate": null, "tcdate": 1542666728965, "tmdate": 1542675204999, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "S1g4cw9Ohm", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your review!\n\nWe agree that the diagnostic data is a key contribution of our work. We wanted to not only have an application-driven measure of progress, but also a targeted measure of performance on specific natural language phenomena that we would expect a general-purpose NLU model to handle well. \n\nRegarding parameter sharing, our intent was to include tasks with very little training data such that automated systems could not do well learning on just those tasks\u2019 data. Competitive systems, then, would need to include some form of knowledge-sharing from outside data.  In only requiring model predictions to evaluate on test, we wanted to avoid restricting future research to any particular paradigm of knowledge sharing. We use multi-task learning and parameter sharing because it is a straightforward baseline with lots of precedent (GenSen, Collobert and Weston, etc.), so we thought it necessary to include.\n\nCould you please clarify how a small *test* set would encourage few-shot learning? To the best of our knowledge, few-shot learning is when you have a small *training* set for the target task.\n\nRe: table 5, we agree! We\u2019ll post an updated version of the paper shortly.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613747, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJ4km2R5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1323/Authors|ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613747}}}, {"id": "B1xRaF2lR7", "original": null, "number": 5, "cdate": 1542666693646, "ddate": null, "tcdate": 1542666693646, "tmdate": 1542666693646, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "ryeyISNYhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "content": {"title": "Author response", "comment": "Thanks for your thoughtful review!\n\nWhile we agree that more analysis would be nice, the central contribution of our paper is to motivate and introduce the benchmark. Thus, our experiments are designed to give baseline numbers for a broad range of models and to highlight the benefits of our design decisions, in order to make the case that our benchmark offers useful improvements over previous evaluation standards like SentEval. \n\nRegarding the diagnostic data, we do believe much of the information you mention is present in the paper. For example, we explicitly give the class distribution for the entire dataset (including statistics by category is a good point - we will soon add the class distribution per coarse-grained category), expert annotator agreement (high, at \\kappa = 0.73), and human performance (R_3 = 0.8 versus 0.28 for the best model). The last statistic in particular indicates that these examples are understandable and solvable by humans and challenging for existing models. These numbers are in-line with other semantic datasets that have been productively used by the community, for example SQuADv2 (humans get ~87 EM); SimLex-999 (0.67 correlation); WordSimilarity-353 (.61 correlation). We agree that human annotations are not perfect, but perfect annotations don\u2019t exist, and datasets can still be useful even when their human annotations are a little noisy, as in the previously mentioned examples.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613747, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJ4km2R5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1323/Authors|ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613747}}}, {"id": "HklEfYnxR7", "original": null, "number": 4, "cdate": 1542666508452, "ddate": null, "tcdate": 1542666508452, "tmdate": 1542666508452, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "S1gHMCM0nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your review!\n\nTo clarify, GLUE is not a benchmark for language modeling (the task of modeling the probability of a piece of text) but rather (classification or regression based) natural language understanding. \n\nRegarding previous work in this space, we mention in the paper what we believe are the two major comparable works: SentEval and DecaNLP, and highlight the benefits over these neighbors afforded by our design decisions.  As your review hints at, we believe that a major benefit of a well-designed benchmark is that it can more clearly distinguish models that make significant improvements and thereby incentivize researchers to work on it. Early results on GLUE seem to suggest that we have been successful in that regard.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613747, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJ4km2R5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1323/Authors|ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613747}}}, {"id": "S1gHMCM0nX", "original": null, "number": 3, "cdate": 1541447181332, "ddate": null, "tcdate": 1541447181332, "tmdate": 1541533234877, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "content": {"title": "Interesting new benchmark", "review": "Summary:\nGLUE is a benchmark consisting of multiple natural language understanding tasks\nthat functions via uploading to a website and receiving a score based on\nprivately held test set labels.\nTasks include acceptability judgement, sentiment prediction, semantic equivalence\ndetection, judgement of premise hypothesis entailment, question paragraph pair\nmatching, etc..\nThe benchmark also includes a diagnostic dataset with logical tasks such as\nlexical entailment and understanding quantifiers.\n\nIn addition to presenting the benchmark itself, the paper also presents models\nfor performance baselines.\nThere is some brief analysis of the ability of Sentence2Vector vs. more complex\nmodels with e.g. attention mechanisms and of single-task vs. multi-task training.\n\nEvaluation:\nThe GLUE benchmark seems like a well designed benchmark that could potentially\nignite new progress in the area of NLU.\nBut since I'm not an expert in the area of language modeling and know almost\nnothing about existing benchmarks I cannot validate the added benefit over\nexisting benchmarks and the novelty of the suggested benchmarking approach.\n\nDetails:\nThe paper is well written, clear and easy to follow.\n\nThe proposed benchmarks seem reasonable and illustrate the difficulty of\nbenchmark tasks that involve logical structure.\n\nPage 5: showing showing (Typo)\n", "rating": "7: Good paper, accept", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "cdate": 1542234255054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921822, "tmdate": 1552335921822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeyISNYhQ", "original": null, "number": 2, "cdate": 1541125447334, "ddate": null, "tcdate": 1541125447334, "tmdate": 1541533234675, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "content": {"title": "Weak reject", "review": "The paper proposes a new benchmark for natural language understanding: GLUE. Models will be evaluated based on a diverse set of existing language understanding tasks which encourages the models to learn shared knowledge across different tasks. The authors empirically show that models trained with multiple tasks in the dataset perform better than models that focused on one specific task. They also point out existing methods are not able achieve good performance in this dataset and request for more general natural language understanding system. The work also collects an expert evaluated diagnostic evaluation dataset for further examination for the models.\n\nQuality: borderline, clarity:good, originality: borderline, significance: good,\n\nPros:\n- The benchmark is set up in a online platform with leaderboard which can be easily accessible to people.\n- The benchmark comes with a diagnostic evaluation dataset with coarse-grained and fine-grained categories that examine different aspect of language understanding abilities.\n- Baseline results for major existing models are provided\n\nCons:\n- The author should provide more detailed analysis and interpretable explanations for the results as opposed to simply stating that the overall performance is better.\nFor example, why attention hurts performance in single task training? Why multi-tasks training actually leads to worse performance on some of the dataset? Do these phenomenons still exist if you train on a different subset of the dataset?\nWhat are the samples that the models failed to perform well? It would be nice to get some more insights and conclusions based on the results obtained from this benchmark to shed some lights on how to improve these models. The results section should be seriously revised.\n\n- The diagnostic evaluation dataset seems to be a way to better understand the model, however, it is hard to see the scope of the data (are the samples under each categories balanced?). Besides, the examples in the dataset seems very confusing even for humans (Table 2).  The evaluation with NLP expert is also far from perfect. I wonder how accurate is this dataset annotated (or even the sentences make sense or not), and how suitable it is for evaluating model\u2019s language understanding abilities. It would be nice if the authors can include some statistics about the dataset.\n\nThe paper proposes a useful benchmark that measures different aspects of language understanding abilities which would be helpful to the community. However, I feel the novelty or take away messages from the experiment section is limited. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Review", "cdate": 1542234255054, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335921822, "tmdate": 1552335921822, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eZR9Jhq7", "original": null, "number": 1, "cdate": 1539205832825, "ddate": null, "tcdate": 1539205832825, "tmdate": 1539205832825, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "BklRvvUFcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "content": {"title": "Clarification Answers", "comment": "Hi Quan,\nThanks for using GLUE! Regarding your questions:\n\n- SST-2, QNLI, RTE, and WNLI are all roughly balanced, so using accuracy is reasonable here (and has been used as the evaluation metric in the past). We use F1 for datasets with class imbalances, and also to maintain comparability with previous work on those datasets.\n\n- For QNLI, each example consists of the original question and a single sentence from the context paragraph that was originally paired with that question. The task is to determine if that context sentence contains an answering span to the question. If I understand correctly, it's the latter of the two options you mentioned, but we do some additional filtering of question, context sentence pairs that are too easy (due to low lexical overlap).\n\nLet us know if you have any additional questions."}, "signatures": ["ICLR.cc/2019/Conference/Paper1323/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613747, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJ4km2R5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1323/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1323/Authors|ICLR.cc/2019/Conference/Paper1323/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613747}}}, {"id": "BklRvvUFcQ", "original": null, "number": 1, "cdate": 1539037030402, "ddate": null, "tcdate": 1539037030402, "tmdate": 1539037030402, "tddate": null, "forum": "rJ4km2R5t7", "replyto": "rJ4km2R5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper1323/Public_Comment", "content": {"comment": "Thank you for the paper and the dataset!\n\nI'm using GLUE in my own research and would like to ask a few clarification questions. \n\nIn Table 1, why is it that only accuracy (and not F1) is used to measure performance on SST-2, QNLI, RTE and WNLI?\n\nFor QNLI, in the sentence, \"The task is to determine whether the context sentence contains the answer to the question.\", is the task is to then:\n\n- Determine whether the answer is contained in any context sentence. The label for a (context, question) pair would then be binary, where 1 indicates at least one sentence in the context contains the answer to the question and 0 otherwise.\nOR\n- Determine the sentence that contains the answer out of all the sentences in the context passage. The label for each sentence in the context passage would then be binary, i.e. a sentence is assigned a gold label of 0 if the answer is not part of the sentence and 1 if it is. \n\n ", "title": "Clarification Questions"}, "signatures": ["~quan_vuong1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1323/Reviewers/Unsubmitted"], "writers": ["~quan_vuong1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "abstract": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.", "keywords": ["natural language understanding", "multi-task learning", "evaluation"], "authorids": ["alexwang@nyu.edu", "amanpreet@nyu.edu", "julianjm@cs.washington.edu", "felixhill@google.com", "omerlevy@cs.washington.edu", "bowman@nyu.edu"], "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "TL;DR": "We present a multi-task benchmark and analysis platform for evaluating generalization in natural language understanding systems.", "pdf": "/pdf/e661931af788bb41220e35ab989a6e051b9e602b.pdf", "paperhash": "wang|glue_a_multitask_benchmark_and_analysis_platform_for_natural_language_understanding", "_bibtex": "@inproceedings{\nwang2018glue,\ntitle={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\nauthor={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJ4km2R5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1323/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311625149, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJ4km2R5t7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1323/Authors", "ICLR.cc/2019/Conference/Paper1323/Reviewers", "ICLR.cc/2019/Conference/Paper1323/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311625149}}}], "count": 10}