{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392735600000, "tcdate": 1392735600000, "number": 1, "id": "A8K4fdGEbvfo0", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "wxobw18IYOxu4", "replyto": "Fy6TFRhAT_wUd", "signatures": ["Arto Klami"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "See the reply for Anonymous 9dec -- we addressed also your remarks in a joint response."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Group-sparse Embeddings in Collective Matrix Factorization", "decision": "submitted, no decision", "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.", "pdf": "https://arxiv.org/abs/1312.5921", "paperhash": "klami|groupsparse_embeddings_in_collective_matrix_factorization", "keywords": [], "conflicts": [], "authors": ["Arto Klami", "Guillaume Bouchard", "Abhishek Tripathi"], "authorids": ["arto.klami@gmail.com", "guillaume.bouchard.free.fr@gmail.com", "abhishektripathi.at@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392735540000, "tcdate": 1392735540000, "number": 1, "id": "njaTjD3NdxzcC", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "wxobw18IYOxu4", "replyto": "Ki1mKRAQP7Cdb", "signatures": ["Arto Klami"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "This is a joint response for both Anonymous 9dec and 75c5, since both  reviews address similar issues.\r\n\r\n\r\nWe thank both reviewers for pointing out the typos, and in particular the sloppy phrase used to describe basics of VB approximation. We have submitted a revised version that fixes these mistakes, as well as addresses the two other issues described below. It should be out by Feb 19th.\r\n\r\n\r\n1) Regarding complexity: While several recent advances were indeed needed to derive the model, the resulting algorithm is reasonably straightforward. To address this issue we now added more detailed description of the algorithm in the Supplementary material. Furthermore, we will later add a link to an open source implementation in R (the documentation still needs a bit of polishing).\r\n\r\n\r\n2) Regarding the scale of the experiments: The artificial data experiments are small mainly to emphasize the effects of jointly modelling multiple matrices. Similarly to how e.g. multi-task learning helps most with small sample sizes, properly modeling the private factors is more important when (some of) the entity sets are small -- given enough data even incorrectly specified models often work reasonably well. Having said that, scalable algorithms are needed especially in settings where we have few examples in important views (i.e. views we want to predict) and many examples for auxiliary data matrices.\r\n\r\nEven though scalability has not been our main focus, the recommender system application in Section 7.3 briefly illustrates the efficiency. In the revised version we now me mention that both of the recommender system data sets are in the order of 1 million observations and the results were obtained in a few minutes on a laptop; this is comparable to the computation time of the competing convex CMF (CCMF) method for a single set of regularization parameters, but CCMF needs cross-validation over two such parameters do be used in practice.\r\n\r\nWe have not tried the algorithm on massive collections, but we expect it could be scaled up fairly nicely with a bit of implementation effort. The slowest part is the gradient computation that would parallelize easily, and switching to stochastic gradients would also be possible. Note also that the time complexity of the proposed VB algorithm is effectively the same as the corresponding MAP problem: the small overhead is only due to the computation of the parameters of the diagonal covariance matrices, the updates of the mean being similar to the updates of the MAP algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Group-sparse Embeddings in Collective Matrix Factorization", "decision": "submitted, no decision", "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.", "pdf": "https://arxiv.org/abs/1312.5921", "paperhash": "klami|groupsparse_embeddings_in_collective_matrix_factorization", "keywords": [], "conflicts": [], "authors": ["Arto Klami", "Guillaume Bouchard", "Abhishek Tripathi"], "authorids": ["arto.klami@gmail.com", "guillaume.bouchard.free.fr@gmail.com", "abhishektripathi.at@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391811840000, "tcdate": 1391811840000, "number": 2, "id": "Fy6TFRhAT_wUd", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "wxobw18IYOxu4", "replyto": "wxobw18IYOxu4", "signatures": ["anonymous reviewer 75c5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Group-sparse Embeddings in Collective Matrix Factorization", "review": "Collective matrix factorization (CMF) is a method for learning entity embeddings by jointly factorizing a collection of matrices, each of which describes a relationship between entities of two types. The set of rows/columns of a matrix corresponds to an entity type, while each row/column of the matrix corresponds to a different entity of the type. This approach assumes that all dimensions of the embeddings of entities of a particular type are relevant for modelling all the relationships they are involved in. This paper extends CMF by relaxing this assumption and allowing embeddings to have dimensions used for modelling only some of the relationships the entities are involved in. This is achieved by extending the model to include a separate precision for each embedding dimension of each type to encourage group-sparse solutions. The authors propose training the resulting models using a variational Bayes algorithm and show how to handle both Gaussian and non-Gaussian observations.\r\n\r\nThe paper is nicely written and makes a small but novel extension to CMF. The resulting approach is simple, and seems scalable and widely applicable. The experiments are fairly small-scale but are sufficient for illustrating the advantages of the method.\r\n\r\nCorrections:\r\nIn the section dealing non-Gaussian observations, the pseudo-data is referred to as Y instead of Z_m.\r\n\r\nThe description of variational Bayes as 'minimizing the KL divergence between the observation probability and ...' is not quite right, as it seems to describe KL(P||Q) instead of KL(Q||P).\r\n\r\nSection 8: 'unability' -> 'inability'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Group-sparse Embeddings in Collective Matrix Factorization", "decision": "submitted, no decision", "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.", "pdf": "https://arxiv.org/abs/1312.5921", "paperhash": "klami|groupsparse_embeddings_in_collective_matrix_factorization", "keywords": [], "conflicts": [], "authors": ["Arto Klami", "Guillaume Bouchard", "Abhishek Tripathi"], "authorids": ["arto.klami@gmail.com", "guillaume.bouchard.free.fr@gmail.com", "abhishektripathi.at@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391730660000, "tcdate": 1391730660000, "number": 1, "id": "Ki1mKRAQP7Cdb", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "wxobw18IYOxu4", "replyto": "wxobw18IYOxu4", "signatures": ["anonymous reviewer 9dec"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Group-sparse Embeddings in Collective Matrix Factorization", "review": "The manuscript articulates a problem with earlier solutions to the Collective Matrix Factorization (CMF) problem in multi-view learning formulations and proposes a novel solution to address the issue. The concern is that current CMF schemes ignore the potential for view-specific structure or noise by implicitly assuming that structure must be shared among all views. The authors solve this problem by putting group-sparse priors on the columns of the matrices. This allows private factors that can be specific to one or even a subset of the matrices. Also note that the use of variational Bayesian learning by the authors provides a large reduction in computational complexity relative to the MAP estimates used in some of the prior literature.\r\n\r\nI agree with the importance of the problem being addressed since, clearly, the need to accommodate view- or subset-specific structure is going to be important in many real-world problems. Also noted is the elimination of the need for tunable regularization parameters.\r\n\r\nThere are a couple of typos in the first paragraph of section 4.2 (top right of page 4). The manuscript is heavily dependent on several of its sources for implementation details of the complete algorithm. This isn't a criticism, since the authors should not repeat details available elsewhere, but I think it is important to understand that this is necessitated by the complexity of the method, and this complexity is a small drawback and potentially an area for future improvement.\r\n\r\nAnother issue is that it would be valuable to see the proposed scheme compared to a wider variety of alternatives in the experimental section (mostly for context that elucidates the importance of CMF itself and therefore their improvement of it for certain applications). However, given the scope of the paper in general, this is a minor point."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Group-sparse Embeddings in Collective Matrix Factorization", "decision": "submitted, no decision", "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.", "pdf": "https://arxiv.org/abs/1312.5921", "paperhash": "klami|groupsparse_embeddings_in_collective_matrix_factorization", "keywords": [], "conflicts": [], "authors": ["Arto Klami", "Guillaume Bouchard", "Abhishek Tripathi"], "authorids": ["arto.klami@gmail.com", "guillaume.bouchard.free.fr@gmail.com", "abhishektripathi.at@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387929720000, "tcdate": 1387929720000, "number": 66, "id": "wxobw18IYOxu4", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "wxobw18IYOxu4", "signatures": ["arto.klami@gmail.com"], "readers": ["everyone"], "content": {"title": "Group-sparse Embeddings in Collective Matrix Factorization", "decision": "submitted, no decision", "abstract": "CMF is a technique for simultaneously learning low-rank representations based on a collection of matrices with shared entities. A typical example is the joint modeling of user-item, item-property, and user-feature matrices in a recommender system. The key idea in CMF is that the embeddings are shared across the matrices, which enables transferring information between them. The existing solutions, however, break down when the individual matrices have low-rank structure not shared with others. In this work we present a novel CMF solution that allows each of the matrices to have a separate low-rank structure that is independent of the other matrices, as well as structures that are shared only by a subset of them. We compare MAP and variational Bayesian solutions based on alternating optimization algorithms and show that the model automatically infers the nature of each factor using group-wise sparsity. Our approach supports in a principled way continuous, binary and count observations and is efficient for sparse matrices involving missing data. We illustrate the solution on a number of examples, focusing in particular on an interesting use-case of augmented multi-view learning.", "pdf": "https://arxiv.org/abs/1312.5921", "paperhash": "klami|groupsparse_embeddings_in_collective_matrix_factorization", "keywords": [], "conflicts": [], "authors": ["Arto Klami", "Guillaume Bouchard", "Abhishek Tripathi"], "authorids": ["arto.klami@gmail.com", "guillaume.bouchard.free.fr@gmail.com", "abhishektripathi.at@gmail.com"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 5}