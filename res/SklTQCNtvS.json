{"notes": [{"id": "SklTQCNtvS", "original": "BJlPf1LuDH", "number": 1054, "cdate": 1569439268536, "ddate": null, "tcdate": 1569439268536, "tmdate": 1583912019731, "tddate": null, "forum": "SklTQCNtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "TeALElz3OC", "original": null, "number": 1, "cdate": 1583680579116, "ddate": null, "tcdate": 1583680579116, "tmdate": 1583680641586, "tddate": null, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Public_Comment", "content": {"title": "Sign black-box attack", "comment": "Nice work! Looks related to our probability black-box attack: \"Black-box Adversarial Attacks on Video Recognition Models\" (https://arxiv.org/abs/1904.05181), where we use NES to estimate the sign of the gradients for efficient black-box attack. But our estimation is based on the probabilities. Would be nice to see some discussions."}, "signatures": ["~Xingjun_Ma1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xingjun_Ma1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklTQCNtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504200310, "tmdate": 1576860588317, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference/Paper1054/Reviewers", "ICLR.cc/2020/Conference/Paper1054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Public_Comment"}}}, {"id": "Pk5u_ZzrB", "original": null, "number": 1, "cdate": 1576798713368, "ddate": null, "tcdate": 1576798713368, "tmdate": 1576800923088, "tddate": null, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The reviewers had several concerns with the paper related to novelty and comparisons with other approaches. During the discussion phase, these concerns were adequately addressed.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721092, "tmdate": 1576800272052, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Decision"}}}, {"id": "ryl-CgIwiB", "original": null, "number": 3, "cdate": 1573507273327, "ddate": null, "tcdate": 1573507273327, "tmdate": 1573512637610, "tddate": null, "forum": "SklTQCNtvS", "replyto": "HJxBPlpEFB", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for valuable comments and suggestions.\n\nAbout the L-smoothness: This assumption is common for convergence analysis of both first-order and zeroth-order methods for nonconvex optimization, e.g., [3,4] since it is a common starting point to bound the stationary gap. And we couldn\u2019t prove g(theta) is continuous for general deep neural networks. It\u2019s possible that the g(theta) may not be continuous; for example, we think it might be possible to construct some counter-examples using ReLU activation. However, although the assumption may not hold for DNN globally, our algorithm still performs well in practice. What we can assure is that if $g(\\cdot)$ has a Lipschitz continuous gradient, our algorithm has such a theoretical guarantee. This is indeed a sufficient but not necessary condition.\n\n\n\nAbout the analytical results: \nFor conventional RGF methods such as ZO-SGD [1] and ZO-stochastic mirror descent [2], the convergence error is typically given by E[|\\nabla f(x)|_2^2]=O(\\sqrt{d}/\\sqrt{T}). For ZO based sign-SGD, it was shown in [4] that the rate O(\\sqrt{d}/\\sqrt{T}) is actually improved since the error is bounded by the non-squared gradient norm E[|\\nabla f(x)|_2] = O(\\sqrt{d}/\\sqrt{T} + d/\\sqrt{q}) at the cost that sign-based ZO method converges to a neighborhood of a solution, whose size is determined by d and q. The work [4] showed that the query efficiency of generate black-box attacks seems a more important metric than the accuracy of the attack, since one cares more about whether or not the attack can succeed as early as possible. Compared to ZO-SignSGD, our rate yields similar pros and cons in Big O notations. However, we observe that it works much better than ZO-SignSGD in all of our experiments. More importantly, ZO-SignSGD was not designed for hard-label black-box attack, and used a more involved query oracle.  Furthermore, if we apply single query oracle to improve Liu's method,  since Liu's method takes sign for each element of the query direction, then it cannot fully utilize the single query oracle and leads to suboptimal convergence. To show this, in Figure 5 (b) we compare with Liu+Single-query-oracle and show Sign-OPT converges faster and achieves a better solution. We have added this comparison in the revision.\n\n\n[1]S. Ghadimi and G. Lan, \u201cStochastic first-and zeroth-order methods for nonconvex stochastic\nprogramming,\u201d SIAM Journal on Optimization, vol. 23, no. 4, pp. 2341\u20132368, 2013\n[2]J. C. Duchi, M. I. Jordan, M. J. Wainwright, and A. Wibisono, \u201cOptimal rates for zero-order convex optimization: The power of two function evaluations,\u201d\n[3]Bernstein, Jeremy, et al. \"signSGD: Compressed optimisation for non-convex problems.\" arXiv preprint arXiv:1802.04434 (2018).\n[4]Liu, Sijia, et al. \"signSGD via zeroth-order oracle.\" (2018).\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1054/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklTQCNtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference/Paper1054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1054/Reviewers", "ICLR.cc/2020/Conference/Paper1054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1054/Authors|ICLR.cc/2020/Conference/Paper1054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161969, "tmdate": 1576860555177, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference/Paper1054/Reviewers", "ICLR.cc/2020/Conference/Paper1054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Official_Comment"}}}, {"id": "r1gzqlLDjH", "original": null, "number": 2, "cdate": 1573507209933, "ddate": null, "tcdate": 1573507209933, "tmdate": 1573507517729, "tddate": null, "forum": "SklTQCNtvS", "replyto": "H1eZtKICKB", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for valuable comments and suggestions.\n\nAbout the novelty: Although we use the similar loss formulation as Cheng et al, the major difference is the proposal of single query oracle (section 3.1) that substantially reduces the cost of gradient sign estimation, and in section 3.2 and 3.3 we showed that this single query oracle can be used to solve Cheng\u2019s objective function. We believe both of these are novel. In contrast, Cheng\u2019s method requires binary search to estimate the loss and gradient thereby increasing the queries required. \n\nRegarding comparisons to Liu et al, we note that their algorithm is proposed for soft label black box setting, not for hard-label setting. More specifically, in their paper the black-box query will return *probability output* of the model, which is different from the hard-label query case studied in this paper. So the query counts are not comparable. \nIt\u2019s non-trivial to extend a soft-label black box attack to the hard-label setting since they are using totally different information. This is why there are tens of soft-label black box attack methods but only few hard-label attack methods. Again, our novelty is in proposing the single query oracle and showing that using such oracle, a query-efficient gradient-based update can be derived to solve Cheng\u2019s hard-label attack formulation. The relationship between our paper and (Liu et al) is that our convergence analysis is a combination of (Liu et al) with a newly introduced way to handle magnitudes of each element of u_k.  \n\nAlso, we have added a discussion in Section 4.4 in the revision to discuss the importance of single query oracle and the improvement from sign-OPT over (Liu et al) with single query oracle. \n\n\n\nAbout the speed-up compared to Cheng et al:\nSorry for the confusion. For OPT-attack, to evaluate g(\\theta+\\beta u) in each gradient estimation, even with the optimal alpha, we still need to do several binary search to achieve v_right-v_left<\\epsilon, which would take extra number of queries. On the other hand, sign-OPT only uses single query. Moreover, in the implementation of both OPT-attack and Sign-OPT, instead of using one vector, they sample 10-20 vectors from Gaussian distribution and average their estimators. Therefore, Sign-OPT further reduces the number of queries several times. The experiments also show Sign-OPT achieve 5-10 times fewer queries in practice.\n\n\n\nAbout the white-box performance: \nOur method can sometimes outperform white-box attack in terms of distortion, The same finding has also been reported in Boundary attack (Brendel et al., 2018), see the table in their section 3.1. Several recent papers (including other submissions to ICLR this year) also reported similar findings and try to explain this phenomenon. We think the main reason is that hard-label attacks are based on searching on the decision surface, while C&W/PGD attacks are starting from the interior and gradually moving to the boundary, so they have very different optimization paths which lead to different local minima. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1054/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklTQCNtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference/Paper1054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1054/Reviewers", "ICLR.cc/2020/Conference/Paper1054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1054/Authors|ICLR.cc/2020/Conference/Paper1054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161969, "tmdate": 1576860555177, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1054/Authors", "ICLR.cc/2020/Conference/Paper1054/Reviewers", "ICLR.cc/2020/Conference/Paper1054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Official_Comment"}}}, {"id": "HJxBPlpEFB", "original": null, "number": 2, "cdate": 1571242076691, "ddate": null, "tcdate": 1571242076691, "tmdate": 1572972518417, "tddate": null, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1054", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors propose a new algorithm for evaluating adversarial robustness of black-box models. The aim of the proposed algorithm, SIGN-OPT is generating adversarial examples as close as possible to the decision boundary using as less queries of the black-box model as possible.\n\nThe authors follow the approach of (Cheng et al 2019) by modeling the problem as an optimization problem, where the objective function is to find the direction with the shortest distance to the decision boundary. They propose a smart modification of the previous approach by evaluating the sign of the gradient rather than the gradient itself. The advantage is that the sign of the gradient can be evaluated using a single query while the estimation of the gradient needs many queries. \nThe authors have analyzed the proposed algorithm. They showed that using SIGN-OPT, the expectation of the gradient tends to zero in $O(1/\\sqrt(T))$, meaning that a (local) minimum is reached.\nThe algorithm is favorably compared with the state-of-the-art on three image test sets (MNIST, CIFAR-10n ImageNet).\n\nThis paper is technically sound, well-written and propose an interesting modification of a previous algorithm. I vote for acceptance.\n\nHowever, I have some concerns:\n-\tI think that the L-smoothness assumption should be discussed. Is it realistic for Deep Learning models? Does it hold for the three attacked CNN networks?\n-\tThe analytical results of the previous algorithm RGF and the proposed algorithm SIGN-OPT are not compared and discussed. It is a pity.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1054/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1054/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576470257597, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1054/Reviewers"], "noninvitees": [], "tcdate": 1570237743071, "tmdate": 1576470257611, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Official_Review"}}}, {"id": "H1eZtKICKB", "original": null, "number": 3, "cdate": 1571871097249, "ddate": null, "tcdate": 1571871097249, "tmdate": 1572972518383, "tddate": null, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "invitation": "ICLR.cc/2020/Conference/Paper1054/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The paper develops a query efficient algorithm for computing black box adversarial examples given only hard labels in the context of deep neural networks. Intuitively, the only information of the function provided to the algorithm is the label for a given sample. Technically, the authors use the formulation proposed by Cheng et al, and derive a zeroth optimization algorithm that uses less queries with nice convergence properties. Experimentally, the proposed algorithm is very effective on three different standard datasets in vision.\n\nI have decided to weak reject the paper for the following key reasons:\n\n1. Novelty: the technique as such as very similar to Cheng et al, and Liu et al, as the authors themselves mention it in Section 3.2. In particular, the speed-up compared to Cheng et al, is twice -- for a bounded maximum \\alpha in Algorithm 1 in Cheng et al, which is almost always the case, because otherwise it would not be \"adversarial\" in nature. The authors claim that the convergence result has not been proved yet for the proposed algorithm but it follows using the technique used in Bernstein et al 2018, with some minor modifications.\n\n2. Experiments: While the experiments that the authors support the claim, I think they are missing comparison with Liu et al which is crucial. In Fig 4, their method is doing even better than white box attacks, how can this be true? or why is this true?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1054/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1054/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack", "authors": ["Minhao Cheng", "Simranjit Singh", "Patrick H. Chen", "Pin-Yu Chen", "Sijia Liu", "Cho-Jui Hsieh"], "authorids": ["mhcheng@ucla.edu", "simranjit@cs.ucla.edu", "patrickchen@ucla.edu", "pin-yu.chen@ibm.com", "sijia.liu@ibm.com", "chohsieh@cs.ucla.edu"], "keywords": [], "abstract": "We study the most practical problem setup for evaluating adversarial robustness of a machine learning system with limited access:  the hard-label black-box attack setting for generating adversarial examples, where limited model queries are allowed and only the decision is provided to a queried data input. Several algorithms have been proposed for this problem but they typically require huge amount (>20,000) of queries for attacking one example. Among them, one of the state-of-the-art approaches (Cheng et al., 2019) showed that hard-label attack can be modeled as an optimization problem where the objective function can be evaluated by binary search with additional model queries, thereby a zeroth order optimization algorithm can be applied. In this paper, we adopt the same optimization formulation but  propose to directly estimate the sign of gradient at any direction instead of the gradient itself, which enjoys the benefit of single query. \nUsing this single query oracle for retrieving sign of directional derivative, we develop a novel query-efficient Sign-OPT approach for hard-label black-box attack. We provide a convergence analysis of the new algorithm and conduct experiments on several models on MNIST, CIFAR-10 and ImageNet. \nWe find that Sign-OPT attack consistently requires 5X to 10X fewer queries when compared to the current state-of-the-art approaches, and usually converges to an adversarial example with smaller perturbation. ", "pdf": "/pdf/b0e100de9d582c690cc356f1475cfd56649b84a4.pdf", "paperhash": "cheng|signopt_a_queryefficient_hardlabel_adversarial_attack", "code": "https://github.com/cmhcbb/attackbox", "_bibtex": "@inproceedings{\nCheng2020Sign-OPT:,\ntitle={Sign-OPT: A Query-Efficient Hard-label Adversarial Attack},\nauthor={Minhao Cheng and Simranjit Singh and Patrick H. Chen and Pin-Yu Chen and Sijia Liu and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklTQCNtvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b1b4a06f0cafab6c9cc329939c34987d9acd9796.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklTQCNtvS", "replyto": "SklTQCNtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576470257597, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1054/Reviewers"], "noninvitees": [], "tcdate": 1570237743071, "tmdate": 1576470257611, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1054/-/Official_Review"}}}], "count": 7}