{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363298100000, "tcdate": 1363298100000, "number": 1, "id": "tG4Zt9xaZ8G5D", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ttnAE7vaATtaK", "replyto": "Ub0AUfEOKkRO1", "signatures": ["Camille Couprie"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and  helpful comments. We computed and added error bars as suggested in Table 1. However, computing standard deviation for the individual means per class of objects does not apply here: the per class accuracies are not computed image per image. Each number corresponds to a ratio of the total number of correctly classified pixels as a particular class, on the number of pixels belonging to this class in the dataset. \r\nFor the pixel-wise accuracy, we now give the standard deviation in Table 1, as well as the median. As the two variances are equal using depth or not, we computed the statistical significance using a two sample t-test, that results in a t statistic equal to 1.54, which is far from the mean performance of 52.2 and thus we can consider that the two reported means are statistically significant. \r\n\r\nAbout the class-by class improvements displayed in Table 1, we discuss the fact that objects having a constant appearance of depth are in general more inclined to take benefit from depth information. As the major part of the scenes contains categories that respect this property, the improvements achieved using depth involve a smaller number of categories, but a larger volume of data.  \r\n\r\nTo strengthen our comparison of the two networks using or not depth information, we now display the results obtained using only the multiscale network without depth information in Figure 2. \r\n\r\nWe hope that the changes that we made in the paper (which should be updated within the next 24 hours) answer your concerns."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363297980000, "tcdate": 1363297980000, "number": 1, "id": "OOB_F66xrPKGA", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ttnAE7vaATtaK", "replyto": "2-VeRGGdvD-58", "signatures": ["Camille Couprie"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and helpful comments. \r\nThe missing values in the depth acquisition were pre-processed using inpainting code available online on Nathan Siberman\u2019s web page. We added the reference to the paper.\r\n In the paper, we made the observation that the classes for which depth fails to outperform the RGB model are the classes of object for which the depth map does not vary too much. We now stress out better this observation with the addition of some depth maps at Figure 2. \r\n\r\nThe question you are raising about whether or not the depth is always useful, or if there could be better ways to leverage depth data is a very good question, and at the moment is still un-answered. The current RGBD  multiscale network is the best way we found to learn features using depth, now maybe we could improve the system by introducing an appropriate contrast normalization of the depth map, or maybe we could combine the learned features using RGB and the learned features using RGBD\u2026"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363297440000, "tcdate": 1363297440000, "number": 1, "id": "VVbCVyTLqczWn", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ttnAE7vaATtaK", "replyto": "qO9gWZZ1gfqhl", "signatures": ["Camille Couprie"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for your review and pointing out the paper of Ciresan et al., that we added to our list of references. Similarly to us, they apply the idea of using a kind of multi-scale network. However, Ciseran's approach to foveation differs from ours: where we use a multiscale pyramid to provide a foveated input to the network, they artificially blur the input's content, radially, and use non-uniform sampling to connect the network to it. The major advantage of using a pyramid is that the whole pyramid can be applied convolutionally, to larger input sizes. Once the model is trained, it must be applied as a sliding window to classify each pixel in the input. Using their method, which requires a radial blur centered on each pixel, the model cannot be applied convolutionally. This is a major difference, which dramatically impacts test time. \r\n\r\nNote: Ciseran's 2012 NIPS paper appeared after our first paper (ICML 2012) on the subject."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362368040000, "tcdate": 1362368040000, "number": 3, "id": "Ub0AUfEOKkRO1", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ttnAE7vaATtaK", "replyto": "ttnAE7vaATtaK", "signatures": ["anonymous reviewer 5193"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Indoor Semantic Segmentation using depth information", "review": "This work builds on recent object-segmentation work by Farabet et al., by augmenting the pixel-processing pathways with ones that processes a depth map from a Kinect RGBD camera. This work seems to me a well-motivated and natural extension now that RGBD sensors are readily available.\r\n\r\nThe incremental value of the depth channel is not entirely clear from this paper. In principle, the depth information should be valuable. However, Table 1 shows that for the majority of object types, the network that ignores depth is actually more accurate.  Although the averages at the bottom of Table 1 show that depth-enhanced segmentation is slightly better, I suspect that if those averages included error bars (and they should), the difference would be insignificant.  In fact, all the accuracies in Table 1 should have error bars on them.  The comparisons with the work of Silberman et al. are more favorable to the proposed model, but again, the comparison would be strengthened by discussion of statistical confidence.\r\n\r\nQualitatively, I would have liked to see the ouput from the convolutional network of Farabet et al. without the depth channel, as a point of comparison in Figures 2 and 3. Without that point of comparison, Figures 2 and 3 are difficult to interpret as supporting evidence for the model using depth.\r\n\r\nPro(s)\r\n- establishes baseline RGBD results with convolutional networks\r\n   \r\nCon(s)\r\n- quantitative results lack confidence intervals\r\n- qualitative results missing important comparison to non-rgbd network"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362213660000, "tcdate": 1362213660000, "number": 2, "id": "2-VeRGGdvD-58", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ttnAE7vaATtaK", "replyto": "ttnAE7vaATtaK", "signatures": ["anonymous reviewer 03ba"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Indoor Semantic Segmentation using depth information", "review": "This work applies convolutional neural networks to the task of RGB-D indoor scene segmentation. The authors previously evaulated the same multi-scale conv net architecture on the data using only RGB information, this work demonstrates that for most segmentation classes providing depth information to the conv net increases performance. \r\n\r\nThe model simply adds depth as a separate channel to the existing RGB channels in a conv net. Depth has some unique properties e.g. infinity / missing values depending on the sensor. It would be nice to see some consideration or experiments on how to properly integrate depth data into the existing model. \r\n\r\nThe experiments demonstrate that a conv net using depth information is competitive on the datasets evaluated. However, it is surprising that the model leveraging depth is not better in all cases. Discussion on where the RGB-D model fails to outperform the RGB only model would be a great contribution to add. This is especially apparent in table 1. Does this suggest that depth isn't always useful, or that there could be better ways to leverage depth data?\r\n\r\n\r\nMinor notes:\r\n'modalityies' misspelled on page 1\r\n\r\nOverall:\r\n- A straightforward application of conv nets to RGB-D data, yielding fairly good results\r\n- More discussion on why depth fails to improve performance compared to an RGB only model would strengthen the experimental findings"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362163380000, "tcdate": 1362163380000, "number": 1, "id": "qO9gWZZ1gfqhl", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ttnAE7vaATtaK", "replyto": "ttnAE7vaATtaK", "signatures": ["anonymous reviewer 777f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Indoor Semantic Segmentation using depth information", "review": "Segmentation with multi-scale max pooling CNN, applied to indoor vision, using depth information. Interesting paper! Fine results.\r\n\r\nQuestion: how does that compare to multi-scale max pooling CNN for a previous award-winning application, namely, segmentation of neuronal membranes (Ciresan et al, NIPS 2012)?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358409600000, "tcdate": 1358409600000, "number": 40, "id": "ttnAE7vaATtaK", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "ttnAE7vaATtaK", "signatures": ["camille.couprie@gmail.com"], "readers": ["everyone"], "content": {"title": "Indoor Semantic Segmentation using depth information", "decision": "conferenceOral-iclr2013-conference", "abstract": "This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on hand-crafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. We obtain state-of-the-art on the NYU-v2 depth dataset with an accuracy of 64.5%. We illustrate the labeling of indoor scenes in videos sequences that could be processed in real-time using appropriate hardware such as an FPGA.", "pdf": "https://arxiv.org/abs/1301.3572", "paperhash": "couprie|indoor_semantic_segmentation_using_depth_information", "keywords": [], "conflicts": [], "authors": ["Camille Couprie", "Clement Farabet", "Laurent Najman", "Yann LeCun"], "authorids": ["camille.couprie@gmail.com", "clement.farabet@gmail.com", "l.najman@esiee.fr", "ylecun@gmail.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}