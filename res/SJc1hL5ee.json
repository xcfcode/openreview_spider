{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396508114, "tcdate": 1486396508114, "number": 1, "id": "HyVwnGLOx", "invitation": "ICLR.cc/2017/conference/-/paper324/acceptance", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The submission describes a method for compressing shallow and wide text classification models. The paper is well written, but the proposed method is not particularly novel, as it's comprised of existing model compression techniques. Overall, the contributions are incremental and the potential impact seems rather limited."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396510129, "id": "ICLR.cc/2017/conference/-/paper324/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396510129}}}, {"tddate": null, "tmdate": 1483401654443, "tcdate": 1483401413358, "number": 4, "id": "Sya6_wOSg", "invitation": "ICLR.cc/2017/conference/-/paper324/public/comment", "forum": "SJc1hL5ee", "replyto": "B1Mcr6c4l", "signatures": ["~Edouard_Grave1"], "readers": ["everyone"], "writers": ["~Edouard_Grave1"], "content": {"title": "Rebuttal", "comment": "First, we would like to thank the reviewer.\n\nThe main concern stated by Reviewer 3 is the lack of novelty of the paper. However, we are not aware of previous work using similar techniques for compression of text classification models. We are happy to examine any paper in that field that the reviewer could refer to. While we do agree that similar techniques were applied to other domains, such as computer vision, one of the main contributions of this paper is to perform a careful empirical evaluation for text classification. In particular, we evaluated many different combinations of the methods discussed in the paper, on 9 datasets with different characteristics (see Tables 5, 6, 7, 8 and 9 of the appendix).\n\n** \"From a machine learning point of view, the proper baseline to solve this problem...\"\n\nWe believe that this approach is more complex that the one described in the paper. Moreover, this technique cannot be used to compress existing models. Again, a reference providing results comparable to our approach in terms of the performance between compression rate and accuracy would be welcomed. \n\n** \"what if one simply uses fewer vocabulary elements (e.g based on subword units ...)\"\n\nWe agree that this technique could be used additionally to the ones described in the paper. We actually discuss this in the last paragraph of the \u201cFuture work\u201d section.\n\n** \"However, the machine learning contributions of the paper are marginal to me.\"\n\nWe do not believe that only the contributions cast as a machine learning objective are of interest to ICLR (see the list of relevant topics, e.g. \"Implementation issues, parallelization, software platforms, hardware\").\n\n** \"The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016)\"\n\nWe do not cite any paper by Carreira-Perpinan and Alizadeh.\n\n** \"In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.\" and \"The use of (optimized) product quantization for approximating inner product is not particularly novel.\"\n\nAs stated in the first paragraph of section 4.1, \u201cwe adopt the normalized PQ (NPQ) for the rest of this study.\u201d Note, this is a variation that departs from PQ and OPQ by the way we treat the magnitude information, which to the best of our knowledge is new in this context. \n\n** \"The paper argues the wide and shallow models are the state of the art in small datasets.\"\n\nWe make the claim that \u201clinear classifiers remain competitive with more sophisticated, deeper models\u201d, supported by the results reported by Wang and Manning (2012) and Joulin et al. (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620954, "id": "ICLR.cc/2017/conference/-/paper324/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJc1hL5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper324/reviewers", "ICLR.cc/2017/conference/paper324/areachairs"], "cdate": 1485287620954}}}, {"tddate": null, "tmdate": 1482507658514, "tcdate": 1482507658514, "number": 3, "id": "B1Mcr6c4l", "invitation": "ICLR.cc/2017/conference/-/paper324/official/review", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/conference/paper324/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper324/AnonReviewer3"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper presents a few tricks to compress a wide and shallow text classification model based on n-gram features. These tricks include (1) using (optimized) product quantization to compress embedding weights (2) pruning some of the vocabulary elements (3) hashing to reduce the storage of the vocabulary (this is a minor component of the paper). The paper focuses on models with very large vocabularies and shows a reduction in the size of the models at a relatively minor reduction of the accuracy.\n\nThe problem of compressing neural models is important and interesting. The methods section of the paper is well written with good high level comments and references. However, the machine learning contributions of the paper are marginal to me. The experiments are not too convincing mainly focusing on benchmarks that are not commonly used. The implications of the paper on the state-of-the-art RNN text classification models is unclear.\n\nThe use of (optimized) product quantization for approximating inner product is not particularly novel. Previous work also considered doing this. Most of the reduction in the model sizes comes from pruning vocabulary elements. The method proposed for pruning vocabulary elements is simply based on the assumption that embeddings with larger L2 norm are more important. A coverage heuristic is taken into account too. From a machine learning point of view, the proper baseline to solve this problem is to have a set of (relaxed) binary coefficients for each embedding vector and learn the coefficients jointly with the weights. An L1 regularizer on the coefficients can be used to encourage sparsity. From a practical point of view, I believe an important baseline is missing: what if one simply uses fewer vocabulary elements (e.g based on subword units - see https://arxiv.org/pdf/1508.07909.pdf) and retrain a smaller models?\n\nGiven the lack of novelty and the missing baselines, I believe the paper in its current form is not ready for publication at ICLR.\n\nMore comments:\n- The title does not make it clear that the paper focuses on wide and shallow text classification models. Please revise the title.\n- The paper cites an ArXiv manuscript by Carreira-Perpinan and Alizadeh (2016) several times, which has the same title as the submitted paper. Please make the paper self-contained and include any supplementary material in the appendix.\n- In Fig 2 does the square mark PQ or OPQ? The paper does not distinguish OPQ and PQ properly at multiple places especially in the experiments.\n- The paper argues the wide and shallow models are the state of the art in small datasets. Is this really correct? What about transfer learning?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623382, "id": "ICLR.cc/2017/conference/-/paper324/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper324/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper324/AnonReviewer2", "ICLR.cc/2017/conference/paper324/AnonReviewer1", "ICLR.cc/2017/conference/paper324/AnonReviewer3"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623382}}}, {"tddate": null, "tmdate": 1482188447256, "tcdate": 1482188447256, "number": 2, "id": "H1PiU1UVg", "invitation": "ICLR.cc/2017/conference/-/paper324/official/review", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/conference/paper324/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper324/AnonReviewer1"], "content": {"title": "Effective if simple combination of existing techniques for text-classifier compression", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes a series of tricks for compressing fast (linear) text classification models. The paper is clearly written, and the results are quite strong. The main compression is achieved via product quantization, a technique which has been explored in other applications within the neural network model compression literature. In addition to the Gong et al. work which was cited, it would be worth mentioning Quantized Convolutional Neural Networks for Mobile Devices (CVPR 2016, https://arxiv.org/pdf/1512.06473v3.pdf), which similarly incorporates fine tuning to mitigate losses due to quantization error.\n\nAs such, one criticism of the paper is that it is a more-or-less straightforward application of techniques that have already been shown to be effective elsewhere in the model compression literature, and so isn't particularly surprising or deep from a technical perspective. However, this is as far as I am aware the first work applying these techniques to text classification, and the results are strong enough that I think it will be of interest to those working on models for text-based tasks.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623382, "id": "ICLR.cc/2017/conference/-/paper324/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper324/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper324/AnonReviewer2", "ICLR.cc/2017/conference/paper324/AnonReviewer1", "ICLR.cc/2017/conference/paper324/AnonReviewer3"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623382}}}, {"tddate": null, "tmdate": 1481910183437, "tcdate": 1481909332446, "number": 3, "id": "Hk3L4jZ4e", "invitation": "ICLR.cc/2017/conference/-/paper324/public/comment", "forum": "SJc1hL5ee", "replyto": "HJe8ExgVe", "signatures": ["~Armand_Joulin2"], "readers": ["everyone"], "writers": ["~Armand_Joulin2"], "content": {"title": "Rebuttal", "comment": "First we would like to thank the reviewer.\n\nShe/he seems to question the potential impact research-wise:\n\nShowing that it is possible to obtain state-of-the-art text classifiers that fits in less than 100KB, is a significant achievement that has not been published before (to the best of our knowledge). \n\nModel quantization is a very active field of research and ICLR 2016 best paper award was on that subject, i.e., \u201cDeep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\u201d by Han et al. Our work provides a simple approach that can be used as a strong baseline for any future work in this direction.\n\nWe answer her/his questions below:\n\n\"- it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\"\n\nWe will add a formula measuring the size of a model as a function of the different parameters. \n\nOn small output space the model size is mostly due to A. On YFCC100M, it is due to both A and B.\n\nThe dictionary cost around 1Mb to store which is negligible on big models but becomes significant when quantizing to hundreds of Ko.\n\nWe dump in memory the model as it will be used during testing and measure its size. It doesn\u2019t account for the size of the executable.\n\n\u201csome parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\u201d\n\nWe agree that we should extend and clarify the greedy algorithm.\n\nThe \u201coverhead of a few hundreds of KB\u201d:\nAt test time, we only have access to the hashing function that compute the indices before pruning and we need to verify if a given hashing value has been removed by the pruning. \n\nThis sum up to storing an array V of K uint_8, where V[index_after_pruning] = index_before_pruning. This leads to an overhead of a few hundreds of KB. At test time, we need to explore this array efficiently, hence the binary search.\n\nThis overhead can be removed by using Bloom filters to test if a given index has been kept or not."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620954, "id": "ICLR.cc/2017/conference/-/paper324/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJc1hL5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper324/reviewers", "ICLR.cc/2017/conference/paper324/areachairs"], "cdate": 1485287620954}}}, {"tddate": null, "tmdate": 1481798770744, "tcdate": 1481798728206, "number": 1, "id": "HJe8ExgVe", "invitation": "ICLR.cc/2017/conference/-/paper324/official/review", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/conference/paper324/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper324/AnonReviewer2"], "content": {"title": "Lossy compression techniques applied to FastText with nice results", "rating": "6: Marginally above acceptance threshold", "review": "This paper describes how to approximate the FastText approach such that its memory footprint is reduced by several orders of magnitude, while preserving its classification accuracy. The original FastText approach was based on a linear classifier on top of bag-of-words embeddings. This type of method is extremely fast to train and test, but the model size can be quite large.\n\nThis paper focuses on approximating the original approach with lossy compression techniques. Namely, the embeddings and classifier matrices A and B are compressed with Product Quantization, and an aggressive dictionary pruning is carried out. Experiments on various datasets (either with small or large number of classes) are conducted to tune the parameters and demonstrate the effectiveness of the approach. With a negligible loss in classification accuracy, an important reduction in term of model size (memory footprint) can be achieved, in the order of 100~1000 folds compared to the original size.\n\nThe paper is well written overall. The goal is clearly defined and well carried out, as well as the experiments. Different options for compressing the model data are evaluated and compared (e.g. PQ vs LSH), which is also interesting. Nevertheless the paper does not propose by itself any novel idea for text classification. It just focuses on adapting existing lossy compression techniques, which is not necessarily a problem. Specifically, it introduces:\n  - a straightforward variant of PQ for unnormalized vectors,\n  - dictionary pruning is cast as a set covering problem (which is NP-hard), but a greedy approach is shown to yield excellent results nonetheless,\n  - hashing tricks and bloom filter are simply borrowed from previous papers.\n\nThese techniques are quite generic and could as well be used in other works. \n\n\nHere are some minor problems with the paper:\n\n  - it is not made clear how the full model size is computed. What is exactly in the model? Which proportion of the full size do the A and B matrices, the dictionary, and the rest, account for? It is hard to follow where is the size bottleneck, which also seems to depend on the target application (i.e. small or large number of test classes). It would have been nice to provide a formula to calculate the total model size as a function of all parameters (k,b for PQ and K for dictionary, number of classes).\n  \n  - some parts lack clarity. For instance, the greedy approach to prune the dictionary is exposed in less than 4 lines (top of page 5), though it is far from being straightforward. Likewise, it is not clear why the binary search used for the hashing trick would introduce an overhead of a few hundreds of KB.\n  \n\nOverall this looks like a solid work, but with potentially limited impact research-wise.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623382, "id": "ICLR.cc/2017/conference/-/paper324/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper324/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper324/AnonReviewer2", "ICLR.cc/2017/conference/paper324/AnonReviewer1", "ICLR.cc/2017/conference/paper324/AnonReviewer3"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623382}}}, {"tddate": null, "tmdate": 1481636958362, "tcdate": 1481625769218, "number": 2, "id": "S1bngLTQe", "invitation": "ICLR.cc/2017/conference/-/paper324/public/comment", "forum": "SJc1hL5ee", "replyto": "Hki8CTyQg", "signatures": ["~Armand_Joulin2"], "readers": ["everyone"], "writers": ["~Armand_Joulin2"], "content": {"title": "Re: Question", "comment": "Thank you for your question.\n\nIn table 9 of the appendix, we measure the quality of our model with different size of embeddings.  We indeed observe that using larger embedding with compression allows to obtain models that are significantly smaller while being more accurate. We did not make it a central point of the paper as it is not the main focus and may clutter the message.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620954, "id": "ICLR.cc/2017/conference/-/paper324/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJc1hL5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper324/reviewers", "ICLR.cc/2017/conference/paper324/areachairs"], "cdate": 1485287620954}}}, {"tddate": null, "tmdate": 1481636948583, "tcdate": 1481625645564, "number": 1, "id": "Sk8EeIame", "invitation": "ICLR.cc/2017/conference/-/paper324/public/comment", "forum": "SJc1hL5ee", "replyto": "SJ-gbbw7g", "signatures": ["~Armand_Joulin2"], "readers": ["everyone"], "writers": ["~Armand_Joulin2"], "content": {"title": "Re: comparison to non-deep baselines", "comment": "Thank you for your questions.\n\nA standard linear model without quantization takes from 1.6Mb (AG) to 45Mb (dbpedia) on a dataset with a small output space. On YFCC100M where the output space is large, a linear model requires ~350Gb (since it has around 300K*300K parameters). \n\nAll the compression methods presented in this paper can be used on linear models if the output space is small. It is less clear on datasets with a large output space. \n\nConcerning the use of \u201cbag-of-words + SVM with polynomial kernel of degree 2 or 3\u201d, we are not sure what the reviewer is referring to. The most related work we are aware of, is the paper by Sanchez et al., \u201cImage Classification with the Fisher Vector: Theory and Practice\u201d where the interaction between SVM and PQ is analyzed with Fisher kernel (could be a rbf/polynomial kernel). It is used on the embeddings to capture higher order information (possibly ngrams in the case of text?). However this work does not compress the model but the representations (that is it conserve the original embeddings, increase it with a kernel and then compress the output). On the other hand, we are interested in compressing the model. If the reviewer is thinking of a different work that shows that this approach can be successfully applied to the compression of text classification models, we will add a comparison in our experiments. \n\nFinally, we focus on the comparison with character level CNNs because in theory, their number of parameters is independent of the vocabulary size, making them natural memory efficient text classifiers (Xiao & Cho, 2016). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620954, "id": "ICLR.cc/2017/conference/-/paper324/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJc1hL5ee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper324/reviewers", "ICLR.cc/2017/conference/paper324/areachairs"], "cdate": 1485287620954}}}, {"tddate": null, "tmdate": 1481212232200, "tcdate": 1481212137422, "number": 2, "id": "SJ-gbbw7g", "invitation": "ICLR.cc/2017/conference/-/paper324/pre-review/question", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/conference/paper324/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper324/AnonReviewer2"], "content": {"title": "comparison to non-deep baselines", "question": "It is well-known that non-deep method still performs well on the text classification task (particularly for dataset with few classes as in Section 4.1). Could the authors also report results on non-deep baselines, such as a simple bag-of-words + SVM with polynomial kernel of degree 2 or 3 (thus emulating bigrams and trigrams)? How does this kind of baseline compare to fastText.zip in terms of accuracy and model size? I believe that such baseline models can be compressed using simple tricks as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481212138019, "id": "ICLR.cc/2017/conference/-/paper324/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper324/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper324/AnonReviewer1", "ICLR.cc/2017/conference/paper324/AnonReviewer2"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481212138019}}}, {"tddate": null, "tmdate": 1480740435122, "tcdate": 1480740435114, "number": 1, "id": "Hki8CTyQg", "invitation": "ICLR.cc/2017/conference/-/paper324/pre-review/question", "forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "signatures": ["ICLR.cc/2017/conference/paper324/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper324/AnonReviewer1"], "content": {"title": "Question", "question": "Did you also explore increasing the dimensionality of embeddings (e.g., doubling)? I.e., with 8x compression, can you achieve a model that is both smaller and noticeably more accurate?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481212138019, "id": "ICLR.cc/2017/conference/-/paper324/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper324/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper324/AnonReviewer1", "ICLR.cc/2017/conference/paper324/AnonReviewer2"], "reply": {"forum": "SJc1hL5ee", "replyto": "SJc1hL5ee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper324/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481212138019}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478379091753, "tcdate": 1478286306268, "number": 324, "id": "SJc1hL5ee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJc1hL5ee", "signatures": ["~Armand_Joulin2"], "readers": ["everyone"], "content": {"title": "FastText.zip: Compressing text classification models", "abstract": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.  After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store the word embeddings.  While the original technique leads to a loss in accuracy,  we adapt this method to circumvent the quantization artifacts. As a result, our approach produces a text classifier, derived from the fastText approach, which at test time requires only a fraction of the memory compared to the original one, without noticeably sacrificing the quality in terms of classification accuracy. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.", "pdf": "/pdf/c041ccf288b0ed8d1eccac2f3d20aa9ed97c155f.pdf", "TL;DR": "Compressing text classification models", "paperhash": "joulin|fasttextzip_compressing_text_classification_models", "conflicts": ["inria.fr", "fb.com", "ens.fr", "columbia.edu"], "keywords": ["Natural language processing", "Supervised Learning", "Applications"], "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Matthijs Douze", "Herve Jegou", "Tomas Mikolov"], "authorids": ["ajoulin@fb.com", "egrave@fb.com", "bojanowski@fb.com", "matthijs@fb.com", "rvj@fb.com", "tmikolov@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 11}