{"notes": [{"id": "B1e0KsRcYQ", "original": "Skx4wSo9t7", "number": 492, "cdate": 1538087813817, "ddate": null, "tcdate": 1538087813817, "tmdate": 1545355421002, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJgSkv5fgV", "original": null, "number": 1, "cdate": 1544885980534, "ddate": null, "tcdate": 1544885980534, "tmdate": 1545354494210, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Meta_Review", "content": {"metareview": "AR1 is is concerned that the only contribution of this work is  combining second-order pooling with with a codebook style assignments. After discussions, AR1 still maintains that that the proposed factorization is a marginal contribution. AR2 feels that the proposed paper is highly related to numerous current works (e.g. mostly a mixture of existing contributions) and that evaluations have not been improved. AR3 also points that this paper lacks important comparisons for fairly evaluating the effectiveness of the proposed formulation and it lacks detailed description and discussion for the methods.\n\nAC has also pointed several works to the authors which are highly related (but by no means this is not an exhaustive list and authors need to explore google scholar to retrieve more relevant papers than the listed ones):\n\n[1] MoNet: Moments Embedding Network by Gou et al. (e.g. Stanford Cars via Tensor Sketching: 90.8 vs. 90.4 in this submission, Airplane: 88.1 vs. 87.3% in this submission, 85.7 vs. 84.3% in this submission)\n[2] Second-order Democratic Aggregation by Lin et al. (e.g. Stanford Cars: 90.8 vs. 90.4 in this submission)\n[3] Statistically-motivated Second-order Pooling by Yu et al (CUB: 85%)\n[4] DeepKSPD: Learning Kernel-matrix-based SPD Representation for Fine-grained Image Recognition by Engin et al.\n[5] Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks by Q. Wang et al. (512D representations)\n[6] Low-rank Bilinear Pooling for Fine-Grained Classification' by S. Kong et al. (CVPR I believe). They get some reduction of size of 10x less than tensor sketch, higher results than here by some >2% (CUB), and all this obtained in somewhat more sophisticated way.\n\nThe authors brushed under the carpet some comparisons. Some methods above are simply better performing even if cited, e.g. MoNet [1] uses sketching and seems a better performer on several datasets, see [2] that uses sketching (Section 4.4), see [5] which also generates compact representation (8K). [4] may be not compact but the whole point is to compare compact methods with non-compact second-order ones too (e.g. small performance loss for compact methods is OK but big loss warrants a question whether they are still useful). Approach [6] seems to also obtain better results on some sets (common testbed comparisons are essentially encouraged). \n\nAt this point, AC will also point authors to sparse coding methods on matrices (bilinear) and tensors (higher-order) from years 2013-2018 (TPAMI, CVPR, ECCV, ICCV, etc.). These all methods can produce compact representations (512 to 10K or so) of bilinear or higher-order descriptors for classification. This manuscript fails to mention this family of methods.\n\nFor a paper to be improved for the future, the authors should consider the following:\n- make a thorough comparison with existing second-order/bilinear methods in the common testbed (most of the codes are out there on-line)\n- the authors should vary the size of representation (from 512 to 8K or more) and plot this against accuracy\n- the authors should provide theoretical discussion and guarantees on the quality of their low-rank approximations (e.g. sketching has clear bounds on its approximation quality, rates, computational cost). The authors should provide some bounds on the loss of information in the proposed method.\n- authors should discuss the theoretical complexity of proposed method (and other methods in the literature)\n\nAdditionally, the authors should improve their references and the story line. Citing  (Lin et al. (2015)) in Eq. 1 and 2 as if they are the father of bilinear pooling is misleading. Citing (Gao et al. (2016)) in the context of polynomial kernel approximation in Eq. 3 to obtain bilinear pooling should be preceded with earlier works that expand polynomial kernel to obtain bilieanr pooling. AC can think of at least two papers from 2012/2013 which do derive bilinear pooling and could be cited here instead. AC encourages the authors to revise their references and story behind bilinear pooling to give unsuspected readers a full/honest story of bilinear representations and compact methods (whether they are branded as compact or just use sketching etc., whether they use dictionaries or low-rank representations).\n\nIn conclusion, it feels this manuscript is not ready for publication with ICLR and requires a major revision. However, there is some merit in the proposed direction and authors are encouraged to explore further.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Incomplete work."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper492/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353197195, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353197195}}}, {"id": "SkxvoNr927", "original": null, "number": 3, "cdate": 1541194910922, "ddate": null, "tcdate": 1541194910922, "tmdate": 1544583398134, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "content": {"title": "Method needs some clarification. ", "review": "Summary: This paper presents a way to combine existing factorized second order representations with a codebook style hard assignment. The number of parameters required to produce this encoded representation is shown to be very low. Like other factorized representations, the number of computations as well as the size of any intermediate representations is low. The overall embedding is trained for retrieval using a triplet loss. Results are shown on Stanford online, CUB and Cars-196 datasets.\n\nComments:\n\nReview of relevant works seems adequate. The results seem reproducible. \n\nThe only contribution of this paper is combining the factorized second order representations  of (Kim et. al. 2017) with a codebook style assignment (sec. 3.2). Seems marginal.\n\nThe scheme described in Sec. 3.2 needs clarification. The assignment is applied to x as h(x) \\kron x in (7). Then the entire N^2 D^2 dimensional second order descriptor h(x) \\kron x \\kron h(x) \\kron x is projected on a N^2 D^2 dim w_i. The latter is factorized into p_i, q_i \\in \\mathbb{R}^{Nd}, which are further factorized into codebook specific projections u_{i,j}, v_{i,j} \\in \\mathbb{R}^{d}. Is this different from classical assignment, where x is hard assigned to one of the N codewords as h(x), then projected using \\mathbb{R}^d dimensional p_i, q_i specific to that codeword ?\n\nIn section 4.1 and Table 2, is the HPBP with codebook the same as the proposed CHPBP ? The wording in \"Then we re-implement ... naively to a codebook strategy\"  seems confusing.\n\nThe method denoted \"Margin\" in Table 4 seems to be better than the proposed approach on CUB. How does it compare in terms of efficiency, memory/computation ?\n\nIs it possible to see any classification results? Most of the relevant second order embeddings have been evaluated in that setting.\n\n\n===============After rebuttal ===============================\n\nAfter reading all reviews, considering author rebuttal and AC inputs, I believe my initial rating is a bit generous. I would like to downgrade it to 4. It has been pointed out that many recent works that are of a similar flavor, published in CVPR 2018 and ECCV 2018, have slightly better results on the same dataset. Further, the only novelty of this work is the proposed factorization and not the encoding scheme. This alone is not sufficient to merit acceptance. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper492/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "cdate": 1542234448983, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335736551, "tmdate": 1552335736551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lGGaIp14", "original": null, "number": 15, "cdate": 1544543497860, "ddate": null, "tcdate": 1544543497860, "tmdate": 1544543497860, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "SJeqwyyuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Additional experiments on a different task", "comment": "Dear reviewer,\n\nRemark that the revised paper includes new experiments on fine-grain classification (different from the original retrieval experiments) evaluated with the accuracy metric (different from the recall in the retrieval experiments) in the added Section 5 and Table 4, including references suggested by the AC. We believe these additional experiments fully address the concerns about testing on only one task with only one evaluation metric."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "SJeqwyyuhX", "original": null, "number": 1, "cdate": 1541037921544, "ddate": null, "tcdate": 1541037921544, "tmdate": 1544393275457, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "content": {"title": "The proposed representation method is tested on only one task and considers only one evaluation metric", "review": "The paper proposes a second order method to represent images. More exactly, multiple (low-dimensional) projections of Kronecker products of low-dimensional representations are used to represent a limited set of dimensions of second-order representations. It is an extension of HPBP (Kim et al., ICLR 2017) but with codebook assigment. \n\nThe main advantage of the method is that, if the number of projection dimensions is small enough, the number of learned parameters is small and the learning process is fast. The method can be easily used as last layers of a neural network. Although the derivations of the method are straightforward, I think the paper is of interest for the computer vision community. \n\nNonetheless, I think that the experimental evaluation is weak. Indeed, the article only considers the specific problem of transfer learning and considers only one evaluation metric (recall@k). However, recent papers that evaluate their method for that task also use the Normalized Mutual Information (NMI) (e.g. [A,B]) or the F1-score [B] as evaluation metrics. \nThe paper does not compare the same task and datasets as (Kim et al., ICLR 2017) either.\nIt is then difficult to evaluate whether the proposed representation is useful only for the considered task. Other tasks and evaluation metrics should be considered.\nMoreover, only the case where D=32 and R=8 are evaluated. It would be useful to observe the behavior of the approaches for different values of R. \nIn Section 3.2, it is mentioned that feature maps become rapidly intractable if the dimension of z is above 10. Other Factorizations are then proposed. How do these factorizations affect the second order nature of the representation of z? Is the proposed projection in Eq. (10) still a good approximation of the second order information induced by the features x?\n\n\nThe paper says that the method is efficient but does not mention training times. How does the method compare in terms of clockwork times compared to other approaches (on machines with similar architecture)?\n\nIn conclusion, the experimental evaluation of the method is currently too weak.\n\n\n[A] Hyun Oh Song, Stefanie Jegelka, Vivek Rathod, Kevin Murphy: Deep Metric Learning via Facility Location. CVPR 2017\n[B] Wang et al., Deep Metric Learning with Angular Loss, ICCV 2017\n\nafter rebuttal:\nThe authors still did not address my concern about testing on only one task with only one evaluation metric.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper492/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "cdate": 1542234448983, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335736551, "tmdate": 1552335736551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxx7MMokN", "original": null, "number": 13, "cdate": 1544393240219, "ddate": null, "tcdate": 1544393240219, "tmdate": 1544393240219, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "HJgGDF6wkN", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "I agree with AC", "comment": "First of all, I would like to mention that my confidence is 2 because I am not completely up-to-date with the current literature in such codebook representation learning. \nAC's point that all the proposed contributions were proposed in the literature (although in separated papers) makes the contribution of the paper limited.\n\nMoreover, the authors did not address my concern about testing on only one task with only one evaluation metric.\nI then updated my score accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "r1l2o-6YRQ", "original": null, "number": 6, "cdate": 1543258532034, "ddate": null, "tcdate": 1543258532034, "tmdate": 1543309060217, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "SkxjnMNFCX", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Comparison with other recent works", "comment": "\nWe would like to thank you for pointing out these recent references. However, most of them do not focus on compactness and efficiency which is the main focus of this paper and consequently do not offer a fair comparison. We aim at low dimensional features and low complexity, while they focus on improved accuracy regardless of the cost. Recall that we also report very good results on retrieval tasks and datasets, for which compactness is an essential property.\n\nAs a sum-up, references [2, 4] use 262k dimensions compared to our 512dimensions. [1] uses the same factorization as CBP-TS upon which we show improvements. Only [3] offers a fair comparison and achieves similar results as ours.\n\nIn details:\n\n[1] MoNet: MoNet proposes a normalization using Tensor Sketch factorization with 10k dimensions to reach the reported performances. Furthermore, MoNet requires the computation of the SVD during the forward pass, which is a costly operation not comparable with a feed forward only implementation.\n\n[2] Second-order democratic aggregation: The reported results with VGG16 use 262k dimensions. Moreover, the forward pass relies on the Dempened Sinkhorn Algorithm, which is an iterative algorithm and thus not comparable with simple feed forward methods.\n\n[3] Statistically motivated second order pooling: The results reported with VGG16 on CUB are 85% at 2k dimensions and 82.6% at 64 dimensions. We report 84.3% at 512 dimensions, which we believe is comparable to their results assuming the usual logarithmic progression of accuracy versus dimension.\n\n[4] DeepKSPD: The reported results are obtained using the full bilinear dimension (262k) and are thus not comparable to compact representations. Moreover, they use a costly decomposition of the kernel matrix which means that neither compactness (as the full matrix has to be computed) nor efficiency can be achieved using this method.\n\n\nAbout \"whether the proposed here compact representations are really a non-incremental non-trivial achievement that builds on the Bilinear pooling\":\n\nWe are to our knowledge the first to introduce an efficient and compact codebook strategy to bilinear models thanks to a non-trivial factorization scheme, with competitive results. All recently published methods focus on improving the accuracy (for example by proposing better normalization), but are consequently not compatible with\ncompact factorization that make bilinear models tractable. As such, we believe our method offers a nice accuracy improvement to compact and efficient bilinear models, which recently published methods are not able to do due to their lack of factorization.\n\n\nAbout the minor comment: \n\nThe goal of equations 1 and 2 is mainly to introduce our notations and the tensor framework to ease the transition to section 3. Eq.(3) has the same objective: Presenting this known expansion to ease the transition from Eq.(5) to Eq.(6) and from Eq.(7) to Eq.(8). We do nonetheless agree that these are common equations, and we add them only for the sake of clarity.\n\nEdit: We upload a new version of the paper with references [1] and [3] added to Table 4."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "SkxjnMNFCX", "original": null, "number": 5, "cdate": 1543221939065, "ddate": null, "tcdate": 1543221939065, "tmdate": 1543221939065, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Comparison with other recent works.", "comment": "Dear authors and reviewers.\n\nAt a glance, some of the missing recent works which outperform the proposed scheme are missing. For instance compact representations:\n[1] MoNet: Moments Embedding Network by Gou et al. (e.g. Stanford Cars via Tensor Sketching: 90.8 vs. 90.4 in this submission, Airplane: 88.1 vs. 87.3% in this submission, 85.7 vs. 84.3% in this submission)\n[2] Second-order Democratic Aggregation by Lin et al. (e.g. Stanford Cars: 90.8 vs. 90.4 in this submission)\n[3] Statistically-motivated Second-order Pooling by Yu et al (CUB: 85%)\n\nAlso, regular Bilinear representations:\n[4] DeepKSPD: Learning Kernel-matrix-based SPD Representation for Fine-grained Image Recognition by Engin et al.\n\nMoreover, it seems that basic Bilinear pooling (although non-compact) reaches 86.4, 89.3 and 91.8% on CUB, Airplane, and Cars again outperforming the results achieved in this paper.\n\nApproaches such as [4] go even further with Bilinear pooling: 93.2% for Cars vs. 90.4% in this submission, 91.5% for Aircraft.\n\nGiven Bilinear pooling is a very crowded topic in venues such as CVPR, ECCV and ICCV, it begs a question whether the proposed here compact representations are really a non-incremental non-trivial achievement that builds on the Bilinear pooling. It feels as the authors avoid comparisons with some recent very strong baselines. From my own knowledge, recent CVPR'18 and ECCV'18 conferences published at least 6 new papers on this topic; non of which seems to be referenced in this manuscript. Moreover, [3] can challenge the proposed here approach in terms of compactness, [2] in terms of compactness and speed, [4] mainly in terms of excellent results via Bilinear strategy.\n\nFrom minor comments, equations such as Eq. 3 (mere expansion of a polynomial kernel) appeared in many works preceding Gao et al. (2016) backdating to year 2000 or so. Perhaps citing Gao et al. next to such an expansion is not really the most factual take on the prior literature. Dedicating Eq. 1, 2 and 3 to a simple polynomial expansion that ends up being an outer product also feels as perhaps unnecessary and can be left out for a supplementary material."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "H1eqaqTrCm", "original": null, "number": 4, "cdate": 1542998722408, "ddate": null, "tcdate": 1542998722408, "tmdate": 1542998722408, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "SJeqwyyuhX", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Additional experiments and method efficiency", "comment": "First, we would like to thank you for the high-quality review.\n\nWe answer to your concern about experimental evaluation and to questions on the efficiency of our method.\n\nQ) The experimental evaluation is weak.\nA) Since this was also pointed out by the other reviewers, we added section 5 about classification performances for fair and relevant comparison with state-of-the-art bilinear factorization methods on 3 fine-grained visual classification (FGVC) datasets (CUB, CARS and AIRCRAFT). For detailed information, we refer to the general comment and to the new version of the paper. \n\nWe train our method and 4 similar approaches proposed by Reviewer3 to confirm the necessity of the codebook strategy to provide compact but informative representations. These methods are multi-rank extensions of the factorization in Eq.(6) with the improvement proposed in HPBP [Kim et al., 2017] and HPBP formulation that we have trained on classification tasks. The conclusion is that all multi-rank methods lead to the same performance as HPBP but with smaller representation while using the same number of parameters. However, JCF consistently outperforms these methods on the 3 datasets and leads to state-of-the-art results on 2 of them with much more compact representation (512 vs ~10k).\n\nThese results confirm our intuition about the necessity to aggregate only similar features: The multi-rank methods do not have such selection mechanism preceding the projection, hence aggregate dissimilar features together. Moreover, the non-linearity not necessary improves performances as they are added after the projection is made. Although it is still possible to learn a projection coupled with the non-linearity to build a similarity driven aggregation, it would be not enforced by design. As JCF is designed for this similarity driven aggregation which is easier to train and lead to better results.\n\nQ) NMI and F1-score\nA) Due to time concern, we have chosen to focus on providing experimental results on newer datasets (CUB, CARS and AIRCRAFT) and for a different task (classification) for fair and relevant comparison with standard bilinear factorization methods, including HPBP [Kim et al., 2017].\n\nQ) The paper does not compare the same task and datasets as [Kim et al., 2017].\nA) We provide additional results on fine-grained visual classification datasets, including HPBP, with the same number of parameters as it is the standard task to evaluate bilinear representation. For more details, we refer you to the general comment and the new paper version.\n\nQ) Only the case D=32 and R=8 is evaluated\nA) In the original version of the paper, we provide ablation study of the number of clusters (Table 2) and the impact of the rank (Figure 1). Note that in the new version, Figure 1 has been replaced by Table 3. For the comparison to the state-of-the-art, we report the full model and the best compromise in performance loss/number of parameters from these ablation studies.\n\nQ) How do these factorizations affect the second order nature ?\nA) All enforced decompositions are only done on the projection matrix and not on the second order features. Thus, we can rebuild the original projection matrix which will be a sum of forth order tensors to project the original second order features.\n\nQ) How does the method compare in terms of clockwork time ?\nA) In terms of clockwork time, all methods implemented in Table 4 take roughly the same clockwork time as they can efficiently be implemented using standard layer in deep learning or optimized linear algebra computation. Quantitatively, the training (resp. testing) time for one epoch is around 300s (resp. 60s) using 448x448 images and batch of 32 (resp. 64), that is around ~ 50ms/image (resp. ~ 10ms/image). All of these computation times are given on CUB dataset, include the extraction of feature maps with VGG16 and are done on a single Nvidia GTX 1080 Ti."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "S1gjF9THCX", "original": null, "number": 3, "cdate": 1542998659419, "ddate": null, "tcdate": 1542998659419, "tmdate": 1542998659419, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "Hyx-zDkqnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Additional comparisons and discussion", "comment": "We would like to thank you for your high-quality review and the insightful propositions to strengthen our paper.\n\nWe answer to both your concerns on lack of comparison and detailed description/discussion.\n\n1) Lack of comparison\nAlso requested by the other reviewers, we report in table 4 classification accuracy on 3 fine-grained visual classification (FGVC) datasets: CUB and CARS with their respective split for classification and AIRCRAFT. We compare our method to the state-of-the-art on Bilinear factorization schemes to confirm the advantage of the codebook strategy to produce compact but rich representations. We report comparable results to the state-of-the-art with much more compact representations than others.\n\nMoreover, we also follow your proposed comparison and we implement and train the 4 approaches, that is : HPBP from [Kim et al., 2017], the multi-rank extension of the standard compression and the improvement proposed in HPBP to improve the multi-rank formulation (non-linearity and projection). Results and discussion are available in Table 4 from the new paper version.\n\n2) Lack of discussion\nWe add discussion of these results at the end of the section 5. To sum-up, HPBP and multi-rank methods in respectively 2048 dimensions and 512 dimensions are slightly lower than the state-of-the-art (~1%) while our JCF consistently outperforms these multi-rank formulations. This confirm our intuition about the importance of aggregating and projecting only related features together. Moreover, the non-linear multi-rank variants are harder to train and may not bring improvements. These multi-rank strategies do not have a selection mechanism that projects and then aggregates only related features. Moreover, the non-linearity leads to models that may be efficient, but harder to train in practice. Although it is still possible to learn a projection coupled with the non-linearity that would lead to a similarity driven aggregation, it is not enforced by design. The advantage of JCF is to provide by design this similarity driven aggregation, which leads to better results.\n\n3) Performances of JCF(32, 32) vs C-CBP(32)\nWe argue that such variation in performances are usually induced by the difficulty in practice to learn the codebook. For example, C-CBP with a 32 size codebook may in practice only use in average 28 entries, which leads to a sub-efficient representation. This is mainly due to the implicit learning of the codebook (contrarily to the cross-entropy loss used directly in classification for example). Thus, the over-parametrization allows the use of these additional entries to improve its representations."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "B1ehE56B0X", "original": null, "number": 2, "cdate": 1542998580359, "ddate": null, "tcdate": 1542998580359, "tmdate": 1542998580359, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "SkxvoNr927", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Contributions, clarifications and classification experiments", "comment": "We would like to thank you for the high-quality review.\n\nWe clarify our contributions and the pointed formulation. Finally, we also report classification results.\n\nQ) The only contribution of this paper\u2026 Seems marginal.\nA) We are convinced by the novelty of this paper, as the few methods that combined second order representations and codebook strategies faced the very high dimensionality of the representation without proposing compact factorization schemes (as shown in Table 1). E.g. MFAFVNet (Li et al., 2017b) extend this second order pooling to codebook with factorization scheme. However, even with such factorization, their proposed method leads to 500k dimension representations (i.e. 2x  Bilinear Pooling) , 27M parameters and  around 40GOps to compute the output representation. JCF provides 512d representation (1/1000 factor), 4M parameters (1/7) and 3GOps (1/10) with the same performances as well-known second order factorization scheme.\n\nTo sum up: Combining second order representation with a codebook is not trivial due to computation concerns. We are the first to provide an efficient scheme for such combination.\n\nQ) Is this different from classical assignment ?\nA) Without the introduction of rank factorization, there is no difference: the two projections u_{i,j} and v_{i,j} play a similar roles as intra-projection in VLAD representation for example. This insight is developed in section 3.2 between equations (9) and (10). However this approach is not tractable, which is the reason we proposed our joint factorization and codebook method. Moreover, to make the method end-to-end trainable, x is soft-assigned to the codebook instead of the hard-assignment of VLAD.\n\n\nQ) The wording in \u201cThen we re-implement...naively to a codebook strategy\u201d seems confusing.\nA) Thanks to point out this ambiguity, we update this sentence in the new paper version also following \u201cReviewer3\u201d remark on the terms used.\nFor clarity, we implement Bilinear Pooling (BP) and  Compact Bilinear Pooling (CBP) and not HPBP as we do not add the non-linearity nor the projection. However, we also train the projection matrices for CBP. Then, we extend these two methods naively to codebook strategy, that is by computing:\n- W^T[h(x) \\kron x \\kron x] where W \\in \\mathbb{R}^{Nd^2 \\times D} (named C-BP)\n-  CBP extended to codebook (named C-CBP), using equation (10).\n\nQ) \u201cMargin\u201d in Table 4 performs better on CUB, how does it compare ?\nA) The \u201cMargin\u201d method proposed a new sampling strategy that allows to explore much more informative triplet than standard strategies, including ours. Also, they use larger images. Note that we can exploit their sampling method to improve our training procedure.\n\nQ) Is it possible to see any classification results ?\nA) As mentioned in the general comment, we added results on 3 fine-grained visual classification (FGVC) datasets, CUB and CARS using the standard split in FGVC and Aircraft which are 3 common datasets for FGVC task. We compare our method to other factorization scheme and we report comparable results to the state-of-the-art with much more compact representation. For more details, we invite you to read the general comment and the new paper version."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "Skx1kqpSRQ", "original": null, "number": 1, "cdate": 1542998487200, "ddate": null, "tcdate": 1542998487200, "tmdate": 1542998487200, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "content": {"title": "Additional classification experiments", "comment": "First, we would like to thank our three reviewers who provided high-quality reviews and insightful remarks, but also proposed additional experiments to strengthen this paper.\n\nIn this comment, we answer to the major concern of this paper: fair and relevant comparison.\n\nWe agree with the 3 reviewers about the lack of comparison on fine-grained visual classification datasets (FGVC) against other factorization scheme. Hence, we add section 5 to discuss about the performances of our method on FGVC task.\n\nWe provide an overview of this additional section in this comment.\n\nTo evaluate the compactness of our method but also its efficiency, we compare our method on three FGVC datasets named CUB, CARS and AIRCRAFT. We report the results of the 4 equations proposed by \u201cReviewer 3\u201d, that is : (I) HPBP from [Kim et al., 2017] ; (II) the extension of Eq.(6) to multi-rank ; (III) the introduction of the same non-linearity as HPBP in the multi-rank formulation and (IV) with an additional learned combination of ranks.\n\nAll multi-rank approaches are computed with a rank of 8 and a representation dimension of 512 while HPBP uses 2048 dimensions (to be fair in number of parameters). We compare these methods to JCF(N=32, R=8).\n\nOur method consistently outperforms the multi-rank variants. This confirms our intuition about the importance of grouping features by similarity before projection and aggregation. Indeed, multi-rank variants do not have a selection mechanism preceding the projection into the subspace that would allow to selectively choose the projectors based on the input features. Instead, all features are projected using the same projectors and then aggregated. We argue that non-linear multi-rank variants bring only marginal improvements, since the non-linearity happens after the projection is made. Although it is still possible to learn a projection coupled with the non-linearity that would lead to a similarity driven aggregation, it is not enforced by design. Since JCF does the similarity driven aggregation by design, it is easier to train, which we believe explains the results.\n\nThe code for our model and the additional multi-rank experiments will be released after the reviewing process."}, "signatures": ["ICLR.cc/2019/Conference/Paper492/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609748, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1e0KsRcYQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper492/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper492/Authors|ICLR.cc/2019/Conference/Paper492/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers", "ICLR.cc/2019/Conference/Paper492/Authors", "ICLR.cc/2019/Conference/Paper492/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609748}}}, {"id": "Hyx-zDkqnQ", "original": null, "number": 2, "cdate": 1541170952647, "ddate": null, "tcdate": 1541170952647, "tmdate": 1541533949350, "tddate": null, "forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "invitation": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "content": {"title": "Interesting idea of bilinear pooling with codebooks. But, needs more experiments for validating the idea.", "review": "Summary:\nThis paper proposes a novel bilinear representation based on a codebook model.\nThe proposed method is build upon the form of Hadamard Product for efficient representation of a bilinear model.\nThrough introducing the framework of codebook, it is naturally extended into a multiple-rank representation while the efficient pooling scheme is also derived from the (sparse) codeword assignment.\nIn addition, the authors also present an efficient formulation in which the codebook-based projections are factorized via a shared projection to further reduce the parameter size.\nThe experimental results on image retrieval tasks show that the proposed method produces better classification accuracy with a limited amount of parameters.\n\nComments:\nPros:\n+ It is interesting that one-rank bilinear pooling is naturally extended to multiple-rank one via introducing codebooks.\n+ Good performance in the image retrieval tasks.\n\nCons:\n- This paper lacks important comparison for fairly evaluating the effectiveness of the proposed formulation.\n- It also lacks detailed description and discussion for the methods.\n\nDue to the above-mentioned weak points, the reviewer cannot fully understand whether the performance improvement really comes from the proposed formulation or not. Thus, this manuscript is currently judged as border. The detailed comments are shown in the followings.\n\n- Comparison\nEventually, the proposed method is closely related to the multiple-rank representation of a bilinear model;\n\nz_i = x^T W_i x (Eq.5) ~ x^T u_i v_i^T x (one-rank, Eq.6) ~ x^T U_i V_i^T x (multiple-rank), ... Eq.(A)\n\nwhich is a straightforward extension from the one-rank model. From this viewpoint, the proposed form in Eq.10 is regarded as an extension of (A) by introducing non-linearity as\n\nz_i = x^T U_i {h(x)h(x)^T} V_i^T x.  ... Eq.(10)\n\nThus, the main technical contribution is found in the weighting by {h(x)h(x)^T}, but its impact on the performance is not evaluated in the experiments. Furthermore, it is also possible to simply introduce such a non-linearity into the model (A) according to [Kim et al.,2017];\n\nz_i = \\sigma(x^T U_i) \\sigma(V_i^T x) = 1^T {\\sigma(U_i^T x) .* \\sigma(V_i^T x)}, ... Eq.(B)\n\nwhere \".*\" indicates Hadamard Product, and we can more directly apply the method of [Kim et al., 2017] to the multiple-rank model by\n\nz_i = p^T {\\sigma(U_i^T x) .* \\sigma(V_i^T x)}, ... Eq.(C)\n\nwhere p is a R-dimensional vector. On the other hand, it is also necessary to compare the proposed method with [Kim et al.,2017] which is formulated by\n\nz = P^T {\\sigma(U^T x) .* \\sigma(V^T x)}, ... Eq.(D)\n\nwhere U and V are matrices of d x K and P is K x D. The parameter K (shared rank) should be determined so that the total parameter size of (2dK + KD) is compatible to that of the proposed method, 2NdD.\n\nIn summary, for demonstrating the effectiveness of the proposed method in Eq.(10), it is inevitable to compare it with the models (A, B, D) and hopefully (C).\n\n- Presentation\nIn Section 4.2, the performance results of the factorization model in Eq.(13) are merely shown without deep discussion nor analysis on them. In particular, it is unclear why the JCF of N=32 and R=32 outperforms the CHPBP of N=32. Those two methods are different only in the form of U and V:\n(CHPBP) U_i -> U'_i A (JCF),\nwhere U_i and U'_i have the same dimensionality of d x 32, and thus we can say that JCF overly parameterizes the projection by redundantly introducing A of 32 x 32. Thus, the projection capacity of JCF is completely the same as that of CHPBP. Therefore, it needs detailed discussion for the performance improvement shown in Figure 1.\n\nMinor comments are:\n* There are lots of notations, and thus it would be better to show a summary of the notations.\n* In Section 4.1, there is no clear description about the methods of BP and HPBP. Actually, the method of HPBP is different from the one presented in [Kim et al., 2017].\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper492/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Codebook and Factorization for Second Order Representation Learning", "abstract": "Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.", "keywords": ["Second order pooling"], "authorids": ["pierre.jacob@ensea.fr", "picard@ensea.fr", "aymeric.histace@ensea.fr", "edouard.klein@gendarmerie.interieur.gouv.fr"], "authors": ["Pierre jacob", "David Picard", "Aymeric Histace", "Edouard Klein"], "TL;DR": "We propose a joint codebook and factorization scheme to improve second order pooling.", "pdf": "/pdf/51b9cb3926c4323371d090eaf8ccfc50f755d378.pdf", "paperhash": "jacob|efficient_codebook_and_factorization_for_second_order_representation_learning", "_bibtex": "@misc{\njacob2019efficient,\ntitle={Efficient Codebook and Factorization for Second Order Representation Learning},\nauthor={Pierre jacob and David Picard and Aymeric Histace and Edouard Klein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1e0KsRcYQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper492/Official_Review", "cdate": 1542234448983, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1e0KsRcYQ", "replyto": "B1e0KsRcYQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper492/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335736551, "tmdate": 1552335736551, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper492/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}