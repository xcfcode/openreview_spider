{"notes": [{"id": "BkeaEyBYDB", "original": "Ske7V1a_DS", "number": 1671, "cdate": 1569439540869, "ddate": null, "tcdate": 1569439540869, "tmdate": 1577168286736, "tddate": null, "forum": "BkeaEyBYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Lgv5ai_usX", "original": null, "number": 1, "cdate": 1576798729435, "ddate": null, "tcdate": 1576798729435, "tmdate": 1576800907090, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Decision", "content": {"decision": "Reject", "comment": "The reviewers have reached consensus that while the paper is interesting, it could use more time.  We urge the authors to continue their investigations.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726461, "tmdate": 1576800278601, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Decision"}}}, {"id": "r1xmrgpU9H", "original": null, "number": 2, "cdate": 1572421691313, "ddate": null, "tcdate": 1572421691313, "tmdate": 1574405530052, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Update: I thank authors for the rebuttal. I agree that direction of exploring personalization in FL is interesting. With a stronger methodological contribution, this could become a good paper.\n\n----------------------------------------------------------------------------------------------------------------\nThe main contribution of this paper is to notice the connection between Federated Averaging (FedAvg) and Model Agnostic Meta Learning algorithms (MAML). Authors also consider an algorithm that first trains with FedAvg and then continues training using Reptile.\n\nPros:\n\nInterpretation of FedAvg as a meta-learning algorithm is interesting.\n\nCons:\n\nVery limited methodological contribution. Proposed algorithm is essentially two existing algorithms applied one after another.\n\nExperiments are not conducted rigorously enough. There are many arbitrary hyperparameter choices which may bias the conclusions made in the paper. Statement \"We tried SGD with a range of other learning rates, and in all cases we observed this value to work the best.\" is alarming suggesting that authors tried a variation of settings observing test data performance and reported a few selected runs. Although \"each experiment was repeated 9 times with random initialization\", the train/test split of the clients was fixed. Randomizing over client train/test split could help to improve the reliability of the results.\n\nEMNIST-62 is the only dataset analyzed in some detail. This dataset has drastically varying P(y|x) across clients, i.e. some people write zeros as some others write 'o's. This suggests that it is very hard to train a good global model and personalization is necessary. However this doesn't mean that Shakespeare dataset \"does not provide any interesting insights\". Perhaps, it is indeed more interesting and challenging, demanding more advanced methodology.\n\nIn Figure 1, number of communication rounds may be impractical for FL (considering also addition 200 Reptile rounds). On Shakespeare, FedAvg paper reports 54% accuracy achieved in under 50 communication rounds in one of the settings. There are also recent works on improving communication efficiency that were not discussed or studied for personalization quality, e.g. FedProx from \"Federated Optimization in Heterogeneous Networks\" and PFNM from \"Bayesian Nonparametric Federated Learning of Neural Networks\".\n\nQuestions about Figure 2 experiments:\n1. Fine-tuning requires 200 extra epochs over the initially trained model. What's the initial model accuracy when FedAvg is further trained with Adam optimizer for 200 extra communication rounds?\n2. The personalized test accuracy with FedAvg and Reptile fine-tuning reaches the same value in 10 update epochs, even when Reptile fine-tuning gets 200 extra initial training epochs. Does Reptile fine-tuning provide additional benefits to the initial model as compared to running FedAvg for more number of epochs?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Reviewers"], "noninvitees": [], "tcdate": 1570237733978, "tmdate": 1574723077588, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review"}}}, {"id": "Syxuj0h8iB", "original": null, "number": 6, "cdate": 1573469855838, "ddate": null, "tcdate": 1573469855838, "tmdate": 1573469855838, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment", "content": {"title": "Response to all reviewers", "comment": "We thank the reviewers for their time, and \n- Feel discouraged because none of the reviews provide us with feedback to the main message of our work - Section 4\n- Agree that studying datasets other than EMNIST-62 is valuable, but argue that the presented results already challenge existing practices of the field.\n\nWe feel our submission may have been read with incorrect expectations. Our argument does not fit into the usual \u201chere is a new algorithm, here is why it is better than the state-of-the-art\u201d. Rather, it presents novel insights, which challenge what is the objective of the state-of-the-art, and argues that different measures should be the object of study of future works in this area.\n\nAs such, we hope the work has the potential to become influential, and we seek feedback on these arguments. Much of the reviews we received focus on what we don\u2019t claim to be our contribution.\n\nThe most interesting observation, as summarized in Figure 2 - and motivating the main conclusions of our work - is that the same models, trained differently, to a similar initial accuracy, can have very different capacity to personalize to a task of the same type as it was trained on. We are not aware of any observation of this kind in the ML literature. As summarized in the concluding Section 4, we formulate concrete challenges to the main objectives of the existing FL and supervised MAML works, and also motivate questions beyond the areas of FL/MAML.\n\nWe also highlight that traditional measures predicting generalization/overfitting are a surprisingly misleading indicator of how well a model can personalize (Table 2).\n\nWe would like to ask the reviewers to re-read Section 4 - where we summarize why and how we think this paper can influence the future work of other researchers - and provide feedback on \n- Do the presented results support the challenges? If not, why?\n- Are the recommendations likely to impact future works in the field? If not, why?\n- Do any of these already have an answer? If yes, which?\n\nWe would be disappointed to go through the review process without any feedback on these questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1671/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1671/Authors|ICLR.cc/2020/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152580, "tmdate": 1576860532690, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment"}}}, {"id": "rklkJ0hIjB", "original": null, "number": 5, "cdate": 1573469654695, "ddate": null, "tcdate": 1573469654695, "tmdate": 1573469654695, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "HkxMzGThcS", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "We thank the reviewer #4 for their time.\n\nWe agree that the method we experiment with, Algorithm 2, is not particularly complex or novel. Note, however, this is not what we present as our main contribution, either. It is not mentioned in the abstract nor the concluding chapter.\n\nRe: \u201c...to believe that regular reptile is beaten by FedAvg+reptile.\u201d\nRelated to the above point, we never make such a claim, or that this would even be our objective. \n\nLet us restate the design objectives from Section 1: (1) good initial global model, (2) good personalized model, and (3) fast convergence.\n\nIn Section 2, we show that FedAvg and Reptile are essentially the same, with the difference being that FedAvg handles different local data size differently, while this was non-existing concern in the setting in which Reptile was introduced.\n\nRunning with FedAvg with large epoch addresses (2) and, mainly (3), but lacks (1). Then switching to a smaller number of steps, independent of the amount of local data (i.e., Reptile) improves (1), without hurting (2) and not needing many additional iterations (3). This contrasts with the experiments in the original Reptile paper, where on the Omniglot task, 40,000 iterations are presented - which would be very expensive in the context of FL.\n\nWe also show that (and we don\u2019t find this intuitively expected) Reptile with different number of steps show quite different performance - using K=1 degrades the personalized performance.\n\nIn summary, our claim is not that one algorithm \u201cbeats\u201d another in a narrow sense, but rather when focusing on the three objectives simultaneously, a combination works better than either of them separately. Fig 2 then shows that models of similar initial accuracy can have very different capacity to personalize, motivating the case for expanding the scope of MAML algorithms, as suggested in the concluding Section 4.\n\n---\nRe: \u201cthe \"three objectives\".  I feel meta-learning is doing all three too.\u201d\nThe problems commonly studied for supervised MAML (random sine wave and Omniglot) do not admit any meaningful notion of initial accuracy - any model is just a random guess. We think this is the main reason why the gap presented in Figure 2 have not been observed before, and that FL applications should become a common part of benchmarks for MAML algorithms.\n\nRe: Distributed Reptile\nWe are not sure what you refer to. The paper https://arxiv.org/pdf/1803.02999.pdf has only a single short remark (end of Section 3) on anything related to distributed optimziation.\n\nPlease also see our shared response to all reviewers.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1671/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1671/Authors|ICLR.cc/2020/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152580, "tmdate": 1576860532690, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment"}}}, {"id": "ByxphT2UjB", "original": null, "number": 4, "cdate": 1573469620518, "ddate": null, "tcdate": 1573469620518, "tmdate": 1573469620518, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "r1xmrgpU9H", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "We thank the reviewer #3 for their time.\nPlease also see our shared response to all reviewers - we would like to ask for feedback to the main points presented in this work. \n\nResponding to specific points made:\nRe: Experimental setup / rigor\nComment on \u201calarming\u201d choice of learning rate - note we do not make an absolute claim, but a claim relative to the training procedure. The local personalization optimizer (including learning rate) that worked best, was exactly the same as the local optimizer used during training. This is in stark contrast with what we observe for differently trained models in Figure 2, where in one case we had to decrease the learning rate by factor of 200, to prevent the model from diverging. This is of major practical value, as it effectively removes a hyperparameter choice.\nRe: Client train/test split - note that reporting variance would not make sense, as we would not be measuring the same value between such experiments. Moreover, this request is analogous to using different train/test split for any work using (for instance) ImageNet data, which is not aligned with the common practice of the field - so that we would have the same yardstick to compare against, across different publications.\n\nRe: \u201cdataset has drastically varying P(y|x) across clients. (...) it is hard to train a good global model and personalization is necessary\u201d\nCorrect. This is the main point, which reflects the practical setting. Nevertheless, almost the entire field has been studying quality of the global model only, following the example of initial paper introducing FedAvg. In Section 4, we argue that the field should shift its focus, and this is one of our main contributions. Also, note that the difference across clients in EMNIST-62 is primarily in P(x) in this dataset, not the conditional.\n\nRe: \u201cthis doesn't mean that Shakespeare dataset ...\u201d\nWe agree. And we do not make such claim in the submission. We say \u201ceven though the data is non-i.i.d., the next-character prediction is mostly focused on a local structure of the language in general, and is similar across all users\u201d which in the language above would be \u201ceven though P(x) is different, P(y|x) is similar.\u201d\n\nRe: \u201cFedAvg paper reports 54% accuracy achieved in under 50 communication rounds\u201d\nThis does not compare apples and apples. The presentation is not fully reproducible, and is different from the data preprocessing settled on for this dataset in the Leaf and TFF projects (see 1146 clients in FedAvg paper vs. 715 in tff.simulation.datasets.shakespeare due to discarding minor roles) and reports weighted average of accuracies, which we argue is misleading as we care about the future (unknown) performance, and thus report unweighted average.\n\nRe: other recent works:\nWe would like to restate the main point of our paper. We do not claim to propose a new algorithm that is superior in some sense. Rather, we challenge the objective studied in existing works, including those you refer to. We agree that studying other methods in the light of objectives we present is valuable, and are trying to argue that the field should do so. As for our main challenge, this is complementary, not contradictory.\n\nRe: Q\u2019s on Fig 2:\n- 75% on average, similar to Momentum, as presented.\n- In Fig. 1, initial accuracy of FedAvg  trained model is stable (in terms of mean/variance across experiments) during rounds 300-500. Further training with the same parameters did not produce a different result.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1671/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1671/Authors|ICLR.cc/2020/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152580, "tmdate": 1576860532690, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment"}}}, {"id": "B1eIla2IoH", "original": null, "number": 3, "cdate": 1573469422345, "ddate": null, "tcdate": 1573469422345, "tmdate": 1573469422345, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "Byxju1vRFr", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "We thank the reviewer #1 for their time.\n\nRe: \u201cI am not sure about the contribution of this paper\u201d\nThe contribution are the observations that motivate the challenges in Section 4. As formulated in the response to all reviewers, we would like to ask for feedback on these challenges. The contribution is not a novel or superior algorithm - and note we do not claim that in abstract or in the concluding Section.\n\nRe: point 1)\nEven though this connection is not mathematically complex to explain, it has not been formulated, despite FedAvg being proposed before the setting of MAML, and the two fields share many similar characteristics.\nWe use this connection to argue and empirically show that what FedAvg was presented to do, is in fact not correct, and is only a side-effect of the actual objective.\n\nRe: point 2)\nSee also the response to all reviewers. We do not claim that the algorithm is particularly novel or superior in any sense, but use it to explore new relationships which eventually motivate the challenges in Section 4. We would welcome feedback on what we did, in addition to what we did not do. We agree that experiments on other datasets will be valuable, and see this as orthogonal to the recommendations we are arguing for.\n\nRe: point 3)\nThis is what we formulate as the practical requirements in Section 1 - that we need to consider jointly all three of the following objectives - (1) Improved Personalized Model (2) Solid Initial Model and (3) Fast Convergence - with (2) motivated by the fact that many clients will not have data to personalize on. And it is the motivation for what we study in Section 3.2. - start with model with good personalization and improve the initial accuracy."}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1671/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1671/Authors|ICLR.cc/2020/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152580, "tmdate": 1576860532690, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment"}}}, {"id": "Byxju1vRFr", "original": null, "number": 1, "cdate": 1571872627406, "ddate": null, "tcdate": 1571872627406, "tmdate": 1572972438256, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper considers personalization federated learning problem in which the goal is to personalize the global model on a given client/device based on available data on that device/client.  The paper claims not only their proposed method can lead to fast convergence time but also provide a solid initial model per device/client and results in a better-personalized model. To evaluate the performance of their method, EMNIST-62 and Shakespeare data are used.\n\nEven though personalization in federated learning is very interesting and challenging, I am not sure about the contribution of this paper and what is exactly proposed in this paper: \n\n1)  Section 2: this paper shows the relationship between FedAvg and MAML. In my view, the connection is very straight forward and can be shown in a couple of sentences. I might be missing something here, but it is not obvious to me what this paper adds to the connection between MAML and FedAvg.  \n\n2)  Personalized FedAvg Section: The same is about section 3. In my view, Algorithm 2 doesn't say anything new rather than to use Adam in local machine and SGD on global models and to optimize for \"E\" steps. But what if we use other datasets rather than EMNIST-62 and Shakespeare? will these recommendations still hold, i.e. using SGD on server and Adam on the devices? Per section 3 of this paper, Algorithm 2 indeed is the result of the experimental adaptation of the FedAvg algorithm so generalization to other datasets won't be obvious and it is a big question to me.\n \n3) Also, the paper mentioned that this method can work even if there is no local data available on some of the devices/clients. I wasn't able to understand how personalization possible if there is no data to personalize. Wouldn't a device/client just use the global model?\n\nIn summary, I find the contribution and novelty of this paper limited and the empirical findings of this paper can't be always applicable to other datasets and scenarios. Plus, I am not convinced this paper shows anything different than FedAvg rather than some recommendations about local and global optimizer selections."}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Reviewers"], "noninvitees": [], "tcdate": 1570237733978, "tmdate": 1574723077588, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review"}}}, {"id": "HkxMzGThcS", "original": null, "number": 3, "cdate": 1572815370442, "ddate": null, "tcdate": 1572815370442, "tmdate": 1572972438170, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies the application of techniques from meta-learning (a method\nto train a single model which can then be easily adjusted to perform well on\nmultiple tasks) to federated learning (the task of distributed training of\nmodels on distributed datasets).  The paper notes that standard meta-learning\nalgorithms are similar to standard federated learning algorithms, and uses\nthis perspective to produce a merged method and evaluate it empirically.\n\nPros.\n+ The motivation of the paper is clear and indeed these methods seems similar,\n and meta-learning can help with federated learning.\n\nCons.\n- The resulting method appears somewhat underdeveloped; it is simply to run\n some amount of federated learning and then some amount of meta-learning,\n whereas the first parts of the paper led me to believe that a single\n simultaneous merge of the methods is the way to go.  The paper does not\n report any fine-grained evaluation of various such choices, thus I don't know\n why they did that they did, and thus do not find their choices compelling.\n- The Reptile method is already presented in the original paper with\n a distributed counterpart, so why not just run that?  I am not convinced that\n some more minor modification of Reptile could not already do well on this\n paper.\n- The empirical evaluation is not very extensive, so I am also not convinced\n there, and in particular I need convincing of this type to believe that\n regular reptile is beaten by FedAvg+reptile.\n\nMinor comments.\nPage 1, second paragraph, the word \"outperform\".  I'm not sure what the\nperformance measure is; in federated learning, we care about many things, for\ninstance privacy, keeping the work on the distributed clients low, etc.\nPage 2, the \"three objectives\".  I feel meta-learning is doing all three too.\nPage 3, Algorithm 1.  I realize space is a concern, but this was hard to read.\nPage 4, Algorithm 2.  \"relatively larger\" is vague."}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Reviewers"], "noninvitees": [], "tcdate": 1570237733978, "tmdate": 1574723077588, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Review"}}}, {"id": "rkeipu_fOS", "original": null, "number": 2, "cdate": 1570044098795, "ddate": null, "tcdate": 1570044098795, "tmdate": 1570044098795, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "H1ehS-3-_B", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Public_Comment", "content": {"comment": "Thanks for your detailed information ", "title": "Thanks for your reply"}, "signatures": ["~Stone_Jamess1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stone_Jamess1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191444, "tmdate": 1576860566350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Public_Comment"}}}, {"id": "H1ehS-3-_B", "original": null, "number": 1, "cdate": 1569993027631, "ddate": null, "tcdate": 1569993027631, "tmdate": 1569993027631, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "r1eGfIbZdB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment", "content": {"comment": "Thank you for your interest!\n\nYou are correct, the subsequent local gradients are computed with respect to different models, and averaged to form a new model only after a number of local steps. This is motivated by the usual high cost of such averaging operation in federated learning. See for instance Figure 1 in (McMahan, 2017) which proposed FedAvg, for empirical visualization that this idea makes sense if you start from the same point, but not if you have two random models. Our eq (5) is thus only a different view on this existing method, providing additional insight into what is it actually optimizing for.", "title": "Interpretation of Equation (5)"}, "signatures": ["ICLR.cc/2020/Conference/Paper1671/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1671/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1671/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1671/Authors|ICLR.cc/2020/Conference/Paper1671/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504152580, "tmdate": 1576860532690, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Official_Comment"}}}, {"id": "r1eGfIbZdB", "original": null, "number": 1, "cdate": 1569949193657, "ddate": null, "tcdate": 1569949193657, "tmdate": 1569949193657, "tddate": null, "forum": "BkeaEyBYDB", "replyto": "BkeaEyBYDB", "invitation": "ICLR.cc/2020/Conference/Paper1671/-/Public_Comment", "content": {"comment": "I quite don't understand why the eq(5) is derived.\n\nIf you choose the Fedsgd, it means every step you have to update the model and every step the gradient global(g_i) is computed based on the current average global model. \n\nIf you use the fedavg, then the local client will compute gradients k steps, but the gradient local(g_i) is computed based on the local model. \n\nIf you don't average at every step, then the global model parameter and local model parameter are different, So how can you connect this two together? \nCan you explain about it?", "title": "Questions about Equation"}, "signatures": ["~Stone_Jamess1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Stone_Jamess1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Federated Learning Personalization via Model Agnostic Meta Learning", "authors": ["Yihan Jiang", "Jakub Kone\u010dn\u00fd", "Keith Rush", "Sreeram Kannan"], "authorids": ["yihanrogerjiang@gmail.com", "konkey@google.com", "krush@google.com", "ksreeram@uw.edu"], "keywords": ["Federated Learning", "Model Agnostic Meta Learning", "Personalization"], "TL;DR": "Federated Averaging already is a Meta Learning algorithm, while datacenter-trained methods are significantly harder to personalize.", "abstract": "Federated Learning (FL) refers to learning a high quality global model based on decentralized data storage, without ever copying the raw data. A natural scenario arises with data created on mobile phones by the activity of their users. Given the typical data heterogeneity in such situations, it is natural to ask how can the global model be personalized for every such device, individually. In this work, we point out that the setting of Model Agnostic Meta Learning (MAML), where one optimizes for a fast, gradient-based, few-shot adaptation to a heterogeneous distribution of tasks, has a number of similarities with the objective of personalization for FL. We present FL as a natural source of practical applications for MAML algorithms, and make the following observations. 1) The popular FL algorithm, Federated Averaging, can be interpreted as a meta learning algorithm. 2) Careful fine-tuning can yield a global model with higher accuracy, which is at the same time easier to personalize. However, solely optimizing for the global model accuracy yields a weaker personalization result. 3) A model trained using a standard datacenter optimization method is much harder to personalize, compared to one trained using Federated Averaging, supporting the first claim. These results raise new questions for FL, MAML, and broader ML research.", "pdf": "/pdf/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "paperhash": "jiang|improving_federated_learning_personalization_via_model_agnostic_meta_learning", "original_pdf": "/attachment/2c4b713b2c09c0749c1b5b884c91a403d28e2436.pdf", "_bibtex": "@misc{\njiang2020improving,\ntitle={Improving Federated Learning Personalization via Model Agnostic Meta Learning},\nauthor={Yihan Jiang and Jakub Kone{\\v{c}}n{\\'y} and Keith Rush and Sreeram Kannan},\nyear={2020},\nurl={https://openreview.net/forum?id=BkeaEyBYDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkeaEyBYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504191444, "tmdate": 1576860566350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1671/Authors", "ICLR.cc/2020/Conference/Paper1671/Reviewers", "ICLR.cc/2020/Conference/Paper1671/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1671/-/Public_Comment"}}}], "count": 12}