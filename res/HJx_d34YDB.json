{"notes": [{"id": "HJx_d34YDB", "original": "B1x5JkwUIS", "number": 46, "cdate": 1569438831656, "ddate": null, "tcdate": 1569438831656, "tmdate": 1577168242523, "tddate": null, "forum": "HJx_d34YDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yinzhao.zy@alibaba-inc.com", "longjun.clj@alibaba-inc.com", "chaoping.tcp@alibaba-inc.com", "auzj_alex@mail.scut.edu.cn", "weiwu@scut.edu.cn"], "authors": ["Yin Zhao", "Longjun Cai", "Chaoping Tu", "Jie Zhang", "Wu Wei"], "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT", "keywords": ["multi-modal fusion", "affective computing", "temporal context", "residual-based training strategy"], "abstract": "Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but\nhave not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "pdf": "/pdf/33a143281af746485db625d1ffee095bfa413043.pdf", "paperhash": "zhao|video_affective_impact_prediction_with_multimodal_fusion_and_longshort_temporal_context", "original_pdf": "/attachment/33a143281af746485db625d1ffee095bfa413043.pdf", "_bibtex": "@misc{\nzhao2020video,\ntitle={{\\{}VIDEO{\\}} {\\{}AFFECTIVE{\\}} {\\{}IMPACT{\\}} {\\{}PREDICTION{\\}} {\\{}WITH{\\}} {\\{}MULTIMODAL{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}LONG{\\}}-{\\{}SHORT{\\}} {\\{}TEMPORAL{\\}} {\\{}CONTEXT{\\}}},\nauthor={Yin Zhao and Longjun Cai and Chaoping Tu and Jie Zhang and Wu Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx_d34YDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "pyypzkoXPB", "original": null, "number": 1, "cdate": 1576798685865, "ddate": null, "tcdate": 1576798685865, "tmdate": 1576800949091, "tddate": null, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "invitation": "ICLR.cc/2020/Conference/Paper46/-/Decision", "content": {"decision": "Reject", "comment": "There is no author response for this paper. The paper addresses the affective analysis of video sequences in terms of continual emotions of valence and arousal. The authors propose a multi-modal approach (combining modalities such as audio, pose estimation, basic emotions and scene analysis) and a multi-scale temporal feature extractor (to capture short and long temporal context via LSTMs) to tackle the problem. All the reviewers and AC agreed that the paper lacks (1) novelty, as the proposed approach is a combination of the existing well-studied techniques without explanations why and when this could be advantageous beyond the considered task, (2) clarity and motivation -- see R2\u2019s and R3\u2019s concerns and suggestions on how to improve. We hope the reviews are useful for improving the paper.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yinzhao.zy@alibaba-inc.com", "longjun.clj@alibaba-inc.com", "chaoping.tcp@alibaba-inc.com", "auzj_alex@mail.scut.edu.cn", "weiwu@scut.edu.cn"], "authors": ["Yin Zhao", "Longjun Cai", "Chaoping Tu", "Jie Zhang", "Wu Wei"], "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT", "keywords": ["multi-modal fusion", "affective computing", "temporal context", "residual-based training strategy"], "abstract": "Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but\nhave not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "pdf": "/pdf/33a143281af746485db625d1ffee095bfa413043.pdf", "paperhash": "zhao|video_affective_impact_prediction_with_multimodal_fusion_and_longshort_temporal_context", "original_pdf": "/attachment/33a143281af746485db625d1ffee095bfa413043.pdf", "_bibtex": "@misc{\nzhao2020video,\ntitle={{\\{}VIDEO{\\}} {\\{}AFFECTIVE{\\}} {\\{}IMPACT{\\}} {\\{}PREDICTION{\\}} {\\{}WITH{\\}} {\\{}MULTIMODAL{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}LONG{\\}}-{\\{}SHORT{\\}} {\\{}TEMPORAL{\\}} {\\{}CONTEXT{\\}}},\nauthor={Yin Zhao and Longjun Cai and Chaoping Tu and Jie Zhang and Wu Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx_d34YDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730594, "tmdate": 1576800283417, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper46/-/Decision"}}}, {"id": "SygpRQ0sFB", "original": null, "number": 1, "cdate": 1571705813158, "ddate": null, "tcdate": 1571705813158, "tmdate": 1572972645628, "tddate": null, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "invitation": "ICLR.cc/2020/Conference/Paper46/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work design a framework to predict valence and arousal values in the emotional impact. Although its performance is much better than previous works, I have some questions about the current submission:\n-- The writing is not clear enough. I have to guess the technical details based on the context. For example, in Equation 1, how to estimate y_{i}? And why put 1/m behind \\sum? Is there any particular reason for this? It is a classification problem here, why use MSE loss rather than cross-entropy loss?\n\n-- \"If modality i plays an important role, it would push the mapping to complete the f_{i-1} towards f_i to get better performance.\" Are you suggesting that sometimes H() will be about zero?\n\n-- Figure 2 is not clear enough for me. I still can not fully understand what ``progressive\" means here. I will appreciate it if the authors can make this more clear. \n\n-- What concerns me the most is the novelty of this work. It seems solid and effective. However, based on the current content, I can not fully appreciate its novelty. Using deep learning models to extract features from different modalities has been used before. Using deep learning to consider inter-clip and intra-clip relations also has been conducted. Using LSTM for fusion is not new, either. The authors claimed they propose a progressive training strategy for effectively training and information fusion. However, given the current version, I can not fully appreciate it.  \n\nI can change my point if the authors would provide more details and explanations later, which can help me to understand the novelty of this work fully. Thanks."}, "signatures": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yinzhao.zy@alibaba-inc.com", "longjun.clj@alibaba-inc.com", "chaoping.tcp@alibaba-inc.com", "auzj_alex@mail.scut.edu.cn", "weiwu@scut.edu.cn"], "authors": ["Yin Zhao", "Longjun Cai", "Chaoping Tu", "Jie Zhang", "Wu Wei"], "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT", "keywords": ["multi-modal fusion", "affective computing", "temporal context", "residual-based training strategy"], "abstract": "Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but\nhave not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "pdf": "/pdf/33a143281af746485db625d1ffee095bfa413043.pdf", "paperhash": "zhao|video_affective_impact_prediction_with_multimodal_fusion_and_longshort_temporal_context", "original_pdf": "/attachment/33a143281af746485db625d1ffee095bfa413043.pdf", "_bibtex": "@misc{\nzhao2020video,\ntitle={{\\{}VIDEO{\\}} {\\{}AFFECTIVE{\\}} {\\{}IMPACT{\\}} {\\{}PREDICTION{\\}} {\\{}WITH{\\}} {\\{}MULTIMODAL{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}LONG{\\}}-{\\{}SHORT{\\}} {\\{}TEMPORAL{\\}} {\\{}CONTEXT{\\}}},\nauthor={Yin Zhao and Longjun Cai and Chaoping Tu and Jie Zhang and Wu Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx_d34YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575046330485, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper46/Reviewers"], "noninvitees": [], "tcdate": 1570237757933, "tmdate": 1575046330498, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper46/-/Official_Review"}}}, {"id": "SygCYdSRFr", "original": null, "number": 2, "cdate": 1571866757820, "ddate": null, "tcdate": 1571866757820, "tmdate": 1572972645594, "tddate": null, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "invitation": "ICLR.cc/2020/Conference/Paper46/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper tackles the problem of affective impact prediction from multimodal sequences. The authors achieve state-of-the-art performance by using (i) a two-time-scale temporal feature extractor, (ii) progressive training strategy for multi-modal feature fusion, and (iii) pretraining. They divide a long video sequence evenly into several clips. The idea of applying one LSTM for intra-clips (short-time) temporal feature extraction and another LSTM for inter-clips (long-time) temporal feature extraction, resulting in two-time-scale, looks reasonable choice but not novel. The proposed progressive training strategy is mainly used for the modality-specific LSTMs training, which is used during the intra-clip short-time modeling phrase. It seems working well in practice. However, from the explanation, it's not clear why this strategy is good for the *complementary* fusion. Each modality LSTM is trained sequentially with features extracted from other fixed modality LSTMs. Also, the authors seem not explaining why they set the training order for LSTMs at this stage to be the descending order of their performance in the previous stage. In the experiments section, two weak baseline models and several previous state-of-the-art models are compared. However, enough ablation studies on the two-time-scale structure and proposed training strategy are not provided to demonstrate that the proposed method does provide complementary multi-modal feature fusion, which is claimed as a contribution. \n\nOverall, although it achieves state-of-the-art performance on the task, none of the claimed contributions is novel or significant enough. It is a combination of existing ideas (but giving good performance). The proposed training procedure is also weak to be considered as a scientific contribution.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yinzhao.zy@alibaba-inc.com", "longjun.clj@alibaba-inc.com", "chaoping.tcp@alibaba-inc.com", "auzj_alex@mail.scut.edu.cn", "weiwu@scut.edu.cn"], "authors": ["Yin Zhao", "Longjun Cai", "Chaoping Tu", "Jie Zhang", "Wu Wei"], "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT", "keywords": ["multi-modal fusion", "affective computing", "temporal context", "residual-based training strategy"], "abstract": "Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but\nhave not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "pdf": "/pdf/33a143281af746485db625d1ffee095bfa413043.pdf", "paperhash": "zhao|video_affective_impact_prediction_with_multimodal_fusion_and_longshort_temporal_context", "original_pdf": "/attachment/33a143281af746485db625d1ffee095bfa413043.pdf", "_bibtex": "@misc{\nzhao2020video,\ntitle={{\\{}VIDEO{\\}} {\\{}AFFECTIVE{\\}} {\\{}IMPACT{\\}} {\\{}PREDICTION{\\}} {\\{}WITH{\\}} {\\{}MULTIMODAL{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}LONG{\\}}-{\\{}SHORT{\\}} {\\{}TEMPORAL{\\}} {\\{}CONTEXT{\\}}},\nauthor={Yin Zhao and Longjun Cai and Chaoping Tu and Jie Zhang and Wu Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx_d34YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575046330485, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper46/Reviewers"], "noninvitees": [], "tcdate": 1570237757933, "tmdate": 1575046330498, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper46/-/Official_Review"}}}, {"id": "Hkxiqjr8qr", "original": null, "number": 3, "cdate": 1572391826564, "ddate": null, "tcdate": 1572391826564, "tmdate": 1572972645549, "tddate": null, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "invitation": "ICLR.cc/2020/Conference/Paper46/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a framework to predict valence and arousal tasks in videos. The framework mainly employs LSTM in a two-time-scale-structure to take multimodal inputs. In general, the proposed framework groups well-studied techniques to solve a well-known task of multimodal learning. \n\nDNN based Multimodal learning has been heavily investigated for a long time. Numerous frameworks have been proposed with various success. Although the proposed framework is technically sound, the proposed \"residual-based training strategy\" and \"long temporal fusion\" are kind of trivial or lackluster. I can hardly identify any significant contributions that support a publication in top machine learning conferences such as ICLR."}, "signatures": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper46/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yinzhao.zy@alibaba-inc.com", "longjun.clj@alibaba-inc.com", "chaoping.tcp@alibaba-inc.com", "auzj_alex@mail.scut.edu.cn", "weiwu@scut.edu.cn"], "authors": ["Yin Zhao", "Longjun Cai", "Chaoping Tu", "Jie Zhang", "Wu Wei"], "title": "VIDEO AFFECTIVE IMPACT PREDICTION WITH MULTIMODAL FUSION AND LONG-SHORT TEMPORAL CONTEXT", "keywords": ["multi-modal fusion", "affective computing", "temporal context", "residual-based training strategy"], "abstract": "Predicting the emotional impact of videos using machine learning is a challenging task. Feature extraction, multi-modal fusion and temporal context fusion are crucial stages for predicting valence and arousal values in the emotional impact, but\nhave not been successfully exploited. In this paper, we proposed a comprehensive framework with innovative designs of model structure and multi-modal fusion strategy. We select the most suitable modalities for valence and arousal tasks respectively and each modal feature is extracted using the modality-specific pre-trained deep model on large generic dataset. Two-time-scale structures, one for the intra-clip and the other for the inter-clip, are proposed to capture the temporal dependency of video content and emotional states. To combine the complementary information from multiple modalities, an effective and efficient residual-based progressive training strategy is proposed. Each modality is step-wisely combined into the\nmulti-modal model, responsible for completing the missing parts of features. With all those above, our proposed prediction framework achieves better performance with a large margin compared to the state-of-the-art.", "pdf": "/pdf/33a143281af746485db625d1ffee095bfa413043.pdf", "paperhash": "zhao|video_affective_impact_prediction_with_multimodal_fusion_and_longshort_temporal_context", "original_pdf": "/attachment/33a143281af746485db625d1ffee095bfa413043.pdf", "_bibtex": "@misc{\nzhao2020video,\ntitle={{\\{}VIDEO{\\}} {\\{}AFFECTIVE{\\}} {\\{}IMPACT{\\}} {\\{}PREDICTION{\\}} {\\{}WITH{\\}} {\\{}MULTIMODAL{\\}} {\\{}FUSION{\\}} {\\{}AND{\\}} {\\{}LONG{\\}}-{\\{}SHORT{\\}} {\\{}TEMPORAL{\\}} {\\{}CONTEXT{\\}}},\nauthor={Yin Zhao and Longjun Cai and Chaoping Tu and Jie Zhang and Wu Wei},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx_d34YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx_d34YDB", "replyto": "HJx_d34YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper46/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575046330485, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper46/Reviewers"], "noninvitees": [], "tcdate": 1570237757933, "tmdate": 1575046330498, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper46/-/Official_Review"}}}], "count": 5}