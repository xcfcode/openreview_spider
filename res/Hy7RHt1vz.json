{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124450275, "tcdate": 1518470602761, "number": 284, "cdate": 1518470602761, "id": "Hy7RHt1vz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hy7RHt1vz", "signatures": ["~Alex_H_Williams1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions", "abstract": "Deep feedforward neural networks are associated with complicated, nonconvex objective functions. Yet, simple optimization algorithms can identify parameters that generalize well to held-out data. We currently lack detailed descriptions of this learning process, even on a qualitative level. We propose a simple tensor decomposition model to study how hidden representations evolve over learning. This approach precisely extracts the correct dynamics of learning in linear networks, which admit closed form solutions. On deep, nonlinear architectures performing image classification (CIFAR-10), we find empirically that a low-rank tensor model can explain a large fraction of variance while extracting meaningful features, such as stage-like learning and selectivity to inputs.", "paperhash": "stock|learning_dynamics_of_deep_networks_admit_lowrank_tensor_descriptions", "keywords": ["Learning Dynamics", "Deep Networks", "Tensor Decomposition"], "_bibtex": "@misc{\n  stock2018learning,\n  title={Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions},\n  author={Christopher H. Stock and Alex H. Williams and Madhu S. Advani and Andrew M. Saxe and Surya Ganguli},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy7RHt1vz}\n}", "authorids": ["chstock@stanford.edu", "ahwillia@stanford.edu", "sganguli@stanford.edu", "madvani@fas.harvard.edu", "asaxe@fas.harvard.edu"], "authors": ["Christopher H. Stock", "Alex H. Williams", "Madhu S. Advani", "Andrew M. Saxe", "Surya Ganguli"], "TL;DR": "We propose a simple unsupervised learning procedure based on tensor decomposition to concisely describe learning dynamics in deep networks.", "pdf": "/pdf/568fbf4e69b6bfc7e894ae5efd6e1382837d0e5c.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582922426, "tcdate": 1520368546612, "number": 1, "cdate": 1520368546612, "id": "ryoojO3dG", "invitation": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer2"], "content": {"title": "Interesting use of tensor decomposition", "rating": "5: Marginally below acceptance threshold", "review": "In this paper, the dynamics of how neural networks are learned is analyzed. For every training instance, they record activations of units for several test instances, which yields a 3-order tensor (activation x test x training). To reduce the data complexity, CP decomposition is used. Interestingly, CP decomposition can be also helpful to interpret the tensor.\n\nThe experimental results are interesting. However, the results are mostly evaluated in qualitative ways and there is no rigorous evidence that the proposed analysis is really useful. Also, it isn't described how we can use the proposed method for real use cases. For example, the method is possibly useful to debug the learning process of DNNs (e.g. detecting the wrong choice of learning rate). Adding such discussion will enhance the impact of the paper. \n\nPros:\n- The idea of using tensor decomposition to analyze learning dynamics is unique.\n\nCons:\n- Experimental results are qualitative.\n- Not ready to real problems. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions", "abstract": "Deep feedforward neural networks are associated with complicated, nonconvex objective functions. Yet, simple optimization algorithms can identify parameters that generalize well to held-out data. We currently lack detailed descriptions of this learning process, even on a qualitative level. We propose a simple tensor decomposition model to study how hidden representations evolve over learning. This approach precisely extracts the correct dynamics of learning in linear networks, which admit closed form solutions. On deep, nonlinear architectures performing image classification (CIFAR-10), we find empirically that a low-rank tensor model can explain a large fraction of variance while extracting meaningful features, such as stage-like learning and selectivity to inputs.", "paperhash": "stock|learning_dynamics_of_deep_networks_admit_lowrank_tensor_descriptions", "keywords": ["Learning Dynamics", "Deep Networks", "Tensor Decomposition"], "_bibtex": "@misc{\n  stock2018learning,\n  title={Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions},\n  author={Christopher H. Stock and Alex H. Williams and Madhu S. Advani and Andrew M. Saxe and Surya Ganguli},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy7RHt1vz}\n}", "authorids": ["chstock@stanford.edu", "ahwillia@stanford.edu", "sganguli@stanford.edu", "madvani@fas.harvard.edu", "asaxe@fas.harvard.edu"], "authors": ["Christopher H. Stock", "Alex H. Williams", "Madhu S. Advani", "Andrew M. Saxe", "Surya Ganguli"], "TL;DR": "We propose a simple unsupervised learning procedure based on tensor decomposition to concisely describe learning dynamics in deep networks.", "pdf": "/pdf/568fbf4e69b6bfc7e894ae5efd6e1382837d0e5c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922246, "id": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper284/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer3"], "reply": {"forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922246}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582834923, "tcdate": 1520601490342, "number": 2, "cdate": 1520601490342, "id": "HJ55K-lFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer1"], "content": {"title": "The approach of this paper is interesting, however, the result does not seem non-trivial.", "rating": "5: Marginally below acceptance threshold", "review": "--Outline\nThis paper investigates smoothness of each layer of DNNs by introducing a notion of Besov smoothness.\nUsing the notion, authors empirically show that smoothness increases as DNNs get deeper.\nIt is also shown that adding random labels increases smoothness.\n\n--Comment\nThe approach of this paper is interesting, however, the result does not seem non-trivial.\nBasically, transforming by compact operators has a role of smoothing, thus it is obvious such smoothing occurs.\nAlso, adding random labels is equivalent to smoothing (e.g. Gaussian smoothing.)\nSo, I do not think their finding is new.\n\nAdditionally, it is not clear that why this analysis will contribute the generalization problem by Zhang+ 2016 and Kawaguchi+ 2017.\n\n--Minor comments\nScales of figures are so tiny thus I can not read.\nEq(3): In the second term, $\\Omega_{k_3}$ -> $\\Omega_{k_2}$?\nIn last sentences of Section 1.3 and 2.2, first letters are small.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions", "abstract": "Deep feedforward neural networks are associated with complicated, nonconvex objective functions. Yet, simple optimization algorithms can identify parameters that generalize well to held-out data. We currently lack detailed descriptions of this learning process, even on a qualitative level. We propose a simple tensor decomposition model to study how hidden representations evolve over learning. This approach precisely extracts the correct dynamics of learning in linear networks, which admit closed form solutions. On deep, nonlinear architectures performing image classification (CIFAR-10), we find empirically that a low-rank tensor model can explain a large fraction of variance while extracting meaningful features, such as stage-like learning and selectivity to inputs.", "paperhash": "stock|learning_dynamics_of_deep_networks_admit_lowrank_tensor_descriptions", "keywords": ["Learning Dynamics", "Deep Networks", "Tensor Decomposition"], "_bibtex": "@misc{\n  stock2018learning,\n  title={Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions},\n  author={Christopher H. Stock and Alex H. Williams and Madhu S. Advani and Andrew M. Saxe and Surya Ganguli},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy7RHt1vz}\n}", "authorids": ["chstock@stanford.edu", "ahwillia@stanford.edu", "sganguli@stanford.edu", "madvani@fas.harvard.edu", "asaxe@fas.harvard.edu"], "authors": ["Christopher H. Stock", "Alex H. Williams", "Madhu S. Advani", "Andrew M. Saxe", "Surya Ganguli"], "TL;DR": "We propose a simple unsupervised learning procedure based on tensor decomposition to concisely describe learning dynamics in deep networks.", "pdf": "/pdf/568fbf4e69b6bfc7e894ae5efd6e1382837d0e5c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922246, "id": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper284/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer3"], "reply": {"forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922246}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582579308, "tcdate": 1521442568051, "number": 3, "cdate": 1521442568051, "id": "HygGk1pFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "signatures": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer3"], "content": {"title": "This paper provides a new perspective to study the learning dynamics in linear networks.", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a CP tensor decomposition model to study the learning dynamics in linear networks. The authors show that this low-rank tensor model can empirically explain a large fraction of variance while extracting meaningful features on CIFAR-10 dataset.\n\nAlthough this is a novel insight to represent the neural networks by CP decomposition factors, this paper also has the following questions:\n1. The authors only show these factors that can interpret the neural networks intuitively, but the theoretical  causes are not exlored (although this may be difficult).\n\n2. In Fig 2(a), why does the \u2018layer 2\u2019 instead of the 'output layer' have the lowest error ? Or why is there such a rule: the error first increases first and then decreases as the layer goes deeper?\n\n3. Lack of comparative experiment. The paper only analyzes the performance of the proposed method w.r.t some hyper-parameters, e.g., rank R, test images and so on. How about  the performance of other  exsiting methods?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions", "abstract": "Deep feedforward neural networks are associated with complicated, nonconvex objective functions. Yet, simple optimization algorithms can identify parameters that generalize well to held-out data. We currently lack detailed descriptions of this learning process, even on a qualitative level. We propose a simple tensor decomposition model to study how hidden representations evolve over learning. This approach precisely extracts the correct dynamics of learning in linear networks, which admit closed form solutions. On deep, nonlinear architectures performing image classification (CIFAR-10), we find empirically that a low-rank tensor model can explain a large fraction of variance while extracting meaningful features, such as stage-like learning and selectivity to inputs.", "paperhash": "stock|learning_dynamics_of_deep_networks_admit_lowrank_tensor_descriptions", "keywords": ["Learning Dynamics", "Deep Networks", "Tensor Decomposition"], "_bibtex": "@misc{\n  stock2018learning,\n  title={Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions},\n  author={Christopher H. Stock and Alex H. Williams and Madhu S. Advani and Andrew M. Saxe and Surya Ganguli},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy7RHt1vz}\n}", "authorids": ["chstock@stanford.edu", "ahwillia@stanford.edu", "sganguli@stanford.edu", "madvani@fas.harvard.edu", "asaxe@fas.harvard.edu"], "authors": ["Christopher H. Stock", "Alex H. Williams", "Madhu S. Advani", "Andrew M. Saxe", "Surya Ganguli"], "TL;DR": "We propose a simple unsupervised learning procedure based on tensor decomposition to concisely describe learning dynamics in deep networks.", "pdf": "/pdf/568fbf4e69b6bfc7e894ae5efd6e1382837d0e5c.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582922246, "id": "ICLR.cc/2018/Workshop/-/Paper284/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper284/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper284/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper284/AnonReviewer3"], "reply": {"forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582922246}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573587464, "tcdate": 1521573587464, "number": 190, "cdate": 1521573587129, "id": "BkoCCARFz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hy7RHt1vz", "replyto": "Hy7RHt1vz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions", "abstract": "Deep feedforward neural networks are associated with complicated, nonconvex objective functions. Yet, simple optimization algorithms can identify parameters that generalize well to held-out data. We currently lack detailed descriptions of this learning process, even on a qualitative level. We propose a simple tensor decomposition model to study how hidden representations evolve over learning. This approach precisely extracts the correct dynamics of learning in linear networks, which admit closed form solutions. On deep, nonlinear architectures performing image classification (CIFAR-10), we find empirically that a low-rank tensor model can explain a large fraction of variance while extracting meaningful features, such as stage-like learning and selectivity to inputs.", "paperhash": "stock|learning_dynamics_of_deep_networks_admit_lowrank_tensor_descriptions", "keywords": ["Learning Dynamics", "Deep Networks", "Tensor Decomposition"], "_bibtex": "@misc{\n  stock2018learning,\n  title={Learning Dynamics of Deep Networks Admit Low-Rank Tensor Descriptions},\n  author={Christopher H. Stock and Alex H. Williams and Madhu S. Advani and Andrew M. Saxe and Surya Ganguli},\n  year={2018},\n  url={https://openreview.net/forum?id=Hy7RHt1vz}\n}", "authorids": ["chstock@stanford.edu", "ahwillia@stanford.edu", "sganguli@stanford.edu", "madvani@fas.harvard.edu", "asaxe@fas.harvard.edu"], "authors": ["Christopher H. Stock", "Alex H. Williams", "Madhu S. Advani", "Andrew M. Saxe", "Surya Ganguli"], "TL;DR": "We propose a simple unsupervised learning procedure based on tensor decomposition to concisely describe learning dynamics in deep networks.", "pdf": "/pdf/568fbf4e69b6bfc7e894ae5efd6e1382837d0e5c.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}