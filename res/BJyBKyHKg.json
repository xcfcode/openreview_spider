{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028625014, "tcdate": 1490028625014, "number": 1, "id": "Byt8uFTjg", "invitation": "ICLR.cc/2017/workshop/-/paper140/acceptance", "forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028625633, "id": "ICLR.cc/2017/workshop/-/paper140/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028625633}}}, {"tddate": null, "tmdate": 1489707583233, "tcdate": 1489707583233, "number": 4, "id": "BkPSziOsx", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/comment", "forum": "BJyBKyHKg", "replyto": "BJ7Wo9dsg", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "content": {"title": "Re: Clarification", "comment": "I disagree on (3), however that's a matter of interpretation.\n\nWith these clarifications, and the added appendices, I have no remaining concerns.\nI've updated my rating.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456146, "tcdate": 1487366456146, "id": "ICLR.cc/2017/workshop/-/paper140/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers", "ICLR.cc/2017/workshop/paper140/areachairs"], "cdate": 1487366456146}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489707428351, "tcdate": 1489303939359, "number": 2, "id": "ryoYFdfsl", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/review", "forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "content": {"title": "Incremental progress over existing research", "rating": "7: Good paper, accept", "review": "This paper discusses an interesting approach, which is very similar to existing research that the paper does not mention.\n\nEq. (2) is the minimum free energy over trajectories xi given initial state s:\nV(s; beta) = min_{pi'} KL[pi'(xi|s) || pi(xi|s)] / beta - E[R(xi) | s; pi'].\n(EDITED: to fix + to - in the R term.)\n\nThis has been studied extensively, for example:\nLinearly Solvable Markov Decision Problems (Todorov, 2006)\nInformation Theory of Decisions and Actions (Tishby and Polani, 2011)\nOptimal Control as a Graphical Model Inference Problem (Kappen et al., 2012)\n\nRewards as exponential HMM emissions have been studied in:\nAn Approximate Inference Approach to Temporal Optimization in Optimal Control (Rawlik et al., 2011)\n\nThe properties of positive vs. negative values of beta have been studied in:\nFree Energy and the Generalized Optimality Equations for Sequential Decision Making (Ortega and Braun, 2012)\n\nThe risk-aversion properties, and in particular the cliff domain, have been studied in:\nTaming the Noise in Reinforcement Learning via Soft Updates (Fox et al., 2016)\n\n\nThe application of the particle filtering technique in this context seems somewhat novel, however it is hard to assess this contribution in isolation from comparison to the large body of existing research.\n(EDITED: to retract allusion to recent works by Todorov and Kappen)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489303940107, "id": "ICLR.cc/2017/workshop/-/paper140/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/AnonReviewer3", "ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "reply": {"forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489303940107}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489707259186, "tcdate": 1489705723495, "number": 5, "id": "BJ7Wo9dsg", "invitation": "ICLR.cc/2017/workshop/-/paper140/public/comment", "forum": "BJyBKyHKg", "replyto": "B1gJyKOog", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "Re: Clarification", "comment": "Thanks for the great questions.\n\n(1) The PVF can be extended to incorporate sampling from an auxillary pi', but the version we considered can be understood as assuming pi\u2019=pi. The PVF takes a different approach than an EM style approach for L(pi, pi'). Instead the PVF is defined as an SMC estimator of the risk sensitive value function in pi. This estimator is a bound (that is consistent in the number particles) on the risk sensitive objective. The advantage is that it has simple policy gradients that decompose additively in time, and thus we consider directly optimizing the PVF via policy gradients of pi.\n\n(2) This is a great question, which we do not have a complete answer for. The degree to which these algorithms inspired by natural gradients results in \u201crisk sensitivity\u201d is not clear to us, but their objective did not appear to be the risk sensitive one. We felt that these algorithms were distinct enough, and the connection unclear enough, to justify leaving a discussion to a later full conference paper. If the connection is clear to you, we'd be excited to learn more.\n\n(3) For the Cliffworld experiments we trained policies with policy gradients of PVFs with varying risk preference parameter beta. For the finite sample cases we considered, the beta = 0, or standard REINFORCE, variant did not solve the task. We suspect this is due to an optimization bias early on in training. Our interpretation was simply that as beta increases the PVF objective tolerates riskier policies and, in this case, suffers less from the optimization bias on this task.\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456142, "tcdate": 1487366456142, "id": "ICLR.cc/2017/workshop/-/paper140/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366456142}}}, {"tddate": null, "tmdate": 1489698520497, "tcdate": 1489698520497, "number": 3, "id": "B1gJyKOog", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/comment", "forum": "BJyBKyHKg", "replyto": "Hyx0_Udsx", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "content": {"title": "Re: Clarification", "comment": "Thank you for the clarification. I agree.\n\nThe paper still seems to me to be missing 3 things:\n\n1. As far as I see, you never mention how V^pi = max_{pi'}{L} is used. If your motivation is the joint optimization of pi and pi', how is max_{pi}{V^pi} performed?\n\n2. It is true that the joint optimization problem of pi and pi' is not usually addressed in KL-control.\nWithout further constraints, the optimal solution has pi=pi', going back to plain optimal control.\nHowever, algorithms like REPS (Peters et al., 2010), DPP (Azar et al., 2012), Psi-learning (Rawlik et al., 2012) and TRPO (Schulman et al., 2015) do alternate between updating pi and pi'.\nHow does your overall approach (given (1) above) relate to these algorithms?\n\n3. Again, why is the cliff domain showing risk sensitivity rather than risk-sensitive properties that have other underlying causes? Fox et al. (2016) analyze this as an increase in KL-cost along the cliff due to learning not to fall, i.e., the agent is not risk-averse but rather averse to the intrinsic cost of *preventing* the risk (which *is* prevented, in an off-policy sense). Have you reached a different conclusion?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456146, "tcdate": 1487366456146, "id": "ICLR.cc/2017/workshop/-/paper140/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers", "ICLR.cc/2017/workshop/paper140/areachairs"], "cdate": 1487366456146}}}, {"tddate": null, "tmdate": 1489688776143, "tcdate": 1489688776143, "number": 4, "id": "Hyx0_Udsx", "invitation": "ICLR.cc/2017/workshop/-/paper140/public/comment", "forum": "BJyBKyHKg", "replyto": "rysi7MDix", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "RE: Review", "comment": "Thanks for the quick followup. To emphasize, our actual contribution (and the only aspect of this work we claimed as original) is a new risk sensitive value function (PVF) defined by an SMC distribution, which you agreed is novel. The advantage of the PVF over existing methods is that it gives you a risk sensitive bound that decomposes in time, is cheap to estimate, and is a single objective for jointly optimizing the policy and possibly a proposal distribution.\n\nWhat we aren\u2019t clear on, is the ways in which risk sensitive control and KL-control are distinct. Just to clarify our understanding define:\n\nL(pi, pi\u2019) = E[R(xi) | s; pi'] - KL[pi'(xi|s) || pi(xi|s)] / beta\n\nOur understanding of the KL-control literature is that it considers the optimization of L(pi, pi\u2019) over pi\u2019 with pi fixed:\n\nargmax_{pi\u2019} L(pi, pi\u2019)\n\nWhen (in the special cases discussed) the optimal pi\u2019 given pi achieves the risk sensitive objective value at pi, then the risk sensitive control problem is a joint optimization of L(pi, pi\u2019) over both pi and pi\u2019:\n\nargmax_{pi} [max_{pi\u2019} L(pi, pi\u2019)]\n\nIt doesn\u2019t seem that the joint optimization problem is explicitly addressed in the KL-control references. Is it implicitly assumed that one could perform the joint optimization given perfect inference (as in EM)?\n\nIf our understanding is correct, it seems that risk sensitive control and KL-control are not the same. They are clearly closely related, in a way that mirrors the relationship between variational inference and maximum likelihood. We\u2019ve added a sentence to the main body of the paper to that effect.\n\nRegardless of whether these objectives are distinct, we agree that ideas from the KL-control literature are absolutely relevant to risk-sensitive control and our PVF. We\u2019ve now tried to reflect this in our Appendix, and are happy to clarify further. In particular, the Risk Sensitive Path Integral Control (van den Broek et al., 2012), is an interesting case in which you can express the risk sensitive optimal policy as the solution to a path integral problem."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456142, "tcdate": 1487366456142, "id": "ICLR.cc/2017/workshop/-/paper140/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366456142}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489688669474, "tcdate": 1487366455043, "number": 140, "id": "BJyBKyHKg", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "BJyBKyHKg", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "content": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489622521217, "tcdate": 1489622521217, "number": 2, "id": "rkZZIIDjx", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/comment", "forum": "BJyBKyHKg", "replyto": "ry0MMIPig", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "content": {"title": "Retracted", "comment": "I'm sorry, I was alluding to half-remembered talks.\nI've retracted this part of my review."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456146, "tcdate": 1487366456146, "id": "ICLR.cc/2017/workshop/-/paper140/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers", "ICLR.cc/2017/workshop/paper140/areachairs"], "cdate": 1487366456146}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489622408710, "tcdate": 1489605538889, "number": 1, "id": "rysi7MDix", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/comment", "forum": "BJyBKyHKg", "replyto": "Hytw1CUox", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "content": {"title": "No, you really aren't", "comment": "First, let me emphasize that I appreciate this paper's discussion of the risk-sensitive aspect of (what I argue is) KL-control. This aspect is not often discussed, however see:\nRisk Sensitive Path Integral Control (van den Broek et al., 2012)\nYour eq. (2) is exactly their J (Section 2) (albeit for continuous systems).\n\nIt is also true that most works in KL-control omit to mention the formulation as an optimization problem over trajectory distributions, such as the one given in our comments above and solved by your eq. (2). KL-control has a formulation in terms of full-trajectory optimization that is equivalent in the same sense that the Bellman equation solves the optimal control problem. For example, see Section 7.1 in Tishby and Polani (2011), and eq. (17) in:\nA Generalized Path Integral Control Approach to Reinforcement Learning (Theodorou et al., 2010)\n(The KL-control assumption is their eq. (8).)\n\nIt is true that this full-trajectory formulation only coincides with your eq. (2) in the special case of LMDPs (Todorov, 2006) and path integrals:\nLinear Theory for Control of Nonlinear Stochastic Systems (Kappen, 2005).\nThe linear form is only exact under the assumption of full controllability, however is often used as an approximation with attractive properties regardless of this assumption, as you do here.\n(You do not consider it an approximation to a principled approach, but rather motivate it directly.)\n\nDo note that the second equation in your comment is the optimum achieved in your first equation, under the full-controllability assumption.\nThis can be verified by differentiating over pi'.\n\nThis reviewer is well-aware of the challenging space limitation, and was not expecting a survey. A clear and correct statement of the novelty of the contribution is however expected. I maintain that the application of particle filtering to KL-control is your main contribution. As far as I know, it is novel.\n(EDITED: to retract allusion to recent works by Todorov and Kappen)\n\nI strongly disagree with your assertion that \"the problems are clearly completely different\" just because they are not always formulated the same way \u2014 but sometimes are! For even more evidence of this, see Section 3.3 in:\nTrading value and information in MDPs (Rubin et al., 2012).\n\nFinally, you make a good point about the distinction between risk-sensitive optimization, and solutions that simply display risk-aware properties. But this puts the burden back on you: Fox et al. (2016) show risk-averse properties of an RL-learned KL-control policy, however their explanation is alternative to risk-sensitivity, and to some degree excludes your motivation as the underlying mechanism. They demonstrate this specifically on the same cliff domain as you do. Your paper is incomplete without a discussion, and hopefully a resolution of this apparent contradiction."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456146, "tcdate": 1487366456146, "id": "ICLR.cc/2017/workshop/-/paper140/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers", "ICLR.cc/2017/workshop/paper140/areachairs"], "cdate": 1487366456146}}}, {"tddate": null, "tmdate": 1489621526387, "tcdate": 1489621526387, "number": 3, "id": "ry0MMIPig", "invitation": "ICLR.cc/2017/workshop/-/paper140/public/comment", "forum": "BJyBKyHKg", "replyto": "rysi7MDix", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Clarification", "comment": "Can you clarify which recent works by Todorov and Kappen you're referring to?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456142, "tcdate": 1487366456142, "id": "ICLR.cc/2017/workshop/-/paper140/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366456142}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489592580503, "tcdate": 1489588064943, "number": 1, "id": "Hytw1CUox", "invitation": "ICLR.cc/2017/workshop/-/paper140/public/comment", "forum": "BJyBKyHKg", "replyto": "ryoYFdfsl", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "We are optimizing a different objective", "comment": "Thank you for pointing us to these references. We would like to point out that our work addresses a separate question from that addressed in the literature you cite. Our contribution aims to optimize the risk sensitive objective, whose literature we duly cited. In summary:\n\n(1) Most of your references only consider the inner maximization over pi', which is a completely different problem.\n\n(2) The value function you mention and the risk sensitive one do not correspond in all cases.\n\nWhile we agree that a related work section would clarify these distinctions, we would also like to point out that this workshop submission had a strict 3-page limit. We\u2019ve added these references to the Appendix, along with a few others.\n\nIn more detail, the value function, which you reference (see originally Albertini & Runggaldier, 1988),\n\nV^pi(s; beta) = max_{pi'} E[R(xi) | s; pi'] - KL[pi'(xi|s) || pi(xi|s)] / beta,\n\nis the value achieved by the optimal policy, when optimizing over pi' under the cumulative return objective with a KL penalty assuming complete control over the transition dynamics of the environment. This is not the same as the optimization we consider. We consider the risk sensitive objective over pi,\n\nV^pi(s; beta) = beta^-1 log E[exp(beta * R(xi)) | s; pi]\n\nThere are at least two distinctions.\n\n(1) Most of the references you provided consider only the inner maximization of E[R(xi) | s; pi'] - KL[pi'(xi|s) || pi(xi|s)] / beta over pi\u2019 while pi is a fixed reference distribution.  In this case, the problems are clearly completely different. Although in some cases (see (2)), the risk sensitive objective would be optimized by a joint optimization over pi and pi\u2019.\n\n(2) When pi is a policy interacting with a stochastic environment that we don\u2019t control, as in the generic MDP definition we took in the paper, then the KL regularized control problem does not necessarily coincide with the risk sensitive one. They do coincide in the following cases: for some some transition dynamics, such as in path integral control, or generally when the policy completely specifies the transition dynamics of the environment. \n\nNone of the references that you cite address the question of optimizing risk sensitive objectives directly, despite possibly displaying risk aware \"properties\". The degree to which KL penalties modulate risk preferences is an interesting question, but beyond the scope of our contribution.\n\nReferences:\nLogarithmic Transformations for Discrete-Time, Finite-Horizon Stochastic Control Problems (Albertini & Runggaldier, 1988)"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456142, "tcdate": 1487366456142, "id": "ICLR.cc/2017/workshop/-/paper140/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366456142}}}, {"tddate": null, "tmdate": 1489588232627, "tcdate": 1489588232627, "number": 2, "id": "Sy-zgCIsl", "invitation": "ICLR.cc/2017/workshop/-/paper140/public/comment", "forum": "BJyBKyHKg", "replyto": "SkZEvnxjl", "signatures": ["~Chris_J_Maddison1"], "readers": ["everyone"], "writers": ["~Chris_J_Maddison1"], "content": {"title": "Author response", "comment": "We thank you for the thoughtful review and questions. We would like to point out that some of the omissions were due to a strict 3-page limit. To address a few of your concerns:\n\n(1) We did compare gradients of the PVF on the Cliffworld example to REINFORCE gradients and VIMCO gradients, with the details in the Appendix. We clarified this in the main body and added some more results to the Appendix.\n\n(2) The complexity for estimating the PVF is of order O(KT), in other words the same as K independent trajectories of length T. This is because the resampling step can be implemented in order K using the Alias method.\n\n(3) We decided to focus on policy gradient style algorithms, and did not explore any Q learning or related methods. All of the TD lambda algorithms apply to the PVF, and it would be interesting to consider in a full-length paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487366456142, "tcdate": 1487366456142, "id": "ICLR.cc/2017/workshop/-/paper140/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "reply": {"forum": "BJyBKyHKg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487366456142}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489189167698, "tcdate": 1489188649307, "number": 1, "id": "SkZEvnxjl", "invitation": "ICLR.cc/2017/workshop/-/paper140/official/review", "forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "signatures": ["ICLR.cc/2017/workshop/paper140/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper140/AnonReviewer3"], "content": {"title": "Official review", "rating": "6: Marginally above acceptance threshold", "review": "\nThe paper studies a family of risk-sensitive value functions in the context of reinforcement learning. These value functions are parametrized by a single scalar \\beta which governs their \"risk sensitivity\", that is, essentially the value assigned to small short-term rewards relative to large delayed rewards. Large \\beta leads to \"risk-taking\" behavior, making better use of sparse rewards. The authors propose an efficient algorithm for optimizing (an approximation to) these value functions, based on a bootstrap particle filter. The approach is tested in Cliffworld with tabular policies.\n\nThe paper is written clearly, with an extensive appendix describing all details. I have not checked the derivations in the appendix. I am not aware of similar approaches in recent RL literature. However, I am not very knowledgeable about literature in some related fields, like optimal control or bandits.\n\nCould the authors please comment on computational complexity compared to usual policy gradients? Is it correct that it is the same, perhaps up to a constant factor?\n\nPros:\n- Addressing an important problem: exploration/exploitation, sparse rewards\n- Departing from the standard expected reward objective\n- Compiling techniques from different fields in a new and useful way\n- Experiments in Cliffworld with tabular policies support the intuitions behind the approach. Risk-taking agent is able to solve the task unlike its more conservative versions.\n\nCons:\n- No experiments in realistic RL scenarios with function approximators. \n- Basically no discussion of alternative approaches or comparisons to baselines (one that comes to mind is Prioritized DQN, but there may be more)\n\nOverall, despite weak experimental evaluation, I tend to recommend acceptance, since the approach looks interesting, new and promising. The paper is outside of my main field of expertise, therefore I am only moderately certain.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Particle Value Functions", "abstract": "The policy gradients of the expected return objective can react slowly to rare rewards. Yet, in some cases agents may wish to emphasize the low or high returns regardless of their probability. Borrowing from the economics and control literature, we review the risk-sensitive value function that arises from an exponential utility and illustrate its effects on an example. This risk-sensitive value function is not always applicable to reinforcement learning problems, so we introduce the particle value function defined by a particle filter over the distributions of an agent's experience, which bounds the risk-sensitive one. We illustrate the benefit of the policy gradients of this objective in Cliffworld.\n", "pdf": "/pdf/15597c88f2ef8ec198fae0327fe20e5342ee5800.pdf", "TL;DR": "Value functions that use particle filters to allow reinforcement learning agents to seek or avoid areas of high variance in the reward.", "paperhash": "maddison|particle_value_functions", "keywords": ["Reinforcement Learning", "Games"], "conflicts": ["google.com", "stats.ox.ac.uk", "cs.toronto.edu"], "authors": ["Chris J. Maddison", "Dieterich Lawson", "George Tucker", "Nicolas Heess", "Arnaud Doucet", "Andriy Mnih", "Yee Whye Teh"], "authorids": ["cmaddis@stats.ox.ac.uk", "dieterichl@google.com", "gjt@google.com", "heess@google.com", "doucet@stats.ox.ac.uk", "amnih@google.com", "y.w.teh@stats.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489303940107, "id": "ICLR.cc/2017/workshop/-/paper140/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper140/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper140/AnonReviewer3", "ICLR.cc/2017/workshop/paper140/AnonReviewer2"], "reply": {"forum": "BJyBKyHKg", "replyto": "BJyBKyHKg", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper140/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489303940107}}}], "count": 13}