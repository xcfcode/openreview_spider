{"notes": [{"id": "PBfaUXYZzU", "original": "PGkzVY7Kd6c", "number": 825, "cdate": 1601308095367, "ddate": null, "tcdate": 1601308095367, "tmdate": 1614985626895, "tddate": null, "forum": "PBfaUXYZzU", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Gcu279gYLMh", "original": null, "number": 1, "cdate": 1610040533103, "ddate": null, "tcdate": 1610040533103, "tmdate": 1610474142902, "tddate": null, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "invitation": "ICLR.cc/2021/Conference/Paper825/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a weighted balanced accuracy metric to evaluate the performance of imbalanced multiclass classification. The metric is based on a one-versus-all decomposition from multi-class to binary, and then aggregating the metrics on the binary classification sub-problems in a weighted manner. The authors hope to argue that the new metric is more flexible for evaluating classifiers in the imbalanced and importance-varying setting.\n\nThe reviewers agree that the proposed framework is simple and applicable to an important problem. Somehow the novelty and significance of the work is pretty limited, as many related metrics (e.g. micro/macro-averaged metrics) exist in the literature. The authors are encouraged to think about stronger reasoning on how useful the \"new\" metric is. The experiments are also not convincing nor complete enough to verify the benefits of the proposed metric.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040533089, "tmdate": 1610474142886, "id": "ICLR.cc/2021/Conference/Paper825/-/Decision"}}}, {"id": "7d7oK9mzld8", "original": null, "number": 1, "cdate": 1603669846906, "ddate": null, "tcdate": 1603669846906, "tmdate": 1605024597126, "tddate": null, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "invitation": "ICLR.cc/2021/Conference/Paper825/-/Official_Review", "content": {"title": "An interesting little idea without full evaluation", "review": "The paper presents a simple addition to the Balanced Accuracy approach - which the authors refer to as \u2018importance\u2019. However, there is nothing in the formulation of this concept which requires that this is an importance and could in fact be any form of weighting. The paper evaluates the new metric - but only agains the Balanced Accuracy metric (which seems quite restrictive).\n\n## Some major comments on the paper:\n\n1) The proposed evaluation metric appears to be to show whether machine learning approach A is actually better than machine learning approach B. As such one can use the metric to give a value which can be used to compare different approaches. However, in order to judge if a particular approach is better or worse than another you need some way of showing that your metric is correct. The paper lacks a demonstration that your approach actually does give a more appropriate ordering of the machine learning approaches.\n\n2) In the abstract you talk about \u2018importance\u2019, however, this concept is not clear at this point. Later in the introduction you explain what importance refers to. Some of this in the abstract would help. The abstract also seems to terse making it difficult to follow.\n\n3) The second paragraph in the introduction seems odd. You are claiming in the previous paragraph that importance need not be the inverse of rarity, however, the example in the second paragraph seems to re-enforce the idea of importance being the inverse of rarity. The text on Uber also doesn\u2019t seem to fit the paper at all and could easily be dropped.\n\n4) Figure 1a needs more discussion in the text.\n\n5) Paragraph starting \u2018Let us illustrate the problem\u2019 - this talks about a dataset in the most abstract sense. It would seem this is potentially a concocted example without a real dataset behind it. It would be much better to indicate what dataset this is based on - perhaps detailing the scenario in the appendix. \n\n6) The related work on evaluation metrics seems a little short. How do the other proposed approaches compare to your work?\n\n7) \u201cAs we discussed with examples in previous sections, in many real-world classification problems,\u201d - you only give one example and the \u2018real-world\u2019 case that this refers to is not provided. Stronger evidence is needed to support this statement.\n\n8) Most sentiment analysis approaches are not based on RNNs - can you justify why you used this approach?\n\n9) The conclusions are rather short and say little. You also claim in the introduction that you will discuss future directions but don't.\n\n##\u00a0Some more minor comments:\n\n- If a majority is the larges group, what is the \u2018greatest majority\u2019? Surely it\u2019s just the majority?\n\n- \u2018adopt\u2019 -> \u2018adapt\u2019\n\n- There is no punctuation around the equations - For example there should be a full-stop after equations 1 and 2.\n\n- As only equation 6 is referred to in the text why are the others numbered?\n\n- Something going odd in the quotes in \u201c not inflated due to high-frequency classes\u2019 results dominating over\nthe others\u2019. \u201c\n\n- \u201cmust be rewarded higher scores\u201d -> \u201cmust be rewarded with higher scores\u201d\n\n- Is equation 7 really needed?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper825/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper825/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper825/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134174, "tmdate": 1606915809312, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper825/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper825/-/Official_Review"}}}, {"id": "__GtS1g236w", "original": null, "number": 2, "cdate": 1604456157715, "ddate": null, "tcdate": 1604456157715, "tmdate": 1605024597057, "tddate": null, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "invitation": "ICLR.cc/2021/Conference/Paper825/-/Official_Review", "content": {"title": "Not enough novelty and originality", "review": "This paper proposes a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.  \n\nI think the problem this paper deals with is very important and of great interest to the wide range of readers. \nIn addition, the paper is generally clearly written and easy to follow.\n\nHowever, I found the novelty and the originality of this paper is not enough for the ICLR standard.  \nAlthough I am not aware of the paper that presents exactly the same concept as this paper, I feel the generalization this work presents is too straightforward that new insights, findings, and benefits brought by this paper to the community are very limited.\nFor example, the micro average (or simply called \"Accuracy\" in this paper) can be regarded as a special form of equation (6), where weights or importance is given by the relative frequency of each class.\nThis importance criteria can be regarded as opposite of one of the presented criteria, \"Rarity\".\nThere may be some cases where the rarer a class is, the more important the class is as explained in the paper, but there may be other cases where more frequent classes are more important. As such, the concept of including importance of each class into an evaluation metric has been implicitly considered. It is true that the paper provides general form that includes the aforementioned case, but I'm afraid the formulation is not novel enough to bring a new value to the community as I mentioned above.\n\nAnother weakness of the paper lies in the experimental analysis.  \nOverall, the analysis is not convincing enough to verify the benefit of the proposed metric.\nFor example, at the end of \"WBA_rarity vs. Class-insensitive Metrics\" in page 7, the authors states \"This result validates that WBA_rarity provides a more sensitive tool for assessing classification performance\". I do not agree with this statement because it is no surprise that different evaluation metrics give different evaluation results, and this alone is not the ground for the validity of the proposed metric. The same argument can be applied to the \"Impact of WBA in Model Training\" in page 8. It is natural that a model trained with a specific criteria results in performing well on the criteria. This is again not the ground for the claim \"our new framework is more effective than Balanced Accuracy \u2013 ... also in training the models themselves.\"\n\nThe pros and cons of this paper can be summarized as follows.\n\n\n#### Pros\n1. The paper deals with practically important topic, and presents a simple, easy and intuitive solution.\n1. The paper is clearly written and easy to understand.\n\n#### Cons\n1. The novelty and the originality of this paper is not enough that there is only limited benefits to the community, which does not satisfy the ICLR standard.\n1. The experimental analysis is not convincing enough to support the usefullness of the proposed metric.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper825/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper825/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper825/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134174, "tmdate": 1606915809312, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper825/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper825/-/Official_Review"}}}, {"id": "rOfDp6aSwh7", "original": null, "number": 3, "cdate": 1604487285877, "ddate": null, "tcdate": 1604487285877, "tmdate": 1605024596993, "tddate": null, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "invitation": "ICLR.cc/2021/Conference/Paper825/-/Official_Review", "content": {"title": "lacks contribution", "review": "This paper presents a weighted balanced accuracy to evaulate the performance of multi-class classification. Basically, the performance for a multi-class problem can be evaluated by decomposing the original multi-class problem into a number of binary ones based on one-against-rest manner, and then evaulating the performance scores for each of the binary ones using any well-known metric for binary classification, and then, aggregating the performance scores. The main aim of this paper is to present a weighting scheme when aggregating the scores.\n\nI'm inclined to rejection of this paper. Frankly speaking, I don't think this paper has a significant contribution. the weighting schemes to combine binary metric scores to evaulate the performance of multi-class classification have been well-studied, such as macro-averaging, micro-averaging, as well as importance weighting (manual or data-driven e.g. frequency). The proposed idea is just a simple natural extension of balanced accuracy with a weighting scheme. It is nothing new.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper825/AnonReviewer6"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper825/AnonReviewer6"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper825/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134174, "tmdate": 1606915809312, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper825/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper825/-/Official_Review"}}}, {"id": "GQEzKZb5dGL", "original": null, "number": 4, "cdate": 1604902173881, "ddate": null, "tcdate": 1604902173881, "tmdate": 1605024596932, "tddate": null, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "invitation": "ICLR.cc/2021/Conference/Paper825/-/Official_Review", "content": {"title": "Novel insights but more works need to be done", "review": "Summary:\n\nThis paper proposes a new evaluation framework for imbalanced data. Specifically, they introduce an additional weighted term in the formulation of balanced accuracy. By varying the weights, their framework can be adapted to many application domains. Finally, they use two case studies to illustrate the effectiveness of the proposed measure.\n \nPros:\n\n- The proposed framework is simple and effective. \n- The proposed framework can be used in many application domains.\n\n\nCons:\n\nI have several comments regarding the experimental results:\n\n- How do the authors compute the \u201cskew\u201d score for each dataset? This term is not clearly explained in the paper. \n\n- Since the weighted terms e.g., $w_i$ play an important role in the proposed framework, more examples should be given to explain how to choose these parameters in different application domains.\n\n- In section 4.2, the authors state \u201c \u2026 by modifying loss functions of DNNs to capture class importance weights\u2026\u201d. It is not very clear to see how to implement this.\n\n- When using WBA in model training, what are the performances for the baseline measures, e.g., precision, recall? Also, why choose these values for $w_i$, e.g., w1=0.209, w2=0.368, and w3=0.255? \n\n \nOverall, I think the novelty of this work is limited and the experimental results are not very convincing.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper825/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper825/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Class-Weighted Evaluation Metrics for Imbalanced Data Classification", "authorids": ["akhileshgupta@alumni.upenn.edu", "~Nesime_Tatbul1", "~Ryan_Marcus1", "~Shengtian_Zhou1", "~Insup_Lee1", "~Justin_Gottschlich1"], "authors": ["Akhilesh Gupta", "Nesime Tatbul", "Ryan Marcus", "Shengtian Zhou", "Insup Lee", "Justin Gottschlich"], "keywords": ["Imbalanced data classification", "Evaluation metrics", "Log parsing", "Sentiment analysis"], "abstract": "Class distribution skews in imbalanced datasets may lead to models with prediction bias towards majority classes, making fair assessment of classifiers a challenging task. Balanced Accuracy is a popular metric used to evaluate a classifier\u2019s prediction performance under such scenarios. However, this metric falls short when classes vary in importance, especially when class importance is skewed differently from class cardinality distributions. In this paper, we propose a simple and general-purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. Experiments with several state-of-the-art classifiers tested on real-world datasets and benchmarks from two different domains show that our new framework is more effective than Balanced Accuracy \u2013- not only in evaluating and ranking model predictions, but also in training the models themselves.", "one-sentence_summary": "We present an evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gupta|classweighted_evaluation_metrics_for_imbalanced_data_classification", "supplementary_material": "/attachment/1461772b7250658874a316da72d635224648e496.zip", "pdf": "/pdf/c85aa74459987745134234759ed9b1ae1e7a03bd.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3sWZpVAG8b", "_bibtex": "@misc{\ngupta2021classweighted,\ntitle={Class-Weighted Evaluation Metrics for Imbalanced Data Classification},\nauthor={Akhilesh Gupta and Nesime Tatbul and Ryan Marcus and Shengtian Zhou and Insup Lee and Justin Gottschlich},\nyear={2021},\nurl={https://openreview.net/forum?id=PBfaUXYZzU}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PBfaUXYZzU", "replyto": "PBfaUXYZzU", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper825/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134174, "tmdate": 1606915809312, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper825/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper825/-/Official_Review"}}}], "count": 6}