{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730174304, "tcdate": 1509125499067, "number": 529, "cdate": 1518730174295, "id": "rkmtTJZCb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkmtTJZCb", "original": "ryGt6JZCb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260084045, "tcdate": 1517249926173, "number": 619, "cdate": 1517249926155, "id": "ByCYByarM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper presents a method for forward prediction in videos. The paper insufficiently motivates the proposed method and presents very limited empirical evaluations (no ablation studies, etc.) to backup its claims. This makes it difficult for the reader to put the work into  the context of the broader research around learning from unsupervised video data; leading reviewers to complete about perceived lack of novelty and clarity."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642462806, "tcdate": 1511787542574, "number": 1, "cdate": 1511787542574, "id": "BJ1X3tYgf", "invitation": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "signatures": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting paper but I find the claims are not backed up by the experimental evidence", "rating": "4: Ok but not good enough - rejection", "review": "The paper treats the interesting problem of long term video prediction in complex video streams. I think the approach of adding more structure to their representation before making longer term prediction is also a reasonable one. Their approach combines an RNN that predicts an encoding of scene and then generating an image prediction using a VAN (Reed et al.). They show some results on the Human3.6M and the Robot Push dataset. \n\nI find the submission lacking clarity in many places. The main lack of clarity source I think is about what the contribution is. There are sparse mentions in the introduction but I think it would be much more forceful and clear if they would present VAN or Villegas et al method separately and then put the pieces together for their method in a separate section. This would allow the author to clearly delineate their contribution and maybe why those choices were made. Also the use of hierarchical is non-standard and leads to confusion I recommend maybe \"semantical\" or better \"latent structured\" instead. Smaller ambiguities in wording are also in the paper : e.g. related work -> long term prediction \"in this work\" refers to the work mentioned but could as well be the work that they are presenting.  \n\nI find some of the claims not clearly backed by a thorough evaluation and analysis. Claiming to be able to produce encodings of scenes that work well at predicting many steps into the future is a very strong claim. I find the few images provided very little evidence for that fact. I think a toy example where this is clearly the case because we know exactly the factors of variations and they are inferred by the algorithm automatically or some better ones are discovered by the algorithm, that would make it a very strong submission. Reed et al. have a few examples that could be adapted to this setting and the resulting representation, analyzed appropriately, would shed some light into whether this is the right approach for long term video prediction and what are the nobs that should be tweaked in this system. \n\nIn the current format, I think that the authors are on a good path and I hope my suggestions will help them improve their submission, but as it stands I recommend rejection from this conference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642462709, "id": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper529/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer1", "ICLR.cc/2018/Conference/Paper529/AnonReviewer3", "ICLR.cc/2018/Conference/Paper529/AnonReviewer2"], "reply": {"forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642462709}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642462766, "tcdate": 1511901056146, "number": 2, "cdate": 1511901056146, "id": "HyuKwSixM", "invitation": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "signatures": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "interesting architecture alternatives, but lack of strong empirical conclusions regarding the lack of supervising the high level structure", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a method for hierarchical future frame prediction in monocular videos. It builds upon the recent method of Villegas et al. 2017, which generates future RGB frames in two stages: in the first stage, it predicts a human body pose sequence, then it conditions on the pose sequence to predict RGB content, using an image analogy network. This current paper, does not constrain the first stage (high level) prediction to be human poses, but instead it can be any high level representation. Thus, the method does not require human annotations.\n\nThe method has the following two sub-networks:\n1) An image encoder, that given an RGB image, predicts a deep feature encoding. \n2) An LSTM predictor, that conditioned on the last observed frame's encoding,  predicts future high level structure p_t. Once enough frames are generated though, it conditions on its own predictions. \n3) A visual analogy network (VAN), that given predicted high level structure p_t, it predicts the pixel image I_t, by applying the transformation from the first to tth frame, as computed by the vector subtraction of the corresponding high level encodings (2nd equation of the paper). VAN is trained to preserve parallelogram relationships in the joint RGB image and high level structure embedding.\n\nThe authors experiment with  many different neural network connectivities, e.g., not constraining the predicted high level structure to match the encoder's outputs, constraining the predicted high level structure to match the encoder's output (EPEV), and training together the VAN  and predictor so that VAN can tolerate mistakes of the predictor. Results are shown in H3.6m and the pushobject datasets, and are compared against the method of Villegas et all (INDIVIDUAL). The conclusion seems to be that not constraining the predicted high level structure to match the encoder\u2019s output, but biasing the encoder\u2019s output in the observed frames to represent ground-truth pose information, gives the best results. \n\nPros\n1) Interesting alternative training schemes are tested\n\nCons:\n1)Numerous English mistakes, e.g.,  ''an intelligent agents\", ''we explore ways generate\" etc.\n\n2) Equations are not numbered (and thus is hard to refer to them.) E.g., i do not understand the first equation, shouldn\u2019t it be that e_{t-1} is always fixed and equal to the encoding of the last observed (not predicted) frame? Then the subscript cannot be t-1.\n\n3) In H3.6M, the results are only qualitative. The conclusions from the paper are uncertain, partly  due to the difficulty of evaluating the video prediction results.\n\n\nGiven the difficulty of assessing the experimental results quantitatively (one possibility to do so is asking a set of people of which one they think is the most plausible video completion), and given the limited novelty of the paper, though interesting alternative architectures are tried out, it may not be suitable to be part of  ICLR proceedings as a conference paper.  \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642462709, "id": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper529/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer1", "ICLR.cc/2018/Conference/Paper529/AnonReviewer3", "ICLR.cc/2018/Conference/Paper529/AnonReviewer2"], "reply": {"forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642462709}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642462727, "tcdate": 1512368856180, "number": 3, "cdate": 1512368856180, "id": "B1e1owMZM", "invitation": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "signatures": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "insufficient novelty and significance", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents a method for predicting future video frames. The method is based on Villegas et al. (2017), with the main difference being that no ground truth pose is needed to train the network.\n\nThe novelty of the method is limited. It seems that there is very little innovation in terms of network architecture compared to Villegas et al. The difference is mainly on how the network is trained. But it is straightforward to train the architecture of Villegas et al. without pose -- just use any standard choice of loss that compares the predicted frame versus the ground truth frame. I don't see what is non-trivial or difficult about not using pose ground truth in training.\n\nOverall I think the contribution is not significant enough. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642462709, "id": "ICLR.cc/2018/Conference/-/Paper529/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper529/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper529/AnonReviewer1", "ICLR.cc/2018/Conference/Paper529/AnonReviewer3", "ICLR.cc/2018/Conference/Paper529/AnonReviewer2"], "reply": {"forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642462709}}}, {"tddate": null, "ddate": null, "tmdate": 1514513964269, "tcdate": 1513793234374, "number": 3, "cdate": 1513793234374, "id": "S1q0U7OMG", "invitation": "ICLR.cc/2018/Conference/-/Paper529/Official_Comment", "forum": "rkmtTJZCb", "replyto": "rkmtTJZCb", "signatures": ["ICLR.cc/2018/Conference/Paper529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper529/Authors"], "content": {"title": "Revised paper", "comment": "We would like to thank the reviewers for taking the time to read our submission and make numerous suggestions for improvements. We uploaded a revision that fixes some English mistakes, improves clarity, and provides new quantitative results on Humans 3.6M that highlight the benefits of the proposed method. \n\nAll three reviewers view the proposed method as insufficiently novel and/or significant. The main motivation of our work is to enable training of hierarchical video prediction models on data where pose groundtruth labels are impractical to collect or unavailable. As Reviewer2 points out, the simple way to do so would be to take the architecture of Villegas et al. and train it using a loss that compares the predicted frame versus the ground truth frame. This is essentially the E2E method described in our submission, and we include this model as one of the baselines in our work. In our experiments, we found that this E2E baseline is not sufficient, and we show that the EPEV method-- which is the main novel contribution of our work--produces much better results on the humans dataset than the E2E method, as shown in the appendix.\n\nAn important concern was that our submission did not have quantitative evidence to support our claims that our results on the Humans 3.6M dataset were better than the CDNA method from Finn et al. To address this, we ran the following evaluations:\n\n1. We ran a pre trained person detector (ssd_mobilenet_v1_coco from the TensorFlow object detection model zoo) on the generated frames from the EPEV method and the CDNA method, on the Humans dataset. The person detector had a much higher average person-class confidence for EPEV frames, compared to CDNA. See section 4.3.1 of the most recent revision for details.\n\n2. We also did a evaluation with a service similar to Mechanical Turk where workers rated the EPEV method as more realistic 53.6% of the time, the CDNA method as more realistic 11.1% of the time and the generated videos as being about the same 35.3% of the time. See section 4.3.2 of the most recent revision for details.\n\nWe believe this provides evidence that the proposed method is both qualitatively and quantitatively better compared to Finn et al. at predictions made on the Humans 3.6M dataset."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732100, "id": "ICLR.cc/2018/Conference/-/Paper529/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkmtTJZCb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper529/Authors|ICLR.cc/2018/Conference/Paper529/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper529/Authors|ICLR.cc/2018/Conference/Paper529/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper529/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper529/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper529/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper529/Reviewers", "ICLR.cc/2018/Conference/Paper529/Authors", "ICLR.cc/2018/Conference/Paper529/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732100}}}, {"tddate": null, "ddate": null, "tmdate": 1514513872676, "tcdate": 1514513661872, "number": 4, "cdate": 1514513661872, "id": "ByLWr7XXf", "invitation": "ICLR.cc/2018/Conference/-/Paper529/Official_Comment", "forum": "rkmtTJZCb", "replyto": "S1q0U7OMG", "signatures": ["ICLR.cc/2018/Conference/Paper529/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper529/Authors"], "content": {"title": "Revised paper Update", "comment": "We also made the following revisions in a more recent update:\n\n1. We seperated the description of the method from Villegas et al from the description of our method. We believe this addresses the comment from reviewer 1 about clarifying our key contribution. See section 2 and section 3.\n\n2. We evaluated our method on a toy dataset and showed that it can make reasonable predictions 1,000 frames into the future about 97% of the time, where the CDNA baseline can only do this about 25% of the time. We believe this addresses the comment from reviewer 1 about needing evidence to support our claim that our method works well for long term prediction. See section 4.2."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Video Prediction", "abstract": "Much recent research has been devoted to video prediction and generation,  but mostly for short-scale time horizons. The hierarchical video prediction method by Villegas et al. (2017) is an example of a state of the art method for long term video prediction.  However, their method has limited applicability in practical settings as it requires a ground truth pose (e.g.,  poses of joints of a human) at training time.   This paper presents a long term hierarchical video prediction model that does not have such a restriction. We show that the network learns its own higher-level structure (e.g., pose equivalent hidden variables) that works better in cases where the ground truth pose does not fully capture all of the information needed to  predict  the  next  frame.   This  method  gives  sharper  results  than  other  video prediction methods which do not require a ground truth pose, and its efficiency is shown on the Humans 3.6M and Robot Pushing datasets.", "pdf": "/pdf/bd96a063e018ce7c3e313c464aea295d72490386.pdf", "TL;DR": "We show ways to train a hierarchical video prediction model without needing pose labels.", "paperhash": "wichers|unsupervised_hierarchical_video_prediction", "_bibtex": "@misc{\nwichers2018unsupervised,\ntitle={Unsupervised Hierarchical Video Prediction},\nauthor={Nevan Wichers and Dumitru Erhan and Honglak Lee},\nyear={2018},\nurl={https://openreview.net/forum?id=rkmtTJZCb},\n}", "keywords": ["video prediction", "visual analogy network", "unsupervised", "hierarchical"], "authors": ["Nevan Wichers", "Dumitru Erhan", "Honglak Lee"], "authorids": ["wichersn@google.com", "dumitru@google.com", "honglak@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825732100, "id": "ICLR.cc/2018/Conference/-/Paper529/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkmtTJZCb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper529/Authors|ICLR.cc/2018/Conference/Paper529/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper529/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper529/Authors|ICLR.cc/2018/Conference/Paper529/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper529/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper529/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper529/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper529/Reviewers", "ICLR.cc/2018/Conference/Paper529/Authors", "ICLR.cc/2018/Conference/Paper529/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825732100}}}], "count": 7}