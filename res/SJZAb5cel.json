{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396660986, "tcdate": 1486396660986, "number": 1, "id": "Hk6eaz8ux", "invitation": "ICLR.cc/2017/conference/-/paper530/acceptance", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "There is a bit of spread in the reviewer scores, but ultimately the paper does not meet the high bar for acceptance to ICLR. The lack of author responses to the reviews does not help either."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396661497, "id": "ICLR.cc/2017/conference/-/paper530/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396661497}}}, {"tddate": null, "tmdate": 1485797432116, "tcdate": 1485797432116, "number": 6, "id": "ByxBOeTDx", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "Sk7D-w7Vl", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the comments", "comment": "Thank you very much for your comments.\n\nAccording to your comments, we will improve our paper as much as possible.\n\n>> At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there).\nThank you for pointing out this key observation.\nIndeed, we have discussed the results shown in Table 13 corresponding to the paragraph \"Vertical connections\" in Section 6.3, and we will explain the results more clearly.\n\n>> Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.\nThank you for the suggestion.\nWe try to compare the results.\nThe use of the label embeddings is similar to using the word embeddings; for example, the label embeddings capture similarities between labels as shown in [1].\nThus, the POS and chunking label embeddings could be effective in using the output information in different computation units (e.g., the four different matrix multiplications in Equation (1)).\n\n[1] Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1485768140068, "tcdate": 1485768140068, "number": 5, "id": "B1N0HKnPe", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "HJ0pRhG4x", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the comments", "comment": "Thank you very much for your comments.\n\n>> On the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way.\nThank you for the pointer.\nI will consider mentioning the relationship or difference in our paper.\n\n>> But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there\nAs mentioned in the preliminary response, in our model, five different types of tasks are handled in the single model, and it is not obvious when to stop the training while trying to maximize the scores of all the tasks.\nAs the first step, we focused on maximizing the accuracy of dependency parsing on the development data.\nHowever, the sizes of the training data are different across the different tasks; for example, the semantic relatedness and entailment tasks include only 4,500 sentence pairs for training, and the dependency parsing dataset includes 39,832 sentences with word-level annotations.\nThus, in general, dependency parsing requires more training epochs than the semantic tasks, but currently, our model trains all of the tasks for the same training epochs.\nWe observed that better scores on the development sets of the semantic tasks can be achieved before the accuracy of dependency parsing reaches the best score.\nTherefore, it should be an important research direction to investigate a method for achieving the best scores for all of the tasks at the same time.\n\n>> The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\nThis aspect should be further discussed in our paper.\nAs discussed in Appendix B, 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences.\nIn addition, we have found that 61 parsing results have cycles.\nIn total, more than 95% of the greedy parsing results are well-formed trees.\nI applied 1st order Eisner's algorithm to the 83 parsing results which are not well-formed trees, and as a result, the overall accuracy does not significantly change.\nFor example, in the case of JMT_ABC shown in Table 12, the UAS is now 94.53% (previously, 94.52%), and the LAS is 92.62% (previously, 92.61%).\nNow , all parsing results can be well-formed trees while keeping the accuracy.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1485766871952, "tcdate": 1485766871952, "number": 4, "id": "Hyek-thve", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "B1fPRQmEe", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the review", "comment": "Thank you for the comments.\n\n>> first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\nThat is right.\nTo show the importance of the linguistic hierarchy, we provided the results with and without using the hierarchy of the multi-layer RNNs in Table 12 with the paragraph \"Different layers for different tasks\".\nAlthough the model architecture is different from the suggested baseline [1], the results in Table 12 show that the use of the hierarchy is more important than the number of the model parameters.\n\n>> second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \nThat is right.\nWe put more weight on higher-level tasks (dependency parsing, relatedness, entailment) in the JMT_all setting, while the chunking results with JMT_AB on the test set can be comparable with other published results.\n\n>> third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \nThis aspect should be further discussed in our paper.\nAs discussed in Appendix B, 22 parsing results are not well-formed trees in terms of ROOT nodes on the development data including 1,700 sentences.\nIn addition, we have found that 61 parsing results have cycles.\nIn total, more than 95% of the greedy parsing results are well-formed trees.\nI applied 1st order Eisner's algorithm to the 83 parsing results which are not well-formed trees, and as a result, the overall accuracy does not significantly change.\nFor example, in the case of JMT_ABC shown in Table 12, the UAS is now 94.53% (previously, 94.52%), and the LAS is 92.62% (previously, 92.61%).\nNow, all parsing results can be well-formed trees while keeping the accuracy.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1482023258707, "tcdate": 1482023258707, "number": 3, "id": "Sk7D-w7Vl", "invitation": "ICLR.cc/2017/conference/-/paper530/official/review", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer2"], "content": {"title": "A joint model that actually works, limited novelty, a lot of experiments but possibly missing few important points.", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).\n\nNovelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP \"complexity\") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call \"successive regularization\".\n\nExperiments:\n\n- The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;\n\n- The information \"transferred\" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.\n\n- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553147, "id": "ICLR.cc/2017/conference/-/paper530/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553147}}}, {"tddate": null, "tmdate": 1482010202011, "tcdate": 1482010202011, "number": 2, "id": "B1fPRQmEe", "invitation": "ICLR.cc/2017/conference/-/paper530/official/review", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer3"], "content": {"title": "experimental setup should be improved", "rating": "3: Clear rejection", "review": "this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.\n\nalthough the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:\n\nfirst, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.\n\nsecond, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. \n\nthird, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. \n\nminor issue:\n- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].\n\n[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf\n[2] http://www.cs.cmu.edu/~nasmith/LSP/", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553147, "id": "ICLR.cc/2017/conference/-/paper530/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553147}}}, {"tddate": null, "tmdate": 1481981637726, "tcdate": 1481981637726, "number": 1, "id": "HJ0pRhG4x", "invitation": "ICLR.cc/2017/conference/-/paper530/official/review", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer1"], "content": {"title": "review", "rating": "5: Marginally below acceptance threshold", "review": "The paper introduce a way to train joint models for many NLP tasks. Traditionally, we treat these tasks as \u201cpipeline\u201d \u2014 the later tasks will depending on the output of the previous tasks. Here, the authors propose a neural approach which includes all the tasks in one single model. The higher level tasks takes (1) the predictions from the lower level tasks and (2) the hidden representations of the lower level tasks. Also proposed in this paper, is the successive regularization. Intuitively, this means that, when training the high level tasks, we don\u2019t want to change the model in the lower levels by too much so that the lower level tasks can keep a reasonable accuracy of prediction.\n\nOn the modeling side, I think the proposed model is very similar comparing to (Zhang and Weiss, ACL 2016) and SPINN (Bowman et al, 2016) in a even simpler way. The number of the experiments are good. But I am not sure I am convinced by the numbers in Table 1 since the patterns are not very clear there \u2014 sometimes, the performance of the higher level tasks even goes down when training with more tasks (sometimes it does go up, but also not very significant and stable). The dependency scores, although I don\u2019t think this is a serious problem, comparing the UAS/LAS when the output is not guaranteed to be a well-formed tree isn\u2019t strictly speaking fair.\n\nI admit that the successive regularization make sense intuitively and is a very interesting direction to try. However, without a careful study of the training schema of such model, the current results on successive regularization do not convince me that it should be the right thing to do in such models (the current results are not strong enough to show that). The training methods need to be explored here including things as iteratively train on different tasks, and the relationship between the number of training iterations of a task and it\u2019s training set size (and loss on this task etc).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512553147, "id": "ICLR.cc/2017/conference/-/paper530/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512553147}}}, {"tddate": null, "tmdate": 1481867849629, "tcdate": 1481867849629, "number": 3, "id": "BJf8fWW4e", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "H1mmuwy4e", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the questions.", "comment": "Thank you very much for your detailed questions, and let me respond to each of the questions.\n\n>> Did the authors try to directly feed the label scores (instead of building an embedding)? -- Table 10 reports results without LE, but it is likely that one would try first this kind of approach by simply feeding the scores in the next architectures;\nWe used the label embeddings because previous work (e.g., [1], [2], and [3]) used the embeddings for representing POS labels as well as words.\nThank you for the suggestion and we will try to directly use the label scores for comparison.\n\nReferences:\n[1] Danqi Chen and Christopher D. Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP.\n[2] David Weiss, Chris Alberti, Michael Collins, and Slav Petrov. 2015. Structured Training for Neural Network Transition-Based Parsing. In ACL.\n[3] Makoto Miwa and Mohit Bansal. 2016. End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures. In ACL.\n\n>> Also, have you tried with the label information, but not the hidden state one? I wonder which one is actually important, or if they are redundant (in that respect, the drop observed in Table 10 is minimal when removing LE).\nWe have actually performed additional experiments regarding this question, and the results are reported in Table 13 in the latest version of our submission.\nAs shown in the table, removing the hidden states from the input vectors has smaller effects than removing the label embedding connections.\nThat is, the vertical connections in the deep bi-LSTMs might become redundant if we can explicitly make use of the output information about the lower-level tasks.\nWhen it is not obvious how to directly use output information, then the vertical connections can be helpful.\nWe will add more detailed discussions in the next version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1481762843552, "tcdate": 1481762843545, "number": 3, "id": "H1mmuwy4e", "invitation": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer2"], "content": {"title": "alternative to label embeddings", "question": "Did the authors try to directly feed the label scores (instead of building an embedding)? -- Table 10 reports results without LE, but it is likely that one would try first this kind of approach by simply feeding the scores in the next architectures;\nAlso, have you tried with the label information, but not the hidden state one? I wonder which one is actually important, or if they are redundant (in that respect, the drop observed in Table 10 is minimal when removing LE).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481762844151, "id": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481762844151}}}, {"tddate": null, "tmdate": 1481612016600, "tcdate": 1481612016594, "number": 2, "id": "BJtgsf67l", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "ByQap2Czl", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the questions", "comment": "Thank you very much for your questions, and let me respond to each of the questions.\n\n>> 1. From table 1, it seems training with high level tasks (jointly) will always hurt the low level tasks performance. Any explanation for that?\nFirst, the accuracy of POS tagging does not drop when the POS component is jointly trained with the high-level semantic tasks.\nIn the case of dependency parsing, the accuracy may seem to drop, but the difference is very small (UAS: 94.67 and 94.71, LAS: 92.90 and 92.92), and indeed there is no significant difference.\nWe expected that the higher-level tasks were helpful in improving the lower-level tasks, but currently, we did not observe positive results.\nOur work is the first step towards handling many different types of NLP tasks in a single model, achieving competitive results with those of state-of-the-art models specifically tuned only for single tasks.\n\n>> 2. From table 1, training with only DE seems outperform training the JMT_all system. The POS/parsing information doesn't help the high level task?\nIn the task of semantic relatedness, we used the Mean Squared Error (MSE) as the evaluation metric, and thus, lower numbers indicate better accuracy.\nTherefore, the MSE score of JMT_all (0.233) is better than that of JMT_DE (0.238).\nIn the case of the task of textual entailment (a 3-class classification task), the difference of the scores 86.2 and 86.8 is small, and the score of JMT_all is comparable with the state-of-the-art results.\nRelated to that, we will describe the size of the training data for each task in the next version.\n\nIn our model, five different types of tasks are handled in the single model, and it is not obvious when to stop the training while trying to maximize the scores of all the tasks.\nAs the first step, we focused on maximizing the accuracy of dependency parsing on the development data.\nHowever, the sizes of the training data are different across the different tasks; for example, the semantic relatedness and entailment tasks include only 4,500 sentence pairs for training, and the dependency parsing dataset includes 39,832 sentences with word-level annotations.\nThus, in general, the dependency parsing task requires more training epochs than the semantic tasks, but currently, our model trains all of the tasks for the same training epochs.\nWe observed that better scores on the development sets of the semantic tasks can be achieved before the accuracy of dependency parsing reaches the best score.\nTherefore, it is an interesting research direction to investigate some methods for achieving the best scores for all of the tasks at the same time.\n\n>> 3. The training/test sets in POS/parsing are different (but overlapped). Is it possible for parsing to get gold POS information indirectly in this training method?\nThat is right, and the POS component of the dependency parsing model is trained by using the gold POS information of the overlapped data, although the gold POS information is not used when training the dependency parsing component.\nIt should be noted that the sentences in the development and test sets of dependency parsing are not included in POS and chunking training data.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1480671291122, "tcdate": 1480671291116, "number": 1, "id": "SyXBlpCfg", "invitation": "ICLR.cc/2017/conference/-/paper530/public/comment", "forum": "SJZAb5cel", "replyto": "rJ2Bb4AGx", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "writers": ["~Kazuma_Hashimoto1"], "content": {"title": "Response to the questions", "comment": "Thank you very much for your questions and suggestions.\nLet me respond to each of the questions.\n\n>> - without output label embeddings how do you connect two tasks?\nIt is true that the tasks are not always directly connected.\nFor example, the POS information is not directly fed into the dependency parsing layer.\nHowever, in our model, there are vertical connections in the multi-layer bi-LSTMs, as explained in the example of the input vector g^{(2)}_t in Section 2.3.\nTherefore, all of the hidden states in the multi-layer bi-LSTMs are propagated to higher layers even when we do not use the output label embeddings.\n\n>> - depth and shortcut connections effect relatedness and entailment tasks too much. how do you explain this? could this be related to using the same corpus (PTB) for all the previous tasks?\nFirst, in the case of depth (Table 7), the size of training data is an important factor.\nFor example, the training data for dependency parsing includes 39,832 sentences, and a dependency annotation is assigned for each word in the sentences.\nBy contrast, the training data for the relatedness and entailment tasks includes only 4,500 sentences, and a gold annotation is assigned for each sentence.\nThus, the size of the training data for the relatedness and entailment tasks is much smaller than that for the other three tasks.\nOur results in Table 7 show that such small datasets are not sufficient to train deep networks (4 layer bi-LSTMs for the relatedness task and 5 layer bi-LSTMs for the entailment task).\n\nSecond, in the case of shortcut connections (Table 9), the difference of the tasks is an important factor.\nThe first three tasks (POS tagging, chunking, and dependency parsing) are tasks primarily related to syntax, and semantic similarities between words are not always important in these tasks.\nBy contrast, capturing semantic similarities is very important in the semantic tasks (relatedness and entailment).\nWithout the shortcut connections, the word embeddings are only fed into the first (POS) layer, and the relatedness and entailment layers receive the word information modified by the first three layers.\nTherefore, the semantic similarities captured by the word embeddings could be washed out before handling the two semantic tasks.\nPlease also refer to Appendix D to see how the word embeddings are modified according to each task.\n\nWe will add the above discussions to the manuscript.\n\n>> - for the first three tasks section splittings are not the same to compare with literature (I guess). Would not it be safer to use the same section split so that a gold-tag information would not flow one task to the other?\nAs described in Section 5.1, we used the commonly-used data splits for POS tagging, chunking, and dependency parsing.\nTherefore, some of the sections are used only in one task.\nWe will make this setting clearer in the next version.\n\n>> - table 12,13 reports higher numbers for chunkings. I guess those are word level accuracies not F1 scores. is this correct?\nThey are indeed the F1 scores.\nAs explained in Footnote 5, the development and test sentences are included in the training data of dependency parsing, following the standard data splits.\nTherefore, it is more like a situation where some of the training data is included in the test data, so the results of chunking cannot be compared with previously published results.\nIn contrast, the results of the Single and JMT_AB settings can be compared with previously reported results.\n\n>> - related work would be more thorough if you include joint learning studies from NLP literature such as https://transacl.org/ojs/index.php/tacl/article/viewFile/412/81\nThank you for the pointer.\nWe will expand the related work section in the next version.\n\n>> - predicting the head and the dependency label does not guarantee producing well-formed dependency trees. did you try using a decoder like eisner's algorithm or maximum-spanning-tree algorithm?\nWe did not try to make all of the dependency results become well-formed trees.\nIn our primary analysis on the dependency parsing results in Appendix B, we found 22 sentences with multiple root nodes or no root nodes out of the 1,700 development sentences.\nThe other possibility is the issue of cyclic graphs.\nThank you for the suggestion and we will try this direction.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287534369, "id": "ICLR.cc/2017/conference/-/paper530/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJZAb5cel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper530/reviewers", "ICLR.cc/2017/conference/paper530/areachairs"], "cdate": 1485287534369}}}, {"tddate": null, "tmdate": 1480670650862, "tcdate": 1480670650859, "number": 2, "id": "ByQap2Czl", "invitation": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer1"], "content": {"title": "questions", "question": "1. From table 1, it seems training with high level tasks (jointly) will always hurt the low level tasks performance. Any explanation for that?\n2. From table 1, training with only DE seems outperform training the JMT_all system. The POS/parsing information doesn't help the high level task?\n3. The training/test sets in POS/parsing are different (but overlapped). Is it possible for parsing to get gold POS information indirectly in this training method?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481762844151, "id": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481762844151}}}, {"tddate": null, "tmdate": 1480634692522, "tcdate": 1480634692517, "number": 1, "id": "rJ2Bb4AGx", "invitation": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "forum": "SJZAb5cel", "replyto": "SJZAb5cel", "signatures": ["ICLR.cc/2017/conference/paper530/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper530/AnonReviewer3"], "content": {"title": "questions & suggestions", "question": "questions:\n- without output label embeddings how do you connect two tasks?\n- depth and shortcut connections effect relatedness and entailment tasks too much. how do you explain this? could this be related to using the same corpus (PTB) for all the previous tasks?\n- for the first three tasks section splittings are not the same to compare with literature (I guess). Would not it be safer to use the same section split so that a gold-tag information would not flow one task to the other?\n- table 12,13 reports higher numbers for chunkings. I guess those are word level accuracies not F1 scores. is this correct?\n\nsuggestions:\n- related work would be more thorough if you include joint learning studies from NLP literature such as https://transacl.org/ojs/index.php/tacl/article/viewFile/412/81\n- predicting the head and the dependency label does not guarantee producing well-formed dependency trees. did you try using a decoder like eisner's algorithm or maximum-spanning-tree algorithm?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481762844151, "id": "ICLR.cc/2017/conference/-/paper530/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper530/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper530/AnonReviewer3", "ICLR.cc/2017/conference/paper530/AnonReviewer1", "ICLR.cc/2017/conference/paper530/AnonReviewer2"], "reply": {"forum": "SJZAb5cel", "replyto": "SJZAb5cel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper530/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481762844151}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1479513934309, "tcdate": 1478300105266, "number": 530, "id": "SJZAb5cel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJZAb5cel", "signatures": ["~Kazuma_Hashimoto1"], "readers": ["everyone"], "content": {"title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "abstract": "Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce such a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. All layers include shortcut connections to both word representations and lower-level task predictions. We use a simple regularization term to allow for optimizing all model weights to improve one task's loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end trainable model obtains state-of-the-art results on chunking, dependency parsing, semantic relatedness and textual entailment. It also performs competitively on POS tagging. Our dependency parsing layer relies only on a single feed-forward pass and does not require a beam search.", "pdf": "/pdf/2f13fb612ef82e430be72ddf7b671e6be4eaf965.pdf", "TL;DR": "A single deep multi-task learning model for five different NLP tasks.", "paperhash": "hashimoto|a_joint_manytask_model_growing_a_neural_network_for_multiple_nlp_tasks", "keywords": ["Natural language processing", "Deep learning"], "conflicts": ["logos.t.u-tokyo.ac.jp", "salesforce.com"], "authors": ["Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher"], "authorids": ["hassy@logos.t.u-tokyo.ac.jp", "cxiong@salesforce.com", "tsuruoka@logos.t.u-tokyo.ac.jp", "rsocher@salesforce.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 14}