{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730181276, "tcdate": 1509099857249, "number": 338, "cdate": 1518730181267, "id": "r1YUtYx0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1YUtYx0-", "original": "S1OIKYeRW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": ["HyJf8QJDz"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260090331, "tcdate": 1517249669411, "number": 404, "cdate": 1517249669396, "id": "Bk6FNyaHG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper proposes a new way to understand why neural networks generalize well. They introduce the concept of ensemble robustness and try to explain DNN generalization based on this concept. The reviewers feel the paper is a bit premature for publication in a top conference although this new way of explaining generalization is quite interesting.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642434238, "tcdate": 1511561108468, "number": 1, "cdate": 1511561108468, "id": "rJhcwfLgf", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Paper that provides a novel theoretical framework for stochastic learning of Deep Networks, the proposed framework is an extension of an existing framework and the contribution is a bit limited in its present form.", "rating": "4: Ok but not good enough - rejection", "review": "Summary:\nThis paper presents an adaptation of the algorithmic robustness of Xu&Mannor'12 to a notion robustness of ensemble of hypothesis allowing the authors to study generalization ability of stochastic learning algorithms for Deep Learning Networks. \nGeneralization can be established as long as the sensitiveness of the learning algorithm to adversarial perturbations is bounded.\nThe paper presents learning bounds and an experimental showing correlation between empirical ensemble robustness and generalization error.\n\nQuality:\nGlobally correct\n\nClarity:\nPaper clear\n\nOriginality:\nLimited with respect to the original definition of algorithmic robustness\n\nSignificance:\nThe paper provides a new theoretical analysis for stochastic learning of Deep Networks but the contribution is limited in its present form.\n\n\nPros:\n-New theoretical study for DL algorithms \n-Focus on adversarial learning\nCons\n-I find the contribution a bit limited\n-Some aspects have to be precised/more argumented\n-Experimental study could have been more complete\n\n\nComments:\n---------\n\n\n*About the proposed framework.\nThe idea of taking a max over instances of partition C_i (Def 3) already appeared in the proof of results of Xu&Mannor, and the originality of the contribution is essentially to add an expectation over the result of the algorithm.\n\n\nIn Xu&Mannor paper, there is a notion of weak robustness that is proved to be necessary and sufficient to generalize. The contribution of the authors would be stronger if they can discuss an equivalent notion in their context.\n\nThe partition considered by the framework is never discussed nor taken into account, while this is an important aspect of the analysis. In particular, there is a tradeoff between \\epsilon(s) and K:  using a very fine tiling it is always possible to have a very small \\epsilon(s) at the price of a very large K (if you think of a covering number, K can be exponential in the size of the tiling and hard to calculate). \nIn the context of adversarial examples, this is actually important because it can be very likely that the adversarial example can belong to a partition set different from the set the original example belong to. \nIn this context, I am not sure to understand the validity of the framework because we can then compare 2 instances of different set which is outside of the framework. \nSo I wonder if the way the adervarial examples are generates should be taken into account for the definition of the partition.\nAdditionnally, the result is given in the contect of IID data, and with a multinomial distribution according to the partition set - adversarial generation can violate this IID assumption.\n\nIn the experimental setup, the partition set is not explained and we have no guarantee to compare instances of the same set. Nothing is said about $r$ and its impact on the results. This is a clear weak aspect of the experimental analysis\nIn the experimental setup, as far as I understand the setup, I find the term \"generalization error\" a bit abusive since it is actually the error on the test set. \nUsing cross validation or considering multiple training/test sets would be more appropriate.\n\n\nIn the proof of Lemma 2, I am not sure to understand where the term 1/n comes from in the term 2M^2/2 (before \"We then bound the term H as follows\")\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642434128, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer1", "ICLR.cc/2018/Conference/Paper338/AnonReviewer2", "ICLR.cc/2018/Conference/Paper338/AnonReviewer3"], "reply": {"forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642434128}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642434195, "tcdate": 1511734221557, "number": 2, "cdate": 1511734221557, "id": "SkLRjndlG", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Paper proposes to study generalization ability via new notion of stability. Improtant problem and a beginning of interesting idea but paper is not ready for a publication since results are not sufficiently strong.", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a study of the generalization ability of deep learning algorithms using an extension of notion of stability called ensemble robustness. It requires that algorithm is stable on average with respect to randomness of the algorithm. The paper then gives bounds on the generalization error of a randomized algorithm in terms of stability parameter and provides empirical study attempting to connect theory with practice.\n\nWhile I believe that paper is trying to tackle an important problem and maybe on the right path to find notions that are responsible for generalization in NNs, I believe that contributions in this work are not sufficiently strong for acceptance.\n\nFirstly, it should be noted that the notion of generalization considered in this work is significantly weaker than standard notions of generalization in learning theory since (a) results are not high probability results (b) the bounds are with respect to randomness of both sample and sample (which gives extra slack).\n\nStabiltiy parameter epsilon_bar(n) is not studied anywhere. How does it scale with sample size n for standard algorithms? How do we know it does not make bounds vacuous?\n\nIt is only allude it to that NN learning algorithms may poses ensemble robustness. It is not clear and not shown anywhere that they do. Indeed, simulations demonstrate that this could be the case but this still presents a significant gap between theory and practice (just like any other analysis that paper criticizes in intro).\n\nMinor:\n\n1. After Theorem 2: \"... can substantially improve ...\" not sure if improvement is substantial since it is still not a high probability bound.\n\n2. In intro, \"Thus statistical learning theory ... struggle to explain generalization ...\". Note that the work of Zhang et al does not establish that learning theory struggle to explain generalization ability of NNs since results in that paper do not study margin bounds. To this end refer to some recent work Bartlett et al, Cortes et al., Neyshabur et al.\n\n3. Typos in def. 3. missing z in \"If s \\in C_i...\". No bar on epsilon.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642434128, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer1", "ICLR.cc/2018/Conference/Paper338/AnonReviewer2", "ICLR.cc/2018/Conference/Paper338/AnonReviewer3"], "reply": {"forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642434128}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642434151, "tcdate": 1512106333012, "number": 3, "cdate": 1512106333012, "id": "r1BvYvAeG", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Clear accept", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper studied the generalization ability of learning algorithms from the robustness viewpoint in a deep learning context. To achieve this goal, the authors extended the notion of the (K, \\epsilon)- robustness proposed in Xu and Mannor, 2012 and introduced the ensemble robustness. \n\nPros: \n\n1, The problem studied in this paper is interesting. Both robustness and generalization are important properties of learning algorithms. It is good to see that the authors made some efforts towards this direction.\n2, The paper is well shaped and is easy to follow. The analysis conducted in this paper is sound. Numerical experiments are also convincing. \n3, The extended notion \"ensemble robustness\" is shown to be very useful in studying the generalization properties of several deep learning algorithms. \n\nCons:    \n\n1,  The terminology \"ensemble\" seems odd to me, and seems not to be informative enough.\n2,  Given that the stability is considered as a weak notion of robustness, and the fact that the stability of a learning algorithm and its relations to the generalization property have been well studied, in my view, it is quite necessary to mention the relation of the present study with stability arguments. \n3, After Definition 3, the author stated that ensemble robustness is a weak notion of robustness proposed in Xu and Manner, 2012. It is better to present an example here immediately to illustrate. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642434128, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer1", "ICLR.cc/2018/Conference/Paper338/AnonReviewer2", "ICLR.cc/2018/Conference/Paper338/AnonReviewer3"], "reply": {"forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642434128}}}, {"tddate": null, "ddate": null, "tmdate": 1515115987511, "tcdate": 1515115987511, "number": 7, "cdate": 1515115987511, "id": "r1oRSIn7M", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "ry3UJraMG", "signatures": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer2"], "content": {"title": "reply", "comment": "\"Regarding epsilon_bar(n): While the study of epsion_bar(n) is hard in the context of general algorithms and deep networks, it can be done for simpler learning algorithms. For example, for linear SVM, \\epsilon_bar(n) will be relevant to the covering number (robustness and regularization of support vector machines, Xu et. al. 09).\"\n\n--> But how do we now the bound is not trivial in case of deep nets?\n\n\n\"Regarding high probability bounds: Can the reviewer explain what he means by these two comments? (a) Our theorems are given in the PAC epsilon/delta formulation which is, in fact, a high probability bound. (b) We do not understand what the reviewer means by the randomness of both sample and sample. \"\n\n--> Standard results in ML are logarithmic in 1/delta, these results are only linear in 1/delta which is a very weak result.\n\n--> I meant randomness of sample and algorithms.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1515115699508, "tcdate": 1515115699508, "number": 6, "cdate": 1515115699508, "id": "Hyon4U3Qz", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "Hk6PtE2XM", "signatures": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/AnonReviewer2"], "content": {"title": "Bounds do NOT hold with high probability", "comment": "Logarithmic dependence on 1/delta is what is understood under \"high probability\". This is standard in ML theory see for instance definition of PAC learning."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1515113241456, "tcdate": 1515113241456, "number": 3, "cdate": 1515113241456, "id": "Hyb7ir3Xf", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "forum": "r1YUtYx0-", "replyto": "H1T7gLalM", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Explanation could use some polishing and further discussion", "comment": "\"Robustness and Stability are different properties, to see that observe that robustness is a global property while stability is local, and that robustness concerns properties of a single hypothesis, while stability concerns two (one for the original data set and one for the modified one). \"\n\nI don't think \"global vs local\" is the best way to distinguish robustness from stability. Both robustness and stability bound the affect of local perturbations. The key difference, IMO, is that robustness deals with perturbations of the test example, whereas stability deals with perturbations of a single training example. Robustness also constrains the test example perturbations to be within a certain partition of the instance space, whereas stability allows the perturbations to range over the entire instance space.\n\nFor algorithms that are both robust and stable, which analysis yields better bounds? Since the paper is concerned with deep learning, consider the stability results in Hardt et al. (2016) or Kuzborskij & Lampert (2017) for learning with non-convex objectives. If one were to combine these results with Elisseeff et al.'s generalization bounds, would the resulting bounds be better or worse than the ones in this paper? I'm just saying that more comparison to related work would make the paper stronger."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791688375, "id": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Area_Chair"], "cdate": 1512791688375}}}, {"tddate": null, "ddate": null, "tmdate": 1515108708667, "tcdate": 1515108708667, "number": 2, "cdate": 1515108708667, "id": "Hk6PtE2XM", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "forum": "r1YUtYx0-", "replyto": "SkLRjndlG", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Bounds do in fact hold with high probability", "comment": "With all due respect, I feel that this review is mistaken about the bounds not holding with high probability. Theorems 1 & 2 clearly state that the bounds hold \"with probability at least $1 \u2212 \\delta$ with respect to the random draw of the s and h.\"\n\nThat said, the bounds are _linear_ in $1/\\delta$, which is not ideal; it would be stronger if they were logarithmic in $1/\\delta$. (Note: Theorem 2 has a term that is linear in $1/\\delta$, which becomes the dominating term.)\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791688375, "id": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Area_Chair"], "cdate": 1512791688375}}}, {"tddate": null, "ddate": null, "tmdate": 1514127188537, "tcdate": 1514127188537, "number": 5, "cdate": 1514127188537, "id": "ry3UJraMG", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "SkLRjndlG", "signatures": ["ICLR.cc/2018/Conference/Paper338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/Authors"], "content": {"title": "Reply", "comment": "We thank the reviewer for his feedback. \n\nRegarding epsilon_bar(n): While the study of epsion_bar(n) is hard in the context of general algorithms and deep networks, it can be done for simpler learning algorithms. For example, for linear SVM, \\epsilon_bar(n) will be relevant to the covering number (robustness and regularization of support vector machines, Xu et. al. 09).\n\nRegarding the robustness of NNs:\nWe agree it is hard to show explicitly that NNs are robust. This is exactly the goal of this paper, trying to bridge the gap between theory and practice. We want to emphasize that the goal of this paper is not to criticize other methods, but to provide a different perspective.  \n\nRegarding high probability bounds: Can the reviewer explain what he means by these two comments? (a) Our theorems are given in the PAC epsilon/delta formulation which is, in fact, a high probability bound. (b) We do not understand what the reviewer means by the randomness of both sample and sample.  \n\nAll minor comments that the reviewers mentioned were fixed in the pdf. \n\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1514126971673, "tcdate": 1514126971673, "number": 4, "cdate": 1514126971673, "id": "BJ4tCEaGM", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "rJhcwfLgf", "signatures": ["ICLR.cc/2018/Conference/Paper338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/Authors"], "content": {"title": "Reply", "comment": "We thank the reviewer for pointing these issues out and agree that they were not explained well.  We have revised the paper to explain the data partitioning principles better and address here the main points the reviewer raises. \n\nRegarding partition for sets: \nGenerally, there is a trade-off between epsilon(s) and, K, the larger K is the smaller \\epsilon(s) due to the finer tiling as the reviewer suggested. This tradeoff is also evident in the bound of Theorem 1, where the right-hand side increases with K and \\epsilon(s) so there is a minimum point (see Corollaries 4&5 in Xu&Mannor 2012 for choosing the minimal K).  \n \nHowever, in the context of Deep Neural Networks, we chose k=n (training data size), to be an implicit partition such that each set contains a small R2 ball around each training example, without specifying the partition explicitly. We then approximate the loss in this partition using the adversarial example, i.e., approximating the maximal loss in the partition using the adversarial example. While this approximation is loose, we show that empirically, it is correlated with generalization. Under this partition, there is no violation of the IID assumption for general stochastic algorithms, but it is violated in the case of adversarial training as the reviewer suggested. However, simulations suggest that correlation exists for both.\n\n\nRegarding weak robustness: We are more interested in the standard generalizability and found weak robustness to be out of the scope of this work. We do believe however that similar bound can be derived for weak robustness of randomized algorithms using the same techniques we used in this work. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1514126535271, "tcdate": 1514126535271, "number": 3, "cdate": 1514126535271, "id": "rJ1R2E6Mz", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "r1BvYvAeG", "signatures": ["ICLR.cc/2018/Conference/Paper338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/Authors"], "content": {"title": "Reply", "comment": "We thank the reviewer for his feedback. \n\nRegarding the 3 cons the reviewer mentioned:\n\n1. We agree that a better terminology may be found, at the moment we decided to stick to the original one. \n2. We have addressed point two in the forum and in the new version of the pdf (related work Section). \n3. Good point. We moved this discussion to after theorem two and revisited the discussion after theorem 2 to explain this issue better.\n \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1514125655061, "tcdate": 1514125655061, "number": 2, "cdate": 1514125655061, "id": "B1yPYVpMf", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["ICLR.cc/2018/Conference/Paper338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/Authors"], "content": {"title": "General comments for the reviewers", "comment": "We thank the reviewers for their constructive feedback, which we found very helpful to improve the quality of this work. For each of the reviewers, a personal response is posted in the forum. Also, a new revision of the paper is available following the reviewer remarks. For the reviewer convenience, additions/corrections are marked in a red color in the text to distinguish new text from old one.\n\nHere, we would like to emphasize the contributions of this paper and its importance to the ICLR community as we see it. This paper revisits the robustness=generalization theory (Xu & Mannor, 2012) in the context of deep networks. We introduce new theorems that deal with stochastic algorithms (the most deployed ones) and provide a complimentary empirical study on the connection between robustness and generalization of Deep Neural Networks. We provide for the first time, an empirical study on the global (as we define it) robustness of the Deep Neural Networks, and its connections to generalization and adversarial examples, which has been puzzling the Deep Learning community lately. Moreover, we have shown that taking an expectation over robustness indeed improves the correlation between robustness and generalization, which we later demonstrate how to evaluate efficiently through Bayesian networks. Finally, we believe that the study of different approaches for generalization of Deep Neural Nets is of high importance, and we believe that this work makes an interesting step in this direction. \n\nFor each of the reviewers, a personal response is posted in the forum. Also, a new revision of the paper is available following the reviewer remarks. For the reviewer convenience, additions/corrections are marked in a red color in the text to distinguish new text from old one. Main modifications:\n \n\u00b7       Intro: a few clarifications about our claims and fixing of citations following R2 comments.\n\u00b7       Intro: better discussion on adversarial training and Parseval networks.\n\u00b7       Related work: discussion on stability following comment in the forum + R1.\n\u00b7       Better explanation of partitions sets, experimental considerations of them \u2013 Sections 4, and 5. (R3)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1512034340649, "tcdate": 1512034340649, "number": 1, "cdate": 1512034340649, "id": "H1T7gLalM", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "forum": "r1YUtYx0-", "replyto": "SJx-TCsxz", "signatures": ["ICLR.cc/2018/Conference/Paper338/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper338/Authors"], "content": {"title": "Reply to: Please discuss relationship to randomized stability-based bounds", "comment": "Thank you for your comment. Stability and robustness are two examples of desired properties of a learning algorithm that can also guarantee generalization under some conditions.\n\nA stable algorithm produces an output hypothesis that is stable to small changes in the data set, i.e., if a training example is replaced with another example from the same distribution, the training error will not change much. Elisseeff et al. (JMLR, 2005), indeed showed that algorithm that fulfills this requirement generalize well. \n\nRobustness, on the other hand, is a different property of learning algorithms. A Robust algorithm produces a hypothesis that is robust to bounded perturbations of the entire data set, as we explain in more detail in our paper. Robustness and Stability are different properties, to see that observe that robustness is a global property while stability is local, and that robustness concerns properties of a single hypothesis, while stability concerns two (one for the original data set and one for the modified one).  \n\nWe emphasize that a learning algorithm may be both stable and robust, e.g., SVM, \"Robustness and Regularization of Support Vector Machines,\" Huan Xu, Constantine Caramanis, Shie Mannor 2009). However, there also exist algorithms that are robust but not stable, e.g., Lasso Regression, \"Robust Regression and Lasso,\" Huan Xu, Constantine Caramanis, Shie Mannor 2008). \n\nWe will further expand the discussion on these issues in a future revision of the paper. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825735381, "id": "ICLR.cc/2018/Conference/-/Paper338/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper338/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper338/Authors|ICLR.cc/2018/Conference/Paper338/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper338/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper338/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper338/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825735381}}}, {"tddate": null, "ddate": null, "tmdate": 1511939319999, "tcdate": 1511939319999, "number": 1, "cdate": 1511939319999, "id": "SJx-TCsxz", "invitation": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "forum": "r1YUtYx0-", "replyto": "r1YUtYx0-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Please discuss relationship to randomized stability-based bounds", "comment": "Ensemble robustness is conceptually very similar to randomized algorithm stability. The latter concept has been thoroughly analyzed by Elisseeff et al. (JMLR, 2005), who derived a number of generalization bounds for randomized algorithms based on different notions of stability (uniform, hypothesis, pointwise hypothesis). Given the similarity between robustness and stability, it seems to me that the submitted paper should discuss the connections to Elisseeff et al.'s work (which is not cited) and compare the bounds in both."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms", "abstract": "The question why deep learning algorithms generalize so well has attracted increasing\nresearch interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided complete\nexplanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this work, we focus\non the robustness approach (Xu & Mannor, 2012), i.e., if the error of a hypothesis\nwill not change much due to perturbations of its training examples, then it\nwill also generalize well. As most deep learning algorithms are stochastic (e.g.,\nStochastic Gradient Descent, Dropout, and Bayes-by-backprop), we revisit the robustness\narguments of Xu & Mannor, and introduce a new approach \u2013 ensemble\nrobustness \u2013 that concerns the robustness of a population of hypotheses. Through\nthe lens of ensemble robustness, we reveal that a stochastic learning algorithm can\ngeneralize well as long as its sensitiveness to adversarial perturbations is bounded\nin average over training examples. Moreover, an algorithm may be sensitive to\nsome adversarial examples (Goodfellow et al., 2015) but still generalize well. To\nsupport our claims, we provide extensive simulations for different deep learning\nalgorithms and different network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.", "pdf": "/pdf/fe30c4710fe1a12a2c0094777825b217669d8195.pdf", "TL;DR": "Explaining the generalization of stochastic deep learning algorithms, theoretically and empirically, via ensemble robustness", "paperhash": "zahavy|ensemble_robustness_and_generalization_of_stochastic_deep_learning_algorithms", "_bibtex": "@misc{\nzahavy2018ensemble,\ntitle={Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms},\nauthor={Tom Zahavy and Bingyi Kang and Alex Sivak and Jiashi Feng and Huan Xu and Shie Mannor},\nyear={2018},\nurl={https://openreview.net/forum?id=r1YUtYx0-},\n}", "keywords": ["Robustness", "Generalization", "Deep Learning", "Adversarial Learning"], "authors": ["Tom Zahavy", "Bingyi Kang", "Alex Sivak", "Jiashi Feng", "Huan Xu", "Shie Mannor"], "authorids": ["tomzahavy@gmail.com", "bingykang@gmail.com", "silex@campus.technion.ac.il", "jshfeng@gmail.com", "huan.xu@isye.gatech.edu", "shiemannor@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791688375, "id": "ICLR.cc/2018/Conference/-/Paper338/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "r1YUtYx0-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper338/Authors", "ICLR.cc/2018/Conference/Paper338/Reviewers", "ICLR.cc/2018/Conference/Paper338/Area_Chair"], "cdate": 1512791688375}}}], "count": 15}