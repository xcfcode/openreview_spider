{"notes": [{"id": "BygY4grYDr", "original": "HJgqJDlKvH", "number": 2255, "cdate": 1569439792616, "ddate": null, "tcdate": 1569439792616, "tmdate": 1577168275732, "tddate": null, "forum": "BygY4grYDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fu6heF04gq", "original": null, "number": 1, "cdate": 1576798744487, "ddate": null, "tcdate": 1576798744487, "tmdate": 1576800891675, "tddate": null, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Decision", "content": {"decision": "Reject", "comment": "As the reviewers point out, the core contribution might be potentially important but the current execution of the paper makes it difficult to gauge this importance. In the light of this, this paper does not seem ready for appearance in a conference like ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718692, "tmdate": 1576800269212, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Decision"}}}, {"id": "SkeyqBiynS", "original": null, "number": 3, "cdate": 1574053254636, "ddate": null, "tcdate": 1574053254636, "tmdate": 1574053254636, "tddate": null, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The authors study the \u2018non-saturating\u2019 variant of training for GANs and show that it is equivalent to a regular training procedure minimizing a \u201csoftened\u201d reverse KL divergence as opposed to Jensen-Shannon divergence. They show a connection between the two training procedures for more general f-divergence losses. For instance, they show that\n1. non-saturating training on original GAN loss minimizes KL(p/2+q/2 || p) \n2. non-saturating training on KL(p||q) loss minimizes KL(q||p)\n\nThe authors start by arguing about previous analyses of the non-saturating training scheme and present why they do not arrive at the complete picture. Then they go on to introduce f-divergences and f-GAN training before explaining in their notation what precisely non-saturating training means. Then they show that it corresponds to minimization of not the original f-divergence but a hybrid (f,g) divergence. \n\nOverall the paper presents most of its insights as speculative statements and does not do a good enough job of attempting to concretely formalize them.\n\nQuestions:\n1. Why is argument of Nowozin et al wrong when q is parametric?\n2. What are tail weights of an f-divergence?\n3. Unclear why one needs the variational lower bound in f-GAN training.\n4. Why does hybrid training (f,g) converge to the minimization of f? Can the discussion in Appendix F be formalized into a Theorem or a Lemma?\n5. One of the main takeaways of the paper appears to be that initial phases of JS divergence minimization leads to flat gradients which can be problematic - question: is this an artifact of JS being bounded? Could unbounded divergences avoid running into this issue?\n\nThe content organization and highlighting of the main result in the paper can be significantly improved. Since the paper is exposing a theoretical connection as its primary result, I would also recommend a higher level of formalism overall.\n1. Figure 2 is references much earlier than it appears\n2. Tail weights are referenced before they are defined\n3. Formal statement of non-saturating gradient based training captured within a subsection or box. Hard to locate in current draft. Need to read 5 sections to get to it.\n4. Appendix C should be in main body.\n5. Some definitions need to be defined more formally. For instance,\n  a. Hybrid optimization\n  b. Non-saturating training\n\nIt is unclear how significant the contribution of the paper is. It is a clean mathematical observation but the consequences of the connection are not explored and fleshed out. For instance, can by realizing that the non-saturating gradient training is optimizing a different f-divergence can we explain why non-saturating gradient training is more useful? Any insights backed by some simple example settings of synthesized probability distributions would also be useful. The paper also does not propose any new training methods based on the insights uncovered and it is not clear how significant the connection uncovered is with the information presented in the current draft. I believe it is an interesting direction that the paper probes and with a more deeper look into the phenomenon and related directions can be ready for publishing in a venue such as ICLR. In it\u2019s current form I unfortunately don\u2019t think the contributions of the paper are significant enough for acceptance.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779890370, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Reviewers"], "noninvitees": [], "tcdate": 1570237725463, "tmdate": 1575779890381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review"}}}, {"id": "BJezD3zRtH", "original": null, "number": 2, "cdate": 1571855449943, "ddate": null, "tcdate": 1571855449943, "tmdate": 1573908654443, "tddate": null, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "######## Updated Review #########\n\nI would like to thank the author(s) for their rebuttal, which I have carefully read. I also appreciate the effort made to improve the paper. My overall evaluation of the paper stands unchanged. \n\n\n\n#############################\n\n\n\n\n\n#### My review is based on the updated paper downloaded from the anonymous link. ####\n\nThis paper discusses alternative training strategies for f-GANs. While the discussion has some interesting points, the presentation needs to be much improved. It is not easy to follow this paper in its current form, and the main results are not properly emphasized. As such, I am not certain of its real contribution. f-GANs are not routinely used in practice (except for the vanilla JSD and RKL), and as far as I can tell the saturating gradient issue is no longer a central concern (it has been well addressed years ago, with, e.g. WGANs). I am voting to reject this submission, but I am willing to re-evaluate this paper if the author(s) significantly improves their writing. \n\n- In Section 2, what does it mean by \"the Fisher metric of the family\"? The concept of Fisher metric is defined anywhere in the text, and there is no reference to it.\n- It is not is intuitive why the second derivative of f_R takes the form $u^{-3} f''(u^{-1})$ (given above Eqn (2)). Please elaborate. \n- Please avoid the use of subjective phrases such as \"imagining\", \"get a feel\", etc. I am guessing the author(s) are trying to suggest taking a visual inspection of the discrepancies projected on the log-likelihood ratio axis (which is 1D) and figure out which f-div might be more appropriate. \n- Fig. 3 needs legends. There are two solid (dotted, resp) lines in the Figure, and I am guessing one of them is for the saturating and the other for the non-saturating gradient. This needs to be specified because the line specs are identical. \n- After going through the entire paper, I would highly recommend the author(s) to take a course in academic writing. The main contributions are not highlighted and some of the key concepts are not even properly defined. For example, analysis of the non-saturating gradient, which is supposedly the main result of this submission, appeared in pp. 7, by which time most readers have exhausted their patience. The vanilla version of the non-saturating scheme never appeared in the main text. The notation system is also non-standard, where the notation $\\bar{\\lambda}$, normally reserved for average/expectation, has been used to denote the gradient wrt $\\lambda$. The writing can be very unprofessional at times, for example, \"the answers to these questions are yes, yes and no respectively\".  \n- I do not know why the author(s) inserted one toy experiment in the paper, as it serves no purpose. Each model converges to their respectively optimal, as expected. There is no discussion of how to choose an appropriate f-div or (f,g) hybrid in practice. \n- I totally agree with the author(s) on the point that this is a note (sec 7, second paragraph), rather than an academic paper. There is a lot of derivations in the paper, but insightful discussions are limited in the sense that it is not clear how to connect these results to improve practice. The section titles also read like bullet points in a note. \n- Not certain about the significance of this paper. The derivations are fairly standard and I can not find any useful proposals that might benefit the practice of f-GAN training. There is a number of papers discussing non-saturating GAN training (see sec 8), and I do not think this paper adds too much value to this discussion. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779890370, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Reviewers"], "noninvitees": [], "tcdate": 1570237725463, "tmdate": 1575779890381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review"}}}, {"id": "Bkx1JiIFjB", "original": null, "number": 4, "cdate": 1573640918922, "ddate": null, "tcdate": 1573640918922, "tmdate": 1573640918922, "tddate": null, "forum": "BygY4grYDr", "replyto": "SJgXA_V2FB", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment", "content": {"title": "Updates in light of reviewer #2's comments", "comment": "> The expressions in Section 3 can be a useful tool to investigate the statistical properties of estimators with f-divergences. However, I think that the usefulness of alternative expressions in Section 3 is not very clear, though an intuitive interpretation is presented.\n\nWe completely agree that the present paper does not immediately suggest ways to improve current GAN training practice. As mentioned when responding to reviewer #3, we hope that progress over time comes from both better theoretical understanding and better experimental results. In this case, we would argue that our result is simple to state and understand (\"non-saturating GAN training doesn't actually optimize Jensen-Shannon\") and provides a better understanding of a state-of-the-art method for GAN training (used for example for StyleGAN).\n\n> Moreover, the numerical experiment is not sufficient to support some new expressions. Detailed theoretical analysis of the non-saturating training based on the proposed expression would be required for publication from ICLR. \n\nThe paper outlines a reinterpretation of what current practice is already doing. Implementing optimization of the softened reverse KL using our approach is (intentionally) identical to the standard implementation of current non-saturating GAN training. To try to understand your concern better, is it that the paper does not provide a practically improved way to train GANs, or that you don't feel the paper empirically justifies its theoretical reformulation of current GAN training sufficiently?\n\n> In addition, the authors could shorten the paper within 8 pages that is the standard length of ICLR paper.\n\nApologies, it's now of the standard length.\n\n> In the paper \"On topological properties of f-divergences\" (1967), Csiszar intensively studied non-saturating properties of f-divergences\nin the paper. It would be helpful for readers to add some comments on the relation between the theoretical development in this paper and Csiszar's paper.\n\nWe were unfortunately unable to locate a copy of this paper. We tried searching the back issues of Studia Scientiarum Mathematicarum Hungarica and the usual aggregators. If you have any suggestions we would be very grateful."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygY4grYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2255/Authors|ICLR.cc/2020/Conference/Paper2255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144063, "tmdate": 1576860537256, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment"}}}, {"id": "r1lFVP8tiB", "original": null, "number": 3, "cdate": 1573639984571, "ddate": null, "tcdate": 1573639984571, "tmdate": 1573639984571, "tddate": null, "forum": "BygY4grYDr", "replyto": "BJezD3zRtH", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment", "content": {"title": "Updates in light of reviewer #3's comments (continued)", "comment": "> I totally agree with the author(s) on the point that this is a note (sec 7, second paragraph), rather than an academic paper. There is a lot of derivations in the paper, but insightful discussions are limited in the sense that it is not clear how to connect these results to improve practice. The section titles also read like bullet points in a note.\n\nThe references to a note are now fixed. We completely agree that the paper does not immediately suggest ways to improve current best practice. However we would argue that progress over time comes from both better theoretical understanding and better experimental results. In this case, the result is simple to state and understand (\"non-saturating GAN training doesn't actually optimize Jensen-Shannon\") and provides a better understanding of a state-of-the-art method for GAN training (used for example for StyleGAN).\n\n> Not certain about the significance of this paper. The derivations are fairly standard and I can not find any useful proposals that might benefit the practice of f-GAN training. There is a number of papers discussing non-saturating GAN training (see sec 8), and I do not think this paper adds too much value to this discussion. \n\nWe acknowledge that our derivation of f-GANs takes up quite a bit of space in the paper while being similar in substance to that of the original f-GAN paper. The main differences are a more elementary derivation of the variational lower bound, without using Legendre transforms / Fenchel conjugates (but it's completely trivially equivalent), and our use of a standardized form for the output of the critic (in our case the optimal critic is always log p - log q).\n\nIn terms of contributions to the general discussion, the present paper addresses a number of deficiencies in the previous literature in this area (this has been made more prominent in the new draft by moving the related work section straight after the introduction), and extends the current literature by showing for the first time that non-saturating GAN training can be viewed as divergence minimization, but for a divergence that has very different properties from Jensen-Shannon. While a relatively straightforward result and pleasingly straightforward to state, we hope / believe it's one that deserves to be well-known by GAN practitioners and theoreticians."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygY4grYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2255/Authors|ICLR.cc/2020/Conference/Paper2255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144063, "tmdate": 1576860537256, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment"}}}, {"id": "Syg7S7Utjr", "original": null, "number": 2, "cdate": 1573638971011, "ddate": null, "tcdate": 1573638971011, "tmdate": 1573638971011, "tddate": null, "forum": "BygY4grYDr", "replyto": "BJezD3zRtH", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment", "content": {"title": "Updates in light of reviewer #3's comments", "comment": "> This paper discusses alternative training strategies for f-GANs. While the discussion has some interesting points, the presentation needs to be much improved. It is not easy to follow this paper in its current form, and the main results are not properly emphasized. As such, I am not certain of its real contribution. f-GANs are not routinely used in practice (except for the vanilla JSD and RKL), and as far as I can tell the saturating gradient issue is no longer a central concern (it has been well addressed years ago, with, e.g. WGANs). I am voting to reject this submission, but I am willing to re-evaluate this paper if the author(s) significantly improves their writing. \n\nWe view the theoretical result that conventional non-saturating GAN training can be viewed as divergence minimization as the main result. We've tried to make that main contribution more prominent in the abstract and moved the section containing the main result earlier in the paper.\n\nThe saturation issue is well-addressed practically by both non-saturating training and WGANs, and we would argue both are prominently used in practice. For example, the ground-breaking StyleGAN paper used non-saturating training, finding it performed better than WGANs. This makes the current paper relevant by extending our best theoretical understanding of current practice.\n\nWe agree that f-GANs are not routinely used in practice, and consider our results on \"non-saturating\" variants of f-GANs are secondary. We cover f-GANs in some detail mainly because our main result is framed in those terms.\n\n> In Section 2, what does it mean by \"the Fisher metric of the family\"? The concept of Fisher metric is defined anywhere in the text, and there is no reference to it.\n\nClarified the wording in the main text, removing the reference to the Fisher metric, and added detail in an appendix.\n\n> It is not is intuitive why the second derivative of f_R takes the form  (given above Eqn (2)). Please elaborate.\n\nWe added an intermediate step in the derivation.\n\n> Please avoid the use of subjective phrases such as \"imagining\", \"get a feel\", etc. I am guessing the author(s) are trying to suggest taking a visual inspection of the discrepancies projected on the log-likelihood ratio axis (which is 1D) and figure out which f-div might be more appropriate.\n\nWe will address this before camera ready publication.\n\n> Fig. 3 needs legends. There are two solid (dotted, resp) lines in the Figure, and I am guessing one of them is for the saturating and the other for the non-saturating gradient. This needs to be specified because the line specs are identical.\n\nApologies, we will address this before camera ready publication.\n\n> After going through the entire paper, I would highly recommend the author(s) to take a course in academic writing. The main contributions are not highlighted and some of the key concepts are not even properly defined. For example, analysis of the non-saturating gradient, which is supposedly the main result of this submission, appeared in pp. 7, by which time most readers have exhausted their patience. The vanilla version of the non-saturating scheme never appeared in the main text. The notation system is also non-standard, where the notation , normally reserved for average/expectation, has been used to denote the gradient wrt . The writing can be very unprofessional at times, for example, \"the answers to these questions are yes, yes and no respectively\".\n\nAddressing specifics first, we moved the main result several sections earlier in the paper. Overlines are used for a lot of different mathematical concepts, and the use for a gradient is fairly standard when talking about adjoints and derivatives. Removed the \"yes, yes, no\" line specifically (that was indeed a slightly strange choice of register). In terms of the broader point, we're sincerely sorry it was hard to follow, and thank you for helping us try to improve the manuscript.\n\n> I do not know why the author(s) inserted one toy experiment in the paper, as it serves no purpose. Each model converges to their respectively optimal, as expected. There is no discussion of how to choose an appropriate f-div or (f,g) hybrid in practice. \n\nMoved the toy experiment to the appendix. We included it for two reasons: People are often suspicious of whether theoretical results can really be applied in practice; and this subject area in particular has had several incorrect or questionably correct attempted derivations previously, and we wanted to provide some empirical evidence that we weren't also making a similar sort of error."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BygY4grYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2255/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2255/Authors|ICLR.cc/2020/Conference/Paper2255/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144063, "tmdate": 1576860537256, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Authors", "ICLR.cc/2020/Conference/Paper2255/Reviewers", "ICLR.cc/2020/Conference/Paper2255/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Comment"}}}, {"id": "SJgXA_V2FB", "original": null, "number": 1, "cdate": 1571731658847, "ddate": null, "tcdate": 1571731658847, "tmdate": 1572972362793, "tddate": null, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "invitation": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "In this paper, firstly, a useful expression of the class of f-divergences is proposed. The authors investigate theoretical properties of some popular f-divergences from newly developed tools. Then, the expression is used to investigate GANs with the non-saturating training scheme.\n\nThe expressions in Section 3 can be a useful tool to investigate the statistical properties of estimators with f-divergences. However, I think that the usefulness of alternative expressions in Section 3 is not very clear, though an intuitive interpretation is presented. Moreover, the numerical experiment is not sufficient to support some new expressions. Detailed theoretical analysis of the non-saturating training based on the proposed expression would be required for publication from ICLR.  In addition, the authors could shorten the paper within 8 pages that is the standard length of ICLR paper. In the paper \"On topological properties of f-divergences\" (1967), Csiszar intensively studied non-saturating properties of f-divergences\nin the paper. It would be helpful for readers to add some comments on the relation between the theoretical development in this paper and Csiszar's paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2255/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["matt.shannon.personal@gmail.com"], "title": "The divergences minimized by non-saturating GAN training", "authors": ["Matt Shannon"], "TL;DR": "Non-saturating GAN training effectively minimizes a reverse KL-like f-divergence.", "abstract": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been\ntheoretically insightful, has spurred discussion, and has lead to theoretically and practically interesting\nextensions such as f-GANs and Wasserstein GANs. For both classic GANs and f-GANs, there is an original variant of training and a \"non-saturating\" variant which uses an alternative form of generator gradient. The original variant is theoretically easier to study, but for GANs the alternative variant performs better in practice. The non-saturating scheme is often regarded as a simple modification to deal with optimization issues, but we show that in fact the non-saturating scheme for GANs is effectively optimizing a reverse KL-like f-divergence. We also develop a number of theoretical tools to help compare and classify f-divergences. We hope these results may help to clarify some of the theoretical discussion surrounding the divergence minimization view of GAN training.", "keywords": ["GAN"], "pdf": "/pdf/7354af906337cbc3edb4da0638983dd2c47f3be3.pdf", "paperhash": "shannon|the_divergences_minimized_by_nonsaturating_gan_training", "original_pdf": "/attachment/0c88e599a9cd89ced30d24bfa7732a457e1c128b.pdf", "_bibtex": "@misc{\nshannon2020the,\ntitle={The divergences minimized by non-saturating {\\{}GAN{\\}} training},\nauthor={Matt Shannon},\nyear={2020},\nurl={https://openreview.net/forum?id=BygY4grYDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BygY4grYDr", "replyto": "BygY4grYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2255/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575779890370, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2255/Reviewers"], "noninvitees": [], "tcdate": 1570237725463, "tmdate": 1575779890381, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2255/-/Official_Review"}}}], "count": 8}